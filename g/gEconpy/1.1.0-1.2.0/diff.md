# Comparing `tmp/gEconpy-1.1.0.tar.gz` & `tmp/gEconpy-1.2.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "gEconpy-1.1.0.tar", last modified: Tue Feb 14 21:46:17 2023, max compression
+gzip compressed data, was "gEconpy-1.2.0.tar", last modified: Tue Apr 18 22:38:24 2023, max compression
```

## Comparing `gEconpy-1.1.0.tar` & `gEconpy-1.2.0.tar`

### file list

```diff
@@ -1,66 +1,77 @@
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.024087 gEconpy-1.1.0/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    35149 2023-02-06 19:26:53.000000 gEconpy-1.1.0/LICENSE
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    13825 2023-02-14 21:46:17.024202 gEconpy-1.1.0/PKG-INFO
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    13487 2023-02-06 19:26:53.000000 gEconpy-1.1.0/README.md
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.010277 gEconpy-1.1.0/gEconpy/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)      470 2023-02-03 16:57:51.000000 gEconpy-1.1.0/gEconpy/__init__.py
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.013977 gEconpy-1.1.0/gEconpy/classes/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)       71 2023-01-20 19:24:45.000000 gEconpy-1.1.0/gEconpy/classes/__init__.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    26230 2023-02-03 16:53:58.000000 gEconpy-1.1.0/gEconpy/classes/block.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     5339 2023-02-03 16:53:58.000000 gEconpy-1.1.0/gEconpy/classes/containers.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    61449 2023-02-14 19:48:31.000000 gEconpy-1.1.0/gEconpy/classes/model.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     3363 2022-12-21 19:51:00.000000 gEconpy-1.1.0/gEconpy/classes/progress_bar.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     2436 2023-01-21 19:25:49.000000 gEconpy-1.1.0/gEconpy/classes/time_aware_symbol.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     1779 2023-01-20 19:06:33.000000 gEconpy-1.1.0/gEconpy/classes/transformers.py
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.015320 gEconpy-1.1.0/gEconpy/estimation/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)        0 2022-12-20 23:44:17.000000 gEconpy-1.1.0/gEconpy/estimation/__init__.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     8813 2023-02-03 16:53:58.000000 gEconpy-1.1.0/gEconpy/estimation/estimate.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    11465 2023-02-05 17:30:02.000000 gEconpy-1.1.0/gEconpy/estimation/estimation_utilities.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     8205 2023-02-03 16:20:41.000000 gEconpy-1.1.0/gEconpy/estimation/kalman_filter.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     1337 2023-02-03 16:36:27.000000 gEconpy-1.1.0/gEconpy/estimation/kalman_smoother.py
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.015802 gEconpy-1.1.0/gEconpy/exceptions/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)        0 2022-12-20 23:44:17.000000 gEconpy-1.1.0/gEconpy/exceptions/__init__.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    11423 2023-02-03 16:20:41.000000 gEconpy-1.1.0/gEconpy/exceptions/exceptions.py
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.016881 gEconpy-1.1.0/gEconpy/numba_linalg/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     8120 2023-01-20 19:06:33.000000 gEconpy-1.1.0/gEconpy/numba_linalg/LAPACK.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)        0 2022-12-20 23:44:17.000000 gEconpy-1.1.0/gEconpy/numba_linalg/__init__.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     1764 2023-01-20 19:06:32.000000 gEconpy-1.1.0/gEconpy/numba_linalg/intrinsics.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    24015 2023-02-03 16:53:58.000000 gEconpy-1.1.0/gEconpy/numba_linalg/overloads.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     2692 2023-01-31 18:13:17.000000 gEconpy-1.1.0/gEconpy/numba_linalg/utilities.py
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.019030 gEconpy-1.1.0/gEconpy/parser/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)        0 2022-12-20 23:44:17.000000 gEconpy-1.1.0/gEconpy/parser/__init__.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     1540 2023-02-03 16:19:37.000000 gEconpy-1.1.0/gEconpy/parser/constants.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)      352 2022-12-21 19:51:01.000000 gEconpy-1.1.0/gEconpy/parser/file_loaders.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    11634 2023-02-03 16:57:51.000000 gEconpy-1.1.0/gEconpy/parser/gEcon_parser.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    53998 2023-02-14 17:47:44.000000 gEconpy-1.1.0/gEconpy/parser/parse_distributions.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    12861 2023-02-03 16:53:58.000000 gEconpy-1.1.0/gEconpy/parser/parse_equations.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     8002 2023-02-03 16:57:51.000000 gEconpy-1.1.0/gEconpy/parser/parse_plaintext.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     4125 2023-02-03 16:20:41.000000 gEconpy-1.1.0/gEconpy/parser/validation.py
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.019540 gEconpy-1.1.0/gEconpy/plotting/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)      469 2023-02-03 16:53:58.000000 gEconpy-1.1.0/gEconpy/plotting/__init__.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    30867 2023-02-03 16:20:42.000000 gEconpy-1.1.0/gEconpy/plotting/plotting.py
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.020569 gEconpy-1.1.0/gEconpy/sampling/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)      473 2023-02-03 16:53:58.000000 gEconpy-1.1.0/gEconpy/sampling/__init__.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     6458 2023-02-03 16:53:58.000000 gEconpy-1.1.0/gEconpy/sampling/posterior_utilities.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     8977 2023-02-03 16:20:41.000000 gEconpy-1.1.0/gEconpy/sampling/prior_utilities.py
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.022020 gEconpy-1.1.0/gEconpy/shared/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)      125 2023-01-20 19:24:45.000000 gEconpy-1.1.0/gEconpy/shared/__init__.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    11107 2023-02-03 16:53:58.000000 gEconpy-1.1.0/gEconpy/shared/dynare_convert.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    29920 2023-02-03 16:53:58.000000 gEconpy-1.1.0/gEconpy/shared/statsmodel_convert.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)      161 2023-01-20 19:06:32.000000 gEconpy-1.1.0/gEconpy/shared/typing.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     9937 2023-02-03 16:20:42.000000 gEconpy-1.1.0/gEconpy/shared/utilities.py
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.023357 gEconpy-1.1.0/gEconpy/solvers/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)        0 2022-12-20 23:44:17.000000 gEconpy-1.1.0/gEconpy/solvers/__init__.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     4978 2023-02-03 16:20:41.000000 gEconpy-1.1.0/gEconpy/solvers/cycle_reduction.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    13313 2023-02-03 16:20:42.000000 gEconpy-1.1.0/gEconpy/solvers/gensys.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    11159 2023-02-03 16:20:42.000000 gEconpy-1.1.0/gEconpy/solvers/perturbation.py
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    57698 2023-02-14 19:47:57.000000 gEconpy-1.1.0/gEconpy/solvers/steady_state.py
-drwxr-xr-x   0 jessegrabowski   (501) staff       (20)        0 2023-02-14 21:46:17.011111 gEconpy-1.1.0/gEconpy.egg-info/
--rw-r--r--   0 jessegrabowski   (501) staff       (20)    13825 2023-02-14 21:46:17.000000 gEconpy-1.1.0/gEconpy.egg-info/PKG-INFO
--rw-r--r--   0 jessegrabowski   (501) staff       (20)     1562 2023-02-14 21:46:17.000000 gEconpy-1.1.0/gEconpy.egg-info/SOURCES.txt
--rw-r--r--   0 jessegrabowski   (501) staff       (20)        1 2023-02-14 21:46:17.000000 gEconpy-1.1.0/gEconpy.egg-info/dependency_links.txt
--rw-r--r--   0 jessegrabowski   (501) staff       (20)       69 2023-02-14 21:46:17.000000 gEconpy-1.1.0/gEconpy.egg-info/requires.txt
--rw-r--r--   0 jessegrabowski   (501) staff       (20)        8 2023-02-14 21:46:17.000000 gEconpy-1.1.0/gEconpy.egg-info/top_level.txt
--rw-r--r--   0 jessegrabowski   (501) staff       (20)      181 2023-02-03 16:57:48.000000 gEconpy-1.1.0/pyproject.toml
--rw-r--r--   0 jessegrabowski   (501) staff       (20)      519 2023-02-14 21:46:17.024601 gEconpy-1.1.0/setup.cfg
--rw-r--r--   0 jessegrabowski   (501) staff       (20)       38 2023-01-20 19:06:32.000000 gEconpy-1.1.0/setup.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.634982 gEconpy-1.2.0/
+-rw-rw-rw-   0        0        0    35823 2023-02-22 12:40:07.000000 gEconpy-1.2.0/LICENSE
+-rw-rw-rw-   0        0        0    14070 2023-04-18 22:38:24.634982 gEconpy-1.2.0/PKG-INFO
+-rw-rw-rw-   0        0        0    13759 2023-02-22 12:40:07.000000 gEconpy-1.2.0/README.md
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.560983 gEconpy-1.2.0/gEconpy/
+-rw-rw-rw-   0        0        0      498 2023-04-18 22:37:10.000000 gEconpy-1.2.0/gEconpy/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.586982 gEconpy-1.2.0/gEconpy/classes/
+-rw-rw-rw-   0        0        0       74 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/classes/__init__.py
+-rw-rw-rw-   0        0        0    28710 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/classes/block.py
+-rw-rw-rw-   0        0        0     6144 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/classes/containers.py
+-rw-rw-rw-   0        0        0    66226 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/classes/model.py
+-rw-rw-rw-   0        0        0     3472 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/classes/progress_bar.py
+-rw-rw-rw-   0        0        0     2523 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/classes/time_aware_symbol.py
+-rw-rw-rw-   0        0        0     1856 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/classes/transformers.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.591982 gEconpy-1.2.0/gEconpy/estimation/
+-rw-rw-rw-   0        0        0        0 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/estimation/__init__.py
+-rw-rw-rw-   0        0        0     9085 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/estimation/estimate.py
+-rw-rw-rw-   0        0        0    11831 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/estimation/estimation_utilities.py
+-rw-rw-rw-   0        0        0     8557 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/estimation/kalman_filter.py
+-rw-rw-rw-   0        0        0     1386 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/estimation/kalman_smoother.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.593982 gEconpy-1.2.0/gEconpy/exceptions/
+-rw-rw-rw-   0        0        0        0 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/exceptions/__init__.py
+-rw-rw-rw-   0        0        0    11729 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/exceptions/exceptions.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.597983 gEconpy-1.2.0/gEconpy/numba_tools/
+-rw-rw-rw-   0        0        0     8396 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/numba_tools/LAPACK.py
+-rw-rw-rw-   0        0        0        0 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/numba_tools/__init__.py
+-rw-rw-rw-   0        0        0     1836 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/numba_tools/intrinsics.py
+-rw-rw-rw-   0        0        0    24886 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/numba_tools/overloads.py
+-rw-rw-rw-   0        0        0     9132 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/numba_tools/utilities.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.603982 gEconpy-1.2.0/gEconpy/parser/
+-rw-rw-rw-   0        0        0        0 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/parser/__init__.py
+-rw-rw-rw-   0        0        0     1617 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/parser/constants.py
+-rw-rw-rw-   0        0        0      370 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/parser/file_loaders.py
+-rw-rw-rw-   0        0        0    11985 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/parser/gEcon_parser.py
+-rw-rw-rw-   0        0        0    55526 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/parser/parse_distributions.py
+-rw-rw-rw-   0        0        0    13526 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/parser/parse_equations.py
+-rw-rw-rw-   0        0        0     8288 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/parser/parse_plaintext.py
+-rw-rw-rw-   0        0        0     4264 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/parser/validation.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.605982 gEconpy-1.2.0/gEconpy/plotting/
+-rw-rw-rw-   0        0        0      492 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/plotting/__init__.py
+-rw-rw-rw-   0        0        0    31786 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/plotting/plotting.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.607982 gEconpy-1.2.0/gEconpy/sampling/
+-rw-rw-rw-   0        0        0      490 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/sampling/__init__.py
+-rw-rw-rw-   0        0        0     6643 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/sampling/posterior_utilities.py
+-rw-rw-rw-   0        0        0     9249 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/sampling/prior_utilities.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.611982 gEconpy-1.2.0/gEconpy/shared/
+-rw-rw-rw-   0        0        0      127 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/shared/__init__.py
+-rw-rw-rw-   0        0        0    11458 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/shared/dynare_convert.py
+-rw-rw-rw-   0        0        0    30597 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/shared/statsmodel_convert.py
+-rw-rw-rw-   0        0        0      168 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/shared/typing.py
+-rw-rw-rw-   0        0        0    10340 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/shared/utilities.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.614982 gEconpy-1.2.0/gEconpy/solvers/
+-rw-rw-rw-   0        0        0        0 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/solvers/__init__.py
+-rw-rw-rw-   0        0        0     5114 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/solvers/cycle_reduction.py
+-rw-rw-rw-   0        0        0    13757 2023-02-22 12:40:07.000000 gEconpy-1.2.0/gEconpy/solvers/gensys.py
+-rw-rw-rw-   0        0        0    11313 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/solvers/perturbation.py
+-rw-rw-rw-   0        0        0    60257 2023-04-18 22:20:22.000000 gEconpy-1.2.0/gEconpy/solvers/steady_state.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.579981 gEconpy-1.2.0/gEconpy.egg-info/
+-rw-rw-rw-   0        0        0    14070 2023-04-18 22:38:24.000000 gEconpy-1.2.0/gEconpy.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0     1811 2023-04-18 22:38:24.000000 gEconpy-1.2.0/gEconpy.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0        1 2023-04-18 22:38:24.000000 gEconpy-1.2.0/gEconpy.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0       69 2023-04-18 22:38:24.000000 gEconpy-1.2.0/gEconpy.egg-info/requires.txt
+-rw-rw-rw-   0        0        0        8 2023-04-18 22:38:24.000000 gEconpy-1.2.0/gEconpy.egg-info/top_level.txt
+-rw-rw-rw-   0        0        0      579 2023-04-18 22:37:10.000000 gEconpy-1.2.0/pyproject.toml
+-rw-rw-rw-   0        0        0      549 2023-04-18 22:38:24.636982 gEconpy-1.2.0/setup.cfg
+-rw-rw-rw-   0        0        0       41 2023-02-22 12:40:07.000000 gEconpy-1.2.0/setup.py
+drwxrwxrwx   0        0        0        0 2023-04-18 22:38:24.633981 gEconpy-1.2.0/tests/
+-rw-rw-rw-   0        0        0    10623 2023-04-18 22:20:22.000000 gEconpy-1.2.0/tests/test_block.py
+-rw-rw-rw-   0        0        0     3639 2023-04-18 22:20:22.000000 gEconpy-1.2.0/tests/test_containers.py
+-rw-rw-rw-   0        0        0     8818 2023-02-22 12:40:07.000000 gEconpy-1.2.0/tests/test_distribution_parser.py
+-rw-rw-rw-   0        0        0     1800 2023-02-22 12:40:07.000000 gEconpy-1.2.0/tests/test_estimation.py
+-rw-rw-rw-   0        0        0    19212 2023-02-22 12:40:07.000000 gEconpy-1.2.0/tests/test_gensys.py
+-rw-rw-rw-   0        0        0     6594 2023-04-18 22:20:22.000000 gEconpy-1.2.0/tests/test_kalman_filter.py
+-rw-rw-rw-   0        0        0    26905 2023-04-18 22:20:22.000000 gEconpy-1.2.0/tests/test_model.py
+-rw-rw-rw-   0        0        0    21844 2023-04-18 22:20:22.000000 gEconpy-1.2.0/tests/test_parser.py
+-rw-rw-rw-   0        0        0    14712 2023-02-22 12:40:07.000000 gEconpy-1.2.0/tests/test_steady_state.py
+-rw-rw-rw-   0        0        0     2379 2023-02-22 12:40:07.000000 gEconpy-1.2.0/tests/test_time_aware_symbols.py
```

### Comparing `gEconpy-1.1.0/LICENSE` & `gEconpy-1.2.0/LICENSE`

 * *Ordering differences only*

 * *Files 7% similar despite different names*

```diff
@@ -1,674 +1,674 @@
-                    GNU GENERAL PUBLIC LICENSE
-                       Version 3, 29 June 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-                            Preamble
-
-  The GNU General Public License is a free, copyleft license for
-software and other kinds of works.
-
-  The licenses for most software and other practical works are designed
-to take away your freedom to share and change the works.  By contrast,
-the GNU General Public License is intended to guarantee your freedom to
-share and change all versions of a program--to make sure it remains free
-software for all its users.  We, the Free Software Foundation, use the
-GNU General Public License for most of our software; it applies also to
-any other work released this way by its authors.  You can apply it to
-your programs, too.
-
-  When we speak of free software, we are referring to freedom, not
-price.  Our General Public Licenses are designed to make sure that you
-have the freedom to distribute copies of free software (and charge for
-them if you wish), that you receive source code or can get it if you
-want it, that you can change the software or use pieces of it in new
-free programs, and that you know you can do these things.
-
-  To protect your rights, we need to prevent others from denying you
-these rights or asking you to surrender the rights.  Therefore, you have
-certain responsibilities if you distribute copies of the software, or if
-you modify it: responsibilities to respect the freedom of others.
-
-  For example, if you distribute copies of such a program, whether
-gratis or for a fee, you must pass on to the recipients the same
-freedoms that you received.  You must make sure that they, too, receive
-or can get the source code.  And you must show them these terms so they
-know their rights.
-
-  Developers that use the GNU GPL protect your rights with two steps:
-(1) assert copyright on the software, and (2) offer you this License
-giving you legal permission to copy, distribute and/or modify it.
-
-  For the developers' and authors' protection, the GPL clearly explains
-that there is no warranty for this free software.  For both users' and
-authors' sake, the GPL requires that modified versions be marked as
-changed, so that their problems will not be attributed erroneously to
-authors of previous versions.
-
-  Some devices are designed to deny users access to install or run
-modified versions of the software inside them, although the manufacturer
-can do so.  This is fundamentally incompatible with the aim of
-protecting users' freedom to change the software.  The systematic
-pattern of such abuse occurs in the area of products for individuals to
-use, which is precisely where it is most unacceptable.  Therefore, we
-have designed this version of the GPL to prohibit the practice for those
-products.  If such problems arise substantially in other domains, we
-stand ready to extend this provision to those domains in future versions
-of the GPL, as needed to protect the freedom of users.
-
-  Finally, every program is threatened constantly by software patents.
-States should not allow patents to restrict development and use of
-software on general-purpose computers, but in those that do, we wish to
-avoid the special danger that patents applied to a free program could
-make it effectively proprietary.  To prevent this, the GPL assures that
-patents cannot be used to render the program non-free.
-
-  The precise terms and conditions for copying, distribution and
-modification follow.
-
-                       TERMS AND CONDITIONS
-
-  0. Definitions.
-
-  "This License" refers to version 3 of the GNU General Public License.
-
-  "Copyright" also means copyright-like laws that apply to other kinds of
-works, such as semiconductor masks.
-
-  "The Program" refers to any copyrightable work licensed under this
-License.  Each licensee is addressed as "you".  "Licensees" and
-"recipients" may be individuals or organizations.
-
-  To "modify" a work means to copy from or adapt all or part of the work
-in a fashion requiring copyright permission, other than the making of an
-exact copy.  The resulting work is called a "modified version" of the
-earlier work or a work "based on" the earlier work.
-
-  A "covered work" means either the unmodified Program or a work based
-on the Program.
-
-  To "propagate" a work means to do anything with it that, without
-permission, would make you directly or secondarily liable for
-infringement under applicable copyright law, except executing it on a
-computer or modifying a private copy.  Propagation includes copying,
-distribution (with or without modification), making available to the
-public, and in some countries other activities as well.
-
-  To "convey" a work means any kind of propagation that enables other
-parties to make or receive copies.  Mere interaction with a user through
-a computer network, with no transfer of a copy, is not conveying.
-
-  An interactive user interface displays "Appropriate Legal Notices"
-to the extent that it includes a convenient and prominently visible
-feature that (1) displays an appropriate copyright notice, and (2)
-tells the user that there is no warranty for the work (except to the
-extent that warranties are provided), that licensees may convey the
-work under this License, and how to view a copy of this License.  If
-the interface presents a list of user commands or options, such as a
-menu, a prominent item in the list meets this criterion.
-
-  1. Source Code.
-
-  The "source code" for a work means the preferred form of the work
-for making modifications to it.  "Object code" means any non-source
-form of a work.
-
-  A "Standard Interface" means an interface that either is an official
-standard defined by a recognized standards body, or, in the case of
-interfaces specified for a particular programming language, one that
-is widely used among developers working in that language.
-
-  The "System Libraries" of an executable work include anything, other
-than the work as a whole, that (a) is included in the normal form of
-packaging a Major Component, but which is not part of that Major
-Component, and (b) serves only to enable use of the work with that
-Major Component, or to implement a Standard Interface for which an
-implementation is available to the public in source code form.  A
-"Major Component", in this context, means a major essential component
-(kernel, window system, and so on) of the specific operating system
-(if any) on which the executable work runs, or a compiler used to
-produce the work, or an object code interpreter used to run it.
-
-  The "Corresponding Source" for a work in object code form means all
-the source code needed to generate, install, and (for an executable
-work) run the object code and to modify the work, including scripts to
-control those activities.  However, it does not include the work's
-System Libraries, or general-purpose tools or generally available free
-programs which are used unmodified in performing those activities but
-which are not part of the work.  For example, Corresponding Source
-includes interface definition files associated with source files for
-the work, and the source code for shared libraries and dynamically
-linked subprograms that the work is specifically designed to require,
-such as by intimate data communication or control flow between those
-subprograms and other parts of the work.
-
-  The Corresponding Source need not include anything that users
-can regenerate automatically from other parts of the Corresponding
-Source.
-
-  The Corresponding Source for a work in source code form is that
-same work.
-
-  2. Basic Permissions.
-
-  All rights granted under this License are granted for the term of
-copyright on the Program, and are irrevocable provided the stated
-conditions are met.  This License explicitly affirms your unlimited
-permission to run the unmodified Program.  The output from running a
-covered work is covered by this License only if the output, given its
-content, constitutes a covered work.  This License acknowledges your
-rights of fair use or other equivalent, as provided by copyright law.
-
-  You may make, run and propagate covered works that you do not
-convey, without conditions so long as your license otherwise remains
-in force.  You may convey covered works to others for the sole purpose
-of having them make modifications exclusively for you, or provide you
-with facilities for running those works, provided that you comply with
-the terms of this License in conveying all material for which you do
-not control copyright.  Those thus making or running the covered works
-for you must do so exclusively on your behalf, under your direction
-and control, on terms that prohibit them from making any copies of
-your copyrighted material outside their relationship with you.
-
-  Conveying under any other circumstances is permitted solely under
-the conditions stated below.  Sublicensing is not allowed; section 10
-makes it unnecessary.
-
-  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
-
-  No covered work shall be deemed part of an effective technological
-measure under any applicable law fulfilling obligations under article
-11 of the WIPO copyright treaty adopted on 20 December 1996, or
-similar laws prohibiting or restricting circumvention of such
-measures.
-
-  When you convey a covered work, you waive any legal power to forbid
-circumvention of technological measures to the extent such circumvention
-is effected by exercising rights under this License with respect to
-the covered work, and you disclaim any intention to limit operation or
-modification of the work as a means of enforcing, against the work's
-users, your or third parties' legal rights to forbid circumvention of
-technological measures.
-
-  4. Conveying Verbatim Copies.
-
-  You may convey verbatim copies of the Program's source code as you
-receive it, in any medium, provided that you conspicuously and
-appropriately publish on each copy an appropriate copyright notice;
-keep intact all notices stating that this License and any
-non-permissive terms added in accord with section 7 apply to the code;
-keep intact all notices of the absence of any warranty; and give all
-recipients a copy of this License along with the Program.
-
-  You may charge any price or no price for each copy that you convey,
-and you may offer support or warranty protection for a fee.
-
-  5. Conveying Modified Source Versions.
-
-  You may convey a work based on the Program, or the modifications to
-produce it from the Program, in the form of source code under the
-terms of section 4, provided that you also meet all of these conditions:
-
-    a) The work must carry prominent notices stating that you modified
-    it, and giving a relevant date.
-
-    b) The work must carry prominent notices stating that it is
-    released under this License and any conditions added under section
-    7.  This requirement modifies the requirement in section 4 to
-    "keep intact all notices".
-
-    c) You must license the entire work, as a whole, under this
-    License to anyone who comes into possession of a copy.  This
-    License will therefore apply, along with any applicable section 7
-    additional terms, to the whole of the work, and all its parts,
-    regardless of how they are packaged.  This License gives no
-    permission to license the work in any other way, but it does not
-    invalidate such permission if you have separately received it.
-
-    d) If the work has interactive user interfaces, each must display
-    Appropriate Legal Notices; however, if the Program has interactive
-    interfaces that do not display Appropriate Legal Notices, your
-    work need not make them do so.
-
-  A compilation of a covered work with other separate and independent
-works, which are not by their nature extensions of the covered work,
-and which are not combined with it such as to form a larger program,
-in or on a volume of a storage or distribution medium, is called an
-"aggregate" if the compilation and its resulting copyright are not
-used to limit the access or legal rights of the compilation's users
-beyond what the individual works permit.  Inclusion of a covered work
-in an aggregate does not cause this License to apply to the other
-parts of the aggregate.
-
-  6. Conveying Non-Source Forms.
-
-  You may convey a covered work in object code form under the terms
-of sections 4 and 5, provided that you also convey the
-machine-readable Corresponding Source under the terms of this License,
-in one of these ways:
-
-    a) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by the
-    Corresponding Source fixed on a durable physical medium
-    customarily used for software interchange.
-
-    b) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by a
-    written offer, valid for at least three years and valid for as
-    long as you offer spare parts or customer support for that product
-    model, to give anyone who possesses the object code either (1) a
-    copy of the Corresponding Source for all the software in the
-    product that is covered by this License, on a durable physical
-    medium customarily used for software interchange, for a price no
-    more than your reasonable cost of physically performing this
-    conveying of source, or (2) access to copy the
-    Corresponding Source from a network server at no charge.
-
-    c) Convey individual copies of the object code with a copy of the
-    written offer to provide the Corresponding Source.  This
-    alternative is allowed only occasionally and noncommercially, and
-    only if you received the object code with such an offer, in accord
-    with subsection 6b.
-
-    d) Convey the object code by offering access from a designated
-    place (gratis or for a charge), and offer equivalent access to the
-    Corresponding Source in the same way through the same place at no
-    further charge.  You need not require recipients to copy the
-    Corresponding Source along with the object code.  If the place to
-    copy the object code is a network server, the Corresponding Source
-    may be on a different server (operated by you or a third party)
-    that supports equivalent copying facilities, provided you maintain
-    clear directions next to the object code saying where to find the
-    Corresponding Source.  Regardless of what server hosts the
-    Corresponding Source, you remain obligated to ensure that it is
-    available for as long as needed to satisfy these requirements.
-
-    e) Convey the object code using peer-to-peer transmission, provided
-    you inform other peers where the object code and Corresponding
-    Source of the work are being offered to the general public at no
-    charge under subsection 6d.
-
-  A separable portion of the object code, whose source code is excluded
-from the Corresponding Source as a System Library, need not be
-included in conveying the object code work.
-
-  A "User Product" is either (1) a "consumer product", which means any
-tangible personal property which is normally used for personal, family,
-or household purposes, or (2) anything designed or sold for incorporation
-into a dwelling.  In determining whether a product is a consumer product,
-doubtful cases shall be resolved in favor of coverage.  For a particular
-product received by a particular user, "normally used" refers to a
-typical or common use of that class of product, regardless of the status
-of the particular user or of the way in which the particular user
-actually uses, or expects or is expected to use, the product.  A product
-is a consumer product regardless of whether the product has substantial
-commercial, industrial or non-consumer uses, unless such uses represent
-the only significant mode of use of the product.
-
-  "Installation Information" for a User Product means any methods,
-procedures, authorization keys, or other information required to install
-and execute modified versions of a covered work in that User Product from
-a modified version of its Corresponding Source.  The information must
-suffice to ensure that the continued functioning of the modified object
-code is in no case prevented or interfered with solely because
-modification has been made.
-
-  If you convey an object code work under this section in, or with, or
-specifically for use in, a User Product, and the conveying occurs as
-part of a transaction in which the right of possession and use of the
-User Product is transferred to the recipient in perpetuity or for a
-fixed term (regardless of how the transaction is characterized), the
-Corresponding Source conveyed under this section must be accompanied
-by the Installation Information.  But this requirement does not apply
-if neither you nor any third party retains the ability to install
-modified object code on the User Product (for example, the work has
-been installed in ROM).
-
-  The requirement to provide Installation Information does not include a
-requirement to continue to provide support service, warranty, or updates
-for a work that has been modified or installed by the recipient, or for
-the User Product in which it has been modified or installed.  Access to a
-network may be denied when the modification itself materially and
-adversely affects the operation of the network or violates the rules and
-protocols for communication across the network.
-
-  Corresponding Source conveyed, and Installation Information provided,
-in accord with this section must be in a format that is publicly
-documented (and with an implementation available to the public in
-source code form), and must require no special password or key for
-unpacking, reading or copying.
-
-  7. Additional Terms.
-
-  "Additional permissions" are terms that supplement the terms of this
-License by making exceptions from one or more of its conditions.
-Additional permissions that are applicable to the entire Program shall
-be treated as though they were included in this License, to the extent
-that they are valid under applicable law.  If additional permissions
-apply only to part of the Program, that part may be used separately
-under those permissions, but the entire Program remains governed by
-this License without regard to the additional permissions.
-
-  When you convey a copy of a covered work, you may at your option
-remove any additional permissions from that copy, or from any part of
-it.  (Additional permissions may be written to require their own
-removal in certain cases when you modify the work.)  You may place
-additional permissions on material, added by you to a covered work,
-for which you have or can give appropriate copyright permission.
-
-  Notwithstanding any other provision of this License, for material you
-add to a covered work, you may (if authorized by the copyright holders of
-that material) supplement the terms of this License with terms:
-
-    a) Disclaiming warranty or limiting liability differently from the
-    terms of sections 15 and 16 of this License; or
-
-    b) Requiring preservation of specified reasonable legal notices or
-    author attributions in that material or in the Appropriate Legal
-    Notices displayed by works containing it; or
-
-    c) Prohibiting misrepresentation of the origin of that material, or
-    requiring that modified versions of such material be marked in
-    reasonable ways as different from the original version; or
-
-    d) Limiting the use for publicity purposes of names of licensors or
-    authors of the material; or
-
-    e) Declining to grant rights under trademark law for use of some
-    trade names, trademarks, or service marks; or
-
-    f) Requiring indemnification of licensors and authors of that
-    material by anyone who conveys the material (or modified versions of
-    it) with contractual assumptions of liability to the recipient, for
-    any liability that these contractual assumptions directly impose on
-    those licensors and authors.
-
-  All other non-permissive additional terms are considered "further
-restrictions" within the meaning of section 10.  If the Program as you
-received it, or any part of it, contains a notice stating that it is
-governed by this License along with a term that is a further
-restriction, you may remove that term.  If a license document contains
-a further restriction but permits relicensing or conveying under this
-License, you may add to a covered work material governed by the terms
-of that license document, provided that the further restriction does
-not survive such relicensing or conveying.
-
-  If you add terms to a covered work in accord with this section, you
-must place, in the relevant source files, a statement of the
-additional terms that apply to those files, or a notice indicating
-where to find the applicable terms.
-
-  Additional terms, permissive or non-permissive, may be stated in the
-form of a separately written license, or stated as exceptions;
-the above requirements apply either way.
-
-  8. Termination.
-
-  You may not propagate or modify a covered work except as expressly
-provided under this License.  Any attempt otherwise to propagate or
-modify it is void, and will automatically terminate your rights under
-this License (including any patent licenses granted under the third
-paragraph of section 11).
-
-  However, if you cease all violation of this License, then your
-license from a particular copyright holder is reinstated (a)
-provisionally, unless and until the copyright holder explicitly and
-finally terminates your license, and (b) permanently, if the copyright
-holder fails to notify you of the violation by some reasonable means
-prior to 60 days after the cessation.
-
-  Moreover, your license from a particular copyright holder is
-reinstated permanently if the copyright holder notifies you of the
-violation by some reasonable means, this is the first time you have
-received notice of violation of this License (for any work) from that
-copyright holder, and you cure the violation prior to 30 days after
-your receipt of the notice.
-
-  Termination of your rights under this section does not terminate the
-licenses of parties who have received copies or rights from you under
-this License.  If your rights have been terminated and not permanently
-reinstated, you do not qualify to receive new licenses for the same
-material under section 10.
-
-  9. Acceptance Not Required for Having Copies.
-
-  You are not required to accept this License in order to receive or
-run a copy of the Program.  Ancillary propagation of a covered work
-occurring solely as a consequence of using peer-to-peer transmission
-to receive a copy likewise does not require acceptance.  However,
-nothing other than this License grants you permission to propagate or
-modify any covered work.  These actions infringe copyright if you do
-not accept this License.  Therefore, by modifying or propagating a
-covered work, you indicate your acceptance of this License to do so.
-
-  10. Automatic Licensing of Downstream Recipients.
-
-  Each time you convey a covered work, the recipient automatically
-receives a license from the original licensors, to run, modify and
-propagate that work, subject to this License.  You are not responsible
-for enforcing compliance by third parties with this License.
-
-  An "entity transaction" is a transaction transferring control of an
-organization, or substantially all assets of one, or subdividing an
-organization, or merging organizations.  If propagation of a covered
-work results from an entity transaction, each party to that
-transaction who receives a copy of the work also receives whatever
-licenses to the work the party's predecessor in interest had or could
-give under the previous paragraph, plus a right to possession of the
-Corresponding Source of the work from the predecessor in interest, if
-the predecessor has it or can get it with reasonable efforts.
-
-  You may not impose any further restrictions on the exercise of the
-rights granted or affirmed under this License.  For example, you may
-not impose a license fee, royalty, or other charge for exercise of
-rights granted under this License, and you may not initiate litigation
-(including a cross-claim or counterclaim in a lawsuit) alleging that
-any patent claim is infringed by making, using, selling, offering for
-sale, or importing the Program or any portion of it.
-
-  11. Patents.
-
-  A "contributor" is a copyright holder who authorizes use under this
-License of the Program or a work on which the Program is based.  The
-work thus licensed is called the contributor's "contributor version".
-
-  A contributor's "essential patent claims" are all patent claims
-owned or controlled by the contributor, whether already acquired or
-hereafter acquired, that would be infringed by some manner, permitted
-by this License, of making, using, or selling its contributor version,
-but do not include claims that would be infringed only as a
-consequence of further modification of the contributor version.  For
-purposes of this definition, "control" includes the right to grant
-patent sublicenses in a manner consistent with the requirements of
-this License.
-
-  Each contributor grants you a non-exclusive, worldwide, royalty-free
-patent license under the contributor's essential patent claims, to
-make, use, sell, offer for sale, import and otherwise run, modify and
-propagate the contents of its contributor version.
-
-  In the following three paragraphs, a "patent license" is any express
-agreement or commitment, however denominated, not to enforce a patent
-(such as an express permission to practice a patent or covenant not to
-sue for patent infringement).  To "grant" such a patent license to a
-party means to make such an agreement or commitment not to enforce a
-patent against the party.
-
-  If you convey a covered work, knowingly relying on a patent license,
-and the Corresponding Source of the work is not available for anyone
-to copy, free of charge and under the terms of this License, through a
-publicly available network server or other readily accessible means,
-then you must either (1) cause the Corresponding Source to be so
-available, or (2) arrange to deprive yourself of the benefit of the
-patent license for this particular work, or (3) arrange, in a manner
-consistent with the requirements of this License, to extend the patent
-license to downstream recipients.  "Knowingly relying" means you have
-actual knowledge that, but for the patent license, your conveying the
-covered work in a country, or your recipient's use of the covered work
-in a country, would infringe one or more identifiable patents in that
-country that you have reason to believe are valid.
-
-  If, pursuant to or in connection with a single transaction or
-arrangement, you convey, or propagate by procuring conveyance of, a
-covered work, and grant a patent license to some of the parties
-receiving the covered work authorizing them to use, propagate, modify
-or convey a specific copy of the covered work, then the patent license
-you grant is automatically extended to all recipients of the covered
-work and works based on it.
-
-  A patent license is "discriminatory" if it does not include within
-the scope of its coverage, prohibits the exercise of, or is
-conditioned on the non-exercise of one or more of the rights that are
-specifically granted under this License.  You may not convey a covered
-work if you are a party to an arrangement with a third party that is
-in the business of distributing software, under which you make payment
-to the third party based on the extent of your activity of conveying
-the work, and under which the third party grants, to any of the
-parties who would receive the covered work from you, a discriminatory
-patent license (a) in connection with copies of the covered work
-conveyed by you (or copies made from those copies), or (b) primarily
-for and in connection with specific products or compilations that
-contain the covered work, unless you entered into that arrangement,
-or that patent license was granted, prior to 28 March 2007.
-
-  Nothing in this License shall be construed as excluding or limiting
-any implied license or other defenses to infringement that may
-otherwise be available to you under applicable patent law.
-
-  12. No Surrender of Others' Freedom.
-
-  If conditions are imposed on you (whether by court order, agreement or
-otherwise) that contradict the conditions of this License, they do not
-excuse you from the conditions of this License.  If you cannot convey a
-covered work so as to satisfy simultaneously your obligations under this
-License and any other pertinent obligations, then as a consequence you may
-not convey it at all.  For example, if you agree to terms that obligate you
-to collect a royalty for further conveying from those to whom you convey
-the Program, the only way you could satisfy both those terms and this
-License would be to refrain entirely from conveying the Program.
-
-  13. Use with the GNU Affero General Public License.
-
-  Notwithstanding any other provision of this License, you have
-permission to link or combine any covered work with a work licensed
-under version 3 of the GNU Affero General Public License into a single
-combined work, and to convey the resulting work.  The terms of this
-License will continue to apply to the part which is the covered work,
-but the special requirements of the GNU Affero General Public License,
-section 13, concerning interaction through a network will apply to the
-combination as such.
-
-  14. Revised Versions of this License.
-
-  The Free Software Foundation may publish revised and/or new versions of
-the GNU General Public License from time to time.  Such new versions will
-be similar in spirit to the present version, but may differ in detail to
-address new problems or concerns.
-
-  Each version is given a distinguishing version number.  If the
-Program specifies that a certain numbered version of the GNU General
-Public License "or any later version" applies to it, you have the
-option of following the terms and conditions either of that numbered
-version or of any later version published by the Free Software
-Foundation.  If the Program does not specify a version number of the
-GNU General Public License, you may choose any version ever published
-by the Free Software Foundation.
-
-  If the Program specifies that a proxy can decide which future
-versions of the GNU General Public License can be used, that proxy's
-public statement of acceptance of a version permanently authorizes you
-to choose that version for the Program.
-
-  Later license versions may give you additional or different
-permissions.  However, no additional obligations are imposed on any
-author or copyright holder as a result of your choosing to follow a
-later version.
-
-  15. Disclaimer of Warranty.
-
-  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
-APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
-HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
-OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
-THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
-IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
-ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
-
-  16. Limitation of Liability.
-
-  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
-WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
-THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
-GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
-USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
-DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
-PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
-EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
-SUCH DAMAGES.
-
-  17. Interpretation of Sections 15 and 16.
-
-  If the disclaimer of warranty and limitation of liability provided
-above cannot be given local legal effect according to their terms,
-reviewing courts shall apply local law that most closely approximates
-an absolute waiver of all civil liability in connection with the
-Program, unless a warranty or assumption of liability accompanies a
-copy of the Program in return for a fee.
-
-                     END OF TERMS AND CONDITIONS
-
-            How to Apply These Terms to Your New Programs
-
-  If you develop a new program, and you want it to be of the greatest
-possible use to the public, the best way to achieve this is to make it
-free software which everyone can redistribute and change under these terms.
-
-  To do so, attach the following notices to the program.  It is safest
-to attach them to the start of each source file to most effectively
-state the exclusion of warranty; and each file should have at least
-the "copyright" line and a pointer to where the full notice is found.
-
-    <one line to give the program's name and a brief idea of what it does.>
-    Copyright (C) <year>  <name of author>
-
-    This program is free software: you can redistribute it and/or modify
-    it under the terms of the GNU General Public License as published by
-    the Free Software Foundation, either version 3 of the License, or
-    (at your option) any later version.
-
-    This program is distributed in the hope that it will be useful,
-    but WITHOUT ANY WARRANTY; without even the implied warranty of
-    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-    GNU General Public License for more details.
-
-    You should have received a copy of the GNU General Public License
-    along with this program.  If not, see <https://www.gnu.org/licenses/>.
-
-Also add information on how to contact you by electronic and paper mail.
-
-  If the program does terminal interaction, make it output a short
-notice like this when it starts in an interactive mode:
-
-    <program>  Copyright (C) <year>  <name of author>
-    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
-    This is free software, and you are welcome to redistribute it
-    under certain conditions; type `show c' for details.
-
-The hypothetical commands `show w' and `show c' should show the appropriate
-parts of the General Public License.  Of course, your program's commands
-might be different; for a GUI interface, you would use an "about box".
-
-  You should also get your employer (if you work as a programmer) or school,
-if any, to sign a "copyright disclaimer" for the program, if necessary.
-For more information on this, and how to apply and follow the GNU GPL, see
-<https://www.gnu.org/licenses/>.
-
-  The GNU General Public License does not permit incorporating your program
-into proprietary programs.  If your program is a subroutine library, you
-may consider it more useful to permit linking proprietary applications with
-the library.  If this is what you want to do, use the GNU Lesser General
-Public License instead of this License.  But first, please read
-<https://www.gnu.org/licenses/why-not-lgpl.html>.
+                    GNU GENERAL PUBLIC LICENSE
+                       Version 3, 29 June 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The GNU General Public License is a free, copyleft license for
+software and other kinds of works.
+
+  The licenses for most software and other practical works are designed
+to take away your freedom to share and change the works.  By contrast,
+the GNU General Public License is intended to guarantee your freedom to
+share and change all versions of a program--to make sure it remains free
+software for all its users.  We, the Free Software Foundation, use the
+GNU General Public License for most of our software; it applies also to
+any other work released this way by its authors.  You can apply it to
+your programs, too.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+them if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces of it in new
+free programs, and that you know you can do these things.
+
+  To protect your rights, we need to prevent others from denying you
+these rights or asking you to surrender the rights.  Therefore, you have
+certain responsibilities if you distribute copies of the software, or if
+you modify it: responsibilities to respect the freedom of others.
+
+  For example, if you distribute copies of such a program, whether
+gratis or for a fee, you must pass on to the recipients the same
+freedoms that you received.  You must make sure that they, too, receive
+or can get the source code.  And you must show them these terms so they
+know their rights.
+
+  Developers that use the GNU GPL protect your rights with two steps:
+(1) assert copyright on the software, and (2) offer you this License
+giving you legal permission to copy, distribute and/or modify it.
+
+  For the developers' and authors' protection, the GPL clearly explains
+that there is no warranty for this free software.  For both users' and
+authors' sake, the GPL requires that modified versions be marked as
+changed, so that their problems will not be attributed erroneously to
+authors of previous versions.
+
+  Some devices are designed to deny users access to install or run
+modified versions of the software inside them, although the manufacturer
+can do so.  This is fundamentally incompatible with the aim of
+protecting users' freedom to change the software.  The systematic
+pattern of such abuse occurs in the area of products for individuals to
+use, which is precisely where it is most unacceptable.  Therefore, we
+have designed this version of the GPL to prohibit the practice for those
+products.  If such problems arise substantially in other domains, we
+stand ready to extend this provision to those domains in future versions
+of the GPL, as needed to protect the freedom of users.
+
+  Finally, every program is threatened constantly by software patents.
+States should not allow patents to restrict development and use of
+software on general-purpose computers, but in those that do, we wish to
+avoid the special danger that patents applied to a free program could
+make it effectively proprietary.  To prevent this, the GPL assures that
+patents cannot be used to render the program non-free.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                       TERMS AND CONDITIONS
+
+  0. Definitions.
+
+  "This License" refers to version 3 of the GNU General Public License.
+
+  "Copyright" also means copyright-like laws that apply to other kinds of
+works, such as semiconductor masks.
+
+  "The Program" refers to any copyrightable work licensed under this
+License.  Each licensee is addressed as "you".  "Licensees" and
+"recipients" may be individuals or organizations.
+
+  To "modify" a work means to copy from or adapt all or part of the work
+in a fashion requiring copyright permission, other than the making of an
+exact copy.  The resulting work is called a "modified version" of the
+earlier work or a work "based on" the earlier work.
+
+  A "covered work" means either the unmodified Program or a work based
+on the Program.
+
+  To "propagate" a work means to do anything with it that, without
+permission, would make you directly or secondarily liable for
+infringement under applicable copyright law, except executing it on a
+computer or modifying a private copy.  Propagation includes copying,
+distribution (with or without modification), making available to the
+public, and in some countries other activities as well.
+
+  To "convey" a work means any kind of propagation that enables other
+parties to make or receive copies.  Mere interaction with a user through
+a computer network, with no transfer of a copy, is not conveying.
+
+  An interactive user interface displays "Appropriate Legal Notices"
+to the extent that it includes a convenient and prominently visible
+feature that (1) displays an appropriate copyright notice, and (2)
+tells the user that there is no warranty for the work (except to the
+extent that warranties are provided), that licensees may convey the
+work under this License, and how to view a copy of this License.  If
+the interface presents a list of user commands or options, such as a
+menu, a prominent item in the list meets this criterion.
+
+  1. Source Code.
+
+  The "source code" for a work means the preferred form of the work
+for making modifications to it.  "Object code" means any non-source
+form of a work.
+
+  A "Standard Interface" means an interface that either is an official
+standard defined by a recognized standards body, or, in the case of
+interfaces specified for a particular programming language, one that
+is widely used among developers working in that language.
+
+  The "System Libraries" of an executable work include anything, other
+than the work as a whole, that (a) is included in the normal form of
+packaging a Major Component, but which is not part of that Major
+Component, and (b) serves only to enable use of the work with that
+Major Component, or to implement a Standard Interface for which an
+implementation is available to the public in source code form.  A
+"Major Component", in this context, means a major essential component
+(kernel, window system, and so on) of the specific operating system
+(if any) on which the executable work runs, or a compiler used to
+produce the work, or an object code interpreter used to run it.
+
+  The "Corresponding Source" for a work in object code form means all
+the source code needed to generate, install, and (for an executable
+work) run the object code and to modify the work, including scripts to
+control those activities.  However, it does not include the work's
+System Libraries, or general-purpose tools or generally available free
+programs which are used unmodified in performing those activities but
+which are not part of the work.  For example, Corresponding Source
+includes interface definition files associated with source files for
+the work, and the source code for shared libraries and dynamically
+linked subprograms that the work is specifically designed to require,
+such as by intimate data communication or control flow between those
+subprograms and other parts of the work.
+
+  The Corresponding Source need not include anything that users
+can regenerate automatically from other parts of the Corresponding
+Source.
+
+  The Corresponding Source for a work in source code form is that
+same work.
+
+  2. Basic Permissions.
+
+  All rights granted under this License are granted for the term of
+copyright on the Program, and are irrevocable provided the stated
+conditions are met.  This License explicitly affirms your unlimited
+permission to run the unmodified Program.  The output from running a
+covered work is covered by this License only if the output, given its
+content, constitutes a covered work.  This License acknowledges your
+rights of fair use or other equivalent, as provided by copyright law.
+
+  You may make, run and propagate covered works that you do not
+convey, without conditions so long as your license otherwise remains
+in force.  You may convey covered works to others for the sole purpose
+of having them make modifications exclusively for you, or provide you
+with facilities for running those works, provided that you comply with
+the terms of this License in conveying all material for which you do
+not control copyright.  Those thus making or running the covered works
+for you must do so exclusively on your behalf, under your direction
+and control, on terms that prohibit them from making any copies of
+your copyrighted material outside their relationship with you.
+
+  Conveying under any other circumstances is permitted solely under
+the conditions stated below.  Sublicensing is not allowed; section 10
+makes it unnecessary.
+
+  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
+
+  No covered work shall be deemed part of an effective technological
+measure under any applicable law fulfilling obligations under article
+11 of the WIPO copyright treaty adopted on 20 December 1996, or
+similar laws prohibiting or restricting circumvention of such
+measures.
+
+  When you convey a covered work, you waive any legal power to forbid
+circumvention of technological measures to the extent such circumvention
+is effected by exercising rights under this License with respect to
+the covered work, and you disclaim any intention to limit operation or
+modification of the work as a means of enforcing, against the work's
+users, your or third parties' legal rights to forbid circumvention of
+technological measures.
+
+  4. Conveying Verbatim Copies.
+
+  You may convey verbatim copies of the Program's source code as you
+receive it, in any medium, provided that you conspicuously and
+appropriately publish on each copy an appropriate copyright notice;
+keep intact all notices stating that this License and any
+non-permissive terms added in accord with section 7 apply to the code;
+keep intact all notices of the absence of any warranty; and give all
+recipients a copy of this License along with the Program.
+
+  You may charge any price or no price for each copy that you convey,
+and you may offer support or warranty protection for a fee.
+
+  5. Conveying Modified Source Versions.
+
+  You may convey a work based on the Program, or the modifications to
+produce it from the Program, in the form of source code under the
+terms of section 4, provided that you also meet all of these conditions:
+
+    a) The work must carry prominent notices stating that you modified
+    it, and giving a relevant date.
+
+    b) The work must carry prominent notices stating that it is
+    released under this License and any conditions added under section
+    7.  This requirement modifies the requirement in section 4 to
+    "keep intact all notices".
+
+    c) You must license the entire work, as a whole, under this
+    License to anyone who comes into possession of a copy.  This
+    License will therefore apply, along with any applicable section 7
+    additional terms, to the whole of the work, and all its parts,
+    regardless of how they are packaged.  This License gives no
+    permission to license the work in any other way, but it does not
+    invalidate such permission if you have separately received it.
+
+    d) If the work has interactive user interfaces, each must display
+    Appropriate Legal Notices; however, if the Program has interactive
+    interfaces that do not display Appropriate Legal Notices, your
+    work need not make them do so.
+
+  A compilation of a covered work with other separate and independent
+works, which are not by their nature extensions of the covered work,
+and which are not combined with it such as to form a larger program,
+in or on a volume of a storage or distribution medium, is called an
+"aggregate" if the compilation and its resulting copyright are not
+used to limit the access or legal rights of the compilation's users
+beyond what the individual works permit.  Inclusion of a covered work
+in an aggregate does not cause this License to apply to the other
+parts of the aggregate.
+
+  6. Conveying Non-Source Forms.
+
+  You may convey a covered work in object code form under the terms
+of sections 4 and 5, provided that you also convey the
+machine-readable Corresponding Source under the terms of this License,
+in one of these ways:
+
+    a) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by the
+    Corresponding Source fixed on a durable physical medium
+    customarily used for software interchange.
+
+    b) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by a
+    written offer, valid for at least three years and valid for as
+    long as you offer spare parts or customer support for that product
+    model, to give anyone who possesses the object code either (1) a
+    copy of the Corresponding Source for all the software in the
+    product that is covered by this License, on a durable physical
+    medium customarily used for software interchange, for a price no
+    more than your reasonable cost of physically performing this
+    conveying of source, or (2) access to copy the
+    Corresponding Source from a network server at no charge.
+
+    c) Convey individual copies of the object code with a copy of the
+    written offer to provide the Corresponding Source.  This
+    alternative is allowed only occasionally and noncommercially, and
+    only if you received the object code with such an offer, in accord
+    with subsection 6b.
+
+    d) Convey the object code by offering access from a designated
+    place (gratis or for a charge), and offer equivalent access to the
+    Corresponding Source in the same way through the same place at no
+    further charge.  You need not require recipients to copy the
+    Corresponding Source along with the object code.  If the place to
+    copy the object code is a network server, the Corresponding Source
+    may be on a different server (operated by you or a third party)
+    that supports equivalent copying facilities, provided you maintain
+    clear directions next to the object code saying where to find the
+    Corresponding Source.  Regardless of what server hosts the
+    Corresponding Source, you remain obligated to ensure that it is
+    available for as long as needed to satisfy these requirements.
+
+    e) Convey the object code using peer-to-peer transmission, provided
+    you inform other peers where the object code and Corresponding
+    Source of the work are being offered to the general public at no
+    charge under subsection 6d.
+
+  A separable portion of the object code, whose source code is excluded
+from the Corresponding Source as a System Library, need not be
+included in conveying the object code work.
+
+  A "User Product" is either (1) a "consumer product", which means any
+tangible personal property which is normally used for personal, family,
+or household purposes, or (2) anything designed or sold for incorporation
+into a dwelling.  In determining whether a product is a consumer product,
+doubtful cases shall be resolved in favor of coverage.  For a particular
+product received by a particular user, "normally used" refers to a
+typical or common use of that class of product, regardless of the status
+of the particular user or of the way in which the particular user
+actually uses, or expects or is expected to use, the product.  A product
+is a consumer product regardless of whether the product has substantial
+commercial, industrial or non-consumer uses, unless such uses represent
+the only significant mode of use of the product.
+
+  "Installation Information" for a User Product means any methods,
+procedures, authorization keys, or other information required to install
+and execute modified versions of a covered work in that User Product from
+a modified version of its Corresponding Source.  The information must
+suffice to ensure that the continued functioning of the modified object
+code is in no case prevented or interfered with solely because
+modification has been made.
+
+  If you convey an object code work under this section in, or with, or
+specifically for use in, a User Product, and the conveying occurs as
+part of a transaction in which the right of possession and use of the
+User Product is transferred to the recipient in perpetuity or for a
+fixed term (regardless of how the transaction is characterized), the
+Corresponding Source conveyed under this section must be accompanied
+by the Installation Information.  But this requirement does not apply
+if neither you nor any third party retains the ability to install
+modified object code on the User Product (for example, the work has
+been installed in ROM).
+
+  The requirement to provide Installation Information does not include a
+requirement to continue to provide support service, warranty, or updates
+for a work that has been modified or installed by the recipient, or for
+the User Product in which it has been modified or installed.  Access to a
+network may be denied when the modification itself materially and
+adversely affects the operation of the network or violates the rules and
+protocols for communication across the network.
+
+  Corresponding Source conveyed, and Installation Information provided,
+in accord with this section must be in a format that is publicly
+documented (and with an implementation available to the public in
+source code form), and must require no special password or key for
+unpacking, reading or copying.
+
+  7. Additional Terms.
+
+  "Additional permissions" are terms that supplement the terms of this
+License by making exceptions from one or more of its conditions.
+Additional permissions that are applicable to the entire Program shall
+be treated as though they were included in this License, to the extent
+that they are valid under applicable law.  If additional permissions
+apply only to part of the Program, that part may be used separately
+under those permissions, but the entire Program remains governed by
+this License without regard to the additional permissions.
+
+  When you convey a copy of a covered work, you may at your option
+remove any additional permissions from that copy, or from any part of
+it.  (Additional permissions may be written to require their own
+removal in certain cases when you modify the work.)  You may place
+additional permissions on material, added by you to a covered work,
+for which you have or can give appropriate copyright permission.
+
+  Notwithstanding any other provision of this License, for material you
+add to a covered work, you may (if authorized by the copyright holders of
+that material) supplement the terms of this License with terms:
+
+    a) Disclaiming warranty or limiting liability differently from the
+    terms of sections 15 and 16 of this License; or
+
+    b) Requiring preservation of specified reasonable legal notices or
+    author attributions in that material or in the Appropriate Legal
+    Notices displayed by works containing it; or
+
+    c) Prohibiting misrepresentation of the origin of that material, or
+    requiring that modified versions of such material be marked in
+    reasonable ways as different from the original version; or
+
+    d) Limiting the use for publicity purposes of names of licensors or
+    authors of the material; or
+
+    e) Declining to grant rights under trademark law for use of some
+    trade names, trademarks, or service marks; or
+
+    f) Requiring indemnification of licensors and authors of that
+    material by anyone who conveys the material (or modified versions of
+    it) with contractual assumptions of liability to the recipient, for
+    any liability that these contractual assumptions directly impose on
+    those licensors and authors.
+
+  All other non-permissive additional terms are considered "further
+restrictions" within the meaning of section 10.  If the Program as you
+received it, or any part of it, contains a notice stating that it is
+governed by this License along with a term that is a further
+restriction, you may remove that term.  If a license document contains
+a further restriction but permits relicensing or conveying under this
+License, you may add to a covered work material governed by the terms
+of that license document, provided that the further restriction does
+not survive such relicensing or conveying.
+
+  If you add terms to a covered work in accord with this section, you
+must place, in the relevant source files, a statement of the
+additional terms that apply to those files, or a notice indicating
+where to find the applicable terms.
+
+  Additional terms, permissive or non-permissive, may be stated in the
+form of a separately written license, or stated as exceptions;
+the above requirements apply either way.
+
+  8. Termination.
+
+  You may not propagate or modify a covered work except as expressly
+provided under this License.  Any attempt otherwise to propagate or
+modify it is void, and will automatically terminate your rights under
+this License (including any patent licenses granted under the third
+paragraph of section 11).
+
+  However, if you cease all violation of this License, then your
+license from a particular copyright holder is reinstated (a)
+provisionally, unless and until the copyright holder explicitly and
+finally terminates your license, and (b) permanently, if the copyright
+holder fails to notify you of the violation by some reasonable means
+prior to 60 days after the cessation.
+
+  Moreover, your license from a particular copyright holder is
+reinstated permanently if the copyright holder notifies you of the
+violation by some reasonable means, this is the first time you have
+received notice of violation of this License (for any work) from that
+copyright holder, and you cure the violation prior to 30 days after
+your receipt of the notice.
+
+  Termination of your rights under this section does not terminate the
+licenses of parties who have received copies or rights from you under
+this License.  If your rights have been terminated and not permanently
+reinstated, you do not qualify to receive new licenses for the same
+material under section 10.
+
+  9. Acceptance Not Required for Having Copies.
+
+  You are not required to accept this License in order to receive or
+run a copy of the Program.  Ancillary propagation of a covered work
+occurring solely as a consequence of using peer-to-peer transmission
+to receive a copy likewise does not require acceptance.  However,
+nothing other than this License grants you permission to propagate or
+modify any covered work.  These actions infringe copyright if you do
+not accept this License.  Therefore, by modifying or propagating a
+covered work, you indicate your acceptance of this License to do so.
+
+  10. Automatic Licensing of Downstream Recipients.
+
+  Each time you convey a covered work, the recipient automatically
+receives a license from the original licensors, to run, modify and
+propagate that work, subject to this License.  You are not responsible
+for enforcing compliance by third parties with this License.
+
+  An "entity transaction" is a transaction transferring control of an
+organization, or substantially all assets of one, or subdividing an
+organization, or merging organizations.  If propagation of a covered
+work results from an entity transaction, each party to that
+transaction who receives a copy of the work also receives whatever
+licenses to the work the party's predecessor in interest had or could
+give under the previous paragraph, plus a right to possession of the
+Corresponding Source of the work from the predecessor in interest, if
+the predecessor has it or can get it with reasonable efforts.
+
+  You may not impose any further restrictions on the exercise of the
+rights granted or affirmed under this License.  For example, you may
+not impose a license fee, royalty, or other charge for exercise of
+rights granted under this License, and you may not initiate litigation
+(including a cross-claim or counterclaim in a lawsuit) alleging that
+any patent claim is infringed by making, using, selling, offering for
+sale, or importing the Program or any portion of it.
+
+  11. Patents.
+
+  A "contributor" is a copyright holder who authorizes use under this
+License of the Program or a work on which the Program is based.  The
+work thus licensed is called the contributor's "contributor version".
+
+  A contributor's "essential patent claims" are all patent claims
+owned or controlled by the contributor, whether already acquired or
+hereafter acquired, that would be infringed by some manner, permitted
+by this License, of making, using, or selling its contributor version,
+but do not include claims that would be infringed only as a
+consequence of further modification of the contributor version.  For
+purposes of this definition, "control" includes the right to grant
+patent sublicenses in a manner consistent with the requirements of
+this License.
+
+  Each contributor grants you a non-exclusive, worldwide, royalty-free
+patent license under the contributor's essential patent claims, to
+make, use, sell, offer for sale, import and otherwise run, modify and
+propagate the contents of its contributor version.
+
+  In the following three paragraphs, a "patent license" is any express
+agreement or commitment, however denominated, not to enforce a patent
+(such as an express permission to practice a patent or covenant not to
+sue for patent infringement).  To "grant" such a patent license to a
+party means to make such an agreement or commitment not to enforce a
+patent against the party.
+
+  If you convey a covered work, knowingly relying on a patent license,
+and the Corresponding Source of the work is not available for anyone
+to copy, free of charge and under the terms of this License, through a
+publicly available network server or other readily accessible means,
+then you must either (1) cause the Corresponding Source to be so
+available, or (2) arrange to deprive yourself of the benefit of the
+patent license for this particular work, or (3) arrange, in a manner
+consistent with the requirements of this License, to extend the patent
+license to downstream recipients.  "Knowingly relying" means you have
+actual knowledge that, but for the patent license, your conveying the
+covered work in a country, or your recipient's use of the covered work
+in a country, would infringe one or more identifiable patents in that
+country that you have reason to believe are valid.
+
+  If, pursuant to or in connection with a single transaction or
+arrangement, you convey, or propagate by procuring conveyance of, a
+covered work, and grant a patent license to some of the parties
+receiving the covered work authorizing them to use, propagate, modify
+or convey a specific copy of the covered work, then the patent license
+you grant is automatically extended to all recipients of the covered
+work and works based on it.
+
+  A patent license is "discriminatory" if it does not include within
+the scope of its coverage, prohibits the exercise of, or is
+conditioned on the non-exercise of one or more of the rights that are
+specifically granted under this License.  You may not convey a covered
+work if you are a party to an arrangement with a third party that is
+in the business of distributing software, under which you make payment
+to the third party based on the extent of your activity of conveying
+the work, and under which the third party grants, to any of the
+parties who would receive the covered work from you, a discriminatory
+patent license (a) in connection with copies of the covered work
+conveyed by you (or copies made from those copies), or (b) primarily
+for and in connection with specific products or compilations that
+contain the covered work, unless you entered into that arrangement,
+or that patent license was granted, prior to 28 March 2007.
+
+  Nothing in this License shall be construed as excluding or limiting
+any implied license or other defenses to infringement that may
+otherwise be available to you under applicable patent law.
+
+  12. No Surrender of Others' Freedom.
+
+  If conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot convey a
+covered work so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you may
+not convey it at all.  For example, if you agree to terms that obligate you
+to collect a royalty for further conveying from those to whom you convey
+the Program, the only way you could satisfy both those terms and this
+License would be to refrain entirely from conveying the Program.
+
+  13. Use with the GNU Affero General Public License.
+
+  Notwithstanding any other provision of this License, you have
+permission to link or combine any covered work with a work licensed
+under version 3 of the GNU Affero General Public License into a single
+combined work, and to convey the resulting work.  The terms of this
+License will continue to apply to the part which is the covered work,
+but the special requirements of the GNU Affero General Public License,
+section 13, concerning interaction through a network will apply to the
+combination as such.
+
+  14. Revised Versions of this License.
+
+  The Free Software Foundation may publish revised and/or new versions of
+the GNU General Public License from time to time.  Such new versions will
+be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+  Each version is given a distinguishing version number.  If the
+Program specifies that a certain numbered version of the GNU General
+Public License "or any later version" applies to it, you have the
+option of following the terms and conditions either of that numbered
+version or of any later version published by the Free Software
+Foundation.  If the Program does not specify a version number of the
+GNU General Public License, you may choose any version ever published
+by the Free Software Foundation.
+
+  If the Program specifies that a proxy can decide which future
+versions of the GNU General Public License can be used, that proxy's
+public statement of acceptance of a version permanently authorizes you
+to choose that version for the Program.
+
+  Later license versions may give you additional or different
+permissions.  However, no additional obligations are imposed on any
+author or copyright holder as a result of your choosing to follow a
+later version.
+
+  15. Disclaimer of Warranty.
+
+  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
+APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
+HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
+OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
+IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
+ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. Limitation of Liability.
+
+  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
+THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
+GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
+USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
+DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
+PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
+EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGES.
+
+  17. Interpretation of Sections 15 and 16.
+
+  If the disclaimer of warranty and limitation of liability provided
+above cannot be given local legal effect according to their terms,
+reviewing courts shall apply local law that most closely approximates
+an absolute waiver of all civil liability in connection with the
+Program, unless a warranty or assumption of liability accompanies a
+copy of the Program in return for a fee.
+
+                     END OF TERMS AND CONDITIONS
+
+            How to Apply These Terms to Your New Programs
+
+  If you develop a new program, and you want it to be of the greatest
+possible use to the public, the best way to achieve this is to make it
+free software which everyone can redistribute and change under these terms.
+
+  To do so, attach the following notices to the program.  It is safest
+to attach them to the start of each source file to most effectively
+state the exclusion of warranty; and each file should have at least
+the "copyright" line and a pointer to where the full notice is found.
+
+    <one line to give the program's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This program is free software: you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation, either version 3 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License
+    along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
+Also add information on how to contact you by electronic and paper mail.
+
+  If the program does terminal interaction, make it output a short
+notice like this when it starts in an interactive mode:
+
+    <program>  Copyright (C) <year>  <name of author>
+    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
+    This is free software, and you are welcome to redistribute it
+    under certain conditions; type `show c' for details.
+
+The hypothetical commands `show w' and `show c' should show the appropriate
+parts of the General Public License.  Of course, your program's commands
+might be different; for a GUI interface, you would use an "about box".
+
+  You should also get your employer (if you work as a programmer) or school,
+if any, to sign a "copyright disclaimer" for the program, if necessary.
+For more information on this, and how to apply and follow the GNU GPL, see
+<https://www.gnu.org/licenses/>.
+
+  The GNU General Public License does not permit incorporating your program
+into proprietary programs.  If your program is a subroutine library, you
+may consider it more useful to permit linking proprietary applications with
+the library.  If this is what you want to do, use the GNU Lesser General
+Public License instead of this License.  But first, please read
+<https://www.gnu.org/licenses/why-not-lgpl.html>.
```

### Comparing `gEconpy-1.1.0/PKG-INFO` & `gEconpy-1.2.0/PKG-INFO`

 * *Files 8% similar despite different names*

```diff
@@ -1,286 +1,282 @@
-Metadata-Version: 2.1
-Name: gEconpy
-Version: 1.1.0
-Summary: A package for solving, estimating, and analyzing DSGE models
-Home-page: https://github.com/jessegrabowski/gEcon.py
-Author: Jesse Grabowski
-Author-email: jessegrabowski@gmail.com
-License: UNKNOWN
-Platform: UNKNOWN
-Description-Content-Type: text/markdown
-License-File: LICENSE
-
-# gEconpy
-A collection of tools for working with DSGE models in python, inspired by the fantastic R package gEcon, http://gecon.r-forge.r-project.org/.
-
-Like gEcon, gEconpy solves first order conditions automatically, helping the researcher avoid math errors while facilitating rapid prototyping of models. By working in the optimization problem space rather than the FoC space, modifications to the model are much simpler. Adding an additional term to the utility function, for example, requires modifying only 2-3 lines of code, whereas in FoC space it may require re-solving the entire model by hand.
-
-gEconpy uses the GCN file originally created for the gEcon package. gEcon GCN files are fully compatable with gEconpy, and includes all the great features of GCN files, including:
-* Automatically solve first order conditions
-* Users can include steady-state values in equations without explictly solving for them by hand first!
-* Users can declare "calibrated parameters", requesting a parameter value be found to induce a specific steady-state relationship
-
-gEconpy is still in an unfinished alpha state, but I encourage anyone interested in DSGE modeling to give it a try and and report any bugs you might find.
-
-## Contributing:
-Contributions from anyone are welcome, regardless of previous experience. Please check the Issues tab for open issues, or to create a new issue. 
-
-# Representing a DSGE Model
-Like the R package gEcon, gEconpy uses .GCN files to represent a DSGE model. A GCN file is divided into blocks, each of which represents an optimization problem. Here is one block from the example Real Business Cycle (RBC) model included in the package.
-
-```
-block HOUSEHOLD
-{
-	definitions
-	{
-		u[] = C[] ^ (1 - sigma_C) / (1 - sigma_C) -
-		      L[] ^ (1 + sigma_L) / (1 + sigma_L);
-	};
-
-	controls
-	{
-		C[], L[], I[], K[];
-	};
-
-	objective
-	{
-		U[] = u[] + beta * E[][U[1]];
-	};
-
-	constraints
-	{
-		C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
-		K[] = (1 - delta) * K[-1] + I[] : q[];
-	};
-
-	calibration
-	{
-		# Fixed parameters
-		beta  = 0.99;
-		delta = 0.02;
-
-		# Parameters to estimate
-		sigma_C ~ N(loc=1.5, scale=0.1, lower=1.0) = 1.5;
-		sigma_L ~ N(loc=2.0, scale=0.1, lower=1.0) = 2.0;
-	};
-};
-```
-## Basic Basics
-A .GCN file uses an easy-to-read syntax. Whitespace is not meaningful - lines are terminated with a ";", so long equations can be split into multiple lines for readability. There are no reserved keywords* to avoid -- feel free to write beta instead of betta!
-
-Model variables are written with a name followed by square brackets, as in `U[]`. The square brackets give the time index the variable enters with. Following Dynare conventions, capital stock `K[-1]` enters with a lag, while all other variables enter in the present. Expectations are denoted by wrapping variables with `E[]`, as in `E[U[1]]` for expected utility at t+1. So I lied, there are a couple reserved keywords (hence the asterisk). Finally, you can refer directly to steady-state values as `K[ss]`.
-
-Parameters are written exactly as variables, except they have no square brackets `[]`.
-
-## Anatomy of a Block
-Blocks are divided into five components: `definitions`, `controls`, `objective`, `constraints`, `identities`, `shocks`, and, `calibration`. In this block, we see five of the seven. The blocks have the following functions:
-
-1. `definitions` contains equations that are **not** stored in the model. Instead, they are immediately substituted into all equations **within the same block**. In this example, a definition is used for the instantaneous utility function. It will be immediately substitutited into the Bellman equation written in the `objective` block.
-2. `controls` are the variables under the agent's control. The objective function represented by the block will be solved by forming a Lagrange function and taking derivatives with respect to the controls.
-3. The `objective` block contains only a single equation, and gives the function an agent will try to maximize over an infinite time horizon. In this case, the agent has a CRRA utility function.
-4. `constraints` give the resource constraints that the agent's maximization must respect. All constraints are given their own Lagrange multipiers.
-5. `identities` are equations that are not part of an optimization problem, but that are a part of the model. Unlike equations defined in the `definitions` block, `identities` are saved in the model's system of equations.
-6. `shocks` are where the user defines exogenous shocks, as in `varexo` in Dynare.
-7. The `calibration` block where free parameters, calibrated parameters, and parameter prior distributions are defined.
-
-## Parameter Values and Priors
-
-All parameters must be given values. In the household block above, all parameters are given a value directly. `beta` and `delta` are set fixed, while `sigma_C` and `sigma_L` are given priors and starting values. The `~` operator denotes a Prior, while `=` denotes a fixed value. All parameters must have a fixed value -- this is used as the "default" value when building and solving the model. Priors, on the other hand, are optional. At present, the user can choose from `Normal`, `HalfNormal`, `TruncatedNormal`, `Beta`, `Gamma`, `Inverse_Gamma`, and `Uniform` priors, with more to come as I improve the integration with PyMC. Distributons can be parameterized either using the `loc`, `scale`, `shape` synatx of `scipy.stats`, or directly using the common parameter values from the literature (such as a `mu` and `sigma` for a normal).
-
-
-As an alterantive to setting a parameter value directly, the user can declare a parameter to be calibrated. To do this, give a steady-state relationship that the parameter should be calibrated to ensure is true. The following GCN code block for the firm's optimization problem shows how this is done:
-
-```
-block FIRM
-{
-    controls
-    {
-        K[-1], L[];
-    };
-
-    objective
-    {
-        TC[] = -(r[] * K[-1] + w[] * L[]);
-    };
-
-    constraints
-    {
-        Y[] = A[] * K[-1] ^ alpha * L[] ^ (1 - alpha) : mc[];
-    };
-
-    identities
-    {
-        # Perfect competition
-        mc[] = 1;
-    };
-
-    calibration
-    {
-	L[ss] / K[ss] = 0.36 -> alpha;
-    };
-};
-```
-The `alpha` parameter is set so that in the steady state, the ratio of labor to capital is 0.36. On the back end, gEconpy will use an optimizer to find a value of `alpha` that satsifies the user's condition. Note that calibrated parameters cannot have prior distributions!
-
-## Lagrange Multipliers and First Order Conditions
-As mentioned, all constraints will automatically have a Lagrange multiplier assigned to them. The user name these multipliers himself by putting a colon ":" after an equation, followed by the Lagrange multipler name. From the code above:
-
-```
-C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
-K[] = (1 - delta) * K[-1] + I[] : q[];
-```
-
-The multiplier associated with the budget constraint has been given the name "lambda", as is usual in the literature, while the law of motion of capital has been given the name `q[]`. If the user wanted, she could use these variables in further computations within the block, for example `Q[] = q[] / lambda[]`, Tobin's Q, could be added in the `identities` block.
-
-Interally, first order conditions are solved by first making all substitutions from `definitions`, then forming the following Lagrangian function:
-`L = objective.RHS - lm1 * (control_1.LHS - control_1.RHS) - lm2 * (control_2.LHS - control_2.RHS) ... - lm_k * (control_k.LHS - control_k.RHS)`
-
-Next, the derivative of this Lagrangian is taken with respect to all control variables and all lagrange multipliers. Derivaties are are computed "though time" using `TimeAwareSymbols`, an extension of a normal Sympy symbol. For a control variable x, the total derivative over time is built up as `dL[]/dx[] + beta * dL[+1]/dx + beta * beta * dL[+2]/dx[] ...`. This unrolling terminates when `dL[+n]/dx[] = 0`.
-
-The result of this unrolling and taking derivatives process are the first order conditions (FoC). All model FoCs, along with objectives, constraints, and identities, are saved into the system of equations that represents the model.
-
-## Steady State
-
-After finding FoCs, the system will be ready to find a steady state and solve for a first-order linear approximation. To help the process, the user can write a `STEADY_STATE` block. This is a special reserved keyword block that can be placed anywhere in the GCN file. It should contain only `defintions` and `identities` as components. Here is an example of a steady state block for the RBC model:
-
-```
-block STEADY_STATE
-{
-    definitions
-    {
-      # If this is empty you can delete this, but it is also nice for writing common parameter or variable combinations.
-    };
-
-    identities
-    {
-        A[ss] = 1;
-        P[ss] = 1;
-        r[ss] = P[ss] * (1 / beta - (1 - delta));
-        w[ss] = (1 - alpha) * P[ss] ^ (1 / (1 - alpha)) * (alpha / r[ss]) ^ (alpha / (1 - alpha));
-        Y[ss] = (r[ss] / (r[ss] - delta * alpha)) ^ (sigma_C / (sigma_C + sigma_L)) *
-            (w[ss] / P[ss] * (w[ss] / P[ss] / (1 - alpha)) ^ sigma_L) ^ (1 / (sigma_C + sigma_L));
-
-        I[ss] = (delta * alpha / r[ss]) * Y[ss];
-        C[ss] = Y[ss] ^ (-sigma_L / sigma_C) * ((1 - alpha) ^ (-sigma_L) * (w[ss] / P[ss]) ^ (1 + sigma_L)) ^ (1 / sigma_C);
-        K[ss] = alpha * Y[ss] * P[ss] / r[ss];
-        L[ss] = (1 - alpha) * Y[ss] * P[ss] / w[ss];
-
-
-        U[ss] = (1 / (1 - beta)) * (C[ss] ^ (1 - sigma_C) / (1 - sigma_C) - L[ss] ^ (1 + sigma_L) / (1 + sigma_L));
-        lambda[ss] = C[ss] ^ (-sigma_C) / P[ss];
-        q[ss] = lambda[ss];
-        TC[ss] = -(r[ss] * K[ss] + w[ss] * L[ss]);
-    };
-};
-```
-
-It is not necessary to write an empty `definitions` component; this was done just to show where it goes. All information from the model block, including parameters and variables, are available to use in the `STEADY_STATE` block regardless of where they appear relative to each other (you can put the STEADY_STATE block at the top if you wish -- the file is not parsed top-to-bottom).
-
-Note that these equations are not checked in any way -- if you put something in the `STEADY_STATE` block, it is taken as the Word of God, and model solving proceeds from there. If you are having trouble finding a steady state, be sure to double check these equations.
-
-Finally, you **do not** have to provide the complete steady state system! You can include only equations, and the rest will be passed to an optimizer to be solved.
-
-## Solving the model
-
-Once a GCN file is written, using gEcon to do analysis is easy, as this code block shows:
-```python
-file_path = 'GCN Files/RBC_basic.gcn'
-model = gEconModel(file_path, verbose=True)
-```
-
-When the model is loaded, you will get a message about the number of equations and variables, as well as some other basic model descriptions. You can then solve for the stead state:
-```python
-model.steady_state()
->>> Steady state found! Sum of squared residuals is 2.9196536232567403e-19
-```
-
-And get the linearized state space representation
-
-```python
-model.solve_model()
->>>Solution found, sum of squared residuals:  7.075155451456433e-30
->>>Norm of deterministic part: 0.000000000
->>>Norm of stochastic part:    0.000000000
-```
-
-To see how to do simulations, IRFs, and compute moments, see the example notebook.
-
-# Other Features
-
-## Dynare Code Generation
-
-Since Dynare is still the gold standard in DSGE modeling, and this is a wacky open source package written by a literally who?, gEconpy has the ability to automatically convert a solved model into a Dynare mod file. This is done as follows:
-
-```python
-from gEconpy.shared.dynare_convert import make_mod_file
-print(make_mod_file(model))
-```
-
-Output:
-```
-var A, C, I, K, L, TC, U, Y, mc, q, r, var_lambda, w;
-varexo epsilon_A;
-
-parameters param_alpha, param_beta, param_delta, rho_A;
-parameters sigma_C, sigma_L;
-
-param_alpha = 0.35;
-param_beta = 0.99;
-param_delta = 0.02;
-rho_A = 0.95;
-sigma_C = 1.5;
-sigma_L = 2.0;
-
-model;
--C - I + K(-1) * r + L * w = 0;
-I - K + K(-1) *(1 - param_delta) = 0;
-C ^(1 - sigma_C) /(1 - sigma_C) - L ^(sigma_L + 1) /(sigma_L + 1) - U + U(1) * param_beta = 0;
--var_lambda + C ^(- sigma_C) = 0;
--L ^ sigma_L + var_lambda * w = 0;
-q - var_lambda = 0;
-param_beta *(q(1) *(1 - param_delta) + r(1) * var_lambda(1)) - q = 0;
-1 - mc = 0;
-A * K(-1) ^ param_alpha * L ^(1 - param_alpha) - Y = 0;
--K(-1) * r - L * w - TC = 0;
-A * K(-1) ^(param_alpha - 1) * L ^(1 - param_alpha) * param_alpha * mc - r = 0;
-A * K(-1) ^ param_alpha * mc *(1 - param_alpha) / L ^ param_alpha - w = 0;
-epsilon_A + rho_A * log(A(-1)) - log(A) = 0;
-end;
-
-initval;
-A = 1.0000;
-C = 2.3584;
-I = 0.7146;
-K = 35.7323;
-L = 0.8201;
-TC = -3.0731;
-U = -148.6156;
-Y = 3.0731;
-var_lambda = 0.2761;
-mc = 1.0000;
-q = 0.2761;
-r = 0.0301;
-w = 2.4358;
-end;
-
-steady;
-check(qz_zero_threshold=1e-20);
-
-shocks;
-var epsilon_A;
-stderr 0.01;
-end;
-
-stoch_simul(order=1, irf=100, qz_zero_threshold=1e-20);
-```
-
-### Warings about Dynare Code
-* No efforts are made to provide symbolic solutions to the steady-state! If you include steady state values in your equations and do not provide a symbolic solution in the STEADY_STATE block, the .mod file will not work "out of the box".
-* If your model includes calibrated equations, the generated Dynare code **will not** work out of the box. You need to analyically compute the steady state values and add a deterministic relationship (that beings with `#`) to the model block.
-
-
-## Estimation
-
-Coming soon!
-
-
+Metadata-Version: 2.1
+Name: gEconpy
+Version: 1.2.0
+Summary: A package for solving, estimating, and analyzing DSGE models
+Home-page: https://github.com/jessegrabowski/gEcon.py
+Author: Jesse Grabowski
+Author-email: jessegrabowski@gmail.com
+Description-Content-Type: text/markdown
+License-File: LICENSE
+
+# gEconpy
+A collection of tools for working with DSGE models in python, inspired by the fantastic R package gEcon, http://gecon.r-forge.r-project.org/.
+
+Like gEcon, gEconpy solves first order conditions automatically, helping the researcher avoid math errors while facilitating rapid prototyping of models. By working in the optimization problem space rather than the FoC space, modifications to the model are much simpler. Adding an additional term to the utility function, for example, requires modifying only 2-3 lines of code, whereas in FoC space it may require re-solving the entire model by hand.
+
+gEconpy uses the GCN file originally created for the gEcon package. gEcon GCN files are fully compatable with gEconpy, and includes all the great features of GCN files, including:
+* Automatically solve first order conditions
+* Users can include steady-state values in equations without explictly solving for them by hand first!
+* Users can declare "calibrated parameters", requesting a parameter value be found to induce a specific steady-state relationship
+
+gEconpy is still in an unfinished alpha state, but I encourage anyone interested in DSGE modeling to give it a try and and report any bugs you might find.
+
+## Contributing:
+Contributions from anyone are welcome, regardless of previous experience. Please check the Issues tab for open issues, or to create a new issue. 
+
+# Representing a DSGE Model
+Like the R package gEcon, gEconpy uses .GCN files to represent a DSGE model. A GCN file is divided into blocks, each of which represents an optimization problem. Here is one block from the example Real Business Cycle (RBC) model included in the package.
+
+```
+block HOUSEHOLD
+{
+	definitions
+	{
+		u[] = C[] ^ (1 - sigma_C) / (1 - sigma_C) -
+		      L[] ^ (1 + sigma_L) / (1 + sigma_L);
+	};
+
+	controls
+	{
+		C[], L[], I[], K[];
+	};
+
+	objective
+	{
+		U[] = u[] + beta * E[][U[1]];
+	};
+
+	constraints
+	{
+		C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
+		K[] = (1 - delta) * K[-1] + I[] : q[];
+	};
+
+	calibration
+	{
+		# Fixed parameters
+		beta  = 0.99;
+		delta = 0.02;
+
+		# Parameters to estimate
+		sigma_C ~ N(loc=1.5, scale=0.1, lower=1.0) = 1.5;
+		sigma_L ~ N(loc=2.0, scale=0.1, lower=1.0) = 2.0;
+	};
+};
+```
+## Basic Basics
+A .GCN file uses an easy-to-read syntax. Whitespace is not meaningful - lines are terminated with a ";", so long equations can be split into multiple lines for readability. There are no reserved keywords* to avoid -- feel free to write beta instead of betta!
+
+Model variables are written with a name followed by square brackets, as in `U[]`. The square brackets give the time index the variable enters with. Following Dynare conventions, capital stock `K[-1]` enters with a lag, while all other variables enter in the present. Expectations are denoted by wrapping variables with `E[]`, as in `E[U[1]]` for expected utility at t+1. So I lied, there are a couple reserved keywords (hence the asterisk). Finally, you can refer directly to steady-state values as `K[ss]`.
+
+Parameters are written exactly as variables, except they have no square brackets `[]`.
+
+## Anatomy of a Block
+Blocks are divided into five components: `definitions`, `controls`, `objective`, `constraints`, `identities`, `shocks`, and, `calibration`. In this block, we see five of the seven. The blocks have the following functions:
+
+1. `definitions` contains equations that are **not** stored in the model. Instead, they are immediately substituted into all equations **within the same block**. In this example, a definition is used for the instantaneous utility function. It will be immediately substitutited into the Bellman equation written in the `objective` block.
+2. `controls` are the variables under the agent's control. The objective function represented by the block will be solved by forming a Lagrange function and taking derivatives with respect to the controls.
+3. The `objective` block contains only a single equation, and gives the function an agent will try to maximize over an infinite time horizon. In this case, the agent has a CRRA utility function.
+4. `constraints` give the resource constraints that the agent's maximization must respect. All constraints are given their own Lagrange multipiers.
+5. `identities` are equations that are not part of an optimization problem, but that are a part of the model. Unlike equations defined in the `definitions` block, `identities` are saved in the model's system of equations.
+6. `shocks` are where the user defines exogenous shocks, as in `varexo` in Dynare.
+7. The `calibration` block where free parameters, calibrated parameters, and parameter prior distributions are defined.
+
+## Parameter Values and Priors
+
+All parameters must be given values. In the household block above, all parameters are given a value directly. `beta` and `delta` are set fixed, while `sigma_C` and `sigma_L` are given priors and starting values. The `~` operator denotes a Prior, while `=` denotes a fixed value. All parameters must have a fixed value -- this is used as the "default" value when building and solving the model. Priors, on the other hand, are optional. At present, the user can choose from `Normal`, `HalfNormal`, `TruncatedNormal`, `Beta`, `Gamma`, `Inverse_Gamma`, and `Uniform` priors, with more to come as I improve the integration with PyMC. Distributons can be parameterized either using the `loc`, `scale`, `shape` synatx of `scipy.stats`, or directly using the common parameter values from the literature (such as a `mu` and `sigma` for a normal).
+
+
+As an alterantive to setting a parameter value directly, the user can declare a parameter to be calibrated. To do this, give a steady-state relationship that the parameter should be calibrated to ensure is true. The following GCN code block for the firm's optimization problem shows how this is done:
+
+```
+block FIRM
+{
+    controls
+    {
+        K[-1], L[];
+    };
+
+    objective
+    {
+        TC[] = -(r[] * K[-1] + w[] * L[]);
+    };
+
+    constraints
+    {
+        Y[] = A[] * K[-1] ^ alpha * L[] ^ (1 - alpha) : mc[];
+    };
+
+    identities
+    {
+        # Perfect competition
+        mc[] = 1;
+    };
+
+    calibration
+    {
+	L[ss] / K[ss] = 0.36 -> alpha;
+    };
+};
+```
+The `alpha` parameter is set so that in the steady state, the ratio of labor to capital is 0.36. On the back end, gEconpy will use an optimizer to find a value of `alpha` that satsifies the user's condition. Note that calibrated parameters cannot have prior distributions!
+
+## Lagrange Multipliers and First Order Conditions
+As mentioned, all constraints will automatically have a Lagrange multiplier assigned to them. The user name these multipliers himself by putting a colon ":" after an equation, followed by the Lagrange multipler name. From the code above:
+
+```
+C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
+K[] = (1 - delta) * K[-1] + I[] : q[];
+```
+
+The multiplier associated with the budget constraint has been given the name "lambda", as is usual in the literature, while the law of motion of capital has been given the name `q[]`. If the user wanted, she could use these variables in further computations within the block, for example `Q[] = q[] / lambda[]`, Tobin's Q, could be added in the `identities` block.
+
+Interally, first order conditions are solved by first making all substitutions from `definitions`, then forming the following Lagrangian function:
+`L = objective.RHS - lm1 * (control_1.LHS - control_1.RHS) - lm2 * (control_2.LHS - control_2.RHS) ... - lm_k * (control_k.LHS - control_k.RHS)`
+
+Next, the derivative of this Lagrangian is taken with respect to all control variables and all lagrange multipliers. Derivaties are are computed "though time" using `TimeAwareSymbols`, an extension of a normal Sympy symbol. For a control variable x, the total derivative over time is built up as `dL[]/dx[] + beta * dL[+1]/dx + beta * beta * dL[+2]/dx[] ...`. This unrolling terminates when `dL[+n]/dx[] = 0`.
+
+The result of this unrolling and taking derivatives process are the first order conditions (FoC). All model FoCs, along with objectives, constraints, and identities, are saved into the system of equations that represents the model.
+
+## Steady State
+
+After finding FoCs, the system will be ready to find a steady state and solve for a first-order linear approximation. To help the process, the user can write a `STEADY_STATE` block. This is a special reserved keyword block that can be placed anywhere in the GCN file. It should contain only `defintions` and `identities` as components. Here is an example of a steady state block for the RBC model:
+
+```
+block STEADY_STATE
+{
+    definitions
+    {
+      # If this is empty you can delete this, but it is also nice for writing common parameter or variable combinations.
+    };
+
+    identities
+    {
+        A[ss] = 1;
+        P[ss] = 1;
+        r[ss] = P[ss] * (1 / beta - (1 - delta));
+        w[ss] = (1 - alpha) * P[ss] ^ (1 / (1 - alpha)) * (alpha / r[ss]) ^ (alpha / (1 - alpha));
+        Y[ss] = (r[ss] / (r[ss] - delta * alpha)) ^ (sigma_C / (sigma_C + sigma_L)) *
+            (w[ss] / P[ss] * (w[ss] / P[ss] / (1 - alpha)) ^ sigma_L) ^ (1 / (sigma_C + sigma_L));
+
+        I[ss] = (delta * alpha / r[ss]) * Y[ss];
+        C[ss] = Y[ss] ^ (-sigma_L / sigma_C) * ((1 - alpha) ^ (-sigma_L) * (w[ss] / P[ss]) ^ (1 + sigma_L)) ^ (1 / sigma_C);
+        K[ss] = alpha * Y[ss] * P[ss] / r[ss];
+        L[ss] = (1 - alpha) * Y[ss] * P[ss] / w[ss];
+
+
+        U[ss] = (1 / (1 - beta)) * (C[ss] ^ (1 - sigma_C) / (1 - sigma_C) - L[ss] ^ (1 + sigma_L) / (1 + sigma_L));
+        lambda[ss] = C[ss] ^ (-sigma_C) / P[ss];
+        q[ss] = lambda[ss];
+        TC[ss] = -(r[ss] * K[ss] + w[ss] * L[ss]);
+    };
+};
+```
+
+It is not necessary to write an empty `definitions` component; this was done just to show where it goes. All information from the model block, including parameters and variables, are available to use in the `STEADY_STATE` block regardless of where they appear relative to each other (you can put the STEADY_STATE block at the top if you wish -- the file is not parsed top-to-bottom).
+
+Note that these equations are not checked in any way -- if you put something in the `STEADY_STATE` block, it is taken as the Word of God, and model solving proceeds from there. If you are having trouble finding a steady state, be sure to double check these equations.
+
+Finally, you **do not** have to provide the complete steady state system! You can include only equations, and the rest will be passed to an optimizer to be solved.
+
+## Solving the model
+
+Once a GCN file is written, using gEcon to do analysis is easy, as this code block shows:
+```python
+file_path = 'GCN Files/RBC_basic.gcn'
+model = gEconModel(file_path, verbose=True)
+```
+
+When the model is loaded, you will get a message about the number of equations and variables, as well as some other basic model descriptions. You can then solve for the stead state:
+```python
+model.steady_state()
+>>> Steady state found! Sum of squared residuals is 2.9196536232567403e-19
+```
+
+And get the linearized state space representation
+
+```python
+model.solve_model()
+>>>Solution found, sum of squared residuals:  7.075155451456433e-30
+>>>Norm of deterministic part: 0.000000000
+>>>Norm of stochastic part:    0.000000000
+```
+
+To see how to do simulations, IRFs, and compute moments, see the example notebook.
+
+# Other Features
+
+## Dynare Code Generation
+
+Since Dynare is still the gold standard in DSGE modeling, and this is a wacky open source package written by a literally who?, gEconpy has the ability to automatically convert a solved model into a Dynare mod file. This is done as follows:
+
+```python
+from gEconpy.shared.dynare_convert import make_mod_file
+print(make_mod_file(model))
+```
+
+Output:
+```
+var A, C, I, K, L, TC, U, Y, mc, q, r, var_lambda, w;
+varexo epsilon_A;
+
+parameters param_alpha, param_beta, param_delta, rho_A;
+parameters sigma_C, sigma_L;
+
+param_alpha = 0.35;
+param_beta = 0.99;
+param_delta = 0.02;
+rho_A = 0.95;
+sigma_C = 1.5;
+sigma_L = 2.0;
+
+model;
+-C - I + K(-1) * r + L * w = 0;
+I - K + K(-1) *(1 - param_delta) = 0;
+C ^(1 - sigma_C) /(1 - sigma_C) - L ^(sigma_L + 1) /(sigma_L + 1) - U + U(1) * param_beta = 0;
+-var_lambda + C ^(- sigma_C) = 0;
+-L ^ sigma_L + var_lambda * w = 0;
+q - var_lambda = 0;
+param_beta *(q(1) *(1 - param_delta) + r(1) * var_lambda(1)) - q = 0;
+1 - mc = 0;
+A * K(-1) ^ param_alpha * L ^(1 - param_alpha) - Y = 0;
+-K(-1) * r - L * w - TC = 0;
+A * K(-1) ^(param_alpha - 1) * L ^(1 - param_alpha) * param_alpha * mc - r = 0;
+A * K(-1) ^ param_alpha * mc *(1 - param_alpha) / L ^ param_alpha - w = 0;
+epsilon_A + rho_A * log(A(-1)) - log(A) = 0;
+end;
+
+initval;
+A = 1.0000;
+C = 2.3584;
+I = 0.7146;
+K = 35.7323;
+L = 0.8201;
+TC = -3.0731;
+U = -148.6156;
+Y = 3.0731;
+var_lambda = 0.2761;
+mc = 1.0000;
+q = 0.2761;
+r = 0.0301;
+w = 2.4358;
+end;
+
+steady;
+check(qz_zero_threshold=1e-20);
+
+shocks;
+var epsilon_A;
+stderr 0.01;
+end;
+
+stoch_simul(order=1, irf=100, qz_zero_threshold=1e-20);
+```
+
+### Warings about Dynare Code
+* No efforts are made to provide symbolic solutions to the steady-state! If you include steady state values in your equations and do not provide a symbolic solution in the STEADY_STATE block, the .mod file will not work "out of the box".
+* If your model includes calibrated equations, the generated Dynare code **will not** work out of the box. You need to analyically compute the steady state values and add a deterministic relationship (that beings with `#`) to the model block.
+
+
+## Estimation
+
+Coming soon!
```

### Comparing `gEconpy-1.1.0/README.md` & `gEconpy-1.2.0/README.md`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,272 +1,272 @@
-# gEconpy
-A collection of tools for working with DSGE models in python, inspired by the fantastic R package gEcon, http://gecon.r-forge.r-project.org/.
-
-Like gEcon, gEconpy solves first order conditions automatically, helping the researcher avoid math errors while facilitating rapid prototyping of models. By working in the optimization problem space rather than the FoC space, modifications to the model are much simpler. Adding an additional term to the utility function, for example, requires modifying only 2-3 lines of code, whereas in FoC space it may require re-solving the entire model by hand.
-
-gEconpy uses the GCN file originally created for the gEcon package. gEcon GCN files are fully compatable with gEconpy, and includes all the great features of GCN files, including:
-* Automatically solve first order conditions
-* Users can include steady-state values in equations without explictly solving for them by hand first!
-* Users can declare "calibrated parameters", requesting a parameter value be found to induce a specific steady-state relationship
-
-gEconpy is still in an unfinished alpha state, but I encourage anyone interested in DSGE modeling to give it a try and and report any bugs you might find.
-
-## Contributing:
-Contributions from anyone are welcome, regardless of previous experience. Please check the Issues tab for open issues, or to create a new issue. 
-
-# Representing a DSGE Model
-Like the R package gEcon, gEconpy uses .GCN files to represent a DSGE model. A GCN file is divided into blocks, each of which represents an optimization problem. Here is one block from the example Real Business Cycle (RBC) model included in the package.
-
-```
-block HOUSEHOLD
-{
-	definitions
-	{
-		u[] = C[] ^ (1 - sigma_C) / (1 - sigma_C) -
-		      L[] ^ (1 + sigma_L) / (1 + sigma_L);
-	};
-
-	controls
-	{
-		C[], L[], I[], K[];
-	};
-
-	objective
-	{
-		U[] = u[] + beta * E[][U[1]];
-	};
-
-	constraints
-	{
-		C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
-		K[] = (1 - delta) * K[-1] + I[] : q[];
-	};
-
-	calibration
-	{
-		# Fixed parameters
-		beta  = 0.99;
-		delta = 0.02;
-
-		# Parameters to estimate
-		sigma_C ~ N(loc=1.5, scale=0.1, lower=1.0) = 1.5;
-		sigma_L ~ N(loc=2.0, scale=0.1, lower=1.0) = 2.0;
-	};
-};
-```
-## Basic Basics
-A .GCN file uses an easy-to-read syntax. Whitespace is not meaningful - lines are terminated with a ";", so long equations can be split into multiple lines for readability. There are no reserved keywords* to avoid -- feel free to write beta instead of betta!
-
-Model variables are written with a name followed by square brackets, as in `U[]`. The square brackets give the time index the variable enters with. Following Dynare conventions, capital stock `K[-1]` enters with a lag, while all other variables enter in the present. Expectations are denoted by wrapping variables with `E[]`, as in `E[U[1]]` for expected utility at t+1. So I lied, there are a couple reserved keywords (hence the asterisk). Finally, you can refer directly to steady-state values as `K[ss]`.
-
-Parameters are written exactly as variables, except they have no square brackets `[]`.
-
-## Anatomy of a Block
-Blocks are divided into five components: `definitions`, `controls`, `objective`, `constraints`, `identities`, `shocks`, and, `calibration`. In this block, we see five of the seven. The blocks have the following functions:
-
-1. `definitions` contains equations that are **not** stored in the model. Instead, they are immediately substituted into all equations **within the same block**. In this example, a definition is used for the instantaneous utility function. It will be immediately substitutited into the Bellman equation written in the `objective` block.
-2. `controls` are the variables under the agent's control. The objective function represented by the block will be solved by forming a Lagrange function and taking derivatives with respect to the controls.
-3. The `objective` block contains only a single equation, and gives the function an agent will try to maximize over an infinite time horizon. In this case, the agent has a CRRA utility function.
-4. `constraints` give the resource constraints that the agent's maximization must respect. All constraints are given their own Lagrange multipiers.
-5. `identities` are equations that are not part of an optimization problem, but that are a part of the model. Unlike equations defined in the `definitions` block, `identities` are saved in the model's system of equations.
-6. `shocks` are where the user defines exogenous shocks, as in `varexo` in Dynare.
-7. The `calibration` block where free parameters, calibrated parameters, and parameter prior distributions are defined.
-
-## Parameter Values and Priors
-
-All parameters must be given values. In the household block above, all parameters are given a value directly. `beta` and `delta` are set fixed, while `sigma_C` and `sigma_L` are given priors and starting values. The `~` operator denotes a Prior, while `=` denotes a fixed value. All parameters must have a fixed value -- this is used as the "default" value when building and solving the model. Priors, on the other hand, are optional. At present, the user can choose from `Normal`, `HalfNormal`, `TruncatedNormal`, `Beta`, `Gamma`, `Inverse_Gamma`, and `Uniform` priors, with more to come as I improve the integration with PyMC. Distributons can be parameterized either using the `loc`, `scale`, `shape` synatx of `scipy.stats`, or directly using the common parameter values from the literature (such as a `mu` and `sigma` for a normal).
-
-
-As an alterantive to setting a parameter value directly, the user can declare a parameter to be calibrated. To do this, give a steady-state relationship that the parameter should be calibrated to ensure is true. The following GCN code block for the firm's optimization problem shows how this is done:
-
-```
-block FIRM
-{
-    controls
-    {
-        K[-1], L[];
-    };
-
-    objective
-    {
-        TC[] = -(r[] * K[-1] + w[] * L[]);
-    };
-
-    constraints
-    {
-        Y[] = A[] * K[-1] ^ alpha * L[] ^ (1 - alpha) : mc[];
-    };
-
-    identities
-    {
-        # Perfect competition
-        mc[] = 1;
-    };
-
-    calibration
-    {
-	L[ss] / K[ss] = 0.36 -> alpha;
-    };
-};
-```
-The `alpha` parameter is set so that in the steady state, the ratio of labor to capital is 0.36. On the back end, gEconpy will use an optimizer to find a value of `alpha` that satsifies the user's condition. Note that calibrated parameters cannot have prior distributions!
-
-## Lagrange Multipliers and First Order Conditions
-As mentioned, all constraints will automatically have a Lagrange multiplier assigned to them. The user name these multipliers himself by putting a colon ":" after an equation, followed by the Lagrange multipler name. From the code above:
-
-```
-C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
-K[] = (1 - delta) * K[-1] + I[] : q[];
-```
-
-The multiplier associated with the budget constraint has been given the name "lambda", as is usual in the literature, while the law of motion of capital has been given the name `q[]`. If the user wanted, she could use these variables in further computations within the block, for example `Q[] = q[] / lambda[]`, Tobin's Q, could be added in the `identities` block.
-
-Interally, first order conditions are solved by first making all substitutions from `definitions`, then forming the following Lagrangian function:
-`L = objective.RHS - lm1 * (control_1.LHS - control_1.RHS) - lm2 * (control_2.LHS - control_2.RHS) ... - lm_k * (control_k.LHS - control_k.RHS)`
-
-Next, the derivative of this Lagrangian is taken with respect to all control variables and all lagrange multipliers. Derivaties are are computed "though time" using `TimeAwareSymbols`, an extension of a normal Sympy symbol. For a control variable x, the total derivative over time is built up as `dL[]/dx[] + beta * dL[+1]/dx + beta * beta * dL[+2]/dx[] ...`. This unrolling terminates when `dL[+n]/dx[] = 0`.
-
-The result of this unrolling and taking derivatives process are the first order conditions (FoC). All model FoCs, along with objectives, constraints, and identities, are saved into the system of equations that represents the model.
-
-## Steady State
-
-After finding FoCs, the system will be ready to find a steady state and solve for a first-order linear approximation. To help the process, the user can write a `STEADY_STATE` block. This is a special reserved keyword block that can be placed anywhere in the GCN file. It should contain only `defintions` and `identities` as components. Here is an example of a steady state block for the RBC model:
-
-```
-block STEADY_STATE
-{
-    definitions
-    {
-      # If this is empty you can delete this, but it is also nice for writing common parameter or variable combinations.
-    };
-
-    identities
-    {
-        A[ss] = 1;
-        P[ss] = 1;
-        r[ss] = P[ss] * (1 / beta - (1 - delta));
-        w[ss] = (1 - alpha) * P[ss] ^ (1 / (1 - alpha)) * (alpha / r[ss]) ^ (alpha / (1 - alpha));
-        Y[ss] = (r[ss] / (r[ss] - delta * alpha)) ^ (sigma_C / (sigma_C + sigma_L)) *
-            (w[ss] / P[ss] * (w[ss] / P[ss] / (1 - alpha)) ^ sigma_L) ^ (1 / (sigma_C + sigma_L));
-
-        I[ss] = (delta * alpha / r[ss]) * Y[ss];
-        C[ss] = Y[ss] ^ (-sigma_L / sigma_C) * ((1 - alpha) ^ (-sigma_L) * (w[ss] / P[ss]) ^ (1 + sigma_L)) ^ (1 / sigma_C);
-        K[ss] = alpha * Y[ss] * P[ss] / r[ss];
-        L[ss] = (1 - alpha) * Y[ss] * P[ss] / w[ss];
-
-
-        U[ss] = (1 / (1 - beta)) * (C[ss] ^ (1 - sigma_C) / (1 - sigma_C) - L[ss] ^ (1 + sigma_L) / (1 + sigma_L));
-        lambda[ss] = C[ss] ^ (-sigma_C) / P[ss];
-        q[ss] = lambda[ss];
-        TC[ss] = -(r[ss] * K[ss] + w[ss] * L[ss]);
-    };
-};
-```
-
-It is not necessary to write an empty `definitions` component; this was done just to show where it goes. All information from the model block, including parameters and variables, are available to use in the `STEADY_STATE` block regardless of where they appear relative to each other (you can put the STEADY_STATE block at the top if you wish -- the file is not parsed top-to-bottom).
-
-Note that these equations are not checked in any way -- if you put something in the `STEADY_STATE` block, it is taken as the Word of God, and model solving proceeds from there. If you are having trouble finding a steady state, be sure to double check these equations.
-
-Finally, you **do not** have to provide the complete steady state system! You can include only equations, and the rest will be passed to an optimizer to be solved.
-
-## Solving the model
-
-Once a GCN file is written, using gEcon to do analysis is easy, as this code block shows:
-```python
-file_path = 'GCN Files/RBC_basic.gcn'
-model = gEconModel(file_path, verbose=True)
-```
-
-When the model is loaded, you will get a message about the number of equations and variables, as well as some other basic model descriptions. You can then solve for the stead state:
-```python
-model.steady_state()
->>> Steady state found! Sum of squared residuals is 2.9196536232567403e-19
-```
-
-And get the linearized state space representation
-
-```python
-model.solve_model()
->>>Solution found, sum of squared residuals:  7.075155451456433e-30
->>>Norm of deterministic part: 0.000000000
->>>Norm of stochastic part:    0.000000000
-```
-
-To see how to do simulations, IRFs, and compute moments, see the example notebook.
-
-# Other Features
-
-## Dynare Code Generation
-
-Since Dynare is still the gold standard in DSGE modeling, and this is a wacky open source package written by a literally who?, gEconpy has the ability to automatically convert a solved model into a Dynare mod file. This is done as follows:
-
-```python
-from gEconpy.shared.dynare_convert import make_mod_file
-print(make_mod_file(model))
-```
-
-Output:
-```
-var A, C, I, K, L, TC, U, Y, mc, q, r, var_lambda, w;
-varexo epsilon_A;
-
-parameters param_alpha, param_beta, param_delta, rho_A;
-parameters sigma_C, sigma_L;
-
-param_alpha = 0.35;
-param_beta = 0.99;
-param_delta = 0.02;
-rho_A = 0.95;
-sigma_C = 1.5;
-sigma_L = 2.0;
-
-model;
--C - I + K(-1) * r + L * w = 0;
-I - K + K(-1) *(1 - param_delta) = 0;
-C ^(1 - sigma_C) /(1 - sigma_C) - L ^(sigma_L + 1) /(sigma_L + 1) - U + U(1) * param_beta = 0;
--var_lambda + C ^(- sigma_C) = 0;
--L ^ sigma_L + var_lambda * w = 0;
-q - var_lambda = 0;
-param_beta *(q(1) *(1 - param_delta) + r(1) * var_lambda(1)) - q = 0;
-1 - mc = 0;
-A * K(-1) ^ param_alpha * L ^(1 - param_alpha) - Y = 0;
--K(-1) * r - L * w - TC = 0;
-A * K(-1) ^(param_alpha - 1) * L ^(1 - param_alpha) * param_alpha * mc - r = 0;
-A * K(-1) ^ param_alpha * mc *(1 - param_alpha) / L ^ param_alpha - w = 0;
-epsilon_A + rho_A * log(A(-1)) - log(A) = 0;
-end;
-
-initval;
-A = 1.0000;
-C = 2.3584;
-I = 0.7146;
-K = 35.7323;
-L = 0.8201;
-TC = -3.0731;
-U = -148.6156;
-Y = 3.0731;
-var_lambda = 0.2761;
-mc = 1.0000;
-q = 0.2761;
-r = 0.0301;
-w = 2.4358;
-end;
-
-steady;
-check(qz_zero_threshold=1e-20);
-
-shocks;
-var epsilon_A;
-stderr 0.01;
-end;
-
-stoch_simul(order=1, irf=100, qz_zero_threshold=1e-20);
-```
-
-### Warings about Dynare Code
-* No efforts are made to provide symbolic solutions to the steady-state! If you include steady state values in your equations and do not provide a symbolic solution in the STEADY_STATE block, the .mod file will not work "out of the box".
-* If your model includes calibrated equations, the generated Dynare code **will not** work out of the box. You need to analyically compute the steady state values and add a deterministic relationship (that beings with `#`) to the model block.
-
-
-## Estimation
-
-Coming soon!
+# gEconpy
+A collection of tools for working with DSGE models in python, inspired by the fantastic R package gEcon, http://gecon.r-forge.r-project.org/.
+
+Like gEcon, gEconpy solves first order conditions automatically, helping the researcher avoid math errors while facilitating rapid prototyping of models. By working in the optimization problem space rather than the FoC space, modifications to the model are much simpler. Adding an additional term to the utility function, for example, requires modifying only 2-3 lines of code, whereas in FoC space it may require re-solving the entire model by hand.
+
+gEconpy uses the GCN file originally created for the gEcon package. gEcon GCN files are fully compatable with gEconpy, and includes all the great features of GCN files, including:
+* Automatically solve first order conditions
+* Users can include steady-state values in equations without explictly solving for them by hand first!
+* Users can declare "calibrated parameters", requesting a parameter value be found to induce a specific steady-state relationship
+
+gEconpy is still in an unfinished alpha state, but I encourage anyone interested in DSGE modeling to give it a try and and report any bugs you might find.
+
+## Contributing:
+Contributions from anyone are welcome, regardless of previous experience. Please check the Issues tab for open issues, or to create a new issue. 
+
+# Representing a DSGE Model
+Like the R package gEcon, gEconpy uses .GCN files to represent a DSGE model. A GCN file is divided into blocks, each of which represents an optimization problem. Here is one block from the example Real Business Cycle (RBC) model included in the package.
+
+```
+block HOUSEHOLD
+{
+	definitions
+	{
+		u[] = C[] ^ (1 - sigma_C) / (1 - sigma_C) -
+		      L[] ^ (1 + sigma_L) / (1 + sigma_L);
+	};
+
+	controls
+	{
+		C[], L[], I[], K[];
+	};
+
+	objective
+	{
+		U[] = u[] + beta * E[][U[1]];
+	};
+
+	constraints
+	{
+		C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
+		K[] = (1 - delta) * K[-1] + I[] : q[];
+	};
+
+	calibration
+	{
+		# Fixed parameters
+		beta  = 0.99;
+		delta = 0.02;
+
+		# Parameters to estimate
+		sigma_C ~ N(loc=1.5, scale=0.1, lower=1.0) = 1.5;
+		sigma_L ~ N(loc=2.0, scale=0.1, lower=1.0) = 2.0;
+	};
+};
+```
+## Basic Basics
+A .GCN file uses an easy-to-read syntax. Whitespace is not meaningful - lines are terminated with a ";", so long equations can be split into multiple lines for readability. There are no reserved keywords* to avoid -- feel free to write beta instead of betta!
+
+Model variables are written with a name followed by square brackets, as in `U[]`. The square brackets give the time index the variable enters with. Following Dynare conventions, capital stock `K[-1]` enters with a lag, while all other variables enter in the present. Expectations are denoted by wrapping variables with `E[]`, as in `E[U[1]]` for expected utility at t+1. So I lied, there are a couple reserved keywords (hence the asterisk). Finally, you can refer directly to steady-state values as `K[ss]`.
+
+Parameters are written exactly as variables, except they have no square brackets `[]`.
+
+## Anatomy of a Block
+Blocks are divided into five components: `definitions`, `controls`, `objective`, `constraints`, `identities`, `shocks`, and, `calibration`. In this block, we see five of the seven. The blocks have the following functions:
+
+1. `definitions` contains equations that are **not** stored in the model. Instead, they are immediately substituted into all equations **within the same block**. In this example, a definition is used for the instantaneous utility function. It will be immediately substitutited into the Bellman equation written in the `objective` block.
+2. `controls` are the variables under the agent's control. The objective function represented by the block will be solved by forming a Lagrange function and taking derivatives with respect to the controls.
+3. The `objective` block contains only a single equation, and gives the function an agent will try to maximize over an infinite time horizon. In this case, the agent has a CRRA utility function.
+4. `constraints` give the resource constraints that the agent's maximization must respect. All constraints are given their own Lagrange multipiers.
+5. `identities` are equations that are not part of an optimization problem, but that are a part of the model. Unlike equations defined in the `definitions` block, `identities` are saved in the model's system of equations.
+6. `shocks` are where the user defines exogenous shocks, as in `varexo` in Dynare.
+7. The `calibration` block where free parameters, calibrated parameters, and parameter prior distributions are defined.
+
+## Parameter Values and Priors
+
+All parameters must be given values. In the household block above, all parameters are given a value directly. `beta` and `delta` are set fixed, while `sigma_C` and `sigma_L` are given priors and starting values. The `~` operator denotes a Prior, while `=` denotes a fixed value. All parameters must have a fixed value -- this is used as the "default" value when building and solving the model. Priors, on the other hand, are optional. At present, the user can choose from `Normal`, `HalfNormal`, `TruncatedNormal`, `Beta`, `Gamma`, `Inverse_Gamma`, and `Uniform` priors, with more to come as I improve the integration with PyMC. Distributons can be parameterized either using the `loc`, `scale`, `shape` synatx of `scipy.stats`, or directly using the common parameter values from the literature (such as a `mu` and `sigma` for a normal).
+
+
+As an alterantive to setting a parameter value directly, the user can declare a parameter to be calibrated. To do this, give a steady-state relationship that the parameter should be calibrated to ensure is true. The following GCN code block for the firm's optimization problem shows how this is done:
+
+```
+block FIRM
+{
+    controls
+    {
+        K[-1], L[];
+    };
+
+    objective
+    {
+        TC[] = -(r[] * K[-1] + w[] * L[]);
+    };
+
+    constraints
+    {
+        Y[] = A[] * K[-1] ^ alpha * L[] ^ (1 - alpha) : mc[];
+    };
+
+    identities
+    {
+        # Perfect competition
+        mc[] = 1;
+    };
+
+    calibration
+    {
+	L[ss] / K[ss] = 0.36 -> alpha;
+    };
+};
+```
+The `alpha` parameter is set so that in the steady state, the ratio of labor to capital is 0.36. On the back end, gEconpy will use an optimizer to find a value of `alpha` that satsifies the user's condition. Note that calibrated parameters cannot have prior distributions!
+
+## Lagrange Multipliers and First Order Conditions
+As mentioned, all constraints will automatically have a Lagrange multiplier assigned to them. The user name these multipliers himself by putting a colon ":" after an equation, followed by the Lagrange multipler name. From the code above:
+
+```
+C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
+K[] = (1 - delta) * K[-1] + I[] : q[];
+```
+
+The multiplier associated with the budget constraint has been given the name "lambda", as is usual in the literature, while the law of motion of capital has been given the name `q[]`. If the user wanted, she could use these variables in further computations within the block, for example `Q[] = q[] / lambda[]`, Tobin's Q, could be added in the `identities` block.
+
+Interally, first order conditions are solved by first making all substitutions from `definitions`, then forming the following Lagrangian function:
+`L = objective.RHS - lm1 * (control_1.LHS - control_1.RHS) - lm2 * (control_2.LHS - control_2.RHS) ... - lm_k * (control_k.LHS - control_k.RHS)`
+
+Next, the derivative of this Lagrangian is taken with respect to all control variables and all lagrange multipliers. Derivaties are are computed "though time" using `TimeAwareSymbols`, an extension of a normal Sympy symbol. For a control variable x, the total derivative over time is built up as `dL[]/dx[] + beta * dL[+1]/dx + beta * beta * dL[+2]/dx[] ...`. This unrolling terminates when `dL[+n]/dx[] = 0`.
+
+The result of this unrolling and taking derivatives process are the first order conditions (FoC). All model FoCs, along with objectives, constraints, and identities, are saved into the system of equations that represents the model.
+
+## Steady State
+
+After finding FoCs, the system will be ready to find a steady state and solve for a first-order linear approximation. To help the process, the user can write a `STEADY_STATE` block. This is a special reserved keyword block that can be placed anywhere in the GCN file. It should contain only `defintions` and `identities` as components. Here is an example of a steady state block for the RBC model:
+
+```
+block STEADY_STATE
+{
+    definitions
+    {
+      # If this is empty you can delete this, but it is also nice for writing common parameter or variable combinations.
+    };
+
+    identities
+    {
+        A[ss] = 1;
+        P[ss] = 1;
+        r[ss] = P[ss] * (1 / beta - (1 - delta));
+        w[ss] = (1 - alpha) * P[ss] ^ (1 / (1 - alpha)) * (alpha / r[ss]) ^ (alpha / (1 - alpha));
+        Y[ss] = (r[ss] / (r[ss] - delta * alpha)) ^ (sigma_C / (sigma_C + sigma_L)) *
+            (w[ss] / P[ss] * (w[ss] / P[ss] / (1 - alpha)) ^ sigma_L) ^ (1 / (sigma_C + sigma_L));
+
+        I[ss] = (delta * alpha / r[ss]) * Y[ss];
+        C[ss] = Y[ss] ^ (-sigma_L / sigma_C) * ((1 - alpha) ^ (-sigma_L) * (w[ss] / P[ss]) ^ (1 + sigma_L)) ^ (1 / sigma_C);
+        K[ss] = alpha * Y[ss] * P[ss] / r[ss];
+        L[ss] = (1 - alpha) * Y[ss] * P[ss] / w[ss];
+
+
+        U[ss] = (1 / (1 - beta)) * (C[ss] ^ (1 - sigma_C) / (1 - sigma_C) - L[ss] ^ (1 + sigma_L) / (1 + sigma_L));
+        lambda[ss] = C[ss] ^ (-sigma_C) / P[ss];
+        q[ss] = lambda[ss];
+        TC[ss] = -(r[ss] * K[ss] + w[ss] * L[ss]);
+    };
+};
+```
+
+It is not necessary to write an empty `definitions` component; this was done just to show where it goes. All information from the model block, including parameters and variables, are available to use in the `STEADY_STATE` block regardless of where they appear relative to each other (you can put the STEADY_STATE block at the top if you wish -- the file is not parsed top-to-bottom).
+
+Note that these equations are not checked in any way -- if you put something in the `STEADY_STATE` block, it is taken as the Word of God, and model solving proceeds from there. If you are having trouble finding a steady state, be sure to double check these equations.
+
+Finally, you **do not** have to provide the complete steady state system! You can include only equations, and the rest will be passed to an optimizer to be solved.
+
+## Solving the model
+
+Once a GCN file is written, using gEcon to do analysis is easy, as this code block shows:
+```python
+file_path = 'GCN Files/RBC_basic.gcn'
+model = gEconModel(file_path, verbose=True)
+```
+
+When the model is loaded, you will get a message about the number of equations and variables, as well as some other basic model descriptions. You can then solve for the stead state:
+```python
+model.steady_state()
+>>> Steady state found! Sum of squared residuals is 2.9196536232567403e-19
+```
+
+And get the linearized state space representation
+
+```python
+model.solve_model()
+>>>Solution found, sum of squared residuals:  7.075155451456433e-30
+>>>Norm of deterministic part: 0.000000000
+>>>Norm of stochastic part:    0.000000000
+```
+
+To see how to do simulations, IRFs, and compute moments, see the example notebook.
+
+# Other Features
+
+## Dynare Code Generation
+
+Since Dynare is still the gold standard in DSGE modeling, and this is a wacky open source package written by a literally who?, gEconpy has the ability to automatically convert a solved model into a Dynare mod file. This is done as follows:
+
+```python
+from gEconpy.shared.dynare_convert import make_mod_file
+print(make_mod_file(model))
+```
+
+Output:
+```
+var A, C, I, K, L, TC, U, Y, mc, q, r, var_lambda, w;
+varexo epsilon_A;
+
+parameters param_alpha, param_beta, param_delta, rho_A;
+parameters sigma_C, sigma_L;
+
+param_alpha = 0.35;
+param_beta = 0.99;
+param_delta = 0.02;
+rho_A = 0.95;
+sigma_C = 1.5;
+sigma_L = 2.0;
+
+model;
+-C - I + K(-1) * r + L * w = 0;
+I - K + K(-1) *(1 - param_delta) = 0;
+C ^(1 - sigma_C) /(1 - sigma_C) - L ^(sigma_L + 1) /(sigma_L + 1) - U + U(1) * param_beta = 0;
+-var_lambda + C ^(- sigma_C) = 0;
+-L ^ sigma_L + var_lambda * w = 0;
+q - var_lambda = 0;
+param_beta *(q(1) *(1 - param_delta) + r(1) * var_lambda(1)) - q = 0;
+1 - mc = 0;
+A * K(-1) ^ param_alpha * L ^(1 - param_alpha) - Y = 0;
+-K(-1) * r - L * w - TC = 0;
+A * K(-1) ^(param_alpha - 1) * L ^(1 - param_alpha) * param_alpha * mc - r = 0;
+A * K(-1) ^ param_alpha * mc *(1 - param_alpha) / L ^ param_alpha - w = 0;
+epsilon_A + rho_A * log(A(-1)) - log(A) = 0;
+end;
+
+initval;
+A = 1.0000;
+C = 2.3584;
+I = 0.7146;
+K = 35.7323;
+L = 0.8201;
+TC = -3.0731;
+U = -148.6156;
+Y = 3.0731;
+var_lambda = 0.2761;
+mc = 1.0000;
+q = 0.2761;
+r = 0.0301;
+w = 2.4358;
+end;
+
+steady;
+check(qz_zero_threshold=1e-20);
+
+shocks;
+var epsilon_A;
+stderr 0.01;
+end;
+
+stoch_simul(order=1, irf=100, qz_zero_threshold=1e-20);
+```
+
+### Warings about Dynare Code
+* No efforts are made to provide symbolic solutions to the steady-state! If you include steady state values in your equations and do not provide a symbolic solution in the STEADY_STATE block, the .mod file will not work "out of the box".
+* If your model includes calibrated equations, the generated Dynare code **will not** work out of the box. You need to analyically compute the steady state values and add a deterministic relationship (that beings with `#`) to the model block.
+
+
+## Estimation
+
+Coming soon!
```

### Comparing `gEconpy-1.1.0/gEconpy/classes/block.py` & `gEconpy-1.2.0/gEconpy/classes/block.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,656 +1,694 @@
-from collections import defaultdict
-from typing import Dict, List, Optional, Tuple, Union
-
-import sympy as sp
-
-from gEconpy.classes.containers import SymbolDictionary
-from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
-from gEconpy.exceptions.exceptions import (
-    BlockNotInitializedException,
-    ControlVariableNotFoundException,
-    DynamicCalibratingEquationException,
-    MultipleObjectiveFunctionsException,
-    OptimizationProblemNotDefinedException,
-)
-from gEconpy.parser import parse_equations
-from gEconpy.shared.typing import VariableType
-from gEconpy.shared.utilities import (
-    diff_through_time,
-    expand_subs_for_all_times,
-    set_equality_equals_zero,
-    unpack_keys_and_values,
-)
-
-
-def sort_positive_then_negative(args):
-    """
-    Sort `args` such that the argument containing a negative symbol is returned first.
-
-    Parameters
-    ----------
-    args : list
-        List of sympy expressions
-
-    Returns
-    -------
-    tuple
-        Tuple of sympy expressions, with the element containing a negative symbol returned first.
-        If `args` does not contain exactly 2 elements, or if neither element contains a negative symbol,
-        return None.
-    """
-    if len(args) != 2:
-        return
-
-    if sum(-1 in arg.atoms() for arg in args) != 1:
-        return
-
-    neg_arg = [arg for arg in args if -1 in arg.atoms()][0]
-    pos_arg = [arg for arg in args if arg != neg_arg][0]
-
-    return pos_arg, neg_arg
-
-
-def simple_log_exp_solver(eq: sp.Add, x: VariableType) -> Union[float, sp.Add]:
-    """
-    Parameters
-    ----------
-    eq: sp.Add
-        Equation to solve
-    x: VariableType
-        Variable to solve for
-
-    Returns
-    -------
-    solution: float or VariableType
-
-    It is common to write shocks in DSGE model in the form x = exp(a * log(x)), which sympy cannot solve using the
-    sp.solve function. This simple function logs both sides of the the equation to help the solver, which will be
-    enough in the case of the function written above. If it still can't get an answer, it returns None.
-
-    """
-    args = sort_positive_then_negative(eq.args)
-    # log "both sides" and then try to solve
-    equality = sp.log(args[0]) - sp.log(-1 * args[1])
-    try:
-        result = sp.solve(equality, x)
-        return result
-    except NotImplementedError:
-        return None
-
-
-class Block:
-    """
-    The Block class holds equations and parameters associated with each block of the DSGE model. They hold methods
-    to solve their associated optimization problem. Blocks should be created by a Model.
-
-    TODO: Refactor this into an abstract class with basic functionality, then create some child classes for specific
-    problems, e.g. IdentityBlock, OptimizationBlock, CRRABlock, etc, each with their own optimization machinery.
-
-    TODO: Split components out into their own class/protocol and let them handle their own parsing?
-    """
-
-    def __init__(
-        self,
-        name: str,
-        block_dict: Dict[str, str],
-        assumptions: Optional[Dict[str, dict]] = None,
-        solution_hints: Optional[Dict[str, str]] = None,
-        allow_incomplete_initialization: bool = False,
-    ) -> None:
-        """
-        Initialize a block object
-
-        Parameters
-        ----------
-        name: str
-            The name of the block
-        block_dict: Dict[str, str]
-            Dictionary of component:List[equations] key-value pairs created by gEcon_parser.parsed_block_to_dict.
-        solution_hints: Dict[str, str], optional
-            If not None, a dictionary of flags that help the solve_optimization method combine
-            the FoC into the "expected" solution. Currently unused.
-        allow_incomplete_initialization: bool, optional
-            If True, the block will not raise an exception if an error in the block's implementation is encountered.
-        """
-
-        self.name = name
-        self.short_name = "".join(word[0] for word in name.split("_"))
-
-        self.definitions: Optional[Dict[int, sp.Add]] = None
-        self.controls: Optional[List[TimeAwareSymbol]] = None
-        self.objective: Optional[Dict[int, sp.Add]] = None
-        self.constraints: Optional[Dict[int, sp.Add]] = None
-        self.identities: Optional[Dict[int, sp.Add]] = None
-        self.shocks: Optional[Dict[int, TimeAwareSymbol]] = None
-        self.calibration: Optional[Dict[int, sp.Add]] = None
-
-        self.variables: List[TimeAwareSymbol] = []
-        self.param_dict: SymbolDictionary[str, float] = SymbolDictionary()
-
-        self.params_to_calibrate: Optional[List[sp.Symbol]] = None
-        self.calibrating_equations: Optional[List[sp.Add]] = None
-
-        self.system_equations: List[sp.Add] = []
-        self.multipliers: Dict[int, TimeAwareSymbol] = {}
-        self.eliminated_variables: List[sp.Symbol] = []
-
-        self.n_equations = 0
-        self.initialized = False
-
-        if assumptions is None:
-            assumptions = defaultdict(dict)
-
-        self.initialize_from_dictionary(block_dict, assumptions)
-        self._get_variable_list()
-        self._get_param_dict_and_calibrating_equations()
-
-    def __str__(self):
-        return (
-            f"{self.name} Block of {self.n_equations} equations, initialized: {self.initialized}, "
-            f"solved: {self.system_equations is not None}"
-        )
-
-    def initialize_from_dictionary(self, block_dict: dict, assumptions: dict) -> None:
-        """
-        Initialize the model block with the provided definitions, objective, constraints, identities, and calibration
-        equations. The model block's controls and shocks will also be extracted from the provided block dictionary.
-
-        Parameters
-        ----------
-        block_dict: dict
-            A dictionary of component: list[equations] key-value pairs created by gEcon_parser.parsed_block_to_dict
-        assumptions: dict
-            A dictionary of user-provided Sympy assumptions about variables in the model.
-
-        Returns
-        -------
-        None
-        """
-
-        self.controls = self._parse_variable_list(block_dict, "controls", assumptions)
-        self.shocks = self._parse_variable_list(block_dict, "shocks", assumptions)
-
-        self.definitions = self._parse_equation_list(block_dict, "definitions", assumptions)
-        self.objective = self._parse_equation_list(block_dict, "objective", assumptions)
-        self.constraints = self._parse_equation_list(block_dict, "constraints", assumptions)
-        self.identities = self._parse_equation_list(block_dict, "identities", assumptions)
-        self.calibration = self._parse_equation_list(block_dict, "calibration", assumptions)
-
-        self.initialized = self._validate_initialization()
-
-    def _validate_initialization(self) -> bool:
-        """
-        Check whether the block has been successfully initialized.
-
-        At a high level, gEcon allows for two kinds of blocks: those with and those without an optimization problem.
-        To have an optimization problem, the block needs both the `controls` and `objective` components to be present.
-        Additionally, all control variables need to be represented among the equations in `objective`, `definitions`, and
-        `constraints`.
-
-        Parameters
-        ----------
-        self: Block
-            The block to be checked
-
-        Returns
-        -------
-        bool
-            Indicates whether the block has been successfully initialized.
-
-        Raises
-        ------
-        OptimizationProblemNotDefinedException
-            If either the `controls` or `objective` component is missing
-        MultipleObjectiveFunctionsException
-            If there is more than one objective function defined
-        ControlVariableNotFoundException
-            If a control variable is not found in any of the `objective`, `definitions`, or `constraints` equations.
-        """
-
-        if self.objective is not None and self.controls is None:
-            raise OptimizationProblemNotDefinedException(block_name=self.name, missing="controls")
-
-        if self.objective is None and self.controls is not None:
-            raise OptimizationProblemNotDefinedException(block_name=self.name, missing="objective")
-
-        if self.objective is not None and len(list(self.objective.values())) > 1:
-            raise MultipleObjectiveFunctionsException(
-                block_name=self.name, eqs=list(self.objective.values())
-            )
-
-        if self.controls is not None:
-            for control in self.controls:
-                control_found = False
-                eq_dicts = [
-                    x for x in [self.definitions, self.objective, self.constraints] if x is not None
-                ]
-                for eq_dict in eq_dicts:
-                    for eq in list(eq_dict.values()):
-                        if control in eq.atoms():
-                            control_found = True
-                            break
-                if not control_found:
-                    raise ControlVariableNotFoundException(self.name, control)
-
-        return True
-
-    def _validate_key(self, block_dict: dict, key: str) -> bool:
-        """
-        Check whether a block component is present in the block_dict, and a valid component name. For valid component
-        names, see gEcon_parser.BLOCK_COMPONENTS.
-
-        Parameters
-        ----------
-        block_dict : dict
-            Dictionary of component:List[equations] key-value pairs created by gEcon_parser.parsed_block_to_dict.
-        key : str
-            A component name.
-
-        Returns
-        -------
-        bool
-        """
-        return key in block_dict and hasattr(self, key) and block_dict[key] is not None
-
-    def _extract_lagrange_multipliers(
-        self, equations: List[List[str]], assumptions: dict
-    ) -> Tuple[List[List[str]], List[Union[TimeAwareSymbol, None]]]:
-        """
-        gEcon allows the user to name lagrange multipliers in the GCN file. These multiplier variables need to be saved
-        and used once the optimization problem is solved. This function removes the ": muliplier[]" from each equation
-        and returns them as a list, along with the new equations. A None is placed in the list for each equation
-        with no associated multiplier.
-
-        Parameters
-        ----------
-        equations : list
-            A list of lists of strings, each list representing a model equation. Created by the
-            gEcon_parser.parsed_block_to_dict function.
-        assumptions : dict
-            Assumptions for the model.
-
-        Returns
-        -------
-        list
-            List of lists of strings.
-        list
-            List of Union[TimeAwareSymbols, None].
-        """
-
-        result, multipliers = [], []
-        for eq in equations:
-            if ":" in eq:
-                colon_idx = eq.index(":")
-                multiplier = eq[-1]
-                multiplier = parse_equations.single_symbol_to_sympy(multiplier, assumptions)
-                eq = eq[:colon_idx].copy()
-
-                result.append(eq)
-                multipliers.append(multiplier)
-            else:
-                result.append(eq)
-                multipliers.append(None)
-
-        return result, multipliers
-
-    def _parse_variable_list(
-        self, block_dict: dict, key: str, assumptions: dict = None
-    ) -> Optional[List[sp.Symbol]]:
-        """
-        Two components -- controls and shocks -- expect a simple list of variables, which is a case the
-        gEcon_parser.build_sympy_equations cannot handle.
-
-        Parameters
-        ----------
-        block_dict : list
-            A list of lists of strings, each list representing a model equation. Created by the
-            gEcon_parser.parsed_block_to_dict function.
-        key : str
-            A component name.
-        assumptions : dict, optional
-            Assumptions for the model.
-
-        Returns
-        -------
-        list
-            A list of variables, represented as Sympy objects, or None if the block does not exist.
-        """
-        if not self._validate_key(block_dict, key):
-            return
-
-        raw_list = [item for l in block_dict[key] for item in l]
-        output = []
-        for variable in raw_list:
-            variable = parse_equations.single_symbol_to_sympy(variable, assumptions)
-            output.append(variable)
-
-        return output
-
-    def _get_variable_list(self) -> None:
-        """
-        :return: None
-        Get a list of all unique variables in the Block and store it in the class attribute "variables"
-        """
-        objective, constraints, identities = [], [], []
-        sub_dict = {}
-        if self.definitions is not None:
-            _, definitions = unpack_keys_and_values(self.definitions)
-            sub_dict = {eq.lhs: eq.rhs for eq in definitions}
-
-        if self.objective is not None:
-            _, objective = unpack_keys_and_values(self.objective)
-
-        if self.constraints is not None:
-            _, constraints = unpack_keys_and_values(self.constraints)
-
-        if self.identities is not None:
-            _, identities = unpack_keys_and_values(self.identities)
-
-        all_equations = [eq for l in [objective, constraints, identities] for eq in l]
-        for eq in all_equations:
-            eq = eq.subs(sub_dict)
-            atoms = eq.atoms()
-            variables = [x for x in atoms if isinstance(x, TimeAwareSymbol)]
-            for variable in variables:
-                if variable.to_ss() not in self.variables:
-                    self.variables.append(variable.to_ss())
-
-    def _get_and_record_equation_numbers(self, equations: List[sp.Eq]) -> List[int]:
-        """
-        Get a list of all unique variables in the Block and store it in the class attribute "variables".
-
-        Returns
-        -------
-        list
-            A list of equation number indices
-        """
-        n_equations = len(equations)
-        equation_numbers = range(self.n_equations, self.n_equations + n_equations)
-        self.n_equations += n_equations
-
-        return equation_numbers
-
-    def _parse_equation_list(
-        self, block_dict: dict, key: str, assumptions: Dict[str, str]
-    ) -> Optional[Dict[int, sp.Eq]]:
-        """
-        Convert a list of equations represented as strings into a dictionary of sympy equations, indexed by their
-        equation number.
-
-        Parameters
-        ----------
-        block_dict : list
-            A list of lists of strings, each list representing a model equation. Created by the
-            gEcon_parser.parsed_block_to_dict function.
-        key : str
-            A component name.
-        assumptions : dict
-            A dictionary with assumptions for each variable.
-
-        Returns
-        -------
-        dict
-            A dictionary of sympy equations, indexed by their equation number, or None if the block does not exist.
-        """
-        if not self._validate_key(block_dict, key):
-            return
-
-        equations = block_dict[key]
-        equations, lagrange_multipliers = self._extract_lagrange_multipliers(equations, assumptions)
-
-        equations = parse_equations.build_sympy_equations(equations, assumptions)
-        equation_numbers = self._get_and_record_equation_numbers(equations)
-
-        equations = dict(zip(equation_numbers, equations))
-        lagrange_multipliers = dict(zip(equation_numbers, lagrange_multipliers))
-        self.multipliers.update(lagrange_multipliers)
-
-        return equations
-
-    def _get_param_dict_and_calibrating_equations(self) -> None:
-        """
-        :return: None
-
-        The calibration block, as implemented in gEcon, mixes together parameters, which are fixed values with a
-        user-provided value, with calibrating equations, which are extra conditions added to the steady-state system.
-        This function divides these out so that the Model instance can ask for only one or the other.
-
-        These are divided heuristically: a parameter is assumed to be an equation with up to three atoms, all of which
-        are of class sp.Symbol or sp.Number. Calibrating equations, on the other hand, are comprised of Symbols,
-        TimeAwareSymbols, and numbers. All TimeAwareSymbols must be in the steady state, or else a Exception will be
-        raised.
-        """
-        if not self.initialized:
-            raise BlockNotInitializedException(block_name=self.name)
-
-        # It is possible that an initialized block will not have a calibration component
-        if self.calibration is None:
-            return
-
-        _, equations = unpack_keys_and_values(self.calibration)
-
-        for eq in equations:
-            atoms = eq.atoms()
-
-            # Check if this equation is a normal parameter definition
-            if len(atoms) <= 3 and all([not isinstance(x, TimeAwareSymbol) for x in atoms]):
-                param = eq.lhs
-                value = eq.rhs
-                self.param_dict[param] = value
-
-            # Check if this equation is a valid calibrating equation
-            elif all([isinstance(x, (sp.Number, sp.Symbol, TimeAwareSymbol)) for x in atoms]):
-                if not all([x.time_index == "ss" for x in atoms if isinstance(x, TimeAwareSymbol)]):
-                    raise DynamicCalibratingEquationException(eq=eq, block_name=self.name)
-
-                if self.params_to_calibrate is None:
-                    self.params_to_calibrate = [eq.lhs]
-                else:
-                    self.params_to_calibrate.append(eq.lhs)
-
-                if self.calibrating_equations is not None:
-                    self.calibrating_equations.append(set_equality_equals_zero(eq.rhs))
-                else:
-                    self.calibrating_equations = [set_equality_equals_zero(eq.rhs)]
-
-    def _build_lagrangian(self) -> sp.Add:
-        """
-        Split the calibration block into a dictionary of fixed parameters and a list of equations to be used for
-        calibration.
-
-        A parameter is assumed to be an equation with up to three atoms, all of which are of class sp.Symbol or
-        sp.Number. Calibrating equations, on the other hand, are comprised of Symbols, TimeAwareSymbols, and numbers.
-        In this second case, all TimeAwareSymbols must be in the steady state.
-
-        Returns
-        -------
-        None
-        """
-        objective = list(self.objective.values())[0]
-        constraints = self.constraints
-        multipliers = self.multipliers
-        sub_dict = dict()
-
-        if self.definitions is not None:
-            definitions = list(self.definitions.values())
-            sub_dict = {eq.lhs: eq.rhs for eq in definitions}
-
-        i = 1
-
-        lagrange = objective.rhs.subs(sub_dict)
-        for key, constraint in constraints.items():
-            if multipliers[key] is not None:
-                lm = multipliers[key]
-            else:
-                lm = TimeAwareSymbol(f"lambda__{self.short_name}_{i}", 0)
-                i += 1
-
-            lagrange = lagrange - lm * (
-                constraint.lhs.subs(sub_dict) - constraint.rhs.subs(sub_dict)
-            )
-
-        return lagrange
-
-    def _get_discount_factor(self) -> Optional[sp.Symbol]:
-        """
-        Calculate the discount factor of a Bellman equation.
-
-        A Bellman equation has the form X[] = a[] + b * E[][X[1]], where `a[]` is the value of the objective function at
-        time `t`, and `E[][X[1]]` is the expected continuation value conditioned on the current information set. The
-        parameter `b` (0 < b < 1) is the discount factor that ensures the equation converges to a fixed point. This
-        function extracts `b` from the objective function and returns it as a sympy symbol.
-
-        For single period optimizations, the discount factor is 1.
-
-        TODO: This function currently assumes the continuation value is a single variable, it will fail in the case of
-        TODO: something like X[] = a[] + b * E[][Y[1] + Z[1]], although i don't know how such a function could arise?
-
-        Returns
-        -------
-        sp.Symbol
-            The discount factor of the Bellman equation.
-
-        Raises
-        ------
-        ValueError
-            If the block has multiple t+1 variables in the Bellman equation.
-        """
-
-        _, objective = unpack_keys_and_values(self.objective)
-        objective = objective[0]
-
-        variables = [x for x in objective.atoms() if isinstance(x, TimeAwareSymbol)]
-
-        # Return 1 if there is no continuation value
-        if all([x.time_index in [0, -1] for x in variables]):
-            return 1.0
-
-        else:
-            continuation_value = [x for x in variables if x.time_index == 1]
-            if len(continuation_value) > 1:
-                raise ValueError(
-                    f"Block {self.name} has multiple t+1 variables in the Bellman equation, this is not"
-                    f"currently supported. Rewrite the equation in the form X[] = a[] + b * E[][X[1]],"
-                    f"where a[] is the instantaneous value function at time t, defined in the"
-                    f'"definitions" component of the block.'
-                )
-            discount_factor = objective.rhs.coeff(continuation_value[0])
-            return discount_factor
-
-    def simplify_system_equations(self) -> None:
-        """
-        Simplify the system of equations that define the first-order conditions (FoCs) in the model. This function
-        currently applies a heuristic to remove redundant Lagrange multipliers generated by the solver. User-named
-        lagrange multipliers are not removed, following the example of gEcon.
-
-        TODO: Add solution patterns for CES, CRRA, and CD functions. Check parameter values to allow CES to collapse
-        TODO: to CD, and CRRA to log-utility.
-        """
-
-        system = self.system_equations
-        simplified_system = system.copy()
-        variables = [x for eq in system for x in eq.atoms() if isinstance(x, TimeAwareSymbol)]
-        generated_multipliers = list({x for x in variables if "lambda__" in x.base_name})
-
-        # Strictly heuristic simplification: look for an equation of the form x = y and use it to substitute away
-        # the generated multipliers.
-
-        eliminated_variables = []
-        for x in generated_multipliers:
-            candidates = [eq for eq in simplified_system if x in eq.atoms()]
-            for eq in candidates:
-                # x = y will have 2 atoms, x = -y will have 3
-                if len(eq.atoms()) <= 3:
-                    sub_dict = sp.solve(eq, x, dict=True)[0]
-                    sub_dict = expand_subs_for_all_times(sub_dict)
-                    eliminated_variables.extend(list(sub_dict.keys()))
-                    simplified_system = [eq.subs(sub_dict) for eq in simplified_system]
-                    break
-
-        simplified_system = [eq for eq in simplified_system if eq != 0]
-
-        self.system_equations = simplified_system
-        self.eliminated_variables = eliminated_variables
-
-    def solve_optimization(self, try_simplify: bool = True) -> None:
-        r"""
-        Solve the optimization problem implied by the block structure:
-           max  Sum_{t=0}^\infty [Objective] subject to [Constraints]
-        [Controls]
-
-        By setting up the following Lagrangian:
-        ..math::
-            L = Sum_{t=0}^\infty Objective - lagrange_multiplier[1] * constraint[1] - ... - lagrange_multiplier[n] * constraint[n]
-        And taking the derivative with respect to each control variable in turn.
-
-        Parameters
-        ----------
-        try_simplify : bool
-            Whether to apply simplifications to the FoCs.
-
-        Returns
-        -------
-        None
-
-        Notes
-        -----
-        All first order conditions, along with the constraints and objective are stored in the .system_equations method.
-        No attempt is made to simplify the resulting system if try_simplify = False.
-
-        TODO: Add helper functions to simplify common setups, including CRRA/log-utility (extract Euler equation,
-            labor supply curve, etc), and common production functions (CES, CD -- extract demand curves, prices, or
-            marginal costs)
-
-        TODO: Automatically solving for un-named lagrange multipliers is currently done by the Model class, is this
-                correct?
-        """
-        if not self.initialized:
-            raise ValueError(
-                f"Block {self.name} is not initialized, cannot call Block.solve_optimization() "
-                f"before initialization"
-            )
-
-        sub_dict = dict()
-
-        if self.definitions is not None:
-            _, definitions = unpack_keys_and_values(self.definitions)
-            sub_dict = {eq.lhs: eq.rhs for eq in definitions}
-
-        if self.identities is not None:
-            _, identities = unpack_keys_and_values(self.identities)
-            for eq in identities:
-                self.system_equations.append(set_equality_equals_zero(eq.subs(sub_dict)))
-
-        if self.constraints is not None:
-            _, constraints = unpack_keys_and_values(self.constraints)
-            for eq in constraints:
-                self.system_equations.append(set_equality_equals_zero(eq.subs(sub_dict)))
-
-        if self.controls is None and self.objective is None:
-            return
-
-        # Solve Lagrangian
-        controls = self.controls
-        obj_idx, objective = unpack_keys_and_values(self.objective)
-        obj_idx, objective = obj_idx[0], objective[0]
-
-        self.system_equations.append(set_equality_equals_zero(objective.subs(sub_dict)))
-
-        _, multipliers = unpack_keys_and_values(self.multipliers)
-
-        discount_factor = self._get_discount_factor()
-        lagrange = self._build_lagrangian()
-
-        # Corner case, if the objective function has a named lagrange multiplier
-        # (pointless? but done in some gEcon example GCN files)
-        if multipliers[obj_idx] is not None:
-            self.system_equations.append(
-                multipliers[obj_idx] - diff_through_time(lagrange, objective.lhs, discount_factor)
-            )
-
-        for control in controls:
-            foc = diff_through_time(lagrange, control, discount_factor)
-            self.system_equations.append(foc.powsimp())
-
-        if try_simplify:
-            self.simplify_system_equations()
+from collections import defaultdict
+from typing import Dict, List, Optional, Tuple, Union
+
+import sympy as sp
+
+from gEconpy.classes.containers import SymbolDictionary
+from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
+from gEconpy.exceptions.exceptions import (
+    BlockNotInitializedException,
+    ControlVariableNotFoundException,
+    DynamicCalibratingEquationException,
+    MultipleObjectiveFunctionsException,
+    OptimizationProblemNotDefinedException,
+)
+from gEconpy.parser import parse_equations
+from gEconpy.shared.typing import VariableType
+from gEconpy.shared.utilities import (
+    diff_through_time,
+    expand_subs_for_all_times,
+    set_equality_equals_zero,
+    unpack_keys_and_values,
+)
+
+
+def sort_positive_then_negative(args):
+    """
+    Sort `args` such that the argument containing a negative symbol is returned first.
+
+    Parameters
+    ----------
+    args : list
+        List of sympy expressions
+
+    Returns
+    -------
+    tuple
+        Tuple of sympy expressions, with the element containing a negative symbol returned first.
+        If `args` does not contain exactly 2 elements, or if neither element contains a negative symbol,
+        return None.
+    """
+    if len(args) != 2:
+        return
+
+    if sum(-1 in arg.atoms() for arg in args) != 1:
+        return
+
+    neg_arg = [arg for arg in args if -1 in arg.atoms()][0]
+    pos_arg = [arg for arg in args if arg != neg_arg][0]
+
+    return pos_arg, neg_arg
+
+
+def simple_log_exp_solver(eq: sp.Add, x: VariableType) -> Union[float, sp.Add]:
+    """
+    Parameters
+    ----------
+    eq: sp.Add
+        Equation to solve
+    x: VariableType
+        Variable to solve for
+
+    Returns
+    -------
+    solution: float or VariableType
+
+    It is common to write shocks in DSGE model in the form x = exp(a * log(x)), which sympy cannot solve using the
+    sp.solve function. This simple function logs both sides of the the equation to help the solver, which will be
+    enough in the case of the function written above. If it still can't get an answer, it returns None.
+
+    """
+    args = sort_positive_then_negative(eq.args)
+    # log "both sides" and then try to solve
+    equality = sp.log(args[0]) - sp.log(-1 * args[1])
+    try:
+        result = sp.solve(equality, x)
+        return result
+    except NotImplementedError:
+        return None
+
+
+class Block:
+    """
+    The Block class holds equations and parameters associated with each block of the DSGE model. They hold methods
+    to solve their associated optimization problem. Blocks should be created by a Model.
+
+    TODO: Refactor this into an abstract class with basic functionality, then create some child classes for specific
+    problems, e.g. IdentityBlock, OptimizationBlock, CRRABlock, etc, each with their own optimization machinery.
+
+    TODO: Split components out into their own class/protocol and let them handle their own parsing?
+    """
+
+    def __init__(
+        self,
+        name: str,
+        block_dict: Dict[str, str],
+        assumptions: Optional[Dict[str, dict]] = None,
+        solution_hints: Optional[Dict[str, str]] = None,
+        allow_incomplete_initialization: bool = False,
+    ) -> None:
+        """
+        Initialize a block object
+
+        Parameters
+        ----------
+        name: str
+            The name of the block
+        block_dict: Dict[str, str]
+            Dictionary of component:List[equations] key-value pairs created by gEcon_parser.parsed_block_to_dict.
+        solution_hints: Dict[str, str], optional
+            If not None, a dictionary of flags that help the solve_optimization method combine
+            the FoC into the "expected" solution. Currently unused.
+        allow_incomplete_initialization: bool, optional
+            If True, the block will not raise an exception if an error in the block's implementation is encountered.
+        """
+
+        self.name = name
+        self.short_name = "".join(word[0] for word in name.split("_"))
+
+        self.definitions: Optional[Dict[int, sp.Add]] = None
+        self.controls: Optional[List[TimeAwareSymbol]] = None
+        self.objective: Optional[Dict[int, sp.Add]] = None
+        self.constraints: Optional[Dict[int, sp.Add]] = None
+        self.identities: Optional[Dict[int, sp.Add]] = None
+        self.shocks: Optional[Dict[int, TimeAwareSymbol]] = None
+        self.calibration: Optional[Dict[int, sp.Add]] = None
+
+        self.variables: List[TimeAwareSymbol] = []
+        self.param_dict: SymbolDictionary[str, float] = SymbolDictionary()
+
+        self.params_to_calibrate: Optional[List[sp.Symbol]] = None
+        self.calibrating_equations: Optional[List[sp.Add]] = None
+
+        self.deterministic_params: Optional[List[sp.Symbol]] = None
+        self.deterministic_relationships: Optional[List[sp.Add]] = None
+
+        self.system_equations: List[sp.Add] = []
+        self.multipliers: Dict[int, TimeAwareSymbol] = {}
+        self.eliminated_variables: List[sp.Symbol] = []
+
+        self.equation_flags: Dict[int, Dict[str, bool]] = {}
+
+        self.n_equations = 0
+        self.initialized = False
+
+        if assumptions is None:
+            assumptions = defaultdict(dict)
+
+        self.initialize_from_dictionary(block_dict, assumptions)
+        self._get_variable_list()
+        self._get_param_dict_and_calibrating_equations()
+
+    def __str__(self):
+        return (
+            f"{self.name} Block of {self.n_equations} equations, initialized: {self.initialized}, "
+            f"solved: {self.system_equations is not None}"
+        )
+
+    def initialize_from_dictionary(self, block_dict: dict, assumptions: dict) -> None:
+        """
+        Initialize the model block with the provided definitions, objective, constraints, identities, and calibration
+        equations. The model block's controls and shocks will also be extracted from the provided block dictionary.
+
+        Parameters
+        ----------
+        block_dict: dict
+            A dictionary of component: list[equations] key-value pairs created by gEcon_parser.parsed_block_to_dict
+        assumptions: dict
+            A dictionary of user-provided Sympy assumptions about variables in the model.
+
+        Returns
+        -------
+        None
+        """
+
+        self.controls = self._parse_variable_list(block_dict, "controls", assumptions)
+        self.shocks = self._parse_variable_list(block_dict, "shocks", assumptions)
+
+        self.definitions = self._parse_equation_list(block_dict, "definitions", assumptions)
+        self.objective = self._parse_equation_list(block_dict, "objective", assumptions)
+        self.constraints = self._parse_equation_list(block_dict, "constraints", assumptions)
+        self.identities = self._parse_equation_list(block_dict, "identities", assumptions)
+        self.calibration = self._parse_equation_list(block_dict, "calibration", assumptions)
+
+        self.initialized = self._validate_initialization()
+
+    def _validate_initialization(self) -> bool:
+        """
+        Check whether the block has been successfully initialized.
+
+        At a high level, gEcon allows for two kinds of blocks: those with and those without an optimization problem.
+        To have an optimization problem, the block needs both the `controls` and `objective` components to be present.
+        Additionally, all control variables need to be represented among the equations in `objective`, `definitions`, and
+        `constraints`.
+
+        Parameters
+        ----------
+        self: Block
+            The block to be checked
+
+        Returns
+        -------
+        bool
+            Indicates whether the block has been successfully initialized.
+
+        Raises
+        ------
+        OptimizationProblemNotDefinedException
+            If either the `controls` or `objective` component is missing
+        MultipleObjectiveFunctionsException
+            If there is more than one objective function defined
+        ControlVariableNotFoundException
+            If a control variable is not found in any of the `objective`, `definitions`, or `constraints` equations.
+        """
+
+        if self.objective is not None and self.controls is None:
+            raise OptimizationProblemNotDefinedException(block_name=self.name, missing="controls")
+
+        if self.objective is None and self.controls is not None:
+            raise OptimizationProblemNotDefinedException(block_name=self.name, missing="objective")
+
+        if self.objective is not None and len(list(self.objective.values())) > 1:
+            raise MultipleObjectiveFunctionsException(
+                block_name=self.name, eqs=list(self.objective.values())
+            )
+
+        if self.controls is not None:
+            for control in self.controls:
+                control_found = False
+                eq_dicts = [
+                    x for x in [self.definitions, self.objective, self.constraints] if x is not None
+                ]
+                for eq_dict in eq_dicts:
+                    for eq in list(eq_dict.values()):
+                        if control in eq.atoms():
+                            control_found = True
+                            break
+                if not control_found:
+                    raise ControlVariableNotFoundException(self.name, control)
+
+        return True
+
+    def _validate_key(self, block_dict: dict, key: str) -> bool:
+        """
+        Check whether a block component is present in the block_dict, and a valid component name. For valid component
+        names, see gEcon_parser.BLOCK_COMPONENTS.
+
+        Parameters
+        ----------
+        block_dict : dict
+            Dictionary of component:List[equations] key-value pairs created by gEcon_parser.parsed_block_to_dict.
+        key : str
+            A component name.
+
+        Returns
+        -------
+        bool
+        """
+        return key in block_dict and hasattr(self, key) and block_dict[key] is not None
+
+    def _extract_lagrange_multipliers(
+        self, equations: List[List[str]], assumptions: dict
+    ) -> Tuple[List[List[str]], List[Union[TimeAwareSymbol, None]]]:
+        """
+        gEcon allows the user to name lagrange multipliers in the GCN file. These multiplier variables need to be saved
+        and used once the optimization problem is solved. This function removes the ": muliplier[]" from each equation
+        and returns them as a list, along with the new equations. A None is placed in the list for each equation
+        with no associated multiplier.
+
+        Parameters
+        ----------
+        equations : list
+            A list of lists of strings, each list representing a model equation. Created by the
+            gEcon_parser.parsed_block_to_dict function.
+        assumptions : dict
+            Assumptions for the model.
+
+        Returns
+        -------
+        list
+            List of lists of strings.
+        list
+            List of Union[TimeAwareSymbols, None].
+        """
+
+        result, multipliers = [], []
+        for eq in equations:
+            if ":" in eq:
+                colon_idx = eq.index(":")
+                multiplier = eq[-1]
+                multiplier = parse_equations.single_symbol_to_sympy(multiplier, assumptions)
+                eq = eq[:colon_idx].copy()
+
+                result.append(eq)
+                multipliers.append(multiplier)
+            else:
+                result.append(eq)
+                multipliers.append(None)
+
+        return result, multipliers
+
+    def _parse_variable_list(
+        self, block_dict: dict, key: str, assumptions: dict = None
+    ) -> Optional[List[sp.Symbol]]:
+        """
+        Two components -- controls and shocks -- expect a simple list of variables, which is a case the
+        gEcon_parser.build_sympy_equations cannot handle.
+
+        Parameters
+        ----------
+        block_dict : list
+            A list of lists of strings, each list representing a model equation. Created by the
+            gEcon_parser.parsed_block_to_dict function.
+        key : str
+            A component name.
+        assumptions : dict, optional
+            Assumptions for the model.
+
+        Returns
+        -------
+        list
+            A list of variables, represented as Sympy objects, or None if the block does not exist.
+        """
+        if not self._validate_key(block_dict, key):
+            return
+
+        raw_list = [item for l in block_dict[key] for item in l]
+        output = []
+        for variable in raw_list:
+            variable = parse_equations.single_symbol_to_sympy(variable, assumptions)
+            output.append(variable)
+
+        return output
+
+    def _get_variable_list(self) -> None:
+        """
+        :return: None
+        Get a list of all unique variables in the Block and store it in the class attribute "variables"
+        """
+        objective, constraints, identities = [], [], []
+        sub_dict = {}
+        if self.definitions is not None:
+            _, definitions = unpack_keys_and_values(self.definitions)
+            sub_dict = {eq.lhs: eq.rhs for eq in definitions}
+
+        if self.objective is not None:
+            _, objective = unpack_keys_and_values(self.objective)
+
+        if self.constraints is not None:
+            _, constraints = unpack_keys_and_values(self.constraints)
+
+        if self.identities is not None:
+            _, identities = unpack_keys_and_values(self.identities)
+
+        all_equations = [eq for l in [objective, constraints, identities] for eq in l]
+        for eq in all_equations:
+            eq = eq.subs(sub_dict)
+            atoms = eq.atoms()
+            variables = [x for x in atoms if isinstance(x, TimeAwareSymbol)]
+            for variable in variables:
+                if variable.to_ss() not in self.variables:
+                    self.variables.append(variable.to_ss())
+
+    def _get_and_record_equation_numbers(self, equations: List[sp.Eq]) -> List[int]:
+        """
+        Get a list of all unique variables in the Block and store it in the class attribute "variables".
+
+        Returns
+        -------
+        list
+            A list of equation number indices
+        """
+        n_equations = len(equations)
+        equation_numbers = range(self.n_equations, self.n_equations + n_equations)
+        self.n_equations += n_equations
+
+        return equation_numbers
+
+    def _parse_equation_list(
+        self, block_dict: dict, key: str, assumptions: Dict[str, str]
+    ) -> Optional[Dict[int, sp.Eq]]:
+        """
+        Convert a list of equations represented as strings into a dictionary of sympy equations, indexed by their
+        equation number.
+
+        Parameters
+        ----------
+        block_dict : list
+            A list of lists of strings, each list representing a model equation. Created by the
+            gEcon_parser.parsed_block_to_dict function.
+        key : str
+            A component name.
+        assumptions : dict
+            A dictionary with assumptions for each variable.
+
+        Returns
+        -------
+        dict
+            A dictionary of sympy equations, indexed by their equation number, or None if the block does not exist.
+        """
+        if not self._validate_key(block_dict, key):
+            return
+
+        equations = block_dict[key]
+        equations, lagrange_multipliers = self._extract_lagrange_multipliers(equations, assumptions)
+
+        parser_output = parse_equations.build_sympy_equations(equations, assumptions)
+        equations, flags = list(zip(*parser_output))
+        equation_numbers = self._get_and_record_equation_numbers(equations)
+
+        equations = dict(zip(equation_numbers, equations))
+        flags = dict(zip(equation_numbers, flags))
+
+        lagrange_multipliers = dict(zip(equation_numbers, lagrange_multipliers))
+        self.multipliers.update(lagrange_multipliers)
+        self.equation_flags.update(flags)
+
+        return equations
+
+    def _get_param_dict_and_calibrating_equations(self) -> None:
+        """
+        :return: None
+
+        The calibration block, as implemented in gEcon, mixes together parameters, which are fixed values with a
+        user-provided value, with calibrating equations, which are extra conditions added to the steady-state system.
+        This function divides these out so that the Model instance can ask for only one or the other.
+
+        These are divided heuristically: a parameter is assumed to be an equation with up to three atoms, all of which
+        are of class sp.Symbol or sp.Number. Calibrating equations, on the other hand, are comprised of Symbols,
+        TimeAwareSymbols, and numbers. All TimeAwareSymbols must be in the steady state, or else a Exception will be
+        raised.
+        """
+        if not self.initialized:
+            raise BlockNotInitializedException(block_name=self.name)
+
+        # It is possible that an initialized block will not have a calibration component
+        if self.calibration is None:
+            return
+
+        eq_idxs, equations = unpack_keys_and_values(self.calibration)
+
+        for idx, eq in zip(eq_idxs, equations):
+            atoms = eq.atoms()
+
+            # Check if this equation is a normal parameter definition. If so, it will be exactly in the form x = y
+            if eq.lhs.is_symbol and eq.rhs.is_number:
+                param = eq.lhs
+                value = eq.rhs
+                self.param_dict[param] = value
+
+            elif self.equation_flags[idx]["is_calibrating"]:
+
+                # Check if this equation is a valid calibrating equation
+                if all([isinstance(x, (sp.Number, sp.Symbol, TimeAwareSymbol)) for x in atoms]):
+                    if not all(
+                        [x.time_index == "ss" for x in atoms if isinstance(x, TimeAwareSymbol)]
+                    ):
+                        raise DynamicCalibratingEquationException(eq=eq, block_name=self.name)
+
+                    if self.params_to_calibrate is None:
+                        self.params_to_calibrate = [eq.lhs]
+                    else:
+                        self.params_to_calibrate.append(eq.lhs)
+
+                    if self.calibrating_equations is None:
+                        self.calibrating_equations = [set_equality_equals_zero(eq.rhs)]
+                    else:
+                        self.calibrating_equations.append(set_equality_equals_zero(eq.rhs))
+                else:
+                    raise ValueError(
+                        f"Invalid tokens found in calibrating equation: {eq} of block {self.name}"
+                    )
+
+            else:
+                # What is left should only be "deterministic relationships", parameters that are defined as
+                # functions of other parameters that the user wants to keep track of.
+
+                # Check that these are functions of numbers and parameters only
+
+                if any([isinstance(x, TimeAwareSymbol) for x in atoms]):
+                    raise ValueError(
+                        "Parameters defined as functions in the calibration sub-block cannot be functions "
+                        f"of variables. Found:\n\n {eq} in {self.name}"
+                    )
+                if self.deterministic_params is None:
+                    self.deterministic_params = [eq.lhs]
+                else:
+                    self.deterministic_params.append(eq.lhs)
+
+                if self.deterministic_relationships is None:
+                    self.deterministic_relationships = [set_equality_equals_zero(eq.rhs)]
+                else:
+                    self.deterministic_relationships.append(set_equality_equals_zero(eq.rhs))
+
+    def _build_lagrangian(self) -> sp.Add:
+        """
+        Split the calibration block into a dictionary of fixed parameters and a list of equations to be used for
+        calibration.
+
+        A parameter is assumed to be an equation with up to three atoms, all of which are of class sp.Symbol or
+        sp.Number. Calibrating equations, on the other hand, are comprised of Symbols, TimeAwareSymbols, and numbers.
+        In this second case, all TimeAwareSymbols must be in the steady state.
+
+        Returns
+        -------
+        None
+        """
+        objective = list(self.objective.values())[0]
+        constraints = self.constraints
+        multipliers = self.multipliers
+        sub_dict = dict()
+
+        if self.definitions is not None:
+            definitions = list(self.definitions.values())
+            sub_dict = {eq.lhs: eq.rhs for eq in definitions}
+
+        i = 1
+
+        lagrange = objective.rhs.subs(sub_dict)
+        for key, constraint in constraints.items():
+            if multipliers[key] is not None:
+                lm = multipliers[key]
+            else:
+                lm = TimeAwareSymbol(f"lambda__{self.short_name}_{i}", 0)
+                i += 1
+
+            lagrange = lagrange - lm * (
+                constraint.lhs.subs(sub_dict) - constraint.rhs.subs(sub_dict)
+            )
+
+        return lagrange
+
+    def _get_discount_factor(self) -> Optional[sp.Symbol]:
+        """
+        Calculate the discount factor of a Bellman equation.
+
+        A Bellman equation has the form X[] = a[] + b * E[][X[1]], where `a[]` is the value of the objective function at
+        time `t`, and `E[][X[1]]` is the expected continuation value conditioned on the current information set. The
+        parameter `b` (0 < b < 1) is the discount factor that ensures the equation converges to a fixed point. This
+        function extracts `b` from the objective function and returns it as a sympy symbol.
+
+        For single period optimizations, the discount factor is 1.
+
+        TODO: This function currently assumes the continuation value is a single variable, it will fail in the case of
+        TODO: something like X[] = a[] + b * E[][Y[1] + Z[1]], although i don't know how such a function could arise?
+
+        Returns
+        -------
+        sp.Symbol
+            The discount factor of the Bellman equation.
+
+        Raises
+        ------
+        ValueError
+            If the block has multiple t+1 variables in the Bellman equation.
+        """
+
+        _, objective = unpack_keys_and_values(self.objective)
+        objective = objective[0]
+
+        variables = [x for x in objective.atoms() if isinstance(x, TimeAwareSymbol)]
+
+        # Return 1 if there is no continuation value
+        if all([x.time_index in [0, -1] for x in variables]):
+            return 1.0
+
+        else:
+            continuation_value = [x for x in variables if x.time_index == 1]
+            if len(continuation_value) > 1:
+                raise ValueError(
+                    f"Block {self.name} has multiple t+1 variables in the Bellman equation, this is not"
+                    f"currently supported. Rewrite the equation in the form X[] = a[] + b * E[][X[1]],"
+                    f"where a[] is the instantaneous value function at time t, defined in the"
+                    f'"definitions" component of the block.'
+                )
+            discount_factor = objective.rhs.coeff(continuation_value[0])
+            return discount_factor
+
+    def simplify_system_equations(self) -> None:
+        """
+        Simplify the system of equations that define the first-order conditions (FoCs) in the model. This function
+        currently applies a heuristic to remove redundant Lagrange multipliers generated by the solver. User-named
+        lagrange multipliers are not removed, following the example of gEcon.
+
+        TODO: Add solution patterns for CES, CRRA, and CD functions. Check parameter values to allow CES to collapse
+        TODO: to CD, and CRRA to log-utility.
+        """
+
+        system = self.system_equations
+        simplified_system = system.copy()
+        variables = [x for eq in system for x in eq.atoms() if isinstance(x, TimeAwareSymbol)]
+        generated_multipliers = list({x for x in variables if "lambda__" in x.base_name})
+
+        # Strictly heuristic simplification: look for an equation of the form x = y and use it to substitute away
+        # the generated multipliers.
+
+        eliminated_variables = []
+        for x in generated_multipliers:
+            candidates = [eq for eq in simplified_system if x in eq.atoms()]
+            for eq in candidates:
+                # x = y will have 2 atoms, x = -y will have 3
+                if len(eq.atoms()) <= 3:
+                    sub_dict = sp.solve(eq, x, dict=True)[0]
+                    sub_dict = expand_subs_for_all_times(sub_dict)
+                    eliminated_variables.extend(list(sub_dict.keys()))
+                    simplified_system = [eq.subs(sub_dict) for eq in simplified_system]
+                    break
+
+        simplified_system = [eq for eq in simplified_system if eq != 0]
+
+        self.system_equations = simplified_system
+        self.eliminated_variables = eliminated_variables
+
+    def solve_optimization(self, try_simplify: bool = True) -> None:
+        r"""
+        Solve the optimization problem implied by the block structure:
+           max  Sum_{t=0}^\infty [Objective] subject to [Constraints]
+        [Controls]
+
+        By setting up the following Lagrangian:
+        ..math::
+            L = Sum_{t=0}^\infty Objective - lagrange_multiplier[1] * constraint[1] - ... - lagrange_multiplier[n] * constraint[n]
+        And taking the derivative with respect to each control variable in turn.
+
+        Parameters
+        ----------
+        try_simplify : bool
+            Whether to apply simplifications to the FoCs.
+
+        Returns
+        -------
+        None
+
+        Notes
+        -----
+        All first order conditions, along with the constraints and objective are stored in the .system_equations method.
+        No attempt is made to simplify the resulting system if try_simplify = False.
+
+        TODO: Add helper functions to simplify common setups, including CRRA/log-utility (extract Euler equation,
+            labor supply curve, etc), and common production functions (CES, CD -- extract demand curves, prices, or
+            marginal costs)
+
+        TODO: Automatically solving for un-named lagrange multipliers is currently done by the Model class, is this
+                correct?
+        """
+        if not self.initialized:
+            raise ValueError(
+                f"Block {self.name} is not initialized, cannot call Block.solve_optimization() "
+                f"before initialization"
+            )
+
+        sub_dict = dict()
+
+        if self.definitions is not None:
+            _, definitions = unpack_keys_and_values(self.definitions)
+            sub_dict = {eq.lhs: eq.rhs for eq in definitions}
+
+        if self.identities is not None:
+            _, identities = unpack_keys_and_values(self.identities)
+            for eq in identities:
+                self.system_equations.append(set_equality_equals_zero(eq.subs(sub_dict)))
+
+        if self.constraints is not None:
+            _, constraints = unpack_keys_and_values(self.constraints)
+            for eq in constraints:
+                self.system_equations.append(set_equality_equals_zero(eq.subs(sub_dict)))
+
+        if self.controls is None and self.objective is None:
+            return
+
+        # Solve Lagrangian
+        controls = self.controls
+        obj_idx, objective = unpack_keys_and_values(self.objective)
+        obj_idx, objective = obj_idx[0], objective[0]
+
+        self.system_equations.append(set_equality_equals_zero(objective.subs(sub_dict)))
+
+        _, multipliers = unpack_keys_and_values(self.multipliers)
+
+        discount_factor = self._get_discount_factor()
+        lagrange = self._build_lagrangian()
+
+        # Corner case, if the objective function has a named lagrange multiplier
+        # (pointless? but done in some gEcon example GCN files)
+        if multipliers[obj_idx] is not None:
+            self.system_equations.append(
+                multipliers[obj_idx] - diff_through_time(lagrange, objective.lhs, discount_factor)
+            )
+
+        for control in controls:
+            foc = diff_through_time(lagrange, control, discount_factor)
+            self.system_equations.append(foc.powsimp())
+
+        if try_simplify:
+            self.simplify_system_equations()
```

### Comparing `gEconpy-1.1.0/gEconpy/classes/model.py` & `gEconpy-1.2.0/gEconpy/classes/model.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,1536 +1,1600 @@
-from collections import defaultdict
-from typing import Any, Callable, Dict, List, Optional, Union
-from warnings import warn
-
-import arviz as az
-import emcee
-import numba as nb
-import numpy as np
-import pandas as pd
-import sympy as sp
-import xarray as xr
-from numpy.typing import ArrayLike
-from scipy import linalg, stats
-
-from gEconpy.classes.block import Block
-from gEconpy.classes.containers import SymbolDictionary
-from gEconpy.classes.progress_bar import ProgressBar
-from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
-from gEconpy.estimation.estimate import build_Z_matrix, evaluate_logp
-from gEconpy.estimation.estimation_utilities import (
-    extract_prior_dict,
-    extract_sparse_data_from_model,
-)
-from gEconpy.exceptions.exceptions import (
-    GensysFailedException,
-    MultipleSteadyStateBlocksException,
-    PerturbationSolutionNotFoundException,
-    SteadyStateNotSolvedError,
-    VariableNotFoundException,
-)
-from gEconpy.parser import file_loaders, gEcon_parser
-from gEconpy.parser.constants import STEADY_STATE_NAMES
-from gEconpy.parser.parse_distributions import create_prior_distribution_dictionary
-from gEconpy.parser.parse_equations import single_symbol_to_sympy
-from gEconpy.shared.utilities import (
-    expand_subs_for_all_times,
-    is_variable,
-    make_all_var_time_combos,
-    merge_dictionaries,
-    substitute_all_equations,
-    unpack_keys_and_values,
-)
-from gEconpy.solvers.gensys import interpret_gensys_output
-from gEconpy.solvers.perturbation import PerturbationSolver
-from gEconpy.solvers.steady_state import SteadyStateSolver
-
-VariableType = Union[sp.Symbol, TimeAwareSymbol]
-
-
-class gEconModel:
-    def __init__(
-        self,
-        model_filepath: str,
-        verbose: bool = True,
-        simplify_blocks=True,
-        simplify_constants=True,
-        simplify_tryreduce=True,
-    ) -> None:
-        """
-        Initialize a DSGE model object from a GCN file.
-
-        Parameters
-        ----------
-        model_filepath : str
-            Filepath to the GCN file
-        verbose : bool, optional
-            Flag for verbose output, by default True
-        simplify_blocks : bool, optional
-            Flag to simplify blocks, by default True
-        simplify_constants : bool, optional
-            Flag to simplify constants, by default True
-        simplify_tryreduce : bool, optional
-            Flag to simplify using `try_reduce_vars`, by default True
-        """
-        self.model_filepath: str = model_filepath
-
-        # Model metadata
-        self.options: Optional[Dict[str, bool]] = None
-        self.try_reduce_vars: Optional[List[TimeAwareSymbol]] = None
-
-        self.blocks: Dict[str, Block] = {}
-        self.n_blocks: int = 0
-
-        # Model components
-        self.variables: List[TimeAwareSymbol] = []
-        self.assumptions: Dict[str, dict] = defaultdict(SymbolDictionary)
-        self.shocks: List[TimeAwareSymbol] = []
-        self.system_equations: List[sp.Add] = []
-        self.calibrating_equations: List[sp.Add] = []
-        self.params_to_calibrate: List[sp.Symbol] = []
-
-        self.free_param_dict: SymbolDictionary[sp.Symbol, float] = SymbolDictionary()
-        self.calib_param_dict: SymbolDictionary[sp.Symbol, float] = SymbolDictionary()
-        self.steady_state_relationships: SymbolDictionary[VariableType, sp.Add] = SymbolDictionary()
-
-        self.param_priors: SymbolDictionary[str, Any] = SymbolDictionary()
-        self.shock_priors: SymbolDictionary[str, Any] = SymbolDictionary()
-        self.observation_noise_priors: SymbolDictionary[str, Any] = SymbolDictionary()
-
-        self.n_variables: int = 0
-        self.n_shocks: int = 0
-        self.n_equations: int = 0
-        self.n_calibrating_equations: int = 0
-
-        # Functional representations of the model
-        self.f_ss: Union[Callable, None] = None
-        self.f_ss_resid: Union[Callable, None] = None
-
-        # Steady state information
-        self.steady_state_solved: bool = False
-        self.steady_state_system: List[sp.Add] = []
-        self.steady_state_dict: SymbolDictionary[sp.Symbol, float] = SymbolDictionary()
-        self.residuals: List[float] = []
-
-        # Functional representation of the perturbation system
-        self.build_perturbation_matrices: Union[Callable, None] = None
-
-        # Perturbation solution information
-        self.perturbation_solved: bool = False
-        self.T: pd.DataFrame = None
-        self.R: pd.DataFrame = None
-        self.P: pd.DataFrame = None
-        self.Q: pd.DataFrame = None
-        self.R: pd.DataFrame = None
-        self.S: pd.DataFrame = None
-
-        self.build(
-            verbose=verbose,
-            simplify_blocks=simplify_blocks,
-            simplify_constants=simplify_constants,
-            simplify_tryreduce=simplify_tryreduce,
-        )
-
-        # Assign Solvers
-        self.steady_state_solver = SteadyStateSolver(self)
-        self.perturbation_solver = PerturbationSolver(self)
-
-        # TODO: Here I copy the assumptions from the model (which should be the only source of truth for assumptions)
-        #  into every SymbolDictionary. This setup is really bad; if these dictionaries go out of sync there could be
-        #  disagreements about what the assumptions for a variable should be.
-
-        for d in [
-            self.free_param_dict,
-            self.calib_param_dict,
-            self.steady_state_relationships,
-            self.param_priors,
-            self.shock_priors,
-            self.observation_noise_priors,
-        ]:
-            d._assumptions.update(self.assumptions)
-
-    def build(
-        self,
-        verbose: bool,
-        simplify_blocks: bool,
-        simplify_constants: bool,
-        simplify_tryreduce: bool,
-    ) -> None:
-        """
-        Main parsing function for the model. Build loads the GCN file, decomposes it into blocks, solves optimization
-        problems contained in each block, then extracts parameters, equations, calibrating equations, calibrated
-        parameters, and exogenous shocks into their respective class attributes.
-
-        Priors declared in the GCN file are converted into scipy distribution objects and stored in two dictionaries:
-        self.param_priors and self.shock_priors.
-
-        Gathering block information is done for convenience. For diagnostic purposes the block structure is retained
-        as well.
-
-        Parameters
-        ----------
-        verbose : bool, optional
-            When True, print a build report describing the model structure and warning the user if the number of
-            variables does not match the number of equations.
-        simplify_blocks : bool, optional
-            If True, simplify equations in the model blocks.
-        simplify_constants : bool, optional
-            If True, simplify constants in the model equations.
-        simplify_tryreduce : bool, optional
-            If True, try to reduce the number of variables in the model by eliminating unnecessary equations.
-
-        Returns
-        -------
-        None
-        """
-
-        raw_model = file_loaders.load_gcn(self.model_filepath)
-        parsed_model, prior_dict = gEcon_parser.preprocess_gcn(raw_model)
-
-        self._build_model_blocks(parsed_model, simplify_blocks)
-        self._get_all_block_equations()
-        self._get_all_block_parameters()
-        self._get_all_block_params_to_calibrate()
-        self._get_variables_and_shocks()
-        self._build_prior_dict(prior_dict)
-
-        reduced_vars = None
-        singletons = None
-
-        if simplify_tryreduce:
-            reduced_vars = self._try_reduce()
-        if simplify_constants:
-            singletons = self._simplify_singletons()
-
-        if verbose:
-            self.build_report(reduced_vars, singletons)
-
-    def build_report(self, reduced_vars, singletons):
-        """
-        Write a diagnostic message after building the model. Note that successfully building the model does not
-        guarantee that the model is correctly specified. For example, it is possible to build a model with more
-        equations than parameters. This message will warn the user in this case.
-
-        Returns
-        -------
-        None
-        """
-        if singletons and len(singletons) == 0:
-            singletons = None
-
-        eq_str = "equation" if self.n_equations == 1 else "equations"
-        var_str = "variable" if self.n_variables == 1 else "variables"
-        shock_str = "shock" if self.n_shocks == 1 else "shocks"
-        cal_eq_str = "equation" if self.n_calibrating_equations == 1 else "equations"
-        par_str = "parameter" if self.n_params_to_calibrate == 1 else "parameters"
-
-        n_params = len(self.free_param_dict) + len(self.calib_param_dict)
-
-        param_priors = self.param_priors.keys()
-        shock_priors = self.shock_priors.keys()
-
-        report = "Model Building Complete.\nFound:\n"
-        report += f"\t{self.n_equations} {eq_str}\n"
-        report += f"\t{self.n_variables} {var_str}\n"
-
-        if reduced_vars:
-            report += f"\tThe following variables were eliminated at user request:\n"
-            report += f"\t\t" + ",".join(reduced_vars) + "\n"
-
-        if singletons:
-            report += f'\tThe following "variables" were defined as constants and have been substituted away:\n'
-            report += f"\t\t" + ",".join(singletons) + "\n"
-
-        report += f"\t{self.n_shocks} stochastic {shock_str}\n"
-        report += (
-            f'\t\t {len(shock_priors)} / {self.n_shocks} {"have" if len(shock_priors) == 1 else "has"}'
-            f" a defined prior. \n"
-        )
-
-        report += f"\t{n_params} {par_str}\n"
-        report += (
-            f'\t\t {len(param_priors)} / {n_params} {"have" if len(param_priors) == 1 else "has"} '
-            f"a defined prior. \n"
-        )
-        report += f"\t{self.n_calibrating_equations} calibrating {cal_eq_str}\n"
-        report += f"\t{self.n_params_to_calibrate} {par_str} to calibrate\n "
-
-        if self.n_equations == self.n_variables:
-            report += "Model appears well defined and ready to proceed to solving.\n"
-            print(report)
-        else:
-            print(report)
-            message = (
-                f"The model does not appear correctly specified, there are {self.n_equations} {eq_str} but "
-                f"{self.n_variables} {var_str}. It will not be possible to solve this model. Please check the "
-                f"specification using available diagnostic tools, and check the GCN file for typos."
-            )
-            warn(message)
-
-    def steady_state(
-        self,
-        verbose: Optional[bool] = True,
-        apply_user_simplifications=True,
-        method: Optional[str] = "root",
-        optimizer_kwargs: Optional[Dict[str, Any]] = None,
-        use_jac: Optional[bool] = True,
-        use_hess: Optional[bool] = True,
-        tol: Optional[float] = 1e-6,
-    ) -> None:
-        """
-        Solves for a function f(params) that computes steady state values and calibrated parameter values given
-        parameter values, stores results, and verifies that the residuals of the solution are zero.
-
-        Parameters
-        ----------
-        verbose: bool
-            Flag controlling whether to print results of the steady state solver. Default is True.
-        apply_user_simplifications: bool
-            Whether to simplify system equations using the user-defined steady state relationships defined in the GCN
-            before passing the system to the numerical solver. Default is True.
-        method: str
-            One of "root" or "minimize". Indicates which family of solution algorithms should be used to find a
-            numerical steady state: direct root finding or minimization of squared error. Not that "root" is not
-            suitable if the number of inputs is not equal to the number of outputs, for example if user-provided
-            steady state relationships do not result in elimination of model equations. Default is "root".
-        optimizer_kwargs: dict
-            Dictionary of arguments to be passed to scipy.optimize.root or scipy.optimize.minimize, see those
-            functions for more details.
-        use_jac: bool
-            Whether to symbolically compute the Jacobian matrix of the steady state system (when method is "root") or
-            the Jacobian vector of the loss function (when method is "minimize"). Strongly recommended. Default is True
-        use_hess: bool
-            Whether to symbolically compute the Hessian matrix of the loss function. Ignored if method is "root".
-            If "False", the default BFGS solver will compute a numerical approximation, so not necessarily required.
-            Still recommended. Default is True.
-        tol: float
-            Numerical tolerance for declaring a steady-state solution valid. Default is 1e-6. Note that this only used
-            by the gEconpy model to decide if a steady state has been found, and is **NOT** passed to the scipy
-            solution algorithms. To adjust solution tolerance for these algorithms, use optimizer_kwargs.
-
-        Returns
-        -------
-        None
-        """
-        if not self.steady_state_solved:
-            self.f_ss = self.steady_state_solver.solve_steady_state(
-                apply_user_simplifications=apply_user_simplifications,
-                method=method,
-                optimizer_kwargs=optimizer_kwargs,
-                use_jac=use_jac,
-                use_hess=use_hess,
-            )
-
-            # self.f_ss_resid = self.steady_state_solver.f_ss_resid
-
-        self._process_steady_state_results(verbose, tol=tol)
-
-    def _process_steady_state_results(self, verbose=True, tol=1e-6) -> None:
-        """Process results from steady state solver.
-
-        This function sets the steady state dictionary, calibrated parameter dictionary, and residuals attribute
-        based on the results of the steady state solver. It also sets the `steady_state_solved` attribute to
-        indicate whether the steady state was successfully found. If `verbose` is True, it prints a message
-        indicating whether the steady state was found and the sum of squared residuals.
-
-        Parameters
-        ----------
-        verbose : bool, optional
-            If True, print a message indicating whether the steady state was found and the sum of squared residuals.
-            Default is True.
-        tol: float, optional
-            Numerical tolerance for declaring a steady-state solution has been found. Default is 1e-6.
-
-        Returns
-        -------
-        None
-        """
-        results = self.f_ss(self.free_param_dict)
-        self.steady_state_dict = results["ss_dict"]
-        self.calib_param_dict = results["calib_dict"]
-        self.residuals = results["resids"]
-
-        # self.steady_state_dict, self.calib_param_dict = self.f_ss(self.free_param_dict)
-        self.steady_state_system = self.steady_state_solver.steady_state_system
-        #
-        # self.residuals = np.array(
-        #     self.f_ss_resid(
-        #         **self.steady_state_dict,
-        #         **self.free_param_dict,
-        #         **self.calib_param_dict,
-        #     )
-        # )
-
-        self.steady_state_solved = np.allclose(self.residuals, 0, atol=tol) & results["success"]
-
-        if verbose:
-            if self.steady_state_solved:
-                print(
-                    f"Steady state found! Sum of squared residuals is {(self.residuals ** 2).sum()}"
-                )
-            else:
-                print(
-                    f"Steady state NOT found. Sum of squared residuals is {(self.residuals ** 2).sum()}"
-                )
-
-    def print_steady_state(self):
-        """
-        Prints the steady state values for the model's variables and calibrated parameters.
-
-        Prints an error message if a valid steady state has not yet been found.
-        """
-        if self.steady_state_dict is None:
-            print("Run the steady_state method to find a steady state before calling this method.")
-            return
-
-        if not self.steady_state_solved:
-            print("Values come from the latest solver iteration but are NOT a valid steady state.")
-
-        max_var_name = (
-            max(
-                len(x)
-                for x in list(self.steady_state_dict.keys()) + list(self.calib_param_dict.keys())
-            )
-            + 5
-        )
-        for key, value in self.steady_state_dict.items():
-            print(f"{key:{max_var_name}}{value:>10.3f}")
-
-        if len(self.params_to_calibrate) > 0:
-            print("\n")
-            print("In addition, the following parameter values were calibrated:")
-            for key, value in self.calib_param_dict.items():
-                print(f"{key:{max_var_name}}{value:>10.3f}")
-
-    def solve_model(
-        self,
-        solver="cycle_reduction",
-        not_loglin_variable: Optional[List[str]] = None,
-        order: int = 1,
-        model_is_linear: bool = False,
-        tol: float = 1e-8,
-        max_iter: int = 1000,
-        verbose: bool = True,
-        on_failure="error",
-    ) -> None:
-        """
-        Solve for the linear approximation to the policy function via perturbation. Adapted from R code in the gEcon
-        package by Grzegorz Klima, Karol Podemski, and Kaja Retkiewicz-Wijtiwiak., http://gecon.r-forge.r-project.org/.
-
-        Parameters
-        ----------
-        solver: str, default: 'cycle_reduction'
-            Name of the algorithm to solve the linear solution. Currently "cycle_reduction" and "gensys" are supported.
-            Following Dynare, cycle_reduction is the default, but note that gEcon uses gensys.
-        not_loglin_variable: List, default: None
-            Variables to not log linearize when solving the model. Variables with steady state values close to zero
-            will be automatically selected to not log linearize.
-        order: int, default: 1
-            Order of taylor expansion to use to solve the model. Currently only 1st order approximation is supported.
-        model_is_linear: bool, default: False
-            Flag indicating whether a model has already been linearized by the user.
-        tol: float, default 1e-8
-            Desired level of floating point accuracy in the solution
-        max_iter: int, default: 1000
-            Maximum number of cycle_reduction iterations. Not used if solver is 'gensys'.
-        verbose: bool, default: True
-            Flag indicating whether to print solver results to the terminal
-        on_failure: str, one of ['error', 'ignore'], default: 'error'
-            Instructions on what to do if the algorithm to find a linearized policy matrix. "Error" will raise an error,
-            while "ignore" will return None. "ignore" is useful when repeatedly solving the model, e.g. when sampling.
-
-        Returns
-        -------
-        None
-        """
-
-        param_dict = self.free_param_dict | self.calib_param_dict
-        steady_state_dict = self.steady_state_dict
-
-        if self.build_perturbation_matrices is None:
-            self._perturbation_setup(not_loglin_variable, order, model_is_linear, verbose, bool)
-
-        A, B, C, D = self.build_perturbation_matrices(
-            **param_dict.to_string(), **steady_state_dict.to_string()
-        )
-        _, variables, _ = self.perturbation_solver.make_all_variable_time_combinations()
-
-        if solver == "gensys":
-            gensys_results = self.perturbation_solver.solve_policy_function_with_gensys(
-                A, B, C, D, tol, verbose
-            )
-            G_1, constant, impact, f_mat, f_wt, y_wt, gev, eu, loose = gensys_results
-
-            if G_1 is None or (eu[0] != eu[1] != 1):
-                if on_failure == "error":
-                    raise GensysFailedException(eu)
-                elif on_failure == "ignore":
-                    if verbose:
-                        print(interpret_gensys_output(eu))
-                    self.P = None
-                    self.Q = None
-                    self.R = None
-                    self.S = None
-
-                    self.perturbation_solved = False
-
-                    return
-
-            T = G_1[: self.n_variables, :][:, : self.n_variables]
-            R = impact[: self.n_variables, :]
-
-        elif solver == "cycle_reduction":
-            (
-                T,
-                R,
-                result,
-                log_norm,
-            ) = self.perturbation_solver.solve_policy_function_with_cycle_reduction(
-                A, B, C, D, max_iter, tol, verbose
-            )
-            if T is None:
-                if on_failure == "errror":
-                    raise GensysFailedException(result)
-        else:
-            raise NotImplementedError(
-                'Only "cycle_reduction" and "gensys" are valid values for solver'
-            )
-
-        gEcon_matrices = self.perturbation_solver.statespace_to_gEcon_representation(
-            A, T, R, variables, tol
-        )
-        P, Q, _, _, A_prime, R_prime, S_prime = gEcon_matrices
-
-        resid_norms = self.perturbation_solver.residual_norms(
-            B, C, D, Q, P, A_prime, R_prime, S_prime
-        )
-        norm_deterministic, norm_stochastic = resid_norms
-
-        if verbose:
-            print(f"Norm of deterministic part: {norm_deterministic:0.9f}")
-            print(f"Norm of stochastic part:    {norm_deterministic:0.9f}")
-
-        self.T = pd.DataFrame(
-            T,
-            index=[x.base_name for x in sorted(self.variables, key=lambda x: x.base_name)],
-            columns=[x.base_name for x in sorted(self.variables, key=lambda x: x.base_name)],
-        )
-        self.R = pd.DataFrame(
-            R,
-            index=[x.base_name for x in sorted(self.variables, key=lambda x: x.base_name)],
-            columns=[x.base_name for x in sorted(self.shocks, key=lambda x: x.base_name)],
-        )
-
-        self.perturbation_solved = True
-
-    def _perturbation_setup(
-        self,
-        not_loglin_variables=None,
-        order=1,
-        model_is_linear=False,
-        verbose=True,
-        return_F_matrices=False,
-        tol=1e-8,
-    ):
-        """
-        This function is used to set up the perturbation matrices needed to simulate the model. It linearizes the model
-        around the steady state and constructs matrices A, B, C, and D needed to solve the system.
-
-        Parameters
-        ----------
-        not_loglin_variables: list of str
-            List of variables that should not be log-linearized. This is useful when a variable has a zero or negative
-             steady state value and cannot be log-linearized.
-        order: int
-            The order of the approximation. Currently only order 1 is implemented.
-        model_is_linear: bool
-            If True, assumes that the model is already linearized in the GCN file and directly
-             returns the matrices A, B, C, D.
-        verbose: bool
-            If True, prints warning messages.
-        return_F_matrices: bool
-            If True, returns the matrices A, B, C, D.
-        tol: float
-            The tolerance used to determine if a steady state value is close to zero.
-
-        Returns
-        -------
-        None or list of sympy matrices
-            If return_F_matrices is True, returns the F matrices. Otherwise, does not return anything.
-
-        """
-
-        free_param_dict = self.free_param_dict.copy()
-
-        parameters = list(free_param_dict.keys())
-        variables = list(self.steady_state_dict.keys())
-        params_to_calibrate = list(self.calib_param_dict.keys())
-
-        params_and_variables = parameters + params_to_calibrate + variables
-
-        shocks = self.shocks
-        shock_ss_dict = dict(zip([x.to_ss() for x in shocks], np.zeros(self.n_shocks)))
-        variables_and_shocks = self.variables + shocks
-        valid_names = [x.base_name for x in variables_and_shocks]
-
-        steady_state_dict = self.steady_state_dict.copy()
-
-        # We need shocks to be zero in A, B, C, D but 1 in T; can abuse the T_dummies to accomplish that.
-        if not_loglin_variables is None:
-            not_loglin_variables = []
-
-        not_loglin_variables += [x.base_name for x in shocks]
-
-        # Validate that all user-supplied variables are in the model
-        for variable in not_loglin_variables:
-            if variable not in valid_names:
-                raise VariableNotFoundException(variable)
-
-        # Variables that are zero at the SS can't be log-linearized, check for these here.
-        close_to_zero_warnings = []
-        for variable in variables_and_shocks:
-            if variable.base_name in not_loglin_variables:
-                continue
-
-            if abs(steady_state_dict[variable.to_ss().name]) < tol:
-                not_loglin_variables.append(variable.base_name)
-                close_to_zero_warnings.append(variable)
-
-        if len(close_to_zero_warnings) > 0 and verbose:
-            warn(
-                "The following variables have steady state values close to zero and will not be log linearized: "
-                + ", ".join(x.base_name for x in close_to_zero_warnings)
-            )
-
-        if order != 1:
-            raise NotImplementedError
-
-        if not self.steady_state_solved:
-            raise SteadyStateNotSolvedError()
-
-        if model_is_linear:
-            warn(
-                "Model will be solved as though ALL system equations have already been linearized in the GCN file. No"
-                "checks are performed to ensure this is indeed the case. Proceed with caution."
-            )
-            Fs = self.perturbation_solver.convert_linear_system_to_matrices()
-
-        else:
-            Fs = self.perturbation_solver.log_linearize_model(
-                not_loglin_variables=not_loglin_variables
-            )
-
-        Fs_subbed = [F.subs(shock_ss_dict) for F in Fs]
-        self.build_perturbation_matrices = sp.lambdify(params_and_variables, Fs_subbed)
-
-        if return_F_matrices:
-            return Fs_subbed
-
-    def check_bk_condition(
-        self,
-        free_param_dict: Optional[Dict[str, float]] = None,
-        system_matrices: Optional[List[ArrayLike]] = None,
-        verbose: bool = True,
-        return_value: Optional[str] = "df",
-        tol=1e-8,
-    ) -> Optional[ArrayLike]:
-        """
-        Compute the generalized eigenvalues of system in the form presented in [1]. Per [2], the number of
-        unstable eigenvalues (|v| > 1) should not be greater than the number of forward-looking variables. Failing
-        this test suggests timing problems in the definition of the model.
-
-        Parameters
-        ----------
-        free_param_dict: dict, optional
-            A dictionary of parameter values. If None, the current stored values are used.
-        verbose: bool, default: True
-            Flag to print the results of the test, otherwise the eigenvalues are returned without comment.
-        return_value: string, default: 'eigenvalues'
-            Controls what is returned by the function. Valid values are 'df', 'flag', and None.
-            If df, a dataframe containing eigenvalues is returned. If 'bool', a boolean indicating whether the BK
-            condition is satisfied. If None, nothing is returned.
-        tol: float, 1e-8
-            Convergence tolerance for the gensys solver
-
-        Returns
-        -------
-
-        """
-        if self.build_perturbation_matrices is None:
-            raise PerturbationSolutionNotFoundException()
-
-        if free_param_dict is not None:
-            ss_dict, calib_dict = self.f_ss(free_param_dict)
-        else:
-            free_param_dict = self.free_param_dict
-            ss_dict = self.steady_state_dict
-            calib_dict = self.calib_param_dict
-
-        if system_matrices is not None:
-            A, B, C, D = system_matrices
-        else:
-            A, B, C, D = self.build_perturbation_matrices(
-                **ss_dict, **free_param_dict, **calib_dict
-            )
-        n_forward = (C.sum(axis=0) > 0).sum().astype(int)
-        n_eq, n_vars = A.shape
-
-        # TODO: Compute system eigenvalues -- avoids calling the whole Gensys routine, but there is code duplication
-        #   building Gamma_0 and Gamma_1
-        lead_var_idx = np.where(np.sum(np.abs(C), axis=0) > tol)[0]
-
-        eqs_and_leads_idx = np.r_[np.arange(n_vars), lead_var_idx + n_vars].tolist()
-
-        Gamma_0 = np.vstack([np.hstack([B, C]), np.hstack([-np.eye(n_eq), np.zeros((n_eq, n_eq))])])
-
-        Gamma_1 = np.vstack(
-            [
-                np.hstack([A, np.zeros((n_eq, n_eq))]),
-                np.hstack([np.zeros((n_eq, n_eq)), np.eye(n_eq)]),
-            ]
-        )
-        Gamma_0 = Gamma_0[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
-        Gamma_1 = Gamma_1[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
-
-        # A, B, Q, Z = qzdiv(1.01, *linalg.qz(-Gamma_0, Gamma_1, 'complex'))
-
-        # Using scipy instead of qzdiv appears to offer a huge speedup for nearly the same answer; some eigenvalues
-        # have sign flip relative to qzdiv -- does it matter?
-        A, B, alpha, beta, Q, Z = linalg.ordqz(-Gamma_0, Gamma_1, sort="ouc", output="complex")
-
-        gev = np.c_[np.diagonal(A), np.diagonal(B)]
-
-        eigenval = gev[:, 1] / (gev[:, 0] + tol)
-        pos_idx = np.where(np.abs(eigenval) > 0)
-        eig = np.zeros(((np.abs(eigenval) > 0).sum(), 3))
-        eig[:, 0] = np.abs(eigenval)[pos_idx]
-        eig[:, 1] = np.real(eigenval)[pos_idx]
-        eig[:, 2] = np.imag(eigenval)[pos_idx]
-
-        sorted_idx = np.argsort(eig[:, 0])
-        eig = pd.DataFrame(eig[sorted_idx, :], columns=["Modulus", "Real", "Imaginary"])
-
-        n_g_one = (eig["Modulus"] > 1).sum()
-        condition_not_satisfied = n_forward > n_g_one
-        if verbose:
-            print(
-                f"Model solution has {n_g_one} eigenvalues greater than one in modulus and {n_forward} "
-                f"forward-looking variables."
-                f'\nBlanchard-Kahn condition is{" NOT" if condition_not_satisfied else ""} satisfied.'
-            )
-
-        if return_value is None:
-            return
-
-        if return_value == "df":
-            return eig
-        elif return_value == "bool":
-            return ~condition_not_satisfied
-
-    def compute_stationary_covariance_matrix(self):
-        """
-        Compute the stationary covariance matrix of the solved system via fixed-point iteration. By construction, any
-        linearized DSGE model will have a fixed covariance matrix. In principle, a closed form solution is available
-        (we could solve a discrete Lyapunov equation) but this works fine.
-
-        Returns
-        -------
-        sigma: DataFrame
-        """
-        if not self.perturbation_solved:
-            raise PerturbationSolutionNotFoundException()
-
-        T, R = self.T, self.R
-
-        # TODO: Should this be R @ Q @ R.T ?
-        sigma = linalg.solve_discrete_lyapunov(T.values, R.values @ R.values.T)
-
-        return pd.DataFrame(sigma / 100, index=T.index, columns=T.index)
-
-    def compute_autocorrelation_matrix(self, n_lags=10):
-        """
-        Computes autocorrelations for each model variable using the stationary covariance matrix. See doc string for
-        compute_stationary_covariance_matrix for more information.
-
-        Parameters
-        ----------
-        n_lags: int
-            Number of lags over which to compute the autocorrelation
-
-        Returns
-        -------
-        acorr_mat: DataFrame
-        """
-        if not self.perturbation_solved:
-            raise PerturbationSolutionNotFoundException()
-
-        T, R = self.T, self.R
-
-        Sigma = linalg.solve_discrete_lyapunov(T.values, R.values @ R.values.T)
-        acorr_mat = _compute_autocorrelation_matrix(T.values, Sigma, n_lags=n_lags)
-
-        return pd.DataFrame(acorr_mat, index=T.index, columns=np.arange(n_lags))
-
-    def fit(
-        self,
-        data,
-        estimate_a0=False,
-        estimate_P0=False,
-        a0_prior=None,
-        P0_prior=None,
-        filter_type="univariate",
-        draws=5000,
-        n_walkers=36,
-        moves=None,
-        emcee_x0=None,
-        verbose=True,
-        return_inferencedata=True,
-        burn_in=None,
-        thin=None,
-        skip_initial_state_check=False,
-        **sampler_kwargs,
-    ):
-        """
-        Estimate model parameters via Bayesian inference. Parameter likelihood is computed using the Kalman filter.
-        Posterior distributions are estimated using Markov Chain Monte Carlo (MCMC), specifically the Affine-Invariant
-        Ensemble Sampler algorithm of [1].
-
-        A "traditional" Random Walk Metropolis can be achieved using the moves argument, but by default this function
-        will use a mix of two Differential Evolution (DE) proposal algorithms that have been shown to work well on
-        weakly multi-modal problems. DSGE estimation can be multi-modal in the sense that regions of the posterior
-        space are separated by the constraints on the ability to solve the perturbation problem.
-
-        This function will start all MCMC chains around random draws from the prior distribution. This is in contrast
-        to Dynare and gEcon.estimate, which start MCMC chains around the Maximum Likelihood estimate for parameter
-        values.
-
-        Parameters
-        ----------
-        data: dataframe
-            A pandas dataframe of observed values, with column names corresponding to DSGE model states names.
-        estimate_a0: bool, default: False
-            Whether to estimate the initial values of the DSGE process. If False, x0 will be deterministically set to
-            a vector of zeros, corresponding to the steady state. If True, you must provide a
-        estimate_P0: bool, default: False
-            Whether to estimate the intial covariance matrix of the DSGE process. If False, P0 will be set to the
-            Kalman Filter steady state value by solving the associated discrete Lyapunov equation.
-        a0_prior: dict, optional
-            A dictionary with (variable name, scipy distribution) key-value pairs. If a key "initial_vector" is found,
-            all other keys will be ignored, and the single distribution over all initial states will be used. Otherwise,
-            n_states independent distributions should be included in the dictionary.
-            If estimate_a0 is False, this will be ignored.
-        P0_prior: dict, optional
-            A dictionary with (variable name, scipy distribution) key-value pairs. If a key "initial_covariance" is
-            found, all other keys will be ignored, and this distribution will be taken as over the entire covariance
-            matrix. Otherwise, n_states independent distributions are expected, and are used to construct a diagonal
-            initial covariance matrix.
-        filter_type: string, default: "standard"
-            Select a kalman filter implementation to use. Currently "standard" and "univariate" are supported. Try
-            univariate if you run into errors inverting the P matrix during filtering.
-        draws: integer
-            Number of draws from each MCMC chain, or "walker" in the jargon of emcee.
-        n_walkers: integer
-            The number of "walkers", which roughly correspond to chains in other MCMC packages. Note that one needs
-            many more walkers than chains; [1] recommends as many as possible.
-        cores: integer
-            The number of processing cores, which is passed to Multiprocessing.Pool to do parallel inference. To
-            maintain detailed balance, the pool of walkers must be split, resulting in n_walkers / cores sub-ensembles.
-            Be sure to raise the number of walkers to compensate.
-        moves: List of emcee.moves objects
-            Moves tell emcee how to generate MCMC proposals. See the emcee docs for details.
-        emcee_x0: array
-            An (n_walkers, k_parameters) array of initial values. Emcee will check the condition number of the matrix
-            to ensure all walkers begin in different regions of the parameter space. If MLE estimates are used, they
-            should be jittered to start walkers in a ball around the desired initial point.
-        return_inferencedata: bool, default: True
-            If true, return an Arviz InferenceData object containing posterior samples. If False, the fitted Emcee
-            sampler is returned.
-        burn_in: int, optional
-            Number of initial samples to discard from all chains. This is ignored if return_inferencedata is False.
-        thin: int, optional
-            Return only every n-th sample from each chain. This is done to reduce storage requirements in highly
-            autocorrelated chains by discarding redundant information. Ignored if return_inferencedata is False.
-
-        Returns
-        -------
-        sampler, emcee.Sampler object
-            An emcee.Sampler object with the estimated posterior over model parameters, as well as other diagnotic
-            information.
-
-        References
-        -------
-        ..[1] Foreman-Mackey, Daniel, et al. “Emcee: The MCMC Hammer.” Publications of the Astronomical Society of the
-              Pacific, vol. 125, no. 925, Mar. 2013, pp. 306–12. arXiv.org, https://doi.org/10.1086/670067.
-        """
-        observed_vars = data.columns.tolist()
-        model_var_names = [x.base_name for x in self.variables]
-        if not all([x in model_var_names for x in observed_vars]):
-            orphans = [x for x in observed_vars if x not in model_var_names]
-            raise ValueError(
-                f"Columns of data must correspond to states of the DSGE model. Found the following columns"
-                f'with no associated model state: {", ".join(orphans)}'
-            )
-
-        sparse_data = extract_sparse_data_from_model(self)
-        prior_dict = extract_prior_dict(self)
-
-        if estimate_a0 is False:
-            a0 = None
-        else:
-            if a0_prior is None:
-                raise ValueError(
-                    "If estimate_a0 is True, you must provide a dictionary of prior distributions for"
-                    "the initial values of all individual states"
-                )
-            if not all([var in a0_prior.keys() for var in model_var_names]):
-                missing_keys = set(model_var_names) - set(list(a0_prior.keys()))
-                raise ValueError(
-                    "You must provide one key for each state in the model. "
-                    f'No keys found for: {", ".join(missing_keys)}'
-                )
-            for var in model_var_names:
-                prior_dict[f"{var}__initial"] = a0_prior[var]
-
-        moves = moves or [
-            (emcee.moves.DEMove(), 0.6),
-            (emcee.moves.DESnookerMove(), 0.4),
-        ]
-
-        shock_names = [x.base_name for x in self.shocks]
-
-        k_params = len(prior_dict)
-        Z = build_Z_matrix(observed_vars, model_var_names)
-
-        args = [
-            data,
-            sparse_data,
-            Z,
-            prior_dict,
-            shock_names,
-            observed_vars,
-            filter_type,
-        ]
-        arg_names = [
-            "observed_data",
-            "sparse_data",
-            "Z",
-            "prior_dict",
-            "shock_names",
-            "observed_vars",
-            "filter_type",
-        ]
-
-        if emcee_x0:
-            x0 = emcee_x0
-        else:
-            x0 = np.stack([x.rvs(n_walkers) for x in prior_dict.values()]).T
-
-        param_names = list(prior_dict.keys())
-
-        sampler = emcee.EnsembleSampler(
-            n_walkers,
-            k_params,
-            evaluate_logp,
-            args=args,
-            moves=moves,
-            parameter_names=param_names,
-            **sampler_kwargs,
-        )
-
-        _ = sampler.run_mcmc(
-            x0,
-            draws,
-            progress=verbose,
-            skip_initial_state_check=skip_initial_state_check,
-        )
-
-        if return_inferencedata:
-            sampler_stats = xr.Dataset(
-                data_vars=dict(
-                    acceptance_fraction=(["chain"], sampler.acceptance_fraction),
-                    autocorrelation_time=(
-                        ["parameters"],
-                        sampler.get_autocorr_time(discard=burn_in or 0, quiet=True),
-                    ),
-                ),
-                coords=dict(chain=np.arange(n_walkers), parameters=param_names),
-            )
-
-            idata = az.from_emcee(
-                sampler,
-                var_names=param_names,
-                blob_names=["log_likelihood"],
-                arg_names=arg_names,
-            )
-
-            idata["sample_stats"].update(sampler_stats)
-            idata.observed_data = idata.observed_data.drop(["sparse_data", "prior_dict"])
-            idata.observed_data = idata.observed_data.drop_dims(
-                ["sparse_data_dim_0", "sparse_data_dim_1", "prior_dict_dim_0"]
-            )
-
-            return idata.sel(draw=slice(burn_in, None, thin))
-
-        return sampler
-
-    def sample_param_dict_from_prior(
-        self, n_samples=1, seed=None, param_subset=None, sample_shock_sigma=False
-    ):
-
-        """
-        Sample parameters from the parameter prior distributions.
-
-        Parameters
-        ----------
-        n_samples: int, default: 1
-            Number of samples to draw from the prior distributions.
-        seed: int, default: None
-            Seed for the random number generator.
-        param_subset: list, default: None
-            List of parameter names to sample. If None, all parameters are sampled.
-        sample_shock_sigma: bool, default: False
-            If True, also sample the shock standard deviations.
-
-        Returns
-        -------
-        new_param_dict: dict
-            Dictionary of sampled parameters.
-        """
-
-        if sample_shock_sigma:
-            shock_priors = {k: v.rv_params["scale"] for k, v in self.shock_priors.items()}
-        else:
-            shock_priors = self.shock_priors
-
-        all_priors = merge_dictionaries(
-            self.param_priors, shock_priors, self.observation_noise_priors
-        )
-
-        if param_subset is None:
-            n_variables = len(all_priors)
-            priors_to_sample = all_priors
-        else:
-            n_variables = len(param_subset)
-            priors_to_sample = {k: v for k, v in all_priors.items() if k in param_subset}
-
-        if seed is not None:
-            seed_sequence = np.random.SeedSequence(seed)
-            child_seeds = seed_sequence.spawn(n_variables)
-            streams = [np.random.default_rng(s) for s in child_seeds]
-        else:
-            streams = [None] * n_variables
-
-        new_param_dict = {}
-        for i, (key, d) in enumerate(priors_to_sample.items()):
-            new_param_dict[key] = d.rvs(size=n_samples, random_state=streams[i])
-
-        return new_param_dict
-
-    def impulse_response_function(self, simulation_length: int = 40, shock_size: float = 1.0):
-        """
-        Compute the impulse response functions of the model.
-
-        Parameters
-        ----------
-        simulation_length : int, optional
-            The number of periods to compute the IRFs over. The default is 40.
-        shock_size : float, optional
-            The size of the shock. The default is 1.0.
-
-        Returns
-        -------
-        pandas.DataFrame
-            The IRFs for each variable in the model. The DataFrame has a multi-index
-            with the variable names as the first level and the timestep as the second.
-            The columns are the shocks.
-
-        Raises
-        ------
-        PerturbationSolutionNotFoundException
-            If a perturbation solution has not been found.
-        """
-
-        if not self.perturbation_solved:
-            raise PerturbationSolutionNotFoundException()
-
-        T, R = self.T, self.R
-
-        timesteps = simulation_length
-
-        data = np.zeros((self.n_variables, timesteps, self.n_shocks))
-
-        for i in range(self.n_shocks):
-            shock_path = np.zeros((self.n_shocks, timesteps))
-            shock_path[i, 0] = shock_size
-
-            for t in range(1, timesteps):
-                stochastic = R.values @ shock_path[:, t - 1]
-                deterministic = T.values @ data[:, t - 1, i]
-                data[:, t, i] = deterministic + stochastic
-
-        index = pd.MultiIndex.from_product(
-            [R.index, np.arange(timesteps), R.columns],
-            names=["Variables", "Time", "Shocks"],
-        )
-
-        df = (
-            pd.DataFrame(data.ravel(), index=index, columns=["Values"])
-            .unstack([1, 2])
-            .droplevel(axis=1, level=0)
-            .sort_index(axis=1)
-        )
-
-        return df
-
-    def simulate(
-        self,
-        simulation_length: int = 40,
-        n_simulations: int = 100,
-        shock_dict: Optional[Dict[str, float]] = None,
-        shock_cov_matrix: Optional[ArrayLike] = None,
-        show_progress_bar: bool = False,
-    ):
-
-        """
-        Simulate the model over a certain number of time periods.
-
-        Parameters
-        ----------
-        simulation_length : int, optional(default=40)
-            The number of time periods to simulate.
-        n_simulations : int, optional(default=100)
-            The number of simulations to run.
-        shock_dict : dict, optional(default=None)
-            Dictionary of shocks to use.
-        shock_cov_matrix : arraylike, optional(default=None)
-            Covariance matrix of shocks to use.
-        show_progress_bar : bool, optional(default=False)
-            Whether to show a progress bar for the simulation.
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            The simulated data.
-        """
-
-        if not self.perturbation_solved:
-            raise PerturbationSolutionNotFoundException()
-
-        T, R = self.T, self.R
-        timesteps = simulation_length
-
-        n_shocks = R.shape[1]
-
-        if shock_cov_matrix is not None:
-            assert shock_cov_matrix.shape == (
-                n_shocks,
-                n_shocks,
-            ), f"The shock covariance matrix should have shape {n_shocks} x {n_shocks}"
-            d = stats.multivariate_normal(mean=np.zeros(n_shocks), cov=shock_cov_matrix)
-            epsilons = np.r_[[d.rvs(timesteps) for _ in range(n_simulations)]]
-
-        elif shock_dict is not None:
-            epsilons = np.zeros((n_simulations, timesteps, n_shocks))
-            for i, shock in enumerate(self.shocks):
-                if shock.base_name in shock_dict.keys():
-                    d = stats.norm(loc=0, scale=shock_dict[shock.base_name])
-                    epsilons[:, :, i] = np.r_[[d.rvs(timesteps) for _ in range(n_simulations)]]
-
-        elif all([shock.base_name in self.shock_priors.keys() for shock in self.shocks]):
-            epsilons = np.zeros((n_simulations, timesteps, n_shocks))
-            for i, d in enumerate(self.shock_priors.values()):
-                epsilons[:, :, i] = np.r_[[d.rvs(timesteps) for _ in range(n_simulations)]]
-
-        else:
-            raise ValueError(
-                "To run a simulation, supply either a full covariance matrix, a dictionary of shocks and"
-                "standard deviations, or specify priors on the shocks in your GCN file."
-            )
-
-        data = np.zeros((self.n_variables, timesteps, n_simulations))
-        if epsilons.ndim == 2:
-            epsilons = epsilons[:, :, None]
-
-        progress_bar = ProgressBar(timesteps - 1, verb="Sampling")
-
-        for t in range(1, timesteps):
-            progress_bar.start()
-            stochastic = np.einsum("ij,sj", R.values, epsilons[:, t - 1, :])
-            deterministic = T.values @ data[:, t - 1, :]
-            data[:, t, :] = deterministic + stochastic
-
-            if show_progress_bar:
-                progress_bar.stop()
-
-        index = pd.MultiIndex.from_product(
-            [R.index, np.arange(timesteps), np.arange(n_simulations)],
-            names=["Variables", "Time", "Simulation"],
-        )
-        df = (
-            pd.DataFrame(data.ravel(), index=index, columns=["Values"])
-            .unstack([1, 2])
-            .droplevel(axis=1, level=0)
-        )
-
-        return df
-
-    def _build_prior_dict(self, prior_dict: Dict[str, str]) -> None:
-        """
-        Parameters
-        ----------
-        prior_dict: dict
-            Dictionary of variable_name: distribution_string pairs, prepared by the parse_gcn function.
-
-        Returns
-        -------
-        self.param_dict: dict
-            Dictionary of variable:distribution pairs. Distributions are scipy rv_frozen objects, unless the
-            distribution is parameterized by another distribution, in which case a "CompositeDistribution" object
-            with methods .rvs, .pdf, and .logpdf is returned.
-        """
-
-        priors = create_prior_distribution_dictionary(prior_dict)
-        hyper_parameters = set(prior_dict.keys()) - set(priors.keys())
-
-        # Clean up the hyper-parameters (e.g. shock stds) from the model, they aren't needed anymore
-        for parameter in hyper_parameters:
-            del self.free_param_dict[parameter]
-
-        param_priors = SymbolDictionary()
-        shock_priors = SymbolDictionary()
-        for key, value in priors.items():
-            sympy_key = single_symbol_to_sympy(key, assumptions=self.assumptions)
-            if isinstance(sympy_key, TimeAwareSymbol):
-                shock_priors[sympy_key.base_name] = value
-            else:
-                param_priors[sympy_key.name] = value
-
-        self.param_priors = param_priors
-        self.shock_priors = shock_priors
-
-    def _build_model_blocks(self, parsed_model, simplify_blocks: bool):
-        """
-        Builds blocks of the gEconpy model using strings parsed from the GCN file.
-
-        Parameters
-        ----------
-        parsed_model : str
-            The GCN model as a string.
-        simplify_blocks : bool
-            Whether to try to simplify equations or not.
-        """
-
-        raw_blocks = gEcon_parser.split_gcn_into_block_dictionary(parsed_model)
-
-        self.options = raw_blocks["options"]
-        self.try_reduce_vars = raw_blocks["tryreduce"]
-        self.assumptions = raw_blocks["assumptions"]
-
-        del raw_blocks["options"]
-        del raw_blocks["tryreduce"]
-        del raw_blocks["assumptions"]
-
-        self._get_steady_state_equations(raw_blocks)
-
-        for block_name, block_content in raw_blocks.items():
-            block_dict = gEcon_parser.parsed_block_to_dict(block_content)
-            block = Block(name=block_name, block_dict=block_dict, assumptions=self.assumptions)
-            block.solve_optimization(try_simplify=simplify_blocks)
-
-            self.blocks[block.name] = block
-
-        self.n_blocks = len(self.blocks)
-
-    def _get_all_block_equations(self) -> None:
-        """
-        Extract all equations from the blocks in the model.
-
-        Parameters
-        ----------
-        self : `Model`
-            The model object whose block system equations will be extracted.
-
-        Returns
-        -------
-        None
-
-        Notes
-        -----
-        Updates the `system_equations` attribute of `self` with the extracted equations.
-        Also updates the `n_equations` attribute of `self` with the number of extracted equations.
-        """
-
-        _, blocks = unpack_keys_and_values(self.blocks)
-        for block in blocks:
-            self.system_equations.extend(block.system_equations)
-        self.n_equations = len(self.system_equations)
-
-    def _get_all_block_parameters(self) -> None:
-        """
-        Extract all parameters from all blocks and store them in the model's free_param_dict attribute. The
-        `free_param_dict` attribute is updated in place.
-        """
-
-        _, blocks = unpack_keys_and_values(self.blocks)
-        for block in blocks:
-            self.free_param_dict.update(block.param_dict)
-
-        self.free_param_dict = (
-            SymbolDictionary(self.free_param_dict).sort_keys().to_string().values_to_float()
-        )
-
-    def _get_all_block_params_to_calibrate(self) -> None:
-        """
-        Retrieve the list of parameters to calibrate and the list of
-        equations used to calibrate the parameters from each block of
-        the model.
-        """
-        _, blocks = unpack_keys_and_values(self.blocks)
-        for block in blocks:
-            if block.params_to_calibrate is None:
-                continue
-
-            if len(self.params_to_calibrate) == 0:
-                self.params_to_calibrate = block.params_to_calibrate
-            else:
-                self.params_to_calibrate.extend(block.params_to_calibrate)
-
-            if block.calibrating_equations is None:
-                continue
-
-            if len(self.calibrating_equations) == 0:
-                self.calibrating_equations = block.calibrating_equations
-            else:
-                self.calibrating_equations.extend(block.calibrating_equations)
-
-        self.n_calibrating_equations = len(self.calibrating_equations)
-        self.n_params_to_calibrate = len(self.params_to_calibrate)
-
-    def _get_variables_and_shocks(self) -> None:
-        """
-        Collect all variables and shocks from the blocks and set their counts.
-
-        This method is called after the blocks have been processed. It collects all the shocks and variables from the
-        blocks, sorts them, and sets the n_shocks and n_variables properties.
-        """
-
-        all_shocks = []
-        _, blocks = unpack_keys_and_values(self.blocks)
-
-        for block in blocks:
-            if block.shocks is not None:
-                all_shocks.extend([x for x in block.shocks])
-        self.shocks = all_shocks
-        self.n_shocks = len(all_shocks)
-
-        for eq in self.system_equations:
-            atoms = eq.atoms()
-            variables = [x for x in atoms if is_variable(x)]
-            for variable in variables:
-                if variable.set_t(0) not in self.variables and variable not in all_shocks:
-                    self.variables.append(variable.set_t(0))
-        self.n_variables = len(self.variables)
-
-        self.variables = sorted(self.variables, key=lambda x: x.name)
-        self.shocks = sorted(self.shocks, key=lambda x: x.name)
-
-    def _get_steady_state_equations(self, raw_blocks: Dict[str, List[str]]):
-        """
-        Extract user-provided steady state equations from the `raw_blocks` dictionary and store the resulting
-        relationships in self.steady_state_relationships.
-
-        Parameters
-        ----------
-        raw_blocks : dict
-            Dictionary of block names and block contents extracted from a gEcon model.
-
-        Raises
-        ------
-        MultipleSteadyStateBlocksException
-            If there is more than one block in `raw_blocks` with a name from `STEADY_STATE_NAMES`.
-        """
-
-        block_names = raw_blocks.keys()
-        ss_block_names = [name for name in block_names if name in STEADY_STATE_NAMES]
-        n_ss_blocks = len(ss_block_names)
-
-        if n_ss_blocks == 0:
-            return
-        if n_ss_blocks > 1:
-            raise MultipleSteadyStateBlocksException(ss_block_names)
-
-        block_content = raw_blocks[ss_block_names[0]]
-        block_dict = gEcon_parser.parsed_block_to_dict(block_content)
-        block = Block(name="steady_state", block_dict=block_dict, assumptions=self.assumptions)
-
-        sub_dict = SymbolDictionary()
-        steady_state_dict = SymbolDictionary()
-
-        if block.definitions is not None:
-            _, definitions = unpack_keys_and_values(block.definitions)
-            sub_dict = SymbolDictionary({eq.lhs: eq.rhs for eq in definitions})
-
-        if block.identities is not None:
-            _, identities = unpack_keys_and_values(block.identities)
-            for eq in identities:
-                subbed_rhs = eq.rhs.subs(sub_dict)
-                steady_state_dict[eq.lhs] = subbed_rhs
-                sub_dict[eq.lhs] = subbed_rhs
-
-        self.steady_state_relationships = (
-            steady_state_dict.sort_keys().to_string().values_to_float()
-        )
-
-        del raw_blocks[ss_block_names[0]]
-
-    def _try_reduce(self):
-        """
-        Attempt to reduce the number of equations in the system by removing equations requested in the `tryreduce`
-        block of the GCN file. Equations are considered safe to remove if they are "self-contained" that is, if
-        no other variables depend on their values.
-
-        Returns
-        -------
-        list
-            The names of the variables that were removed. If reduction was not possible, None is returned.
-        """
-
-        if self.try_reduce_vars is None:
-            return
-
-        self.try_reduce_vars = [
-            single_symbol_to_sympy(x, self.assumptions) for x in self.try_reduce_vars
-        ]
-
-        variables = self.variables
-        n_variables = self.n_variables
-
-        occurrence_matrix = np.zeros((n_variables, n_variables))
-        reduced_system = []
-
-        for i, eq in enumerate(self.system_equations):
-            for j, var in enumerate(self.variables):
-                if any([x in eq.atoms() for x in make_all_var_time_combos([var])]):
-                    occurrence_matrix[i, j] += 1
-
-        # Columns with a sum of 1 are variables that appear only in a single equations; these equations can be deleted
-        # without consequence w.r.t solving the system.
-
-        isolated_variables = np.array(variables)[occurrence_matrix.sum(axis=0) == 1]
-        to_remove = set(isolated_variables).intersection(set(self.try_reduce_vars))
-
-        for eq in self.system_equations:
-            if not any([var in eq.atoms() for var in to_remove]):
-                reduced_system.append(eq)
-
-        self.system_equations = reduced_system
-        self.n_equations = len(self.system_equations)
-
-        self.variables = {
-            atom.set_t(0) for eq in reduced_system for atom in eq.atoms() if is_variable(atom)
-        }
-        self.variables -= set(self.shocks)
-        self.variables = sorted(list(self.variables), key=lambda x: x.name)
-        self.n_variables = len(self.variables)
-
-        if self.n_equations != self.n_variables:
-            warn("Reduction was requested but not possible because the system is not well defined.")
-            return
-
-        eliminated_vars = [var.name for var in variables if var not in self.variables]
-
-        return eliminated_vars
-
-    def _simplify_singletons(self):
-        """
-        Simplify the system by removing variables that are deterministically defined as a known value. Common examples
-        include P[] = 1, setting the price level of the economy as the numeraire, or B[] = 0, putting the bond market
-        in net-zero supply.
-
-        In these cases, the variable can be replaced by the deterministic value after all FoC
-        have been computed.
-
-        Returns
-        -------
-        eliminated_vars : List[str]
-            The names of the variables that were removed.
-        """
-
-        system = self.system_equations
-
-        variables = self.variables
-        reduce_dict = {}
-
-        for eq in system:
-            if len(eq.atoms()) < 4:
-                var = [x for x in eq.atoms() if is_variable(x)]
-                if len(var) != 1:
-                    continue
-                var = var[0]
-                sub_dict = expand_subs_for_all_times(sp.solve(eq, var, dict=True)[0])
-                reduce_dict.update(sub_dict)
-
-        reduced_system = substitute_all_equations(system, reduce_dict)
-        reduced_system = [eq for eq in reduced_system if eq != 0]
-
-        self.system_equations = reduced_system
-        self.n_equations = len(reduced_system)
-
-        self.variables = {
-            atom.set_t(0) for eq in reduced_system for atom in eq.atoms() if is_variable(atom)
-        }
-        self.variables -= set(self.shocks)
-        self.variables = sorted(list(self.variables), key=lambda x: x.name)
-        self.n_variables = len(self.variables)
-
-        if self.n_equations != self.n_variables:
-            warn(
-                "Simplification was requested but not possible because the system is not well defined."
-            )
-            return
-
-        eliminated_vars = [var.name for var in variables if var not in self.variables]
-
-        return eliminated_vars
-
-
-# #@njit
-# def _compute_stationary_covariance_matrix(A, C, tol=1e-9, max_iter=10_000):
-#     sigma = np.eye(A.shape[0])
-#     for _ in range(max_iter):
-#         new_sigma = A @ sigma @ A.T + C @ C.T
-#         if ((sigma - new_sigma) ** 2).mean() < tol:
-#             return sigma
-#         else:
-#             sigma = new_sigma
-
-
-@nb.njit(cache=True)
-def _compute_autocorrelation_matrix(A, sigma, n_lags=5):
-    """Compute the autocorrelation matrix for the given state-space model.
-
-    Parameters
-    ----------
-    A : ndarray
-        An array of shape (n_endog, n_endog, n_lags) representing the transition matrix of the
-        state-space system.
-    sigma : ndarray
-        An array of shape (n_endog, n_endog) representing the variance-covariance matrix of the errors of
-        the transition equation.
-    n_lags : int, optional
-        The number of lags for which to compute the autocorrelation matrix.
-
-    Returns
-    -------
-    acov : ndarray
-        An array of shape (n_endog, n_lags) representing the autocorrelation matrix of the state-space process.
-    """
-
-    acov = np.zeros((A.shape[0], n_lags))
-    acov_factor = np.eye(A.shape[0])
-    for i in range(n_lags):
-        cov = acov_factor @ sigma
-        acov[:, i] = np.diag(cov) / np.diag(sigma)
-        acov_factor = A @ acov_factor
-
-    return acov
+from collections import defaultdict
+from functools import reduce
+from typing import Any, Callable, Dict, List, Optional, Union
+from warnings import warn
+
+import arviz as az
+import emcee
+import numba as nb
+import numpy as np
+import pandas as pd
+import sympy as sp
+import xarray as xr
+from numpy.typing import ArrayLike
+from scipy import linalg, stats
+
+from gEconpy.classes.block import Block
+from gEconpy.classes.containers import SymbolDictionary
+from gEconpy.classes.progress_bar import ProgressBar
+from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
+from gEconpy.estimation.estimate import build_Z_matrix, evaluate_logp
+from gEconpy.estimation.estimation_utilities import (
+    extract_prior_dict,
+    extract_sparse_data_from_model,
+)
+from gEconpy.exceptions.exceptions import (
+    GensysFailedException,
+    MultipleSteadyStateBlocksException,
+    PerturbationSolutionNotFoundException,
+    SteadyStateNotSolvedError,
+    VariableNotFoundException,
+)
+from gEconpy.numba_tools.utilities import numba_lambdify
+from gEconpy.parser import file_loaders, gEcon_parser
+from gEconpy.parser.constants import STEADY_STATE_NAMES
+from gEconpy.parser.parse_distributions import create_prior_distribution_dictionary
+from gEconpy.parser.parse_equations import single_symbol_to_sympy
+from gEconpy.shared.utilities import (
+    expand_subs_for_all_times,
+    is_variable,
+    make_all_var_time_combos,
+    merge_dictionaries,
+    substitute_all_equations,
+    unpack_keys_and_values,
+)
+from gEconpy.solvers.gensys import interpret_gensys_output
+from gEconpy.solvers.perturbation import PerturbationSolver
+from gEconpy.solvers.steady_state import SteadyStateSolver
+
+VariableType = Union[sp.Symbol, TimeAwareSymbol]
+
+
+class gEconModel:
+    def __init__(
+        self,
+        model_filepath: str,
+        verbose: bool = True,
+        simplify_blocks=True,
+        simplify_constants=True,
+        simplify_tryreduce=True,
+    ) -> None:
+        """
+        Initialize a DSGE model object from a GCN file.
+
+        Parameters
+        ----------
+        model_filepath : str
+            Filepath to the GCN file
+        verbose : bool, optional
+            Flag for verbose output, by default True
+        simplify_blocks : bool, optional
+            Flag to simplify blocks, by default True
+        simplify_constants : bool, optional
+            Flag to simplify constants, by default True
+        simplify_tryreduce : bool, optional
+            Flag to simplify using `try_reduce_vars`, by default True
+        """
+        self.model_filepath: str = model_filepath
+
+        # Model metadata
+        self.options: Optional[Dict[str, bool]] = None
+        self.try_reduce_vars: Optional[List[TimeAwareSymbol]] = None
+
+        self.blocks: Dict[str, Block] = {}
+        self.n_blocks: int = 0
+
+        # Model components
+        self.variables: List[TimeAwareSymbol] = []
+        self.assumptions: Dict[str, dict] = defaultdict(SymbolDictionary)
+        self.shocks: List[TimeAwareSymbol] = []
+        self.system_equations: List[sp.Add] = []
+        self.calibrating_equations: List[sp.Add] = []
+        self.params_to_calibrate: List[sp.Symbol] = []
+
+        self.deterministic_relationships: List[sp.Add] = []
+        self.deterministic_params: List[sp.Symbol] = []
+
+        self.free_param_dict: SymbolDictionary[sp.Symbol, float] = SymbolDictionary()
+        self.calib_param_dict: SymbolDictionary[sp.Symbol, float] = SymbolDictionary()
+        self.det_param_dict: SymbolDictionary[sp.Symbol, float] = SymbolDictionary()
+        self.steady_state_relationships: SymbolDictionary[VariableType, sp.Add] = SymbolDictionary()
+
+        self.param_priors: SymbolDictionary[str, Any] = SymbolDictionary()
+        self.shock_priors: SymbolDictionary[str, Any] = SymbolDictionary()
+        self.observation_noise_priors: SymbolDictionary[str, Any] = SymbolDictionary()
+
+        self.n_variables: int = 0
+        self.n_shocks: int = 0
+        self.n_equations: int = 0
+        self.n_calibrating_equations: int = 0
+
+        # Functional representations of the model
+        self.f_ss: Union[Callable, None] = None
+        self.f_ss_resid: Union[Callable, None] = None
+
+        # Steady state information
+        self.steady_state_solved: bool = False
+        self.steady_state_system: List[sp.Add] = []
+        self.steady_state_dict: SymbolDictionary[sp.Symbol, float] = SymbolDictionary()
+        self.residuals: List[float] = []
+
+        # Functional representation of the perturbation system
+        self.build_perturbation_matrices: Union[Callable, None] = None
+
+        # Perturbation solution information
+        self.perturbation_solved: bool = False
+        self.T: pd.DataFrame = None
+        self.R: pd.DataFrame = None
+        self.P: pd.DataFrame = None
+        self.Q: pd.DataFrame = None
+        self.R: pd.DataFrame = None
+        self.S: pd.DataFrame = None
+
+        self.build(
+            verbose=verbose,
+            simplify_blocks=simplify_blocks,
+            simplify_constants=simplify_constants,
+            simplify_tryreduce=simplify_tryreduce,
+        )
+
+        # Assign Solvers
+        self.steady_state_solver = SteadyStateSolver(self)
+        self.perturbation_solver = PerturbationSolver(self)
+
+        # TODO: Here I copy the assumptions from the model (which should be the only source of truth for assumptions)
+        #  into every SymbolDictionary. This setup is really bad; if these dictionaries go out of sync there could be
+        #  disagreements about what the assumptions for a variable should be.
+
+        for d in [
+            self.free_param_dict,
+            self.calib_param_dict,
+            self.steady_state_relationships,
+            self.param_priors,
+            self.shock_priors,
+            self.observation_noise_priors,
+        ]:
+            d._assumptions.update(self.assumptions)
+
+    def build(
+        self,
+        verbose: bool,
+        simplify_blocks: bool,
+        simplify_constants: bool,
+        simplify_tryreduce: bool,
+    ) -> None:
+        """
+        Main parsing function for the model. Build loads the GCN file, decomposes it into blocks, solves optimization
+        problems contained in each block, then extracts parameters, equations, calibrating equations, calibrated
+        parameters, and exogenous shocks into their respective class attributes.
+
+        Priors declared in the GCN file are converted into scipy distribution objects and stored in two dictionaries:
+        self.param_priors and self.shock_priors.
+
+        Gathering block information is done for convenience. For diagnostic purposes the block structure is retained
+        as well.
+
+        Parameters
+        ----------
+        verbose : bool, optional
+            When True, print a build report describing the model structure and warning the user if the number of
+            variables does not match the number of equations.
+        simplify_blocks : bool, optional
+            If True, simplify equations in the model blocks.
+        simplify_constants : bool, optional
+            If True, simplify constants in the model equations.
+        simplify_tryreduce : bool, optional
+            If True, try to reduce the number of variables in the model by eliminating unnecessary equations.
+
+        Returns
+        -------
+        None
+        """
+
+        raw_model = file_loaders.load_gcn(self.model_filepath)
+        parsed_model, prior_dict = gEcon_parser.preprocess_gcn(raw_model)
+
+        self._build_model_blocks(parsed_model, simplify_blocks)
+        self._get_all_block_equations()
+        self._get_all_block_parameters()
+        self._get_all_block_params_to_calibrate()
+        self._get_all_block_deterministic_parameters()
+        self._get_variables_and_shocks()
+        self._build_prior_dict(prior_dict)
+
+        self._make_deterministic_substitutions()
+
+        reduced_vars = None
+        singletons = None
+
+        if simplify_tryreduce:
+            reduced_vars = self._try_reduce()
+        if simplify_constants:
+            singletons = self._simplify_singletons()
+
+        if verbose:
+            self.build_report(reduced_vars, singletons)
+
+    def build_report(self, reduced_vars, singletons):
+        """
+        Write a diagnostic message after building the model. Note that successfully building the model does not
+        guarantee that the model is correctly specified. For example, it is possible to build a model with more
+        equations than parameters. This message will warn the user in this case.
+
+        Returns
+        -------
+        None
+        """
+        if singletons and len(singletons) == 0:
+            singletons = None
+
+        eq_str = "equation" if self.n_equations == 1 else "equations"
+        var_str = "variable" if self.n_variables == 1 else "variables"
+        shock_str = "shock" if self.n_shocks == 1 else "shocks"
+        cal_eq_str = "equation" if self.n_calibrating_equations == 1 else "equations"
+        par_str = "parameter" if self.n_params_to_calibrate == 1 else "parameters"
+
+        n_params = len(self.free_param_dict) + len(self.calib_param_dict)
+
+        param_priors = self.param_priors.keys()
+        shock_priors = self.shock_priors.keys()
+
+        report = "Model Building Complete.\nFound:\n"
+        report += f"\t{self.n_equations} {eq_str}\n"
+        report += f"\t{self.n_variables} {var_str}\n"
+
+        if reduced_vars:
+            report += f"\tThe following variables were eliminated at user request:\n"
+            report += f"\t\t" + ",".join(reduced_vars) + "\n"
+
+        if singletons:
+            report += f'\tThe following "variables" were defined as constants and have been substituted away:\n'
+            report += f"\t\t" + ",".join(singletons) + "\n"
+
+        report += f"\t{self.n_shocks} stochastic {shock_str}\n"
+        report += (
+            f'\t\t {len(shock_priors)} / {self.n_shocks} {"have" if len(shock_priors) == 1 else "has"}'
+            f" a defined prior. \n"
+        )
+
+        report += f"\t{n_params} {par_str}\n"
+        report += (
+            f'\t\t {len(param_priors)} / {n_params} {"have" if len(param_priors) == 1 else "has"} '
+            f"a defined prior. \n"
+        )
+        report += f"\t{self.n_calibrating_equations} calibrating {cal_eq_str}\n"
+        report += f"\t{self.n_params_to_calibrate} {par_str} to calibrate\n "
+
+        if self.n_equations == self.n_variables:
+            report += "Model appears well defined and ready to proceed to solving.\n"
+            print(report)
+        else:
+            print(report)
+            message = (
+                f"The model does not appear correctly specified, there are {self.n_equations} {eq_str} but "
+                f"{self.n_variables} {var_str}. It will not be possible to solve this model. Please check the "
+                f"specification using available diagnostic tools, and check the GCN file for typos."
+            )
+            warn(message)
+
+    def steady_state(
+        self,
+        verbose: Optional[bool] = True,
+        model_is_linear: Optional[bool] = False,
+        apply_user_simplifications=True,
+        method: Optional[str] = "root",
+        optimizer_kwargs: Optional[Dict[str, Any]] = None,
+        use_jac: Optional[bool] = True,
+        use_hess: Optional[bool] = True,
+        tol: Optional[float] = 1e-6,
+    ) -> None:
+        """
+        Solves for a function f(params) that computes steady state values and calibrated parameter values given
+        parameter values, stores results, and verifies that the residuals of the solution are zero.
+
+        Parameters
+        ----------
+        verbose: bool
+            Flag controlling whether to print results of the steady state solver. Default is True.
+        model_is_linear: bool, optional
+            If True, the model is assumed to have been linearized by the user. A specialized solving routine is used
+            to find the steady state, which is likely all zeros. If True, all other arguments to this function
+            have no effect (except verbose). Default is False.
+        apply_user_simplifications: bool
+            Whether to simplify system equations using the user-defined steady state relationships defined in the GCN
+            before passing the system to the numerical solver. Default is True.
+        method: str
+            One of "root" or "minimize". Indicates which family of solution algorithms should be used to find a
+            numerical steady state: direct root finding or minimization of squared error. Not that "root" is not
+            suitable if the number of inputs is not equal to the number of outputs, for example if user-provided
+            steady state relationships do not result in elimination of model equations. Default is "root".
+        optimizer_kwargs: dict
+            Dictionary of arguments to be passed to scipy.optimize.root or scipy.optimize.minimize, see those
+            functions for more details.
+        use_jac: bool
+            Whether to symbolically compute the Jacobian matrix of the steady state system (when method is "root") or
+            the Jacobian vector of the loss function (when method is "minimize"). Strongly recommended. Default is True
+        use_hess: bool
+            Whether to symbolically compute the Hessian matrix of the loss function. Ignored if method is "root".
+            If "False", the default BFGS solver will compute a numerical approximation, so not necessarily required.
+            Still recommended. Default is True.
+        tol: float
+            Numerical tolerance for declaring a steady-state solution valid. Default is 1e-6. Note that this only used
+            by the gEconpy model to decide if a steady state has been found, and is **NOT** passed to the scipy
+            solution algorithms. To adjust solution tolerance for these algorithms, use optimizer_kwargs.
+
+        Returns
+        -------
+        None
+        """
+        if not self.steady_state_solved:
+            self.f_ss = self.steady_state_solver.solve_steady_state(
+                apply_user_simplifications=apply_user_simplifications,
+                model_is_linear=model_is_linear,
+                method=method,
+                optimizer_kwargs=optimizer_kwargs,
+                use_jac=use_jac,
+                use_hess=use_hess,
+            )
+
+        self._process_steady_state_results(verbose, tol=tol)
+
+    def _process_steady_state_results(self, verbose=True, tol=1e-6) -> None:
+        """Process results from steady state solver.
+
+        This function sets the steady state dictionary, calibrated parameter dictionary, and residuals attribute
+        based on the results of the steady state solver. It also sets the `steady_state_solved` attribute to
+        indicate whether the steady state was successfully found. If `verbose` is True, it prints a message
+        indicating whether the steady state was found and the sum of squared residuals.
+
+        Parameters
+        ----------
+        verbose : bool, optional
+            If True, print a message indicating whether the steady state was found and the sum of squared residuals.
+            Default is True.
+        tol: float, optional
+            Numerical tolerance for declaring a steady-state solution has been found. Default is 1e-6.
+
+        Returns
+        -------
+        None
+        """
+        results = self.f_ss(self.free_param_dict)
+        self.steady_state_dict = results["ss_dict"]
+        self.calib_param_dict = results["calib_dict"]
+        self.residuals = results["resids"]
+
+        self.steady_state_system = self.steady_state_solver.steady_state_system
+        self.steady_state_solved = np.allclose(self.residuals, 0, atol=tol) & results["success"]
+
+        if verbose:
+            if self.steady_state_solved:
+                print(
+                    f"Steady state found! Sum of squared residuals is {(self.residuals ** 2).sum()}"
+                )
+            else:
+                print(
+                    f"Steady state NOT found. Sum of squared residuals is {(self.residuals ** 2).sum()}"
+                )
+
+    def print_steady_state(self):
+        """
+        Prints the steady state values for the model's variables and calibrated parameters.
+
+        Prints an error message if a valid steady state has not yet been found.
+        """
+        if self.steady_state_dict is None:
+            print("Run the steady_state method to find a steady state before calling this method.")
+            return
+
+        if not self.steady_state_solved:
+            print("Values come from the latest solver iteration but are NOT a valid steady state.")
+
+        max_var_name = (
+            max(
+                len(x)
+                for x in list(self.steady_state_dict.keys()) + list(self.calib_param_dict.keys())
+            )
+            + 5
+        )
+        for key, value in self.steady_state_dict.items():
+            print(f"{key:{max_var_name}}{value:>10.3f}")
+
+        if len(self.params_to_calibrate) > 0:
+            print("\n")
+            print("In addition, the following parameter values were calibrated:")
+            for key, value in self.calib_param_dict.items():
+                print(f"{key:{max_var_name}}{value:>10.3f}")
+
+    def solve_model(
+        self,
+        solver="cycle_reduction",
+        not_loglin_variable: Optional[List[str]] = None,
+        order: int = 1,
+        model_is_linear: bool = False,
+        tol: float = 1e-8,
+        max_iter: int = 1000,
+        verbose: bool = True,
+        on_failure="error",
+    ) -> None:
+        """
+        Solve for the linear approximation to the policy function via perturbation. Adapted from R code in the gEcon
+        package by Grzegorz Klima, Karol Podemski, and Kaja Retkiewicz-Wijtiwiak., http://gecon.r-forge.r-project.org/.
+
+        Parameters
+        ----------
+        solver: str, default: 'cycle_reduction'
+            Name of the algorithm to solve the linear solution. Currently "cycle_reduction" and "gensys" are supported.
+            Following Dynare, cycle_reduction is the default, but note that gEcon uses gensys.
+        not_loglin_variable: List, default: None
+            Variables to not log linearize when solving the model. Variables with steady state values close to zero
+            will be automatically selected to not log linearize.
+        order: int, default: 1
+            Order of taylor expansion to use to solve the model. Currently only 1st order approximation is supported.
+        model_is_linear: bool, default: False
+            Flag indicating whether a model has already been linearized by the user.
+        tol: float, default 1e-8
+            Desired level of floating point accuracy in the solution
+        max_iter: int, default: 1000
+            Maximum number of cycle_reduction iterations. Not used if solver is 'gensys'.
+        verbose: bool, default: True
+            Flag indicating whether to print solver results to the terminal
+        on_failure: str, one of ['error', 'ignore'], default: 'error'
+            Instructions on what to do if the algorithm to find a linearized policy matrix. "Error" will raise an error,
+            while "ignore" will return None. "ignore" is useful when repeatedly solving the model, e.g. when sampling.
+
+        Returns
+        -------
+        None
+        """
+
+        param_dict = self.free_param_dict | self.calib_param_dict
+        steady_state_dict = self.steady_state_dict
+
+        if self.build_perturbation_matrices is None:
+            self._perturbation_setup(not_loglin_variable, order, model_is_linear, verbose, bool)
+
+        A, B, C, D = self.build_perturbation_matrices(
+            np.array(list(param_dict.values())), np.array(list(steady_state_dict.values()))
+        )
+        _, variables, _ = self.perturbation_solver.make_all_variable_time_combinations()
+
+        if solver == "gensys":
+            gensys_results = self.perturbation_solver.solve_policy_function_with_gensys(
+                A, B, C, D, tol, verbose
+            )
+            G_1, constant, impact, f_mat, f_wt, y_wt, gev, eu, loose = gensys_results
+
+            if G_1 is None or (eu[0] != eu[1] != 1):
+                if on_failure == "error":
+                    raise GensysFailedException(eu)
+                elif on_failure == "ignore":
+                    if verbose:
+                        print(interpret_gensys_output(eu))
+                    self.P = None
+                    self.Q = None
+                    self.R = None
+                    self.S = None
+
+                    self.perturbation_solved = False
+
+                    return
+
+            T = G_1[: self.n_variables, :][:, : self.n_variables]
+            R = impact[: self.n_variables, :]
+
+        elif solver == "cycle_reduction":
+            (
+                T,
+                R,
+                result,
+                log_norm,
+            ) = self.perturbation_solver.solve_policy_function_with_cycle_reduction(
+                A, B, C, D, max_iter, tol, verbose
+            )
+            if T is None:
+                if on_failure == "errror":
+                    raise GensysFailedException(result)
+        else:
+            raise NotImplementedError(
+                'Only "cycle_reduction" and "gensys" are valid values for solver'
+            )
+
+        gEcon_matrices = self.perturbation_solver.statespace_to_gEcon_representation(
+            A, T, R, variables, tol
+        )
+        P, Q, _, _, A_prime, R_prime, S_prime = gEcon_matrices
+
+        resid_norms = self.perturbation_solver.residual_norms(
+            B, C, D, Q, P, A_prime, R_prime, S_prime
+        )
+        norm_deterministic, norm_stochastic = resid_norms
+
+        if verbose:
+            print(f"Norm of deterministic part: {norm_deterministic:0.9f}")
+            print(f"Norm of stochastic part:    {norm_deterministic:0.9f}")
+
+        self.T = pd.DataFrame(
+            T,
+            index=[x.base_name for x in sorted(self.variables, key=lambda x: x.base_name)],
+            columns=[x.base_name for x in sorted(self.variables, key=lambda x: x.base_name)],
+        )
+        self.R = pd.DataFrame(
+            R,
+            index=[x.base_name for x in sorted(self.variables, key=lambda x: x.base_name)],
+            columns=[x.base_name for x in sorted(self.shocks, key=lambda x: x.base_name)],
+        )
+
+        self.perturbation_solved = True
+
+    def _perturbation_setup(
+        self,
+        not_loglin_variables=None,
+        order=1,
+        model_is_linear=False,
+        verbose=True,
+        return_F_matrices=False,
+        tol=1e-8,
+    ):
+        """
+        This function is used to set up the perturbation matrices needed to simulate the model. It linearizes the model
+        around the steady state and constructs matrices A, B, C, and D needed to solve the system.
+
+        Parameters
+        ----------
+        not_loglin_variables: list of str
+            List of variables that should not be log-linearized. This is useful when a variable has a zero or negative
+             steady state value and cannot be log-linearized.
+        order: int
+            The order of the approximation. Currently only order 1 is implemented.
+        model_is_linear: bool
+            If True, assumes that the model is already linearized in the GCN file and directly
+             returns the matrices A, B, C, D.
+        verbose: bool
+            If True, prints warning messages.
+        return_F_matrices: bool
+            If True, returns the matrices A, B, C, D.
+        tol: float
+            The tolerance used to determine if a steady state value is close to zero.
+
+        Returns
+        -------
+        None or list of sympy matrices
+            If return_F_matrices is True, returns the F matrices. Otherwise, does not return anything.
+
+        """
+
+        free_param_dict = self.free_param_dict.copy()
+
+        parameters = list(free_param_dict.to_sympy().keys())
+        variables = list(self.steady_state_dict.to_sympy().keys())
+        params_to_calibrate = list(self.calib_param_dict.to_sympy().keys())
+
+        all_params = parameters + params_to_calibrate
+
+        shocks = self.shocks
+        shock_ss_dict = dict(zip([x.to_ss() for x in shocks], np.zeros(self.n_shocks)))
+        variables_and_shocks = self.variables + shocks
+        valid_names = [x.base_name for x in variables_and_shocks]
+
+        steady_state_dict = self.steady_state_dict.copy()
+
+        if not model_is_linear:
+            # We need shocks to be zero in A, B, C, D but 1 in T; can abuse the T_dummies to accomplish that.
+            if not_loglin_variables is None:
+                not_loglin_variables = []
+
+            not_loglin_variables += [x.base_name for x in shocks]
+
+            # Validate that all user-supplied variables are in the model
+            for variable in not_loglin_variables:
+                if variable not in valid_names:
+                    raise VariableNotFoundException(variable)
+
+            # Variables that are zero at the SS can't be log-linearized, check for these here.
+            close_to_zero_warnings = []
+            for variable in variables_and_shocks:
+                if variable.base_name in not_loglin_variables:
+                    continue
+
+                if abs(steady_state_dict[variable.to_ss().name]) < tol:
+                    not_loglin_variables.append(variable.base_name)
+                    close_to_zero_warnings.append(variable)
+
+            if len(close_to_zero_warnings) > 0 and verbose:
+                warn(
+                    "The following variables have steady state values close to zero and will not be log linearized: "
+                    + ", ".join(x.base_name for x in close_to_zero_warnings)
+                )
+
+        if order != 1:
+            raise NotImplementedError
+
+        if not self.steady_state_solved:
+            raise SteadyStateNotSolvedError()
+
+        if model_is_linear:
+            Fs = self.perturbation_solver.convert_linear_system_to_matrices()
+
+        else:
+            Fs = self.perturbation_solver.log_linearize_model(
+                not_loglin_variables=not_loglin_variables
+            )
+
+        Fs_subbed = [F.subs(shock_ss_dict) for F in Fs]
+        self.build_perturbation_matrices = numba_lambdify(
+            exog_vars=all_params, endog_vars=variables, expr=Fs_subbed
+        )
+
+        if return_F_matrices:
+            return Fs_subbed
+
+    def check_bk_condition(
+        self,
+        free_param_dict: Optional[Dict[str, float]] = None,
+        system_matrices: Optional[List[ArrayLike]] = None,
+        verbose: bool = True,
+        return_value: Optional[str] = "df",
+        tol=1e-8,
+    ) -> Optional[ArrayLike]:
+        """
+        Compute the generalized eigenvalues of system in the form presented in [1]. Per [2], the number of
+        unstable eigenvalues (|v| > 1) should not be greater than the number of forward-looking variables. Failing
+        this test suggests timing problems in the definition of the model.
+
+        Parameters
+        ----------
+        free_param_dict: dict, optional
+            A dictionary of parameter values. If None, the current stored values are used.
+        verbose: bool, default: True
+            Flag to print the results of the test, otherwise the eigenvalues are returned without comment.
+        return_value: string, default: 'eigenvalues'
+            Controls what is returned by the function. Valid values are 'df', 'flag', and None.
+            If df, a dataframe containing eigenvalues is returned. If 'bool', a boolean indicating whether the BK
+            condition is satisfied. If None, nothing is returned.
+        tol: float, 1e-8
+            Convergence tolerance for the gensys solver
+
+        Returns
+        -------
+
+        """
+        if self.build_perturbation_matrices is None:
+            raise PerturbationSolutionNotFoundException()
+
+        if free_param_dict is not None:
+            ss_dict, calib_dict = self.f_ss(free_param_dict)
+        else:
+            free_param_dict = self.free_param_dict
+            ss_dict = self.steady_state_dict
+            calib_dict = self.calib_param_dict
+
+        if system_matrices is not None:
+            A, B, C, D = system_matrices
+        else:
+            A, B, C, D = self.build_perturbation_matrices(
+                **ss_dict, **free_param_dict, **calib_dict
+            )
+        n_forward = (C.sum(axis=0) > 0).sum().astype(int)
+        n_eq, n_vars = A.shape
+
+        # TODO: Compute system eigenvalues -- avoids calling the whole Gensys routine, but there is code duplication
+        #   building Gamma_0 and Gamma_1
+        lead_var_idx = np.where(np.sum(np.abs(C), axis=0) > tol)[0]
+
+        eqs_and_leads_idx = np.r_[np.arange(n_vars), lead_var_idx + n_vars].tolist()
+
+        Gamma_0 = np.vstack([np.hstack([B, C]), np.hstack([-np.eye(n_eq), np.zeros((n_eq, n_eq))])])
+
+        Gamma_1 = np.vstack(
+            [
+                np.hstack([A, np.zeros((n_eq, n_eq))]),
+                np.hstack([np.zeros((n_eq, n_eq)), np.eye(n_eq)]),
+            ]
+        )
+        Gamma_0 = Gamma_0[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
+        Gamma_1 = Gamma_1[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
+
+        # A, B, Q, Z = qzdiv(1.01, *linalg.qz(-Gamma_0, Gamma_1, 'complex'))
+
+        # Using scipy instead of qzdiv appears to offer a huge speedup for nearly the same answer; some eigenvalues
+        # have sign flip relative to qzdiv -- does it matter?
+        A, B, alpha, beta, Q, Z = linalg.ordqz(-Gamma_0, Gamma_1, sort="ouc", output="complex")
+
+        gev = np.c_[np.diagonal(A), np.diagonal(B)]
+
+        eigenval = gev[:, 1] / (gev[:, 0] + tol)
+        pos_idx = np.where(np.abs(eigenval) > 0)
+        eig = np.zeros(((np.abs(eigenval) > 0).sum(), 3))
+        eig[:, 0] = np.abs(eigenval)[pos_idx]
+        eig[:, 1] = np.real(eigenval)[pos_idx]
+        eig[:, 2] = np.imag(eigenval)[pos_idx]
+
+        sorted_idx = np.argsort(eig[:, 0])
+        eig = pd.DataFrame(eig[sorted_idx, :], columns=["Modulus", "Real", "Imaginary"])
+
+        n_g_one = (eig["Modulus"] > 1).sum()
+        condition_not_satisfied = n_forward > n_g_one
+        if verbose:
+            print(
+                f"Model solution has {n_g_one} eigenvalues greater than one in modulus and {n_forward} "
+                f"forward-looking variables."
+                f'\nBlanchard-Kahn condition is{" NOT" if condition_not_satisfied else ""} satisfied.'
+            )
+
+        if return_value is None:
+            return
+
+        if return_value == "df":
+            return eig
+        elif return_value == "bool":
+            return ~condition_not_satisfied
+
+    def compute_stationary_covariance_matrix(self):
+        """
+        Compute the stationary covariance matrix of the solved system via fixed-point iteration. By construction, any
+        linearized DSGE model will have a fixed covariance matrix. In principle, a closed form solution is available
+        (we could solve a discrete Lyapunov equation) but this works fine.
+
+        Returns
+        -------
+        sigma: DataFrame
+        """
+        if not self.perturbation_solved:
+            raise PerturbationSolutionNotFoundException()
+
+        T, R = self.T, self.R
+
+        # TODO: Should this be R @ Q @ R.T ?
+        sigma = linalg.solve_discrete_lyapunov(T.values, R.values @ R.values.T)
+
+        return pd.DataFrame(sigma / 100, index=T.index, columns=T.index)
+
+    def compute_autocorrelation_matrix(self, n_lags=10):
+        """
+        Computes autocorrelations for each model variable using the stationary covariance matrix. See doc string for
+        compute_stationary_covariance_matrix for more information.
+
+        Parameters
+        ----------
+        n_lags: int
+            Number of lags over which to compute the autocorrelation
+
+        Returns
+        -------
+        acorr_mat: DataFrame
+        """
+        if not self.perturbation_solved:
+            raise PerturbationSolutionNotFoundException()
+
+        T, R = self.T, self.R
+
+        Sigma = linalg.solve_discrete_lyapunov(T.values, R.values @ R.values.T)
+        acorr_mat = _compute_autocorrelation_matrix(T.values, Sigma, n_lags=n_lags)
+
+        return pd.DataFrame(acorr_mat, index=T.index, columns=np.arange(n_lags))
+
+    def fit(
+        self,
+        data,
+        estimate_a0=False,
+        estimate_P0=False,
+        a0_prior=None,
+        P0_prior=None,
+        filter_type="univariate",
+        draws=5000,
+        n_walkers=36,
+        moves=None,
+        emcee_x0=None,
+        verbose=True,
+        return_inferencedata=True,
+        burn_in=None,
+        thin=None,
+        skip_initial_state_check=False,
+        **sampler_kwargs,
+    ):
+        """
+        Estimate model parameters via Bayesian inference. Parameter likelihood is computed using the Kalman filter.
+        Posterior distributions are estimated using Markov Chain Monte Carlo (MCMC), specifically the Affine-Invariant
+        Ensemble Sampler algorithm of [1].
+
+        A "traditional" Random Walk Metropolis can be achieved using the moves argument, but by default this function
+        will use a mix of two Differential Evolution (DE) proposal algorithms that have been shown to work well on
+        weakly multi-modal problems. DSGE estimation can be multi-modal in the sense that regions of the posterior
+        space are separated by the constraints on the ability to solve the perturbation problem.
+
+        This function will start all MCMC chains around random draws from the prior distribution. This is in contrast
+        to Dynare and gEcon.estimate, which start MCMC chains around the Maximum Likelihood estimate for parameter
+        values.
+
+        Parameters
+        ----------
+        data: dataframe
+            A pandas dataframe of observed values, with column names corresponding to DSGE model states names.
+        estimate_a0: bool, default: False
+            Whether to estimate the initial values of the DSGE process. If False, x0 will be deterministically set to
+            a vector of zeros, corresponding to the steady state. If True, you must provide a
+        estimate_P0: bool, default: False
+            Whether to estimate the intial covariance matrix of the DSGE process. If False, P0 will be set to the
+            Kalman Filter steady state value by solving the associated discrete Lyapunov equation.
+        a0_prior: dict, optional
+            A dictionary with (variable name, scipy distribution) key-value pairs. If a key "initial_vector" is found,
+            all other keys will be ignored, and the single distribution over all initial states will be used. Otherwise,
+            n_states independent distributions should be included in the dictionary.
+            If estimate_a0 is False, this will be ignored.
+        P0_prior: dict, optional
+            A dictionary with (variable name, scipy distribution) key-value pairs. If a key "initial_covariance" is
+            found, all other keys will be ignored, and this distribution will be taken as over the entire covariance
+            matrix. Otherwise, n_states independent distributions are expected, and are used to construct a diagonal
+            initial covariance matrix.
+        filter_type: string, default: "standard"
+            Select a kalman filter implementation to use. Currently "standard" and "univariate" are supported. Try
+            univariate if you run into errors inverting the P matrix during filtering.
+        draws: integer
+            Number of draws from each MCMC chain, or "walker" in the jargon of emcee.
+        n_walkers: integer
+            The number of "walkers", which roughly correspond to chains in other MCMC packages. Note that one needs
+            many more walkers than chains; [1] recommends as many as possible.
+        cores: integer
+            The number of processing cores, which is passed to Multiprocessing.Pool to do parallel inference. To
+            maintain detailed balance, the pool of walkers must be split, resulting in n_walkers / cores sub-ensembles.
+            Be sure to raise the number of walkers to compensate.
+        moves: List of emcee.moves objects
+            Moves tell emcee how to generate MCMC proposals. See the emcee docs for details.
+        emcee_x0: array
+            An (n_walkers, k_parameters) array of initial values. Emcee will check the condition number of the matrix
+            to ensure all walkers begin in different regions of the parameter space. If MLE estimates are used, they
+            should be jittered to start walkers in a ball around the desired initial point.
+        return_inferencedata: bool, default: True
+            If true, return an Arviz InferenceData object containing posterior samples. If False, the fitted Emcee
+            sampler is returned.
+        burn_in: int, optional
+            Number of initial samples to discard from all chains. This is ignored if return_inferencedata is False.
+        thin: int, optional
+            Return only every n-th sample from each chain. This is done to reduce storage requirements in highly
+            autocorrelated chains by discarding redundant information. Ignored if return_inferencedata is False.
+
+        Returns
+        -------
+        sampler, emcee.Sampler object
+            An emcee.Sampler object with the estimated posterior over model parameters, as well as other diagnotic
+            information.
+
+        References
+        -------
+        ..[1] Foreman-Mackey, Daniel, et al. “Emcee: The MCMC Hammer.” Publications of the Astronomical Society of the
+              Pacific, vol. 125, no. 925, Mar. 2013, pp. 306–12. arXiv.org, https://doi.org/10.1086/670067.
+        """
+        observed_vars = data.columns.tolist()
+        model_var_names = [x.base_name for x in self.variables]
+        if not all([x in model_var_names for x in observed_vars]):
+            orphans = [x for x in observed_vars if x not in model_var_names]
+            raise ValueError(
+                f"Columns of data must correspond to states of the DSGE model. Found the following columns"
+                f'with no associated model state: {", ".join(orphans)}'
+            )
+
+        sparse_data = extract_sparse_data_from_model(self)
+        prior_dict = extract_prior_dict(self)
+
+        if estimate_a0 is False:
+            a0 = None
+        else:
+            if a0_prior is None:
+                raise ValueError(
+                    "If estimate_a0 is True, you must provide a dictionary of prior distributions for"
+                    "the initial values of all individual states"
+                )
+            if not all([var in a0_prior.keys() for var in model_var_names]):
+                missing_keys = set(model_var_names) - set(list(a0_prior.keys()))
+                raise ValueError(
+                    "You must provide one key for each state in the model. "
+                    f'No keys found for: {", ".join(missing_keys)}'
+                )
+            for var in model_var_names:
+                prior_dict[f"{var}__initial"] = a0_prior[var]
+
+        moves = moves or [
+            (emcee.moves.DEMove(), 0.6),
+            (emcee.moves.DESnookerMove(), 0.4),
+        ]
+
+        shock_names = [x.base_name for x in self.shocks]
+
+        k_params = len(prior_dict)
+        Z = build_Z_matrix(observed_vars, model_var_names)
+
+        args = [
+            data,
+            sparse_data,
+            Z,
+            prior_dict,
+            shock_names,
+            observed_vars,
+            filter_type,
+        ]
+        arg_names = [
+            "observed_data",
+            "sparse_data",
+            "Z",
+            "prior_dict",
+            "shock_names",
+            "observed_vars",
+            "filter_type",
+        ]
+
+        if emcee_x0:
+            x0 = emcee_x0
+        else:
+            x0 = np.stack([x.rvs(n_walkers) for x in prior_dict.values()]).T
+
+        param_names = list(prior_dict.keys())
+
+        sampler = emcee.EnsembleSampler(
+            n_walkers,
+            k_params,
+            evaluate_logp,
+            args=args,
+            moves=moves,
+            parameter_names=param_names,
+            **sampler_kwargs,
+        )
+
+        _ = sampler.run_mcmc(
+            x0,
+            draws,
+            progress=verbose,
+            skip_initial_state_check=skip_initial_state_check,
+        )
+
+        if return_inferencedata:
+            sampler_stats = xr.Dataset(
+                data_vars=dict(
+                    acceptance_fraction=(["chain"], sampler.acceptance_fraction),
+                    autocorrelation_time=(
+                        ["parameters"],
+                        sampler.get_autocorr_time(discard=burn_in or 0, quiet=True),
+                    ),
+                ),
+                coords=dict(chain=np.arange(n_walkers), parameters=param_names),
+            )
+
+            idata = az.from_emcee(
+                sampler,
+                var_names=param_names,
+                blob_names=["log_likelihood"],
+                arg_names=arg_names,
+            )
+
+            idata["sample_stats"].update(sampler_stats)
+            idata.observed_data = idata.observed_data.drop(["sparse_data", "prior_dict"])
+            idata.observed_data = idata.observed_data.drop_dims(
+                ["sparse_data_dim_0", "sparse_data_dim_1", "prior_dict_dim_0"]
+            )
+
+            return idata.sel(draw=slice(burn_in, None, thin))
+
+        return sampler
+
+    def sample_param_dict_from_prior(
+        self, n_samples=1, seed=None, param_subset=None, sample_shock_sigma=False
+    ):
+
+        """
+        Sample parameters from the parameter prior distributions.
+
+        Parameters
+        ----------
+        n_samples: int, default: 1
+            Number of samples to draw from the prior distributions.
+        seed: int, default: None
+            Seed for the random number generator.
+        param_subset: list, default: None
+            List of parameter names to sample. If None, all parameters are sampled.
+        sample_shock_sigma: bool, default: False
+            If True, also sample the shock standard deviations.
+
+        Returns
+        -------
+        new_param_dict: dict
+            Dictionary of sampled parameters.
+        """
+
+        if sample_shock_sigma:
+            shock_priors = {k: v.rv_params["scale"] for k, v in self.shock_priors.items()}
+        else:
+            shock_priors = self.shock_priors
+
+        all_priors = merge_dictionaries(
+            self.param_priors, shock_priors, self.observation_noise_priors
+        )
+
+        if param_subset is None:
+            n_variables = len(all_priors)
+            priors_to_sample = all_priors
+        else:
+            n_variables = len(param_subset)
+            priors_to_sample = {k: v for k, v in all_priors.items() if k in param_subset}
+
+        if seed is not None:
+            seed_sequence = np.random.SeedSequence(seed)
+            child_seeds = seed_sequence.spawn(n_variables)
+            streams = [np.random.default_rng(s) for s in child_seeds]
+        else:
+            streams = [None] * n_variables
+
+        new_param_dict = {}
+        for i, (key, d) in enumerate(priors_to_sample.items()):
+            new_param_dict[key] = d.rvs(size=n_samples, random_state=streams[i])
+
+        return new_param_dict
+
+    def impulse_response_function(self, simulation_length: int = 40, shock_size: float = 1.0):
+        """
+        Compute the impulse response functions of the model.
+
+        Parameters
+        ----------
+        simulation_length : int, optional
+            The number of periods to compute the IRFs over. The default is 40.
+        shock_size : float, optional
+            The size of the shock. The default is 1.0.
+
+        Returns
+        -------
+        pandas.DataFrame
+            The IRFs for each variable in the model. The DataFrame has a multi-index
+            with the variable names as the first level and the timestep as the second.
+            The columns are the shocks.
+
+        Raises
+        ------
+        PerturbationSolutionNotFoundException
+            If a perturbation solution has not been found.
+        """
+
+        if not self.perturbation_solved:
+            raise PerturbationSolutionNotFoundException()
+
+        T, R = self.T, self.R
+
+        timesteps = simulation_length
+
+        data = np.zeros((self.n_variables, timesteps, self.n_shocks))
+
+        for i in range(self.n_shocks):
+            shock_path = np.zeros((self.n_shocks, timesteps))
+            shock_path[i, 0] = shock_size
+
+            for t in range(1, timesteps):
+                stochastic = R.values @ shock_path[:, t - 1]
+                deterministic = T.values @ data[:, t - 1, i]
+                data[:, t, i] = deterministic + stochastic
+
+        index = pd.MultiIndex.from_product(
+            [R.index, np.arange(timesteps), R.columns],
+            names=["Variables", "Time", "Shocks"],
+        )
+
+        df = (
+            pd.DataFrame(data.ravel(), index=index, columns=["Values"])
+            .unstack([1, 2])
+            .droplevel(axis=1, level=0)
+            .sort_index(axis=1)
+        )
+
+        return df
+
+    def simulate(
+        self,
+        simulation_length: int = 40,
+        n_simulations: int = 100,
+        shock_dict: Optional[Dict[str, float]] = None,
+        shock_cov_matrix: Optional[ArrayLike] = None,
+        show_progress_bar: bool = False,
+    ):
+
+        """
+        Simulate the model over a certain number of time periods.
+
+        Parameters
+        ----------
+        simulation_length : int, optional(default=40)
+            The number of time periods to simulate.
+        n_simulations : int, optional(default=100)
+            The number of simulations to run.
+        shock_dict : dict, optional(default=None)
+            Dictionary of shocks to use.
+        shock_cov_matrix : arraylike, optional(default=None)
+            Covariance matrix of shocks to use.
+        show_progress_bar : bool, optional(default=False)
+            Whether to show a progress bar for the simulation.
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            The simulated data.
+        """
+
+        if not self.perturbation_solved:
+            raise PerturbationSolutionNotFoundException()
+
+        T, R = self.T, self.R
+        timesteps = simulation_length
+
+        n_shocks = R.shape[1]
+
+        if shock_cov_matrix is not None:
+            assert shock_cov_matrix.shape == (
+                n_shocks,
+                n_shocks,
+            ), f"The shock covariance matrix should have shape {n_shocks} x {n_shocks}"
+            d = stats.multivariate_normal(mean=np.zeros(n_shocks), cov=shock_cov_matrix)
+            epsilons = np.r_[[d.rvs(timesteps) for _ in range(n_simulations)]]
+
+        elif shock_dict is not None:
+            epsilons = np.zeros((n_simulations, timesteps, n_shocks))
+            for i, shock in enumerate(self.shocks):
+                if shock.base_name in shock_dict.keys():
+                    d = stats.norm(loc=0, scale=shock_dict[shock.base_name])
+                    epsilons[:, :, i] = np.r_[[d.rvs(timesteps) for _ in range(n_simulations)]]
+
+        elif all([shock.base_name in self.shock_priors.keys() for shock in self.shocks]):
+            epsilons = np.zeros((n_simulations, timesteps, n_shocks))
+            for i, d in enumerate(self.shock_priors.values()):
+                epsilons[:, :, i] = np.r_[[d.rvs(timesteps) for _ in range(n_simulations)]]
+
+        else:
+            raise ValueError(
+                "To run a simulation, supply either a full covariance matrix, a dictionary of shocks and"
+                "standard deviations, or specify priors on the shocks in your GCN file."
+            )
+
+        data = np.zeros((self.n_variables, timesteps, n_simulations))
+        if epsilons.ndim == 2:
+            epsilons = epsilons[:, :, None]
+
+        progress_bar = ProgressBar(timesteps - 1, verb="Sampling")
+
+        for t in range(1, timesteps):
+            progress_bar.start()
+            stochastic = np.einsum("ij,sj", R.values, epsilons[:, t - 1, :])
+            deterministic = T.values @ data[:, t - 1, :]
+            data[:, t, :] = deterministic + stochastic
+
+            if show_progress_bar:
+                progress_bar.stop()
+
+        index = pd.MultiIndex.from_product(
+            [R.index, np.arange(timesteps), np.arange(n_simulations)],
+            names=["Variables", "Time", "Simulation"],
+        )
+        df = (
+            pd.DataFrame(data.ravel(), index=index, columns=["Values"])
+            .unstack([1, 2])
+            .droplevel(axis=1, level=0)
+        )
+
+        return df
+
+    def _build_prior_dict(self, prior_dict: Dict[str, str]) -> None:
+        """
+        Parameters
+        ----------
+        prior_dict: dict
+            Dictionary of variable_name: distribution_string pairs, prepared by the parse_gcn function.
+
+        Returns
+        -------
+        self.param_dict: dict
+            Dictionary of variable:distribution pairs. Distributions are scipy rv_frozen objects, unless the
+            distribution is parameterized by another distribution, in which case a "CompositeDistribution" object
+            with methods .rvs, .pdf, and .logpdf is returned.
+        """
+
+        priors = create_prior_distribution_dictionary(prior_dict)
+        hyper_parameters = set(prior_dict.keys()) - set(priors.keys())
+
+        # Clean up the hyper-parameters (e.g. shock stds) from the model, they aren't needed anymore
+        for parameter in hyper_parameters:
+            del self.free_param_dict[parameter]
+
+        param_priors = SymbolDictionary()
+        shock_priors = SymbolDictionary()
+        for key, value in priors.items():
+            sympy_key = single_symbol_to_sympy(key, assumptions=self.assumptions)
+            if isinstance(sympy_key, TimeAwareSymbol):
+                shock_priors[sympy_key.base_name] = value
+            else:
+                param_priors[sympy_key.name] = value
+
+        self.param_priors = param_priors
+        self.shock_priors = shock_priors
+
+    def _build_model_blocks(self, parsed_model, simplify_blocks: bool):
+        """
+        Builds blocks of the gEconpy model using strings parsed from the GCN file.
+
+        Parameters
+        ----------
+        parsed_model : str
+            The GCN model as a string.
+        simplify_blocks : bool
+            Whether to try to simplify equations or not.
+        """
+
+        raw_blocks = gEcon_parser.split_gcn_into_block_dictionary(parsed_model)
+
+        self.options = raw_blocks["options"]
+        self.try_reduce_vars = raw_blocks["tryreduce"]
+        self.assumptions = raw_blocks["assumptions"]
+
+        del raw_blocks["options"]
+        del raw_blocks["tryreduce"]
+        del raw_blocks["assumptions"]
+
+        self._get_steady_state_equations(raw_blocks)
+
+        for block_name, block_content in raw_blocks.items():
+            block_dict = gEcon_parser.parsed_block_to_dict(block_content)
+            block = Block(name=block_name, block_dict=block_dict, assumptions=self.assumptions)
+            block.solve_optimization(try_simplify=simplify_blocks)
+
+            self.blocks[block.name] = block
+
+        self.n_blocks = len(self.blocks)
+
+    def _get_all_block_equations(self) -> None:
+        """
+        Extract all equations from the blocks in the model.
+
+        Parameters
+        ----------
+        self : `Model`
+            The model object whose block system equations will be extracted.
+
+        Returns
+        -------
+        None
+
+        Notes
+        -----
+        Updates the `system_equations` attribute of `self` with the extracted equations.
+        Also updates the `n_equations` attribute of `self` with the number of extracted equations.
+        """
+
+        _, blocks = unpack_keys_and_values(self.blocks)
+        for block in blocks:
+            self.system_equations.extend(block.system_equations)
+        self.n_equations = len(self.system_equations)
+
+    def _get_all_block_parameters(self) -> None:
+        """
+        Extract all parameters from all blocks and store them in the model's free_param_dict attribute. The
+        `free_param_dict` attribute is updated in place.
+        """
+
+        _, blocks = unpack_keys_and_values(self.blocks)
+        for block in blocks:
+            self.free_param_dict = self.free_param_dict | block.param_dict
+
+        self.free_param_dict = self.free_param_dict.sort_keys().to_string().values_to_float()
+
+    def _get_all_block_params_to_calibrate(self) -> None:
+        """
+        Retrieve the list of parameters to calibrate and the list of
+        equations used to calibrate the parameters from each block of
+        the model.
+        """
+        _, blocks = unpack_keys_and_values(self.blocks)
+        for block in blocks:
+            if block.params_to_calibrate is None:
+                continue
+
+            if len(self.params_to_calibrate) == 0:
+                self.params_to_calibrate = block.params_to_calibrate
+            else:
+                self.params_to_calibrate.extend(block.params_to_calibrate)
+
+            if block.calibrating_equations is None:
+                continue
+
+            if len(self.calibrating_equations) == 0:
+                self.calibrating_equations = block.calibrating_equations
+            else:
+                self.calibrating_equations.extend(block.calibrating_equations)
+
+        alpha_sort_idx = np.argsort([x.name for x in self.params_to_calibrate])
+        self.params_to_calibrate = [self.params_to_calibrate[i] for i in alpha_sort_idx]
+        self.calibrating_equations = [self.calibrating_equations[i] for i in alpha_sort_idx]
+
+        self.n_calibrating_equations = len(self.calibrating_equations)
+        self.n_params_to_calibrate = len(self.params_to_calibrate)
+
+    def _get_all_block_deterministic_parameters(self) -> None:
+        _, blocks = unpack_keys_and_values(self.blocks)
+        for block in blocks:
+            if block.deterministic_params is None:
+                continue
+
+            if len(self.deterministic_params) == 0:
+                self.deterministic_params = block.deterministic_params
+            else:
+                self.deterministic_params.extend(block.deterministic_params)
+
+            if block.deterministic_relationships is None:
+                continue
+
+            if len(self.deterministic_relationships) == 0:
+                self.deterministic_relationships = block.deterministic_relationships
+            else:
+                self.deterministic_relationships.extend(block.deterministic_relationships)
+
+        alpha_sort_idx = np.argsort([x.name for x in self.deterministic_params])
+        self.deterministic_params = [self.deterministic_params[i] for i in alpha_sort_idx]
+        self.deterministic_relationships = [
+            self.deterministic_relationships[i] for i in alpha_sort_idx
+        ]
+
+    def _get_variables_and_shocks(self) -> None:
+        """
+        Collect all variables and shocks from the blocks and set their counts.
+
+        This method is called after the blocks have been processed. It collects all the shocks and variables from the
+        blocks, sorts them, and sets the n_shocks and n_variables properties.
+        """
+
+        all_shocks = []
+        _, blocks = unpack_keys_and_values(self.blocks)
+
+        for block in blocks:
+            if block.shocks is not None:
+                all_shocks.extend([x for x in block.shocks])
+        self.shocks = all_shocks
+        self.n_shocks = len(all_shocks)
+
+        for eq in self.system_equations:
+            atoms = eq.atoms()
+            variables = [x for x in atoms if is_variable(x)]
+            for variable in variables:
+                if variable.set_t(0) not in self.variables and variable not in all_shocks:
+                    self.variables.append(variable.set_t(0))
+
+        self.n_variables = len(self.variables)
+
+        self.variables = sorted(self.variables, key=lambda x: x.name)
+        self.shocks = sorted(self.shocks, key=lambda x: x.name)
+
+    def _get_steady_state_equations(self, raw_blocks: Dict[str, List[str]]):
+        """
+        Extract user-provided steady state equations from the `raw_blocks` dictionary and store the resulting
+        relationships in self.steady_state_relationships.
+
+        Parameters
+        ----------
+        raw_blocks : dict
+            Dictionary of block names and block contents extracted from a gEcon model.
+
+        Raises
+        ------
+        MultipleSteadyStateBlocksException
+            If there is more than one block in `raw_blocks` with a name from `STEADY_STATE_NAMES`.
+        """
+
+        block_names = raw_blocks.keys()
+        ss_block_names = [name for name in block_names if name in STEADY_STATE_NAMES]
+        n_ss_blocks = len(ss_block_names)
+
+        if n_ss_blocks == 0:
+            return
+        if n_ss_blocks > 1:
+            raise MultipleSteadyStateBlocksException(ss_block_names)
+
+        block_content = raw_blocks[ss_block_names[0]]
+        block_dict = gEcon_parser.parsed_block_to_dict(block_content)
+        block = Block(name="steady_state", block_dict=block_dict, assumptions=self.assumptions)
+
+        sub_dict = SymbolDictionary()
+        steady_state_dict = SymbolDictionary()
+
+        if block.definitions is not None:
+            _, definitions = unpack_keys_and_values(block.definitions)
+            sub_dict = SymbolDictionary({eq.lhs: eq.rhs for eq in definitions})
+
+        if block.identities is not None:
+            _, identities = unpack_keys_and_values(block.identities)
+            for eq in identities:
+                subbed_rhs = eq.rhs.subs(sub_dict)
+                steady_state_dict[eq.lhs] = subbed_rhs
+                sub_dict[eq.lhs] = subbed_rhs
+
+        for k, eq in steady_state_dict.items():
+            steady_state_dict[k] = eq.subs(steady_state_dict)
+
+        self.steady_state_relationships = (
+            steady_state_dict.sort_keys().to_string().values_to_float()
+        )
+
+        del raw_blocks[ss_block_names[0]]
+
+    def _try_reduce(self):
+        """
+        Attempt to reduce the number of equations in the system by removing equations requested in the `tryreduce`
+        block of the GCN file. Equations are considered safe to remove if they are "self-contained" that is, if
+        no other variables depend on their values.
+
+        Returns
+        -------
+        list
+            The names of the variables that were removed. If reduction was not possible, None is returned.
+        """
+
+        if self.try_reduce_vars is None:
+            return
+
+        self.try_reduce_vars = [
+            single_symbol_to_sympy(x, self.assumptions) for x in self.try_reduce_vars
+        ]
+
+        variables = self.variables
+        n_variables = self.n_variables
+
+        occurrence_matrix = np.zeros((n_variables, n_variables))
+        reduced_system = []
+
+        for i, eq in enumerate(self.system_equations):
+            for j, var in enumerate(self.variables):
+                if any([x in eq.atoms() for x in make_all_var_time_combos([var])]):
+                    occurrence_matrix[i, j] += 1
+
+        # Columns with a sum of 1 are variables that appear only in a single equations; these equations can be deleted
+        # without consequence w.r.t solving the system.
+
+        isolated_variables = np.array(variables)[occurrence_matrix.sum(axis=0) == 1]
+        to_remove = set(isolated_variables).intersection(set(self.try_reduce_vars))
+
+        for eq in self.system_equations:
+            if not any([var in eq.atoms() for var in to_remove]):
+                reduced_system.append(eq)
+
+        self.system_equations = reduced_system
+        self.n_equations = len(self.system_equations)
+
+        self.variables = {
+            atom.set_t(0) for eq in reduced_system for atom in eq.atoms() if is_variable(atom)
+        }
+        self.variables -= set(self.shocks)
+        self.variables = sorted(list(self.variables), key=lambda x: x.name)
+        self.n_variables = len(self.variables)
+
+        if self.n_equations != self.n_variables:
+            warn("Reduction was requested but not possible because the system is not well defined.")
+            return
+
+        eliminated_vars = [var.name for var in variables if var not in self.variables]
+
+        return eliminated_vars
+
+    def _simplify_singletons(self):
+        """
+        Simplify the system by removing variables that are deterministically defined as a known value. Common examples
+        include P[] = 1, setting the price level of the economy as the numeraire, or B[] = 0, putting the bond market
+        in net-zero supply.
+
+        In these cases, the variable can be replaced by the deterministic value after all FoC
+        have been computed.
+
+        Returns
+        -------
+        eliminated_vars : List[str]
+            The names of the variables that were removed.
+        """
+
+        system = self.system_equations
+
+        variables = self.variables
+        reduce_dict = {}
+
+        for eq in system:
+            if len(eq.atoms()) < 4:
+                var = [x for x in eq.atoms() if is_variable(x)]
+                if len(var) != 1:
+                    continue
+                var = var[0]
+                sub_dict = expand_subs_for_all_times(sp.solve(eq, var, dict=True)[0])
+                reduce_dict.update(sub_dict)
+
+        reduced_system = substitute_all_equations(system, reduce_dict)
+        reduced_system = [eq for eq in reduced_system if eq != 0]
+
+        self.system_equations = reduced_system
+        self.n_equations = len(reduced_system)
+
+        self.variables = {
+            atom.set_t(0) for eq in reduced_system for atom in eq.atoms() if is_variable(atom)
+        }
+        self.variables -= set(self.shocks)
+        self.variables = sorted(list(self.variables), key=lambda x: x.name)
+        self.n_variables = len(self.variables)
+
+        if self.n_equations != self.n_variables:
+            warn(
+                "Simplification was requested but not possible because the system is not well defined."
+            )
+            return
+
+        eliminated_vars = [var.name for var in variables if var not in self.variables]
+
+        return eliminated_vars
+
+    def _make_deterministic_substitutions(self):
+        if self.deterministic_params is None:
+            return
+
+        all_atoms = reduce(
+            lambda left, right: left.union(right), [eq.atoms() for eq in self.system_equations]
+        )
+
+        if not any([det_var in all_atoms for det_var in self.deterministic_params]):
+            return
+
+        det_sub_dict = dict(zip(self.deterministic_params, self.deterministic_relationships))
+
+        # recursively substitute the dictionary on itself, in case there are any relationships between the relationships
+        for i in range(5):
+            all_atoms = reduce(
+                lambda left, right: left.union(right), [eq.atoms() for eq in det_sub_dict.values()]
+            )
+            if any([det_param in all_atoms for det_param in self.deterministic_params]):
+                det_sub_dict = substitute_all_equations(det_sub_dict, det_sub_dict)
+            else:
+                break
+        if i == 5:
+            raise ValueError(
+                "Could not reduce deterministic relationships to functions of only free parameters after"
+                "five recursive substitutions. Check that there are not circular definitions among the"
+                "deterministic parameters."
+            )
+
+        self.system_equations = [eq.subs(det_sub_dict) for eq in self.system_equations]
+
+
+# #@njit
+# def _compute_stationary_covariance_matrix(A, C, tol=1e-9, max_iter=10_000):
+#     sigma = np.eye(A.shape[0])
+#     for _ in range(max_iter):
+#         new_sigma = A @ sigma @ A.T + C @ C.T
+#         if ((sigma - new_sigma) ** 2).mean() < tol:
+#             return sigma
+#         else:
+#             sigma = new_sigma
+
+
+@nb.njit(cache=True)
+def _compute_autocorrelation_matrix(A, sigma, n_lags=5):
+    """Compute the autocorrelation matrix for the given state-space model.
+
+    Parameters
+    ----------
+    A : ndarray
+        An array of shape (n_endog, n_endog, n_lags) representing the transition matrix of the
+        state-space system.
+    sigma : ndarray
+        An array of shape (n_endog, n_endog) representing the variance-covariance matrix of the errors of
+        the transition equation.
+    n_lags : int, optional
+        The number of lags for which to compute the autocorrelation matrix.
+
+    Returns
+    -------
+    acov : ndarray
+        An array of shape (n_endog, n_lags) representing the autocorrelation matrix of the state-space process.
+    """
+
+    acov = np.zeros((A.shape[0], n_lags))
+    acov_factor = np.eye(A.shape[0])
+    for i in range(n_lags):
+        cov = acov_factor @ sigma
+        acov[:, i] = np.diag(cov) / np.diag(sigma)
+        acov_factor = A @ acov_factor
+
+    return acov
```

### Comparing `gEconpy-1.1.0/gEconpy/classes/transformers.py` & `gEconpy-1.2.0/gEconpy/classes/transformers.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,77 +1,77 @@
-from abc import ABC
-
-import numpy as np
-import sympy as sp
-from scipy.special import expit
-
-
-def softplus(x):
-    return np.log(1 + np.exp(x))
-
-
-class Transformer(ABC):
-    def constrain(self, x):
-        raise NotImplementedError
-
-    def unconstrain(self, x):
-        raise NotImplementedError
-
-    def jac_det(self, x):
-        raise NotImplementedError
-
-    def sp_jac_det(self, x):
-        raise NotImplementedError
-
-
-class IdentityTransformer(Transformer):
-    def constrain(self, x):
-        return x
-
-    def unconstrain(self, x):
-        return x
-
-    def jac_det(self, x):
-        return 1
-
-    def sp_jac_det(self, x):
-        return 1
-
-
-class PositiveTransformer(Transformer):
-    def constrain(self, x):
-        return x**2
-
-    def unconstrain(self, x):
-        return x**0.5
-
-    def jac_det(self, x):
-        return 2 * x
-
-    def sp_jac_det(self, x):
-        return 2 * x
-
-
-class IntervalTransformer(Transformer):
-    def __init__(self, low=0, high=1, slope=1):
-        self.low = low
-        self.high = high
-        self.slope = slope
-        self.eps = 1e-8
-
-    def constrain(self, x):
-        sigmoid_x = expit(self.slope * x)
-        return sigmoid_x * self.high + (1 - sigmoid_x) * self.low
-        # low, high, k, eps = self.low, self.high, self.slope, self.eps
-        # return low + (high - low) / (1 + np.exp(-k * x))
-
-    def unconstrain(self, x):
-        low, high, k, eps = self.low, self.high, self.slope, self.eps
-        return np.log((x - low + eps) / (high - x + eps)) / k
-
-        # return np.log(x - self.low) - np.log(self.high - x)
-
-    def jac_det(self, x):
-        return (self.high - self.low) * np.exp(-x) / (1 + np.exp(-x)) ** 2
-
-    def sp_jac_def(self, x):
-        return (self.high - self.low) * sp.exp(-x) / (1 + sp.exp(-x)) ** 2
+from abc import ABC
+
+import numpy as np
+import sympy as sp
+from scipy.special import expit
+
+
+def softplus(x):
+    return np.log(1 + np.exp(x))
+
+
+class Transformer(ABC):
+    def constrain(self, x):
+        raise NotImplementedError
+
+    def unconstrain(self, x):
+        raise NotImplementedError
+
+    def jac_det(self, x):
+        raise NotImplementedError
+
+    def sp_jac_det(self, x):
+        raise NotImplementedError
+
+
+class IdentityTransformer(Transformer):
+    def constrain(self, x):
+        return x
+
+    def unconstrain(self, x):
+        return x
+
+    def jac_det(self, x):
+        return 1
+
+    def sp_jac_det(self, x):
+        return 1
+
+
+class PositiveTransformer(Transformer):
+    def constrain(self, x):
+        return x**2
+
+    def unconstrain(self, x):
+        return x**0.5
+
+    def jac_det(self, x):
+        return 2 * x
+
+    def sp_jac_det(self, x):
+        return 2 * x
+
+
+class IntervalTransformer(Transformer):
+    def __init__(self, low=0, high=1, slope=1):
+        self.low = low
+        self.high = high
+        self.slope = slope
+        self.eps = 1e-8
+
+    def constrain(self, x):
+        sigmoid_x = expit(self.slope * x)
+        return sigmoid_x * self.high + (1 - sigmoid_x) * self.low
+        # low, high, k, eps = self.low, self.high, self.slope, self.eps
+        # return low + (high - low) / (1 + np.exp(-k * x))
+
+    def unconstrain(self, x):
+        low, high, k, eps = self.low, self.high, self.slope, self.eps
+        return np.log((x - low + eps) / (high - x + eps)) / k
+
+        # return np.log(x - self.low) - np.log(self.high - x)
+
+    def jac_det(self, x):
+        return (self.high - self.low) * np.exp(-x) / (1 + np.exp(-x)) ** 2
+
+    def sp_jac_def(self, x):
+        return (self.high - self.low) * sp.exp(-x) / (1 + sp.exp(-x)) ** 2
```

### Comparing `gEconpy-1.1.0/gEconpy/estimation/estimate.py` & `gEconpy-1.2.0/gEconpy/estimation/estimate.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,272 +1,272 @@
-from typing import Any, Dict, List, Optional, Tuple
-
-import numpy as np
-from numpy.typing import ArrayLike
-from scipy import stats
-
-from gEconpy.estimation.estimation_utilities import (
-    build_system_matrices,
-    check_bk_condition,
-    check_finite_matrix,
-    split_random_variables,
-)
-from gEconpy.estimation.kalman_filter import kalman_filter
-from gEconpy.solvers.cycle_reduction import cycle_reduction, solve_shock_matrix
-
-
-def build_and_solve(
-    param_dict: dict, sparse_datas: List, vars_to_estimate: Optional[List] = None
-) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:
-    """
-    A collection of functionality already in the gEcon model object, extracted for speed and memory optimizations
-    when doing parallel fitting. Specifically, this function avoids the need of passing around the (potentially large)
-    model object when repeatedly solving the perturbation problem for policy matrices T and R.
-
-    Parameters
-    ----------
-    param_dict: dictionary of string keys float values
-        A dictionary that maps parameters to be estimated to point values.
-    sparse_datas: list of tuples
-        A list of the equations and CSR indicies needed to construct the A, B, C, and D matrices necessary to solve for
-        T and R.
-    vars_to_estimate: list of strings, default: None
-        The subset of variables to be estimated for. When None, the variable list is assumed to be those assigned
-        priors in the GCN file. This should only be used for debugging.
-
-    Returns
-    -------
-    T: array
-        The "policy matrix" describing how the linear system evolves with time
-    R: array
-        The "selection matrix" describing how exogenous shocks enter into the linear system
-    success: bool
-        A flag indicating whether the system has been successfully solved. This encodes three conditions: successful
-        converge of the perturbation algorithm, the size of the deterministic and stochastic norms of the
-        linear solution, and the blanchard-khan conditions.
-
-    TODO: njit this function by figuring out how to get rid of the sympy lambdify functions inside sparse_datas
-    """
-
-    res = build_system_matrices(param_dict, sparse_datas, vars_to_estimate=vars_to_estimate)
-    A, B, C, D = res
-
-    if not all([check_finite_matrix(x) for x in res]):
-        T = np.zeros_like(A)
-        R = np.zeros((T.shape[0], 1))
-        success = False
-        return T, R, success
-
-    bk_condition_met = check_bk_condition(A, B, C, tol=1e-8)
-
-    try:
-        T, result, log_norm = cycle_reduction(A, B, C, 1000, 1e-8, False)
-        R = solve_shock_matrix(B, C, D, T)
-    except np.linalg.LinAlgError:
-        T = np.zeros_like(A)
-        R = np.zeros((T.shape[0], 1))
-        success = False
-        return T, R, success
-
-    success = (result == "Optimization successful") & (log_norm < 1e-8) & bk_condition_met
-
-    T = np.ascontiguousarray(T)
-    R = np.ascontiguousarray(R)
-
-    return T, R, success
-
-
-def build_Z_matrix(obs_variables: List[str], state_variables: List[str]) -> np.ndarray:
-    """Constructs the design matrix Z for a state-space system.
-
-    Parameters
-    ----------
-    obs_variables : List[str]
-        The names of the observed variables.
-    state_variables : List[str]
-        The names of the state variables.
-
-    Returns
-    -------
-    Z : np.ndarray
-        The design matrix Z.
-    """
-
-    Z = np.array([[x == var for x in state_variables] for var in obs_variables], dtype="float64")
-    return Z
-
-
-def build_Q_and_H(
-    state_sigmas: Dict[str, float],
-    shock_variables: List[str],
-    obs_variables: List[str],
-    obs_sigmas: Optional[Dict[str, float]] = None,
-) -> Tuple[np.ndarray, np.ndarray]:
-    """
-    Build the Q and H matrices for a state-space system.
-
-    Parameters
-    ----------
-    state_sigmas : Dict[str, float]
-        A dictionary of variances associated with shocks in the state-space system.
-    shock_variables : List[str]
-        A list of strings representing shocks.
-    obs_variables : List[str]
-        A list of strings representing the observed variables.
-    obs_sigmas : Optional[Dict[str, float]]
-        A dictionary of variances associated with observed variables. If not provided, all variances are set to 0.
-
-    Returns
-    -------
-    Tuple[np.ndarray, np.ndarray]
-        A tuple containing the Q and H matrices.
-    """
-
-    k_posdef = len(shock_variables)
-    k_obs = len(obs_variables)
-
-    obs_sigmas = obs_sigmas or {}
-
-    i = 0
-    Q = np.zeros((k_posdef, k_posdef))
-    for v in shock_variables:
-        if v in state_sigmas.keys():
-            Q[i, i] = state_sigmas[v]
-        i += 1
-
-    i = 0
-    H = np.zeros((k_obs, k_obs))
-    for v in obs_variables:
-        if v in obs_sigmas.keys():
-            H[i, i] = obs_sigmas[v]
-        i += 1
-
-    Q = np.ascontiguousarray(Q)
-    H = np.ascontiguousarray(H)
-
-    return Q, H
-
-
-def evaluate_prior_logp(
-    all_param_dict: Dict[str, float], prior_dict: Dict[str, stats.rv_continuous]
-) -> float:
-    """
-    Evaluate the log probability density function (PDF) of the prior distribution for a given set of parameters.
-
-    Parameters
-    ----------
-    all_param_dict : dict
-        A dictionary containing the parameters (float values) for which the prior log PDF is to be evaluated.
-    prior_dict : dict
-        A dictionary containing the prior distributions (scipy.stats continuous random variables) for each parameter.
-
-    Returns
-    -------
-    float
-        The log probability of the parameters under the prior distribution.
-    """
-    ll = 0
-
-    for k, v in all_param_dict.items():
-        ll += prior_dict[k].logpdf(v).sum()
-
-    return ll
-
-
-def split_param_dict(
-    all_param_dict: Dict[str, float]
-) -> Tuple[Dict[str, float], Dict[str, float], Dict[str, float]]:
-    """
-    Split a dictionary of parameters into three dictionaries based on their keys.
-
-    Parameters
-    ----------
-    all_param_dict : dict
-        A dictionary of parameters to be split.
-
-    Returns
-    -------
-    tuple
-        A tuple containing
-        (1) a dictionary of deep parameters,
-        (2) a numpy array containing the initial conditions for the state vector, and
-        (3) a numpy array containing the initial conditions for the state variance-covariance matrix.
-    """
-    param_dict = {}
-    a0_dict = {}
-    P0_dict = {}
-
-    initial_names = [x for x in all_param_dict.keys() if x.endswith("__initial")]
-    initial_cov_names = [x for x in all_param_dict.keys() if x.endswith("__initial_cov")]
-
-    for k, v in all_param_dict.items():
-        if k in initial_names:
-            a0_dict[k] = v
-        elif k in initial_cov_names:
-            P0_dict[k] = v
-        else:
-            param_dict[k] = v
-
-    return param_dict, a0_dict, P0_dict
-
-
-def evaluate_logp(
-    all_param_dict: Dict[str, Any],
-    df: np.ndarray,
-    sparse_datas: np.ndarray,
-    Z: np.ndarray,
-    priors: Dict[str, Any],
-    shock_names: List[str],
-    observed_vars: List[str],
-    filter_type: str = "standard",
-) -> Tuple[float, np.ndarray]:
-    """
-    Evaluate the log likelihood of a log-linearized DSGE model using the Kalman Filter.
-
-    Parameters
-    ----------
-    all_param_dict : dict
-        A dictionary of all parameters for the model.
-    df : pd.DataFrame
-        A 2D array of data for which the log probability should be computed.
-    sparse_datas : numpy array
-        A 3D array of sparse matrices for the model.
-    Z : numpy array
-        A 2D array of measurement errors for the model.
-    priors : dict
-        A dictionary of prior distributions for each parameter.
-    shock_names : list
-        A list of names of shocks in the model.
-    observed_vars : list
-        A list of names of observed variables in the model.
-    filter_type : str, optional
-        The type of Kalman Filter to use. The default is 'standard'.
-
-    Returns
-    -------
-    tuple
-        A tuple containing (1) the log likelihood of the model and (2) an array of log likelihoods for each observation.
-    """
-
-    ll = evaluate_prior_logp(all_param_dict, priors)
-    param_dict, a0_dict, P0_dict = split_param_dict(all_param_dict)
-
-    if not np.isfinite(ll):
-        return -np.inf, np.zeros(df.shape[0])
-
-    param_dict, shock_dict, obs_dict = split_random_variables(
-        param_dict, shock_names, observed_vars
-    )
-    T, R, success = build_and_solve(param_dict, sparse_datas)
-
-    if not success:
-        return -np.inf, np.zeros(df.shape[0])
-
-    a0 = np.array(list(a0_dict.values()))[:, None] if len(a0_dict) > 0 else None
-    P0 = np.eye(len(P0_dict)) * np.array(list(P0_dict.keys())) if len(P0_dict) > 0 else None
-
-    Q, H = build_Q_and_H(shock_dict, shock_names, observed_vars, obs_dict)
-
-    *_, ll_obs = kalman_filter(df.values, T, Z, R, H, Q, a0, P0, filter_type=filter_type)
-    ll += ll_obs.sum()
-
-    return ll, ll_obs
+from typing import Any, Dict, List, Optional, Tuple
+
+import numpy as np
+from numpy.typing import ArrayLike
+from scipy import stats
+
+from gEconpy.estimation.estimation_utilities import (
+    build_system_matrices,
+    check_bk_condition,
+    check_finite_matrix,
+    split_random_variables,
+)
+from gEconpy.estimation.kalman_filter import kalman_filter
+from gEconpy.solvers.cycle_reduction import cycle_reduction, solve_shock_matrix
+
+
+def build_and_solve(
+    param_dict: dict, sparse_datas: List, vars_to_estimate: Optional[List] = None
+) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:
+    """
+    A collection of functionality already in the gEcon model object, extracted for speed and memory optimizations
+    when doing parallel fitting. Specifically, this function avoids the need of passing around the (potentially large)
+    model object when repeatedly solving the perturbation problem for policy matrices T and R.
+
+    Parameters
+    ----------
+    param_dict: dictionary of string keys float values
+        A dictionary that maps parameters to be estimated to point values.
+    sparse_datas: list of tuples
+        A list of the equations and CSR indicies needed to construct the A, B, C, and D matrices necessary to solve for
+        T and R.
+    vars_to_estimate: list of strings, default: None
+        The subset of variables to be estimated for. When None, the variable list is assumed to be those assigned
+        priors in the GCN file. This should only be used for debugging.
+
+    Returns
+    -------
+    T: array
+        The "policy matrix" describing how the linear system evolves with time
+    R: array
+        The "selection matrix" describing how exogenous shocks enter into the linear system
+    success: bool
+        A flag indicating whether the system has been successfully solved. This encodes three conditions: successful
+        converge of the perturbation algorithm, the size of the deterministic and stochastic norms of the
+        linear solution, and the blanchard-khan conditions.
+
+    TODO: njit this function by figuring out how to get rid of the sympy lambdify functions inside sparse_datas
+    """
+
+    res = build_system_matrices(param_dict, sparse_datas, vars_to_estimate=vars_to_estimate)
+    A, B, C, D = res
+
+    if not all([check_finite_matrix(x) for x in res]):
+        T = np.zeros_like(A)
+        R = np.zeros((T.shape[0], 1))
+        success = False
+        return T, R, success
+
+    bk_condition_met = check_bk_condition(A, B, C, tol=1e-8)
+
+    try:
+        T, result, log_norm = cycle_reduction(A, B, C, 1000, 1e-8, False)
+        R = solve_shock_matrix(B, C, D, T)
+    except np.linalg.LinAlgError:
+        T = np.zeros_like(A)
+        R = np.zeros((T.shape[0], 1))
+        success = False
+        return T, R, success
+
+    success = (result == "Optimization successful") & (log_norm < 1e-8) & bk_condition_met
+
+    T = np.ascontiguousarray(T)
+    R = np.ascontiguousarray(R)
+
+    return T, R, success
+
+
+def build_Z_matrix(obs_variables: List[str], state_variables: List[str]) -> np.ndarray:
+    """Constructs the design matrix Z for a state-space system.
+
+    Parameters
+    ----------
+    obs_variables : List[str]
+        The names of the observed variables.
+    state_variables : List[str]
+        The names of the state variables.
+
+    Returns
+    -------
+    Z : np.ndarray
+        The design matrix Z.
+    """
+
+    Z = np.array([[x == var for x in state_variables] for var in obs_variables], dtype="float64")
+    return Z
+
+
+def build_Q_and_H(
+    state_sigmas: Dict[str, float],
+    shock_variables: List[str],
+    obs_variables: List[str],
+    obs_sigmas: Optional[Dict[str, float]] = None,
+) -> Tuple[np.ndarray, np.ndarray]:
+    """
+    Build the Q and H matrices for a state-space system.
+
+    Parameters
+    ----------
+    state_sigmas : Dict[str, float]
+        A dictionary of variances associated with shocks in the state-space system.
+    shock_variables : List[str]
+        A list of strings representing shocks.
+    obs_variables : List[str]
+        A list of strings representing the observed variables.
+    obs_sigmas : Optional[Dict[str, float]]
+        A dictionary of variances associated with observed variables. If not provided, all variances are set to 0.
+
+    Returns
+    -------
+    Tuple[np.ndarray, np.ndarray]
+        A tuple containing the Q and H matrices.
+    """
+
+    k_posdef = len(shock_variables)
+    k_obs = len(obs_variables)
+
+    obs_sigmas = obs_sigmas or {}
+
+    i = 0
+    Q = np.zeros((k_posdef, k_posdef))
+    for v in shock_variables:
+        if v in state_sigmas.keys():
+            Q[i, i] = state_sigmas[v]
+        i += 1
+
+    i = 0
+    H = np.zeros((k_obs, k_obs))
+    for v in obs_variables:
+        if v in obs_sigmas.keys():
+            H[i, i] = obs_sigmas[v]
+        i += 1
+
+    Q = np.ascontiguousarray(Q)
+    H = np.ascontiguousarray(H)
+
+    return Q, H
+
+
+def evaluate_prior_logp(
+    all_param_dict: Dict[str, float], prior_dict: Dict[str, stats.rv_continuous]
+) -> float:
+    """
+    Evaluate the log probability density function (PDF) of the prior distribution for a given set of parameters.
+
+    Parameters
+    ----------
+    all_param_dict : dict
+        A dictionary containing the parameters (float values) for which the prior log PDF is to be evaluated.
+    prior_dict : dict
+        A dictionary containing the prior distributions (scipy.stats continuous random variables) for each parameter.
+
+    Returns
+    -------
+    float
+        The log probability of the parameters under the prior distribution.
+    """
+    ll = 0
+
+    for k, v in all_param_dict.items():
+        ll += prior_dict[k].logpdf(v).sum()
+
+    return ll
+
+
+def split_param_dict(
+    all_param_dict: Dict[str, float]
+) -> Tuple[Dict[str, float], Dict[str, float], Dict[str, float]]:
+    """
+    Split a dictionary of parameters into three dictionaries based on their keys.
+
+    Parameters
+    ----------
+    all_param_dict : dict
+        A dictionary of parameters to be split.
+
+    Returns
+    -------
+    tuple
+        A tuple containing
+        (1) a dictionary of deep parameters,
+        (2) a numpy array containing the initial conditions for the state vector, and
+        (3) a numpy array containing the initial conditions for the state variance-covariance matrix.
+    """
+    param_dict = {}
+    a0_dict = {}
+    P0_dict = {}
+
+    initial_names = [x for x in all_param_dict.keys() if x.endswith("__initial")]
+    initial_cov_names = [x for x in all_param_dict.keys() if x.endswith("__initial_cov")]
+
+    for k, v in all_param_dict.items():
+        if k in initial_names:
+            a0_dict[k] = v
+        elif k in initial_cov_names:
+            P0_dict[k] = v
+        else:
+            param_dict[k] = v
+
+    return param_dict, a0_dict, P0_dict
+
+
+def evaluate_logp(
+    all_param_dict: Dict[str, Any],
+    df: np.ndarray,
+    sparse_datas: np.ndarray,
+    Z: np.ndarray,
+    priors: Dict[str, Any],
+    shock_names: List[str],
+    observed_vars: List[str],
+    filter_type: str = "standard",
+) -> Tuple[float, np.ndarray]:
+    """
+    Evaluate the log likelihood of a log-linearized DSGE model using the Kalman Filter.
+
+    Parameters
+    ----------
+    all_param_dict : dict
+        A dictionary of all parameters for the model.
+    df : pd.DataFrame
+        A 2D array of data for which the log probability should be computed.
+    sparse_datas : numpy array
+        A 3D array of sparse matrices for the model.
+    Z : numpy array
+        A 2D array of measurement errors for the model.
+    priors : dict
+        A dictionary of prior distributions for each parameter.
+    shock_names : list
+        A list of names of shocks in the model.
+    observed_vars : list
+        A list of names of observed variables in the model.
+    filter_type : str, optional
+        The type of Kalman Filter to use. The default is 'standard'.
+
+    Returns
+    -------
+    tuple
+        A tuple containing (1) the log likelihood of the model and (2) an array of log likelihoods for each observation.
+    """
+
+    ll = evaluate_prior_logp(all_param_dict, priors)
+    param_dict, a0_dict, P0_dict = split_param_dict(all_param_dict)
+
+    if not np.isfinite(ll):
+        return -np.inf, np.zeros(df.shape[0])
+
+    param_dict, shock_dict, obs_dict = split_random_variables(
+        param_dict, shock_names, observed_vars
+    )
+    T, R, success = build_and_solve(param_dict, sparse_datas)
+
+    if not success:
+        return -np.inf, np.zeros(df.shape[0])
+
+    a0 = np.array(list(a0_dict.values()))[:, None] if len(a0_dict) > 0 else None
+    P0 = np.eye(len(P0_dict)) * np.array(list(P0_dict.keys())) if len(P0_dict) > 0 else None
+
+    Q, H = build_Q_and_H(shock_dict, shock_names, observed_vars, obs_dict)
+
+    *_, ll_obs = kalman_filter(df.values, T, Z, R, H, Q, a0, P0, filter_type=filter_type)
+    ll += ll_obs.sum()
+
+    return ll, ll_obs
```

### Comparing `gEconpy-1.1.0/gEconpy/estimation/estimation_utilities.py` & `gEconpy-1.2.0/gEconpy/estimation/estimation_utilities.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,366 +1,366 @@
-from typing import Callable, Dict, List, Optional, Tuple
-
-import numba as nb
-import numpy as np
-import sympy as sp
-from scipy import linalg
-
-
-@nb.njit
-def check_finite_matrix(a):
-    for v in np.nditer(a):
-        if not np.isfinite(v.item()):
-            return False
-    return True
-
-
-def numba_lambdify_scalar(inputs, expr, sig):
-    """
-    Convert a sympy expression into a Numba-compiled function.
-
-    Parameters
-    ----------
-    inputs : List[str]
-        A list of strings containing the names of the variables in the expression.
-    expr : sympy.Expr
-        The sympy expression to be converted.
-
-    Returns
-    -------
-    numba.types.function
-        A Numba-compiled function equivalent to the input expression.
-
-    Notes
-    -----
-    The function returned by this function is pickleable.
-    """
-    code = sp.printing.ccode(expr)
-    # The code string will contain a single line, so we add line breaks to make it a valid block of code
-    code = "@nb.njit('{}')\ndef f({}):\n{}\n    return {}".format(
-        sig, ",".join(inputs), " " * 4, code
-    )
-    # Compile the code and return the resulting function
-    exec(code)
-    return locals()["f"]
-
-
-def extract_sparse_data_from_model(model, params_to_estimate: Optional[List] = None) -> List:
-    """
-    Extract sparse data from a DSGE model.
-
-    Parameters
-    ----------
-    model : object
-        A gEconpy model object.
-    params_to_estimate : list, optional
-        A list of variables to estimate. The default is None, which estimates all variables.
-
-    Returns
-    -------
-    list
-        A list of sparse data.
-    """
-
-    params_to_estimate = params_to_estimate or model.param_priors.keys()
-    ss_vars = list(model.steady_state_dict.to_sympy().keys())
-
-    param_dict = model.free_param_dict.copy()
-    ss_sub_dict = model.steady_state_relationships.copy()
-    calib_dict = model.calib_param_dict.copy()
-
-    requires_numeric_solution = [x for x in ss_vars if x not in ss_sub_dict.to_sympy()]
-
-    not_estimated_dict = param_dict.copy()
-    for k in param_dict.keys():
-        if k in params_to_estimate:
-            del not_estimated_dict[k]
-
-    names = ["A", "B", "C", "D"]
-    A, B, C, D = (x.tolist() for x in model._perturbation_setup(return_F_matrices=True))
-
-    inputs = params_to_estimate + requires_numeric_solution
-    # n_inputs = len(inputs)
-
-    # signature_str = f"float64({', '.join(['float64'] * n_inputs)})"
-    # function_sig = nb.types.FunctionType(nb.types.float64(*(nb.types.float64,) * n_inputs))
-    #
-    # sparse_datas = nb.typed.List()
-    sparse_datas = []
-
-    for name, matrix in zip(names, [A, B, C, D]):
-        # data = nb.typed.List.empty_list(function_sig)
-        # idxs = nb.typed.List()
-        # pointers = nb.typed.List([0])
-
-        data = []
-        idxs = []
-        pointers = [0]
-
-        for row in matrix:
-            for i, value in enumerate(row):
-                if value != 0:
-                    expr = (
-                        value.subs(ss_sub_dict.to_sympy())
-                        .subs(calib_dict.to_sympy())
-                        .subs(not_estimated_dict.to_sympy())
-                    )
-                    # numba_func = numba_lambdify_scalar(inputs, expr, signature_str)
-                    func = sp.lambdify(inputs, expr)
-                    # data.append(numba_func)
-                    data.append(func)
-                    idxs.append(i)
-            pointers.append(len(idxs))
-
-        shape = (len(matrix), len(matrix[0]))
-        sparse_datas.append((data, idxs, pointers, shape))
-
-    return sparse_datas
-
-
-# @nb.njit
-def matrix_from_csr_data(
-    data: np.ndarray, indices: np.ndarray, idxptrs: np.ndarray, shape: Tuple[int, int]
-) -> np.ndarray:
-    """
-    Convert a CSR matrix into a dense numpy array.
-
-    Parameters
-    ----------
-    data : np.ndarray
-        The data stored in the CSR matrix.
-    indices : np.ndarray
-        The column indices for the non-zero values in `data`.
-    idxptrs : np.ndarray
-        The index pointers for the CSR matrix.
-    shape : tuple[int, int]
-        The shape of the dense matrix to create.
-
-    Returns
-    -------
-    np.ndarray
-        The dense matrix representation of the CSR matrix.
-    """
-    out = np.zeros(shape)
-    for i in range(shape[0]):
-        start = idxptrs[i]
-        end = idxptrs[i + 1]
-        s = slice(start, end)
-        d_idx = range(start, end)
-        col_idxs = indices[s]
-        for j, d in zip(col_idxs, d_idx):
-            out[i, j] = data[d]
-
-    return out
-
-
-def build_system_matrices(
-    param_dict: Dict[str, float],
-    sparse_datas: List[Tuple[Callable, np.ndarray, np.ndarray, Tuple[int, int]]],
-    vars_to_estimate: Optional[List[str]] = None,
-) -> List[np.ndarray]:
-    """
-    Build system matrices for a DSGE model.
-
-    This function builds the A, B, C, and D matrices for a DSGE model given a set of parameters
-    and pre-computed sparse data.
-
-    Parameters
-    ----------
-    param_dict : dict
-        Dictionary of parameter values
-    sparse_datas : list of tuples
-        List of tuples, each tuple representing sparse data for a single matrix. The tuple contains the following
-        elements:
-        data : numpy array
-            Array of values to be placed in the matrix
-        indices : numpy array
-            Array of column indices for the non-zero values in the matrix
-        idxptrs : numpy array
-            Array of row pointers for the non-zero values in the matrix
-        shape : tuple
-            Shape of the matrix as a tuple (n_rows, n_cols)
-    vars_to_estimate : list of str, optional
-        List of parameter names to use in building the matrices, by default None
-    Returns
-    -------
-    list of numpy arrays
-        List of matrices A, B, C, and D
-    """
-
-    result = []
-    if vars_to_estimate:
-        params_to_use = {k: v for k, v in param_dict.to_string().items() if k in vars_to_estimate}
-    else:
-        params_to_use = param_dict.to_string()
-
-    for sparse_data in sparse_datas:
-        fs, indices, idxptrs, shape = sparse_data
-        data = np.zeros(len(fs))
-        i = 0
-        for f in fs:
-            data[i] = f(**params_to_use)
-            i += 1
-        M = matrix_from_csr_data(data, indices, idxptrs, shape)
-        result.append(M)
-    return result
-
-
-@nb.njit
-def compute_eigenvalues(A, B, C, tol=1e-8):
-    """
-    Given the log-linearized coefficient matrices A, B, and C at times t-1, t, and t+1 respectively, compute the
-    eigenvalues of the DSGE system. These eigenvalues are used to determine stability of the DSGE system.
-
-    Parameters
-    ----------
-    A : np.ndarray
-        The log-linearized coefficient matrix of the DSGE system at time t-1
-    B : np.ndarray
-        The log-linearized coefficient matrix of the DSGE system at time t
-    C : np.ndarray
-        The log-linearized coefficient matrix of the DSGE system at time t+1
-    tol : float, optional
-        The tolerance used to check for stability, by default 1e-8
-
-    Returns
-    -------
-    np.ndarray
-        The eigenvalues of the DSGE system, sorted by the magnitude of the real part. Each row of the output array
-        contains the magnitude, real part, and imaginary part of an eigenvalue.
-    """
-
-    n_eq, n_vars = A.shape
-
-    lead_var_idx = np.where(np.sum(np.abs(C), axis=0) > tol)[0]
-
-    eqs_and_leads_idx = np.hstack((np.arange(n_vars), lead_var_idx + n_vars))
-
-    Gamma_0 = np.vstack((np.hstack((B, C)), np.hstack((-np.eye(n_eq), np.zeros((n_eq, n_eq))))))
-
-    Gamma_1 = np.vstack(
-        (
-            np.hstack((A, np.zeros((n_eq, n_eq)))),
-            np.hstack((np.zeros((n_eq, n_eq)), np.eye(n_eq))),
-        )
-    )
-    Gamma_0 = Gamma_0[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
-    Gamma_1 = Gamma_1[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
-
-    A, B, alpha, beta, Q, Z = linalg.ordqz(-Gamma_0, Gamma_1, sort="ouc", output="complex")
-
-    gev = np.vstack((np.diag(A), np.diag(B))).T
-
-    eigenval = gev[:, 1] / (gev[:, 0] + tol)
-    pos_idx = np.where(np.abs(eigenval) > 0)
-    eig = np.zeros(((np.abs(eigenval) > 0).sum(), 3))
-    eig[:, 0] = np.abs(eigenval)[pos_idx]
-    eig[:, 1] = np.real(eigenval)[pos_idx]
-    eig[:, 2] = np.imag(eigenval)[pos_idx]
-
-    sorted_idx = np.argsort(eig[:, 0])
-
-    return eig[sorted_idx, :]
-
-
-@nb.njit
-def check_bk_condition(A, B, C, tol=1e-8):
-    """
-    Check the Blanchard-Kahn condition for the DSGE model specified by the log linearized coefficient matrices
-    A (t-1), B (t), and C (t+1).
-
-    This function computes the eigenvalues of the DSGE system and checks if the number of forward-looking variables
-    is less than or equal to the number of eigenvalues greater than 1. The Blanchard-Kahn condition ensures the
-    stability of the rational expectations equilibrium of the model.
-
-    Parameters
-    ----------
-    A : numpy.ndarray
-        The log-linearized coefficient matrix at time t-1
-    B : numpy.ndarray
-        The log-linearized coefficient matrix at time t
-    C : numpy.ndarray
-        The log-linearized coefficient matrix at time t+1
-    tol : float, optional
-        The tolerance for eigenvalues that are considered equal to 1, by default 1e-8
-
-    Returns
-    -------
-    bool
-        True if the Blanchard-Kahn condition is satisfied, else False
-
-    References
-    ----------
-    Blanchard, Olivier Jean, and Charles M. Kahn. "The solution of linear difference models under rational
-    expectations." Econometrica: Journal of the Econometric Society (1980): 1305-1311.
-    """
-
-    n_forward = int((C.sum(axis=0) > 0).sum())
-
-    try:
-        eig = compute_eigenvalues(A, B, C, tol)
-    # TODO: ValueError is the correct exception to raise here, but numba complains
-    except Exception:
-        return False
-
-    n_g_one = (eig[:, 0] > 1).sum()
-    return n_forward <= n_g_one
-
-
-def split_random_variables(param_dict, shock_names, obs_names):
-    """
-    Split a dictionary of parameters into dictionaries of shocks, observables, and other variables.
-
-    Parameters
-    ----------
-    param_dict : Dict[str, float]
-        A dictionary of parameters and their values.
-    shock_names : List[str]
-        A list of the names of shock variables.
-    obs_names : List[str]
-        A list of the names of observable variables.
-
-    Returns
-    -------
-    Tuple[Dict[str, float], Dict[str, float], Dict[str, float]]
-        A tuple containing three dictionaries: the first has parameters, the second has
-        all shock variances parameters, and the third has observation noise variances.
-    """
-
-    out_param_dict = {}
-    shock_dict = {}
-    obs_dict = {}
-
-    for k, v in param_dict.items():
-        if k in shock_names:
-            shock_dict[k] = v
-        elif k in obs_names:
-            obs_dict[k] = v
-        else:
-            out_param_dict[k] = v
-
-    return out_param_dict, shock_dict, obs_dict
-
-
-def extract_prior_dict(model):
-    """
-    Extract the prior distributions from a gEconModel object.
-
-    Parameters
-    ----------
-    model : gEconModel
-        The gEconModel object to extract priors from.
-
-    Returns
-    -------
-    prior_dict : dict
-        A dictionary containing the prior distributions for the model's parameters, shocks, and observation noise.
-    """
-    prior_dict = {}
-
-    prior_dict.update(model.param_priors)
-    prior_dict.update(
-        {k: model.shock_priors[k].rv_params["scale"] for k in model.shock_priors.keys()}
-    )
-    prior_dict.update(model.observation_noise_priors)
-
-    return prior_dict
+from typing import Callable, Dict, List, Optional, Tuple
+
+import numba as nb
+import numpy as np
+import sympy as sp
+from scipy import linalg
+
+
+@nb.njit
+def check_finite_matrix(a):
+    for v in np.nditer(a):
+        if not np.isfinite(v.item()):
+            return False
+    return True
+
+
+def numba_lambdify_scalar(inputs, expr, sig):
+    """
+    Convert a sympy expression into a Numba-compiled function.
+
+    Parameters
+    ----------
+    inputs : List[str]
+        A list of strings containing the names of the variables in the expression.
+    expr : sympy.Expr
+        The sympy expression to be converted.
+
+    Returns
+    -------
+    numba.types.function
+        A Numba-compiled function equivalent to the input expression.
+
+    Notes
+    -----
+    The function returned by this function is pickleable.
+    """
+    code = sp.printing.ccode(expr)
+    # The code string will contain a single line, so we add line breaks to make it a valid block of code
+    code = "@nb.njit('{}')\ndef f({}):\n{}\n    return {}".format(
+        sig, ",".join(inputs), " " * 4, code
+    )
+    # Compile the code and return the resulting function
+    exec(code)
+    return locals()["f"]
+
+
+def extract_sparse_data_from_model(model, params_to_estimate: Optional[List] = None) -> List:
+    """
+    Extract sparse data from a DSGE model.
+
+    Parameters
+    ----------
+    model : object
+        A gEconpy model object.
+    params_to_estimate : list, optional
+        A list of variables to estimate. The default is None, which estimates all variables.
+
+    Returns
+    -------
+    list
+        A list of sparse data.
+    """
+
+    params_to_estimate = params_to_estimate or model.param_priors.keys()
+    ss_vars = list(model.steady_state_dict.to_sympy().keys())
+
+    param_dict = model.free_param_dict.copy()
+    ss_sub_dict = model.steady_state_relationships.copy()
+    calib_dict = model.calib_param_dict.copy()
+
+    requires_numeric_solution = [x for x in ss_vars if x not in ss_sub_dict.to_sympy()]
+
+    not_estimated_dict = param_dict.copy()
+    for k in param_dict.keys():
+        if k in params_to_estimate:
+            del not_estimated_dict[k]
+
+    names = ["A", "B", "C", "D"]
+    A, B, C, D = (x.tolist() for x in model._perturbation_setup(return_F_matrices=True))
+
+    inputs = params_to_estimate + requires_numeric_solution
+    # n_inputs = len(inputs)
+
+    # signature_str = f"float64({', '.join(['float64'] * n_inputs)})"
+    # function_sig = nb.types.FunctionType(nb.types.float64(*(nb.types.float64,) * n_inputs))
+    #
+    # sparse_datas = nb.typed.List()
+    sparse_datas = []
+
+    for name, matrix in zip(names, [A, B, C, D]):
+        # data = nb.typed.List.empty_list(function_sig)
+        # idxs = nb.typed.List()
+        # pointers = nb.typed.List([0])
+
+        data = []
+        idxs = []
+        pointers = [0]
+
+        for row in matrix:
+            for i, value in enumerate(row):
+                if value != 0:
+                    expr = (
+                        value.subs(ss_sub_dict.to_sympy())
+                        .subs(calib_dict.to_sympy())
+                        .subs(not_estimated_dict.to_sympy())
+                    )
+                    # numba_func = numba_lambdify_scalar(inputs, expr, signature_str)
+                    func = sp.lambdify(inputs, expr)
+                    # data.append(numba_func)
+                    data.append(func)
+                    idxs.append(i)
+            pointers.append(len(idxs))
+
+        shape = (len(matrix), len(matrix[0]))
+        sparse_datas.append((data, idxs, pointers, shape))
+
+    return sparse_datas
+
+
+# @nb.njit
+def matrix_from_csr_data(
+    data: np.ndarray, indices: np.ndarray, idxptrs: np.ndarray, shape: Tuple[int, int]
+) -> np.ndarray:
+    """
+    Convert a CSR matrix into a dense numpy array.
+
+    Parameters
+    ----------
+    data : np.ndarray
+        The data stored in the CSR matrix.
+    indices : np.ndarray
+        The column indices for the non-zero values in `data`.
+    idxptrs : np.ndarray
+        The index pointers for the CSR matrix.
+    shape : tuple[int, int]
+        The shape of the dense matrix to create.
+
+    Returns
+    -------
+    np.ndarray
+        The dense matrix representation of the CSR matrix.
+    """
+    out = np.zeros(shape)
+    for i in range(shape[0]):
+        start = idxptrs[i]
+        end = idxptrs[i + 1]
+        s = slice(start, end)
+        d_idx = range(start, end)
+        col_idxs = indices[s]
+        for j, d in zip(col_idxs, d_idx):
+            out[i, j] = data[d]
+
+    return out
+
+
+def build_system_matrices(
+    param_dict: Dict[str, float],
+    sparse_datas: List[Tuple[Callable, np.ndarray, np.ndarray, Tuple[int, int]]],
+    vars_to_estimate: Optional[List[str]] = None,
+) -> List[np.ndarray]:
+    """
+    Build system matrices for a DSGE model.
+
+    This function builds the A, B, C, and D matrices for a DSGE model given a set of parameters
+    and pre-computed sparse data.
+
+    Parameters
+    ----------
+    param_dict : dict
+        Dictionary of parameter values
+    sparse_datas : list of tuples
+        List of tuples, each tuple representing sparse data for a single matrix. The tuple contains the following
+        elements:
+        data : numpy array
+            Array of values to be placed in the matrix
+        indices : numpy array
+            Array of column indices for the non-zero values in the matrix
+        idxptrs : numpy array
+            Array of row pointers for the non-zero values in the matrix
+        shape : tuple
+            Shape of the matrix as a tuple (n_rows, n_cols)
+    vars_to_estimate : list of str, optional
+        List of parameter names to use in building the matrices, by default None
+    Returns
+    -------
+    list of numpy arrays
+        List of matrices A, B, C, and D
+    """
+
+    result = []
+    if vars_to_estimate:
+        params_to_use = {k: v for k, v in param_dict.to_string().items() if k in vars_to_estimate}
+    else:
+        params_to_use = param_dict.to_string()
+
+    for sparse_data in sparse_datas:
+        fs, indices, idxptrs, shape = sparse_data
+        data = np.zeros(len(fs))
+        i = 0
+        for f in fs:
+            data[i] = f(**params_to_use)
+            i += 1
+        M = matrix_from_csr_data(data, indices, idxptrs, shape)
+        result.append(M)
+    return result
+
+
+@nb.njit
+def compute_eigenvalues(A, B, C, tol=1e-8):
+    """
+    Given the log-linearized coefficient matrices A, B, and C at times t-1, t, and t+1 respectively, compute the
+    eigenvalues of the DSGE system. These eigenvalues are used to determine stability of the DSGE system.
+
+    Parameters
+    ----------
+    A : np.ndarray
+        The log-linearized coefficient matrix of the DSGE system at time t-1
+    B : np.ndarray
+        The log-linearized coefficient matrix of the DSGE system at time t
+    C : np.ndarray
+        The log-linearized coefficient matrix of the DSGE system at time t+1
+    tol : float, optional
+        The tolerance used to check for stability, by default 1e-8
+
+    Returns
+    -------
+    np.ndarray
+        The eigenvalues of the DSGE system, sorted by the magnitude of the real part. Each row of the output array
+        contains the magnitude, real part, and imaginary part of an eigenvalue.
+    """
+
+    n_eq, n_vars = A.shape
+
+    lead_var_idx = np.where(np.sum(np.abs(C), axis=0) > tol)[0]
+
+    eqs_and_leads_idx = np.hstack((np.arange(n_vars), lead_var_idx + n_vars))
+
+    Gamma_0 = np.vstack((np.hstack((B, C)), np.hstack((-np.eye(n_eq), np.zeros((n_eq, n_eq))))))
+
+    Gamma_1 = np.vstack(
+        (
+            np.hstack((A, np.zeros((n_eq, n_eq)))),
+            np.hstack((np.zeros((n_eq, n_eq)), np.eye(n_eq))),
+        )
+    )
+    Gamma_0 = Gamma_0[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
+    Gamma_1 = Gamma_1[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
+
+    A, B, alpha, beta, Q, Z = linalg.ordqz(-Gamma_0, Gamma_1, sort="ouc", output="complex")
+
+    gev = np.vstack((np.diag(A), np.diag(B))).T
+
+    eigenval = gev[:, 1] / (gev[:, 0] + tol)
+    pos_idx = np.where(np.abs(eigenval) > 0)
+    eig = np.zeros(((np.abs(eigenval) > 0).sum(), 3))
+    eig[:, 0] = np.abs(eigenval)[pos_idx]
+    eig[:, 1] = np.real(eigenval)[pos_idx]
+    eig[:, 2] = np.imag(eigenval)[pos_idx]
+
+    sorted_idx = np.argsort(eig[:, 0])
+
+    return eig[sorted_idx, :]
+
+
+@nb.njit
+def check_bk_condition(A, B, C, tol=1e-8):
+    """
+    Check the Blanchard-Kahn condition for the DSGE model specified by the log linearized coefficient matrices
+    A (t-1), B (t), and C (t+1).
+
+    This function computes the eigenvalues of the DSGE system and checks if the number of forward-looking variables
+    is less than or equal to the number of eigenvalues greater than 1. The Blanchard-Kahn condition ensures the
+    stability of the rational expectations equilibrium of the model.
+
+    Parameters
+    ----------
+    A : numpy.ndarray
+        The log-linearized coefficient matrix at time t-1
+    B : numpy.ndarray
+        The log-linearized coefficient matrix at time t
+    C : numpy.ndarray
+        The log-linearized coefficient matrix at time t+1
+    tol : float, optional
+        The tolerance for eigenvalues that are considered equal to 1, by default 1e-8
+
+    Returns
+    -------
+    bool
+        True if the Blanchard-Kahn condition is satisfied, else False
+
+    References
+    ----------
+    Blanchard, Olivier Jean, and Charles M. Kahn. "The solution of linear difference models under rational
+    expectations." Econometrica: Journal of the Econometric Society (1980): 1305-1311.
+    """
+
+    n_forward = int((C.sum(axis=0) > 0).sum())
+
+    try:
+        eig = compute_eigenvalues(A, B, C, tol)
+    # TODO: ValueError is the correct exception to raise here, but numba complains
+    except Exception:
+        return False
+
+    n_g_one = (eig[:, 0] > 1).sum()
+    return n_forward <= n_g_one
+
+
+def split_random_variables(param_dict, shock_names, obs_names):
+    """
+    Split a dictionary of parameters into dictionaries of shocks, observables, and other variables.
+
+    Parameters
+    ----------
+    param_dict : Dict[str, float]
+        A dictionary of parameters and their values.
+    shock_names : List[str]
+        A list of the names of shock variables.
+    obs_names : List[str]
+        A list of the names of observable variables.
+
+    Returns
+    -------
+    Tuple[Dict[str, float], Dict[str, float], Dict[str, float]]
+        A tuple containing three dictionaries: the first has parameters, the second has
+        all shock variances parameters, and the third has observation noise variances.
+    """
+
+    out_param_dict = {}
+    shock_dict = {}
+    obs_dict = {}
+
+    for k, v in param_dict.items():
+        if k in shock_names:
+            shock_dict[k] = v
+        elif k in obs_names:
+            obs_dict[k] = v
+        else:
+            out_param_dict[k] = v
+
+    return out_param_dict, shock_dict, obs_dict
+
+
+def extract_prior_dict(model):
+    """
+    Extract the prior distributions from a gEconModel object.
+
+    Parameters
+    ----------
+    model : gEconModel
+        The gEconModel object to extract priors from.
+
+    Returns
+    -------
+    prior_dict : dict
+        A dictionary containing the prior distributions for the model's parameters, shocks, and observation noise.
+    """
+    prior_dict = {}
+
+    prior_dict.update(model.param_priors)
+    prior_dict.update(
+        {k: model.shock_priors[k].rv_params["scale"] for k in model.shock_priors.keys()}
+    )
+    prior_dict.update(model.observation_noise_priors)
+
+    return prior_dict
```

### Comparing `gEconpy-1.1.0/gEconpy/estimation/kalman_filter.py` & `gEconpy-1.2.0/gEconpy/estimation/kalman_filter.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,312 +1,315 @@
-from typing import Tuple
-
-import numpy as np
-from numba import njit
-from numpy.typing import ArrayLike
-from scipy import linalg
-
-from gEconpy.numba_linalg.overloads import *  # pylint: disable=unused-wildcard-import,wildcard-import
-
-MVN_CONST = np.log(2.0 * np.pi)
-EPS = 1e-12
-
-
-@njit("float64[:, ::1](boolean[::1])")
-def build_mask_matrix(nan_mask: ArrayLike) -> ArrayLike:
-    """
-    The Kalman Filter can "natively" handle missing values by treating observed states as un-observed states for
-    iterations where data is not available. To do this, the Z and H matrices must be modified. This function creates
-    a matrix W such that W @ Z and W @ H have zeros where data is missing.
-    Parameters
-    ----------
-    nan_mask: array
-        A 1d array of boolean flags of length n, indicating whether a state is observed in the current iteration.
-    Returns
-    -------
-    W: array
-        An n x n matrix used to mask missing values in the Z and H matrices
-    """
-    n = nan_mask.shape[0]
-    W = np.eye(n)
-    i = 0
-    for flag in nan_mask:
-        if flag:
-            W[i, i] = 0
-        i += 1
-
-    W = np.ascontiguousarray(W)
-
-    return W
-
-
-@njit
-def standard_kalman_filter(
-    data: ArrayLike,
-    T: ArrayLike,
-    Z: ArrayLike,
-    R: ArrayLike,
-    H: ArrayLike,
-    Q: ArrayLike,
-    a0: ArrayLike,
-    P0: ArrayLike,
-) -> Tuple:
-    """
-    Parameters
-    ----------
-    data: array
-        (T, k_observed) matrix of observed data. Data can include missing values.
-    a0: array
-        (k_states, 1) vector of initial states.
-    P0: array
-        (k_states, k_states) initial state covariance matrix
-    T: array
-        (k_states, k_states) transition matrix
-    Z: array
-        (k_states, k_observed) design matrix
-    R: array
-    H: array
-    Q: array
-    Returns
-    -------
-    """
-    n_steps, k_obs = data.shape
-    k_states, k_posdef = R.shape
-
-    filtered_states = np.zeros((n_steps, k_states))
-    predicted_states = np.zeros((n_steps + 1, k_states))
-    filtered_cov = np.zeros((n_steps, k_states, k_states))
-    predicted_cov = np.zeros((n_steps + 1, k_states, k_states))
-    log_likelihood = np.zeros(n_steps)
-
-    a = a0
-    P = P0
-
-    predicted_states[0] = a
-    predicted_cov[0] = P
-
-    for i in range(n_steps):
-        a_filtered, a_hat, P_filtered, P_hat, ll = kalman_step(data[i].copy(), a, P, T, Z, R, H, Q)
-
-        filtered_states[i] = a_filtered[:, 0]
-        predicted_states[i + 1] = a_hat[:, 0]
-        filtered_cov[i] = P_filtered
-        predicted_cov[i + 1] = P_hat
-        log_likelihood[i] = ll[0]
-
-        a = a_hat
-        P = P_hat
-
-    return (
-        filtered_states,
-        predicted_states,
-        filtered_cov,
-        predicted_cov,
-        log_likelihood,
-    )
-
-
-@njit
-def kalman_step(y, a, P, T, Z, R, H, Q):
-    y = y.reshape(-1, 1)
-    nan_mask = np.isnan(y).ravel()
-    W = build_mask_matrix(nan_mask)
-
-    Z_masked = W @ Z
-    H_masked = W @ H
-    y_masked = y.copy()
-    y_masked[nan_mask] = 0.0
-
-    a_filtered, P_filtered, ll = filter_step(y_masked, Z_masked, H_masked, a, P)
-
-    a_hat, P_hat = predict(a=a_filtered, P=P_filtered, T=T, R=R, Q=Q)
-
-    return a_filtered, a_hat, P_filtered, P_hat, ll
-
-
-@njit(
-    "Tuple((float64[:, ::1], float64[:, ::1], float64[::1]))(float64[:, ::1], float64[:, ::1], float64[:, ::1], "
-    "float64[:, ::1], float64[:, ::1])"
-)
-def filter_step(y, Z, H, a, P):
-    v = y - Z @ a
-
-    PZT = P @ Z.T
-    F = Z @ PZT + H
-
-    # Special case for if everything is missing. Abort before failing to invert F
-    if np.all(Z == 0):
-        a_filtered = np.atleast_2d(a).reshape((-1, 1))
-        P_filtered = P
-        ll = np.zeros(v.shape[0])
-
-        return a_filtered, P_filtered, ll
-
-    F_chol = np.linalg.cholesky(F)
-    K = linalg.solve_triangular(
-        F_chol, linalg.solve_triangular(F_chol, PZT.T, lower=True), trans=1, lower=True
-    ).T
-
-    I_KZ = np.eye(K.shape[0]) - K @ Z
-
-    a_filtered = a + K @ v
-    P_filtered = I_KZ @ P @ I_KZ.T + K @ H @ K.T
-    P_filtered = 0.5 * (P_filtered + P_filtered.T)
-
-    inner_term = linalg.solve_triangular(
-        F_chol, linalg.solve_triangular(F_chol, v, lower=True), lower=True, trans=1
-    )
-    n = y.shape[0]
-    ll = -0.5 * (n * MVN_CONST + (v.T @ inner_term).ravel()) - np.log(np.diag(F_chol)).sum()
-
-    return a_filtered, P_filtered, ll
-
-
-@njit
-def predict(a, P, T, R, Q):
-    a_hat = T @ a
-
-    P_hat = T @ P @ T.T + R @ Q @ R.T
-    P_hat = 0.5 * (P_hat + P_hat.T)
-
-    return a_hat, P_hat
-
-
-@njit
-def univariate_kalman_filter(
-    data: ArrayLike,
-    T: ArrayLike,
-    Z: ArrayLike,
-    R: ArrayLike,
-    H: ArrayLike,
-    Q: ArrayLike,
-    a0: ArrayLike,
-    P0: ArrayLike,
-) -> Tuple:
-    n_steps, k_obs = data.shape
-    k_states, k_posdef = R.shape
-
-    filtered_states = np.zeros((n_steps, k_states))
-    predicted_states = np.zeros((n_steps + 1, k_states))
-    filtered_cov = np.zeros((n_steps, k_states, k_states))
-    predicted_cov = np.zeros((n_steps + 1, k_states, k_states))
-    log_likelihood = np.zeros(n_steps)
-
-    a = a0
-    P = P0
-
-    predicted_states[0] = a[:, 0]
-    predicted_cov[0] = P
-
-    for i in range(n_steps):
-        a_filtered, a_hat, P_filtered, P_hat, ll = univariate_kalman_step(
-            data[i].copy(), a, P, T, Z, R, H, Q
-        )
-
-        filtered_states[i] = a_filtered[:, 0]
-        predicted_states[i + 1] = a_hat[:, 0]
-        filtered_cov[i] = P_filtered
-        predicted_cov[i + 1] = P_hat
-        log_likelihood[i] = ll
-
-        a = a_hat
-        P = P_hat
-
-    return (
-        filtered_states,
-        predicted_states,
-        filtered_cov,
-        predicted_cov,
-        log_likelihood,
-    )
-
-
-@njit
-def univariate_kalman_step(y, a, P, T, Z, R, H, Q):
-    y = y.reshape(-1, 1)
-    nan_mask = np.isnan(y).ravel()
-    W = build_mask_matrix(nan_mask)
-
-    Z_masked = W @ Z
-    H_masked = W @ H
-    y_masked = y.copy()
-    y_masked[nan_mask] = 0.0
-
-    a_filtered, P_filtered, ll = univariate_filter_step(y_masked, Z_masked, H_masked, a, P)
-
-    a_hat, P_hat = predict(a=a_filtered, P=P_filtered, T=T, R=R, Q=Q)
-
-    return a_filtered, a_hat, P_filtered, P_hat, ll
-
-
-@njit
-def univariate_filter_step(y_masked, Z_masked, H_masked, a, P):
-    """
-    Univariate step that avoids inverting the F matrix by filtering one state at a time. Good for when the H matrix
-    isn't full rank (all economics problems)!
-    """
-
-    n_states = y_masked.shape[0]
-    a_filtered = a.copy()
-    P_filtered = P.copy()
-    ll_row = np.zeros(n_states)
-
-    for i in range(n_states):
-        a_filtered, P_filtered, ll = univariate_inner_step(
-            y_masked[i], Z_masked[i, :], H_masked[i, i], a_filtered, P_filtered
-        )
-        ll_row[i] = ll[0]
-
-    ll = -0.5 * ((ll_row != 0).sum() * MVN_CONST + ll_row.sum())
-    P_filtered = 0.5 * (P_filtered + P_filtered.T)
-
-    return a_filtered, P_filtered, ll
-
-
-@njit
-def univariate_inner_step(y, Z_row, sigma_H, a, P):
-    Z_row = np.atleast_2d(Z_row)
-    v = y - Z_row @ a
-
-    PZT = P @ Z_row.T
-    F = Z_row @ PZT + sigma_H
-
-    if F < EPS:
-        a_filtered = a
-        P_filtered = P
-        ll = np.zeros(v.shape[0])
-        return a_filtered, P_filtered, ll.ravel()
-
-    K = PZT / F
-    a_filtered = a + K * v
-    P_filtered = P - np.outer(K, K) * F
-    ll = np.log(F) + v**2 / F
-
-    return a_filtered, P_filtered, ll.ravel()
-
-
-@njit(
-    "Tuple((float64[:, ::1], float64[:, ::1]))(float64[:, ::1], float64[:, ::1], float64[:, ::1], "
-    "optional(float64[:, ::1]), optional(float64[:, ::1]))"
-)
-def make_initial_conditions(T, R, Q, a0, P0):
-    if a0 is None:
-        a0 = np.zeros((T.shape[0], 1))
-    if P0 is None:
-        P0 = linalg.solve_discrete_lyapunov(T, R @ Q @ R.T)
-
-    return a0, P0
-
-
-@njit
-def kalman_filter(data, T, Z, R, H, Q, a0=None, P0=None, filter_type="standard"):
-    if filter_type not in ["standard", "univariate"]:
-        raise NotImplementedError('Only "standard" and "univariate" kalman filters are implemented')
-
-    a0, P0 = make_initial_conditions(T, R, Q, a0, P0)
-
-    if filter_type == "univariate":
-        filter_results = univariate_kalman_filter(data, T, Z, R, H, Q, a0, P0)
-    else:
-        filter_results = standard_kalman_filter(data, T, Z, R, H, Q, a0, P0)
-
-    return filter_results
+from typing import Tuple
+
+import numpy as np
+from numba import njit
+from numpy.typing import ArrayLike
+from scipy import linalg
+
+from gEconpy.numba_tools.overloads import (  # pylint: disable=unused-import
+    solve_discrete_lyapunov_impl,
+    solve_triangular_impl,
+)
+
+MVN_CONST = np.log(2.0 * np.pi)
+EPS = 1e-12
+
+
+@njit("float64[:, ::1](boolean[::1])")
+def build_mask_matrix(nan_mask: ArrayLike) -> ArrayLike:
+    """
+    The Kalman Filter can "natively" handle missing values by treating observed states as un-observed states for
+    iterations where data is not available. To do this, the Z and H matrices must be modified. This function creates
+    a matrix W such that W @ Z and W @ H have zeros where data is missing.
+    Parameters
+    ----------
+    nan_mask: array
+        A 1d array of boolean flags of length n, indicating whether a state is observed in the current iteration.
+    Returns
+    -------
+    W: array
+        An n x n matrix used to mask missing values in the Z and H matrices
+    """
+    n = nan_mask.shape[0]
+    W = np.eye(n)
+    i = 0
+    for flag in nan_mask:
+        if flag:
+            W[i, i] = 0
+        i += 1
+
+    W = np.ascontiguousarray(W)
+
+    return W
+
+
+@njit
+def standard_kalman_filter(
+    data: ArrayLike,
+    T: ArrayLike,
+    Z: ArrayLike,
+    R: ArrayLike,
+    H: ArrayLike,
+    Q: ArrayLike,
+    a0: ArrayLike,
+    P0: ArrayLike,
+) -> Tuple:
+    """
+    Parameters
+    ----------
+    data: array
+        (T, k_observed) matrix of observed data. Data can include missing values.
+    a0: array
+        (k_states, 1) vector of initial states.
+    P0: array
+        (k_states, k_states) initial state covariance matrix
+    T: array
+        (k_states, k_states) transition matrix
+    Z: array
+        (k_states, k_observed) design matrix
+    R: array
+    H: array
+    Q: array
+    Returns
+    -------
+    """
+    n_steps, k_obs = data.shape
+    k_states, k_posdef = R.shape
+
+    filtered_states = np.zeros((n_steps, k_states))
+    predicted_states = np.zeros((n_steps + 1, k_states))
+    filtered_cov = np.zeros((n_steps, k_states, k_states))
+    predicted_cov = np.zeros((n_steps + 1, k_states, k_states))
+    log_likelihood = np.zeros(n_steps)
+
+    a = a0
+    P = P0
+
+    predicted_states[0] = a
+    predicted_cov[0] = P
+
+    for i in range(n_steps):
+        a_filtered, a_hat, P_filtered, P_hat, ll = kalman_step(data[i].copy(), a, P, T, Z, R, H, Q)
+
+        filtered_states[i] = a_filtered[:, 0]
+        predicted_states[i + 1] = a_hat[:, 0]
+        filtered_cov[i] = P_filtered
+        predicted_cov[i + 1] = P_hat
+        log_likelihood[i] = ll[0]
+
+        a = a_hat
+        P = P_hat
+
+    return (
+        filtered_states,
+        predicted_states,
+        filtered_cov,
+        predicted_cov,
+        log_likelihood,
+    )
+
+
+@njit
+def kalman_step(y, a, P, T, Z, R, H, Q):
+    y = y.reshape(-1, 1)
+    nan_mask = np.isnan(y).ravel()
+    W = build_mask_matrix(nan_mask)
+
+    Z_masked = W @ Z
+    H_masked = W @ H
+    y_masked = y.copy()
+    y_masked[nan_mask] = 0.0
+
+    a_filtered, P_filtered, ll = filter_step(y_masked, Z_masked, H_masked, a, P)
+
+    a_hat, P_hat = predict(a=a_filtered, P=P_filtered, T=T, R=R, Q=Q)
+
+    return a_filtered, a_hat, P_filtered, P_hat, ll
+
+
+@njit(
+    "Tuple((float64[:, ::1], float64[:, ::1], float64[::1]))(float64[:, ::1], float64[:, ::1], float64[:, ::1], "
+    "float64[:, ::1], float64[:, ::1])"
+)
+def filter_step(y, Z, H, a, P):
+    v = y - Z @ a
+
+    PZT = P @ Z.T
+    F = Z @ PZT + H
+
+    # Special case for if everything is missing. Abort before failing to invert F
+    if np.all(Z == 0):
+        a_filtered = np.atleast_2d(a).reshape((-1, 1))
+        P_filtered = P
+        ll = np.zeros(v.shape[0])
+
+        return a_filtered, P_filtered, ll
+
+    F_chol = np.linalg.cholesky(F)
+    K = linalg.solve_triangular(
+        F_chol, linalg.solve_triangular(F_chol, PZT.T, lower=True), trans=1, lower=True
+    ).T
+
+    I_KZ = np.eye(K.shape[0]) - K @ Z
+
+    a_filtered = a + K @ v
+    P_filtered = I_KZ @ P @ I_KZ.T + K @ H @ K.T
+    P_filtered = 0.5 * (P_filtered + P_filtered.T)
+
+    inner_term = linalg.solve_triangular(
+        F_chol, linalg.solve_triangular(F_chol, v, lower=True), lower=True, trans=1
+    )
+    n = y.shape[0]
+    ll = -0.5 * (n * MVN_CONST + (v.T @ inner_term).ravel()) - np.log(np.diag(F_chol)).sum()
+
+    return a_filtered, P_filtered, ll
+
+
+@njit
+def predict(a, P, T, R, Q):
+    a_hat = T @ a
+
+    P_hat = T @ P @ T.T + R @ Q @ R.T
+    P_hat = 0.5 * (P_hat + P_hat.T)
+
+    return a_hat, P_hat
+
+
+@njit
+def univariate_kalman_filter(
+    data: ArrayLike,
+    T: ArrayLike,
+    Z: ArrayLike,
+    R: ArrayLike,
+    H: ArrayLike,
+    Q: ArrayLike,
+    a0: ArrayLike,
+    P0: ArrayLike,
+) -> Tuple:
+    n_steps, k_obs = data.shape
+    k_states, k_posdef = R.shape
+
+    filtered_states = np.zeros((n_steps, k_states))
+    predicted_states = np.zeros((n_steps + 1, k_states))
+    filtered_cov = np.zeros((n_steps, k_states, k_states))
+    predicted_cov = np.zeros((n_steps + 1, k_states, k_states))
+    log_likelihood = np.zeros(n_steps)
+
+    a = a0
+    P = P0
+
+    predicted_states[0] = a[:, 0]
+    predicted_cov[0] = P
+
+    for i in range(n_steps):
+        a_filtered, a_hat, P_filtered, P_hat, ll = univariate_kalman_step(
+            data[i].copy(), a, P, T, Z, R, H, Q
+        )
+
+        filtered_states[i] = a_filtered[:, 0]
+        predicted_states[i + 1] = a_hat[:, 0]
+        filtered_cov[i] = P_filtered
+        predicted_cov[i + 1] = P_hat
+        log_likelihood[i] = ll
+
+        a = a_hat
+        P = P_hat
+
+    return (
+        filtered_states,
+        predicted_states,
+        filtered_cov,
+        predicted_cov,
+        log_likelihood,
+    )
+
+
+@njit
+def univariate_kalman_step(y, a, P, T, Z, R, H, Q):
+    y = y.reshape(-1, 1)
+    nan_mask = np.isnan(y).ravel()
+    W = build_mask_matrix(nan_mask)
+
+    Z_masked = W @ Z
+    H_masked = W @ H
+    y_masked = y.copy()
+    y_masked[nan_mask] = 0.0
+
+    a_filtered, P_filtered, ll = univariate_filter_step(y_masked, Z_masked, H_masked, a, P)
+
+    a_hat, P_hat = predict(a=a_filtered, P=P_filtered, T=T, R=R, Q=Q)
+
+    return a_filtered, a_hat, P_filtered, P_hat, ll
+
+
+@njit
+def univariate_filter_step(y_masked, Z_masked, H_masked, a, P):
+    """
+    Univariate step that avoids inverting the F matrix by filtering one state at a time. Good for when the H matrix
+    isn't full rank (all economics problems)!
+    """
+
+    n_states = y_masked.shape[0]
+    a_filtered = a.copy()
+    P_filtered = P.copy()
+    ll_row = np.zeros(n_states)
+
+    for i in range(n_states):
+        a_filtered, P_filtered, ll = univariate_inner_step(
+            y_masked[i], Z_masked[i, :], H_masked[i, i], a_filtered, P_filtered
+        )
+        ll_row[i] = ll[0]
+
+    ll = -0.5 * ((ll_row != 0).sum() * MVN_CONST + ll_row.sum())
+    P_filtered = 0.5 * (P_filtered + P_filtered.T)
+
+    return a_filtered, P_filtered, ll
+
+
+@njit
+def univariate_inner_step(y, Z_row, sigma_H, a, P):
+    Z_row = np.atleast_2d(Z_row)
+    v = y - Z_row @ a
+
+    PZT = P @ Z_row.T
+    F = Z_row @ PZT + sigma_H
+
+    if F < EPS:
+        a_filtered = a
+        P_filtered = P
+        ll = np.zeros(v.shape[0])
+        return a_filtered, P_filtered, ll.ravel()
+
+    K = PZT / F
+    a_filtered = a + K * v
+    P_filtered = P - np.outer(K, K) * F
+    ll = np.log(F) + v**2 / F
+
+    return a_filtered, P_filtered, ll.ravel()
+
+
+@njit(
+    "Tuple((float64[:, ::1], float64[:, ::1]))(float64[:, ::1], float64[:, ::1], float64[:, ::1], "
+    "optional(float64[:, ::1]), optional(float64[:, ::1]))"
+)
+def make_initial_conditions(T, R, Q, a0, P0):
+    if a0 is None:
+        a0 = np.zeros((T.shape[0], 1))
+    if P0 is None:
+        P0 = linalg.solve_discrete_lyapunov(T, R @ Q @ R.T)
+
+    return a0, P0
+
+
+@njit
+def kalman_filter(data, T, Z, R, H, Q, a0=None, P0=None, filter_type="standard"):
+    if filter_type not in ["standard", "univariate"]:
+        raise NotImplementedError('Only "standard" and "univariate" kalman filters are implemented')
+
+    a0, P0 = make_initial_conditions(T, R, Q, a0, P0)
+
+    if filter_type == "univariate":
+        filter_results = univariate_kalman_filter(data, T, Z, R, H, Q, a0, P0)
+    else:
+        filter_results = standard_kalman_filter(data, T, Z, R, H, Q, a0, P0)
+
+    return filter_results
```

### Comparing `gEconpy-1.1.0/gEconpy/estimation/kalman_smoother.py` & `gEconpy-1.2.0/gEconpy/estimation/kalman_smoother.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,49 +1,49 @@
-import numba as nb
-import numpy as np
-
-
-@nb.njit
-def predict(a, P, T, R, Q):
-    a_hat = T @ a
-
-    P_hat = T @ P @ T.T + R @ Q @ R.T
-    P_hat = 0.5 * (P_hat + P_hat.T)
-
-    return a_hat, P_hat
-
-
-@nb.njit
-def kalman_smoother(T, R, Q, filtered_states, filtered_covariances):
-    n_steps, k_states = filtered_states.shape
-
-    smoothed_states = np.zeros((n_steps, k_states))
-    smoothed_covariances = np.zeros((n_steps, k_states, k_states))
-
-    a_smooth = filtered_states[-1].copy()
-    P_smooth = filtered_covariances[-1].copy()
-
-    smoothed_states[-1] = a_smooth
-    smoothed_covariances[-1] = P_smooth
-
-    for t in range(n_steps - 1, -1, -1):
-        a = filtered_states[t]
-        P = filtered_covariances[t]
-        a_smooth, P_smooth = smoother_step(a, P, a_smooth, P_smooth, T, R, Q)
-
-        smoothed_states[t] = a_smooth
-        smoothed_covariances[t] = P_smooth
-
-    return smoothed_states, smoothed_covariances
-
-
-@nb.njit
-def smoother_step(a, P, a_smooth, P_smooth, T, R, Q):
-    a_hat, P_hat = predict(a, P, T, R, Q)
-
-    # Use pinv, otherwise P_hat is singular when there is missing data
-    smoother_gain = (np.linalg.pinv(P_hat) @ T @ P).T
-
-    a_smooth_next = a + smoother_gain @ (a_smooth - a_hat)
-    P_smooth_next = P + smoother_gain @ (P_smooth - P_hat) @ smoother_gain.T
-
-    return a_smooth_next, P_smooth_next
+import numba as nb
+import numpy as np
+
+
+@nb.njit
+def predict(a, P, T, R, Q):
+    a_hat = T @ a
+
+    P_hat = T @ P @ T.T + R @ Q @ R.T
+    P_hat = 0.5 * (P_hat + P_hat.T)
+
+    return a_hat, P_hat
+
+
+@nb.njit
+def kalman_smoother(T, R, Q, filtered_states, filtered_covariances):
+    n_steps, k_states = filtered_states.shape
+
+    smoothed_states = np.zeros((n_steps, k_states))
+    smoothed_covariances = np.zeros((n_steps, k_states, k_states))
+
+    a_smooth = filtered_states[-1].copy()
+    P_smooth = filtered_covariances[-1].copy()
+
+    smoothed_states[-1] = a_smooth
+    smoothed_covariances[-1] = P_smooth
+
+    for t in range(n_steps - 1, -1, -1):
+        a = filtered_states[t]
+        P = filtered_covariances[t]
+        a_smooth, P_smooth = smoother_step(a, P, a_smooth, P_smooth, T, R, Q)
+
+        smoothed_states[t] = a_smooth
+        smoothed_covariances[t] = P_smooth
+
+    return smoothed_states, smoothed_covariances
+
+
+@nb.njit
+def smoother_step(a, P, a_smooth, P_smooth, T, R, Q):
+    a_hat, P_hat = predict(a, P, T, R, Q)
+
+    # Use pinv, otherwise P_hat is singular when there is missing data
+    smoother_gain = (np.linalg.pinv(P_hat) @ T @ P).T
+
+    a_smooth_next = a + smoother_gain @ (a_smooth - a_hat)
+    P_smooth_next = P + smoother_gain @ (P_smooth - P_hat) @ smoother_gain.T
+
+    return a_smooth_next, P_smooth_next
```

### Comparing `gEconpy-1.1.0/gEconpy/numba_linalg/LAPACK.py` & `gEconpy-1.2.0/gEconpy/numba_tools/LAPACK.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,276 +1,276 @@
-import ctypes
-
-from numba.extending import get_cython_function_address
-from numba.np.linalg import _blas_kinds, ensure_lapack
-
-_PTR = ctypes.POINTER
-
-_dbl = ctypes.c_double
-_float = ctypes.c_float
-_char = ctypes.c_char
-_int = ctypes.c_int
-
-_ptr_float = _PTR(_float)
-_ptr_dbl = _PTR(_dbl)
-_ptr_char = _PTR(_char)
-_ptr_int = _PTR(_int)
-
-
-def _get_float_pointer_for_dtype(blas_dtype):
-    if blas_dtype in ["s", "c"]:
-        return _ptr_float
-    elif blas_dtype in ["d", "z"]:
-        return _ptr_dbl
-
-
-class _LAPACK:
-    """
-    Functions to return type signatures for wrapped
-    LAPACK functions.
-    """
-
-    def __init__(self):
-        ensure_lapack()
-
-    @classmethod
-    def test_blas_kinds(cls, dtype):
-        return _blas_kinds[dtype]
-
-    @classmethod
-    def numba_rgees(cls, dtype):
-        d = _blas_kinds[dtype]
-        func_name = f"{d}gees"
-        float_pointer = _get_float_pointer_for_dtype(d)
-        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
-        functype = ctypes.CFUNCTYPE(
-            None,
-            _ptr_int,  # JOBVS
-            _ptr_int,  # SORT
-            _ptr_int,  # SELECT
-            _ptr_int,  # N
-            float_pointer,  # A
-            _ptr_int,  # LDA
-            _ptr_int,  # SDIM
-            float_pointer,  # WR
-            float_pointer,  # WI
-            float_pointer,  # VS
-            _ptr_int,  # LDVS
-            float_pointer,  # WORK
-            _ptr_int,  # LWORK
-            _ptr_int,  # BWORK
-            _ptr_int,
-        )  # INFO
-        return functype(addr)
-
-    @classmethod
-    def numba_cgees(cls, dtype):
-        d = _blas_kinds[dtype]
-        func_name = f"{d}gees"
-        float_pointer = _get_float_pointer_for_dtype(d)
-        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
-        functype = ctypes.CFUNCTYPE(
-            None,
-            _ptr_int,  # JOBVS
-            _ptr_int,  # SORT
-            _ptr_int,  # SELECT
-            _ptr_int,  # N
-            float_pointer,  # A
-            _ptr_int,  # LDA
-            _ptr_int,  # SDIM
-            float_pointer,  # W
-            float_pointer,  # VS
-            _ptr_int,  # LDVS
-            float_pointer,  # WORK
-            _ptr_int,  # LWORK
-            float_pointer,  # RWORK
-            _ptr_int,  # BWORK
-            _ptr_int,
-        )  # INFO
-        return functype(addr)
-
-    @classmethod
-    def numba_rgges(cls, dtype):
-        d = _blas_kinds[dtype]
-        func_name = f"{d}gges"
-        float_pointer = _get_float_pointer_for_dtype(d)
-        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
-
-        functype = ctypes.CFUNCTYPE(
-            None,
-            _ptr_int,  # JOBVSL
-            _ptr_int,  # JOBVSR
-            _ptr_int,  # SORT
-            _ptr_int,  # SELCTG
-            _ptr_int,  # N
-            float_pointer,  # A
-            _ptr_int,  # LDA
-            float_pointer,  # B
-            _ptr_int,  # LDB
-            _ptr_int,  # SDIM
-            float_pointer,  # ALPHAR
-            float_pointer,  # ALPHAI
-            float_pointer,  # BETA
-            float_pointer,  # VSL
-            _ptr_int,  # LDVSL
-            float_pointer,  # VSR
-            _ptr_int,  # LDVSR
-            float_pointer,  # WORK
-            _ptr_int,  # LWORK
-            _ptr_int,  # BWORK
-            _ptr_int,
-        )  # INFO
-        return functype(addr)
-
-    @classmethod
-    def numba_cgges(cls, dtype):
-        d = _blas_kinds[dtype]
-        func_name = f"{d}gges"
-        float_pointer = _get_float_pointer_for_dtype(d)
-        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
-
-        functype = ctypes.CFUNCTYPE(
-            None,
-            _ptr_int,  # JOBVSL
-            _ptr_int,  # JOBVSR
-            _ptr_int,  # SORT
-            _ptr_int,  # SELCTG
-            _ptr_int,  # N
-            float_pointer,  # A, complex
-            _ptr_int,  # LDA
-            float_pointer,  # B, complex
-            _ptr_int,  # LDB
-            _ptr_int,  # SDIM
-            float_pointer,  # ALPHA, complex
-            float_pointer,  # BETA, complex
-            float_pointer,  # VSL, complex
-            _ptr_int,  # LDVSL
-            float_pointer,  # VSR, complex
-            _ptr_int,  # LDVSR
-            float_pointer,  # WORK, complex
-            _ptr_int,  # LWORK
-            float_pointer,  # RWORK
-            _ptr_int,  # BWORK
-            _ptr_int,
-        )  # INFO
-        return functype(addr)
-
-    @classmethod
-    def numba_rtgsen(cls, dtype):
-        d = _blas_kinds[dtype]
-        func_name = f"{d}tgsen"
-        float_pointer = _get_float_pointer_for_dtype(d)
-        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
-
-        functype = ctypes.CFUNCTYPE(
-            None,
-            _ptr_int,  # IJOB
-            _ptr_int,  # WANTQ
-            _ptr_int,  # WANTZ
-            _ptr_int,  # SELECT
-            _ptr_int,  # N
-            float_pointer,  # A
-            _ptr_int,  # LDA
-            float_pointer,  # B
-            _ptr_int,  # LDB
-            float_pointer,  # ALPHAR
-            float_pointer,  # ALPHAI
-            float_pointer,  # BETA
-            float_pointer,  # Q
-            _ptr_int,  # LDQ
-            float_pointer,  # Z
-            _ptr_int,  # LDZ
-            _ptr_int,  # M
-            float_pointer,  # PL
-            float_pointer,  # PR
-            float_pointer,  # DIF
-            float_pointer,  # WORK
-            _ptr_int,  # LWORK
-            _ptr_int,  # IWORK
-            _ptr_int,  # LIWORK
-            _ptr_int,
-        )  # INFO
-        return functype(addr)
-
-    @classmethod
-    def numba_ctgsen(cls, dtype):
-        d = _blas_kinds[dtype]
-        func_name = f"{d}tgsen"
-        float_pointer = _get_float_pointer_for_dtype(d)
-        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
-
-        functype = ctypes.CFUNCTYPE(
-            None,
-            _ptr_int,  # IJOB
-            _ptr_int,  # WANTQ
-            _ptr_int,  # WANTZ
-            _ptr_int,  # SELECT
-            _ptr_int,  # N
-            float_pointer,  # A
-            _ptr_int,  # LDA
-            float_pointer,  # B
-            _ptr_int,  # LDB
-            float_pointer,  # ALPHA
-            float_pointer,  # BETA
-            float_pointer,  # Q
-            _ptr_int,  # LDQ
-            float_pointer,  # Z
-            _ptr_int,  # LDZ
-            _ptr_int,  # M
-            float_pointer,  # PL
-            float_pointer,  # PR
-            float_pointer,  # DIF
-            float_pointer,  # WORK
-            _ptr_int,  # LWORK
-            _ptr_int,  # IWORK
-            _ptr_int,  # LIWORK
-            _ptr_int,
-        )  # INFO
-        return functype(addr)
-
-    @classmethod
-    def numba_xtrsyl(cls, dtype):
-        d = _blas_kinds[dtype]
-        func_name = f"{d}trsyl"
-        float_pointer = _get_float_pointer_for_dtype(d)
-        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
-
-        functype = ctypes.CFUNCTYPE(
-            None,
-            _ptr_int,  # TRANA
-            _ptr_int,  # TRANB
-            _ptr_int,  # ISGN
-            _ptr_int,  # M
-            _ptr_int,  # N
-            float_pointer,  # A
-            _ptr_int,  # LDA
-            float_pointer,  # B
-            _ptr_int,  # LDB
-            float_pointer,  # C
-            _ptr_int,  # LDC
-            float_pointer,  # SCALE
-            _ptr_int,
-        )  # INFO
-        return functype(addr)
-
-    @classmethod
-    def numba_xtrtrs(cls, dtype):
-        d = _blas_kinds[dtype]
-        func_name = f"{d}trtrs"
-        float_pointer = _get_float_pointer_for_dtype(d)
-
-        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
-        functype = ctypes.CFUNCTYPE(
-            None,
-            _ptr_int,  # UPLO
-            _ptr_int,  # TRANS
-            _ptr_int,  # DIAG
-            _ptr_int,  # N
-            _ptr_int,  # NRHS
-            float_pointer,  # A
-            _ptr_int,  # LDA
-            float_pointer,  # B
-            _ptr_int,  # LDB
-            _ptr_int,
-        )  # INFO
-
-        return functype(addr)
+import ctypes
+
+from numba.extending import get_cython_function_address
+from numba.np.linalg import _blas_kinds, ensure_lapack
+
+_PTR = ctypes.POINTER
+
+_dbl = ctypes.c_double
+_float = ctypes.c_float
+_char = ctypes.c_char
+_int = ctypes.c_int
+
+_ptr_float = _PTR(_float)
+_ptr_dbl = _PTR(_dbl)
+_ptr_char = _PTR(_char)
+_ptr_int = _PTR(_int)
+
+
+def _get_float_pointer_for_dtype(blas_dtype):
+    if blas_dtype in ["s", "c"]:
+        return _ptr_float
+    elif blas_dtype in ["d", "z"]:
+        return _ptr_dbl
+
+
+class _LAPACK:
+    """
+    Functions to return type signatures for wrapped
+    LAPACK functions.
+    """
+
+    def __init__(self):
+        ensure_lapack()
+
+    @classmethod
+    def test_blas_kinds(cls, dtype):
+        return _blas_kinds[dtype]
+
+    @classmethod
+    def numba_rgees(cls, dtype):
+        d = _blas_kinds[dtype]
+        func_name = f"{d}gees"
+        float_pointer = _get_float_pointer_for_dtype(d)
+        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
+        functype = ctypes.CFUNCTYPE(
+            None,
+            _ptr_int,  # JOBVS
+            _ptr_int,  # SORT
+            _ptr_int,  # SELECT
+            _ptr_int,  # N
+            float_pointer,  # A
+            _ptr_int,  # LDA
+            _ptr_int,  # SDIM
+            float_pointer,  # WR
+            float_pointer,  # WI
+            float_pointer,  # VS
+            _ptr_int,  # LDVS
+            float_pointer,  # WORK
+            _ptr_int,  # LWORK
+            _ptr_int,  # BWORK
+            _ptr_int,
+        )  # INFO
+        return functype(addr)
+
+    @classmethod
+    def numba_cgees(cls, dtype):
+        d = _blas_kinds[dtype]
+        func_name = f"{d}gees"
+        float_pointer = _get_float_pointer_for_dtype(d)
+        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
+        functype = ctypes.CFUNCTYPE(
+            None,
+            _ptr_int,  # JOBVS
+            _ptr_int,  # SORT
+            _ptr_int,  # SELECT
+            _ptr_int,  # N
+            float_pointer,  # A
+            _ptr_int,  # LDA
+            _ptr_int,  # SDIM
+            float_pointer,  # W
+            float_pointer,  # VS
+            _ptr_int,  # LDVS
+            float_pointer,  # WORK
+            _ptr_int,  # LWORK
+            float_pointer,  # RWORK
+            _ptr_int,  # BWORK
+            _ptr_int,
+        )  # INFO
+        return functype(addr)
+
+    @classmethod
+    def numba_rgges(cls, dtype):
+        d = _blas_kinds[dtype]
+        func_name = f"{d}gges"
+        float_pointer = _get_float_pointer_for_dtype(d)
+        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
+
+        functype = ctypes.CFUNCTYPE(
+            None,
+            _ptr_int,  # JOBVSL
+            _ptr_int,  # JOBVSR
+            _ptr_int,  # SORT
+            _ptr_int,  # SELCTG
+            _ptr_int,  # N
+            float_pointer,  # A
+            _ptr_int,  # LDA
+            float_pointer,  # B
+            _ptr_int,  # LDB
+            _ptr_int,  # SDIM
+            float_pointer,  # ALPHAR
+            float_pointer,  # ALPHAI
+            float_pointer,  # BETA
+            float_pointer,  # VSL
+            _ptr_int,  # LDVSL
+            float_pointer,  # VSR
+            _ptr_int,  # LDVSR
+            float_pointer,  # WORK
+            _ptr_int,  # LWORK
+            _ptr_int,  # BWORK
+            _ptr_int,
+        )  # INFO
+        return functype(addr)
+
+    @classmethod
+    def numba_cgges(cls, dtype):
+        d = _blas_kinds[dtype]
+        func_name = f"{d}gges"
+        float_pointer = _get_float_pointer_for_dtype(d)
+        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
+
+        functype = ctypes.CFUNCTYPE(
+            None,
+            _ptr_int,  # JOBVSL
+            _ptr_int,  # JOBVSR
+            _ptr_int,  # SORT
+            _ptr_int,  # SELCTG
+            _ptr_int,  # N
+            float_pointer,  # A, complex
+            _ptr_int,  # LDA
+            float_pointer,  # B, complex
+            _ptr_int,  # LDB
+            _ptr_int,  # SDIM
+            float_pointer,  # ALPHA, complex
+            float_pointer,  # BETA, complex
+            float_pointer,  # VSL, complex
+            _ptr_int,  # LDVSL
+            float_pointer,  # VSR, complex
+            _ptr_int,  # LDVSR
+            float_pointer,  # WORK, complex
+            _ptr_int,  # LWORK
+            float_pointer,  # RWORK
+            _ptr_int,  # BWORK
+            _ptr_int,
+        )  # INFO
+        return functype(addr)
+
+    @classmethod
+    def numba_rtgsen(cls, dtype):
+        d = _blas_kinds[dtype]
+        func_name = f"{d}tgsen"
+        float_pointer = _get_float_pointer_for_dtype(d)
+        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
+
+        functype = ctypes.CFUNCTYPE(
+            None,
+            _ptr_int,  # IJOB
+            _ptr_int,  # WANTQ
+            _ptr_int,  # WANTZ
+            _ptr_int,  # SELECT
+            _ptr_int,  # N
+            float_pointer,  # A
+            _ptr_int,  # LDA
+            float_pointer,  # B
+            _ptr_int,  # LDB
+            float_pointer,  # ALPHAR
+            float_pointer,  # ALPHAI
+            float_pointer,  # BETA
+            float_pointer,  # Q
+            _ptr_int,  # LDQ
+            float_pointer,  # Z
+            _ptr_int,  # LDZ
+            _ptr_int,  # M
+            float_pointer,  # PL
+            float_pointer,  # PR
+            float_pointer,  # DIF
+            float_pointer,  # WORK
+            _ptr_int,  # LWORK
+            _ptr_int,  # IWORK
+            _ptr_int,  # LIWORK
+            _ptr_int,
+        )  # INFO
+        return functype(addr)
+
+    @classmethod
+    def numba_ctgsen(cls, dtype):
+        d = _blas_kinds[dtype]
+        func_name = f"{d}tgsen"
+        float_pointer = _get_float_pointer_for_dtype(d)
+        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
+
+        functype = ctypes.CFUNCTYPE(
+            None,
+            _ptr_int,  # IJOB
+            _ptr_int,  # WANTQ
+            _ptr_int,  # WANTZ
+            _ptr_int,  # SELECT
+            _ptr_int,  # N
+            float_pointer,  # A
+            _ptr_int,  # LDA
+            float_pointer,  # B
+            _ptr_int,  # LDB
+            float_pointer,  # ALPHA
+            float_pointer,  # BETA
+            float_pointer,  # Q
+            _ptr_int,  # LDQ
+            float_pointer,  # Z
+            _ptr_int,  # LDZ
+            _ptr_int,  # M
+            float_pointer,  # PL
+            float_pointer,  # PR
+            float_pointer,  # DIF
+            float_pointer,  # WORK
+            _ptr_int,  # LWORK
+            _ptr_int,  # IWORK
+            _ptr_int,  # LIWORK
+            _ptr_int,
+        )  # INFO
+        return functype(addr)
+
+    @classmethod
+    def numba_xtrsyl(cls, dtype):
+        d = _blas_kinds[dtype]
+        func_name = f"{d}trsyl"
+        float_pointer = _get_float_pointer_for_dtype(d)
+        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
+
+        functype = ctypes.CFUNCTYPE(
+            None,
+            _ptr_int,  # TRANA
+            _ptr_int,  # TRANB
+            _ptr_int,  # ISGN
+            _ptr_int,  # M
+            _ptr_int,  # N
+            float_pointer,  # A
+            _ptr_int,  # LDA
+            float_pointer,  # B
+            _ptr_int,  # LDB
+            float_pointer,  # C
+            _ptr_int,  # LDC
+            float_pointer,  # SCALE
+            _ptr_int,
+        )  # INFO
+        return functype(addr)
+
+    @classmethod
+    def numba_xtrtrs(cls, dtype):
+        d = _blas_kinds[dtype]
+        func_name = f"{d}trtrs"
+        float_pointer = _get_float_pointer_for_dtype(d)
+
+        addr = get_cython_function_address("scipy.linalg.cython_lapack", func_name)
+        functype = ctypes.CFUNCTYPE(
+            None,
+            _ptr_int,  # UPLO
+            _ptr_int,  # TRANS
+            _ptr_int,  # DIAG
+            _ptr_int,  # N
+            _ptr_int,  # NRHS
+            float_pointer,  # A
+            _ptr_int,  # LDA
+            float_pointer,  # B
+            _ptr_int,  # LDB
+            _ptr_int,
+        )  # INFO
+
+        return functype(addr)
```

### Comparing `gEconpy-1.1.0/gEconpy/numba_linalg/intrinsics.py` & `gEconpy-1.2.0/gEconpy/numba_tools/intrinsics.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,72 +1,72 @@
-from numba.core import cgutils, types
-from numba.extending import intrinsic
-
-
-@intrinsic
-def val_to_dptr(typingctx, data):
-    def impl(context, builder, signature, args):
-        ptr = cgutils.alloca_once_value(builder, args[0])
-        return ptr
-
-    sig = types.CPointer(types.float64)(types.float64)
-    return sig, impl
-
-
-@intrinsic
-def val_to_zptr(typingctx, data):
-    def impl(context, builder, signature, args):
-        ptr = cgutils.alloca_once_value(builder, args[0])
-        return ptr
-
-    sig = types.CPointer(types.complex128)(types.complex128)
-    return sig, impl
-
-
-@intrinsic
-def val_to_sptr(typingctx, data):
-    def impl(context, builder, signature, args):
-        ptr = cgutils.alloca_once_value(builder, args[0])
-        return ptr
-
-    sig = types.CPointer(types.float32)(types.float32)
-    return sig, impl
-
-
-@intrinsic
-def val_to_int_ptr(typingctx, data):
-    def impl(context, builder, signature, args):
-        ptr = cgutils.alloca_once_value(builder, args[0])
-        return ptr
-
-    sig = types.CPointer(types.int32)(types.int32)
-    return sig, impl
-
-
-@intrinsic
-def int_ptr_to_val(typingctx, data):
-    def impl(context, builder, signature, args):
-        val = builder.load(args[0])
-        return val
-
-    sig = types.int32(types.CPointer(types.int32))
-    return sig, impl
-
-
-@intrinsic
-def dptr_to_val(typingctx, data):
-    def impl(context, builder, signature, args):
-        val = builder.load(args[0])
-        return val
-
-    sig = types.float64(types.CPointer(types.float64))
-    return sig, impl
-
-
-@intrinsic
-def sptr_to_val(typingctx, data):
-    def impl(context, builder, signature, args):
-        val = builder.load(args[0])
-        return val
-
-    sig = types.float32(types.CPointer(types.float32))
-    return sig, impl
+from numba.core import cgutils, types
+from numba.extending import intrinsic
+
+
+@intrinsic
+def val_to_dptr(typingctx, data):
+    def impl(context, builder, signature, args):
+        ptr = cgutils.alloca_once_value(builder, args[0])
+        return ptr
+
+    sig = types.CPointer(types.float64)(types.float64)
+    return sig, impl
+
+
+@intrinsic
+def val_to_zptr(typingctx, data):
+    def impl(context, builder, signature, args):
+        ptr = cgutils.alloca_once_value(builder, args[0])
+        return ptr
+
+    sig = types.CPointer(types.complex128)(types.complex128)
+    return sig, impl
+
+
+@intrinsic
+def val_to_sptr(typingctx, data):
+    def impl(context, builder, signature, args):
+        ptr = cgutils.alloca_once_value(builder, args[0])
+        return ptr
+
+    sig = types.CPointer(types.float32)(types.float32)
+    return sig, impl
+
+
+@intrinsic
+def val_to_int_ptr(typingctx, data):
+    def impl(context, builder, signature, args):
+        ptr = cgutils.alloca_once_value(builder, args[0])
+        return ptr
+
+    sig = types.CPointer(types.int32)(types.int32)
+    return sig, impl
+
+
+@intrinsic
+def int_ptr_to_val(typingctx, data):
+    def impl(context, builder, signature, args):
+        val = builder.load(args[0])
+        return val
+
+    sig = types.int32(types.CPointer(types.int32))
+    return sig, impl
+
+
+@intrinsic
+def dptr_to_val(typingctx, data):
+    def impl(context, builder, signature, args):
+        val = builder.load(args[0])
+        return val
+
+    sig = types.float64(types.CPointer(types.float64))
+    return sig, impl
+
+
+@intrinsic
+def sptr_to_val(typingctx, data):
+    def impl(context, builder, signature, args):
+        val = builder.load(args[0])
+        return val
+
+    sig = types.float32(types.CPointer(types.float32))
+    return sig, impl
```

### Comparing `gEconpy-1.1.0/gEconpy/numba_linalg/overloads.py` & `gEconpy-1.2.0/gEconpy/numba_tools/overloads.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,874 +1,874 @@
-import numpy as np
-import scipy
-from numba.core import types
-from numba.extending import overload
-from numba.np.linalg import (
-    _check_finite_matrix,
-    _copy_to_fortran_order,
-    _handle_err_maybe_convergence_problem,
-    ensure_lapack,
-)
-from scipy import linalg
-
-from gEconpy.numba_linalg.intrinsics import int_ptr_to_val, val_to_int_ptr
-from gEconpy.numba_linalg.LAPACK import _LAPACK
-from gEconpy.numba_linalg.utilities import (
-    _check_scipy_linalg_matrix,
-    _get_underlying_float,
-    _iuc,
-    _lhp,
-    _ouc,
-    _rhp,
-    direct_lyapunov_solution,
-)
-
-
-@overload(scipy.linalg.solve_triangular)
-def solve_triangular_impl(A, B, trans=0, lower=False, unit_diagonal=False):
-    ensure_lapack()
-
-    _check_scipy_linalg_matrix(A, "solve_triangular")
-    _check_scipy_linalg_matrix(B, "solve_triangular")
-
-    dtype = A.dtype
-    w_type = _get_underlying_float(dtype)
-
-    numba_trtrs = _LAPACK().numba_xtrtrs(dtype)
-
-    def impl(A, B, trans=0, lower=False, unit_diagonal=False):
-        _N = np.int32(A.shape[-1])
-        if A.shape[-2] != _N:
-            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
-
-        if A.shape[0] != B.shape[0]:
-            raise linalg.LinAlgError("Dimensions of A and B do not conform")
-
-        A_copy = _copy_to_fortran_order(A)
-        B_copy = _copy_to_fortran_order(B)
-
-        # if isinstance(trans, str):
-        # if trans not in ['N', 'C', 'T']:
-        #     raise ValueError('Parameter "trans" should be one of N, C, T or 0, 1, 2')
-        # transval = ord(trans)
-
-        # else:
-        if trans not in [0, 1, 2]:
-            raise ValueError('Parameter "trans" should be one of N, C, T or 0, 1, 2')
-        if trans == 0:
-            transval = ord("N")
-        elif trans == 1:
-            transval = ord("T")
-        else:
-            transval = ord("C")
-
-        UPLO = val_to_int_ptr(ord("L") if lower else ord("U"))
-        TRANS = val_to_int_ptr(transval)
-        DIAG = val_to_int_ptr(ord("U") if unit_diagonal else ord("N"))
-        N = val_to_int_ptr(_N)
-        NRHS = val_to_int_ptr(B.shape[1])
-        LDA = val_to_int_ptr(_N)
-        LDB = val_to_int_ptr(_N)
-        INFO = val_to_int_ptr(0)
-
-        numba_trtrs(
-            UPLO,
-            TRANS,
-            DIAG,
-            N,
-            NRHS,
-            A_copy.view(w_type).ctypes,
-            LDA,
-            B_copy.view(w_type).ctypes,
-            LDB,
-            INFO,
-        )
-
-        return B_copy
-
-    return impl
-
-
-@overload(scipy.linalg.schur)
-def schur_impl(A, output):
-    ensure_lapack()
-
-    _check_scipy_linalg_matrix(A, "schur")
-
-    dtype = A.dtype
-    w_type = _get_underlying_float(dtype)
-
-    numba_rgees = _LAPACK().numba_rgees(dtype)
-    numba_cgees = _LAPACK().numba_cgees(dtype)
-
-    def real_schur_impl(A, output):
-        """
-        schur() implementation for real arrays
-        """
-        _N = np.int32(A.shape[-1])
-        if A.shape[-2] != _N:
-            msg = "Last 2 dimensions of the array must be square"
-            raise linalg.LinAlgError(msg)
-
-        _check_finite_matrix(A)
-        A_copy = _copy_to_fortran_order(A)
-
-        JOBVS = val_to_int_ptr(ord("V"))
-        SORT = val_to_int_ptr(ord("N"))
-        SELECT = val_to_int_ptr(0.0)
-
-        N = val_to_int_ptr(_N)
-        LDA = val_to_int_ptr(_N)
-        SDIM = val_to_int_ptr(_N)
-        WR = np.empty(_N, dtype=dtype)
-        WI = np.empty(_N, dtype=dtype)
-        _LDVS = _N
-        LDVS = val_to_int_ptr(_N)
-        VS = np.empty((_LDVS, _N), dtype=dtype)
-        LWORK = val_to_int_ptr(-1)
-        WORK = np.empty(1, dtype=dtype)
-        BWORK = val_to_int_ptr(1)
-        INFO = val_to_int_ptr(1)
-
-        # workspace query
-        numba_rgees(
-            JOBVS,
-            SORT,
-            SELECT,
-            N,
-            A_copy.ctypes,
-            LDA,
-            SDIM,
-            WR.ctypes,
-            WI.ctypes,
-            VS.ctypes,
-            LDVS,
-            WORK.ctypes,
-            LWORK,
-            BWORK,
-            INFO,
-        )
-        WS_SIZE = np.int32(WORK[0].real)
-        LWORK = val_to_int_ptr(WS_SIZE)
-        WORK = np.empty(WS_SIZE, dtype=dtype)
-
-        # Actual work
-        numba_rgees(
-            JOBVS,
-            SORT,
-            SELECT,
-            N,
-            A_copy.ctypes,
-            LDA,
-            SDIM,
-            WR.ctypes,
-            WI.ctypes,
-            VS.ctypes,
-            LDVS,
-            WORK.ctypes,
-            LWORK,
-            BWORK,
-            INFO,
-        )
-
-        # if np.any(WI) and output == 'complex':
-        #     raise ValueError("schur() argument must not cause a domain change.")
-        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
-
-        return A_copy, VS.T
-
-    def complex_schur_impl(A, output):
-        """
-        schur() implementation for complex arrays
-        """
-
-        _N = np.int32(A.shape[-1])
-        if A.shape[-2] != _N:
-            msg = "Last 2 dimensions of the array must be square"
-            raise linalg.LinAlgError(msg)
-
-        _check_finite_matrix(A)
-        A_copy = _copy_to_fortran_order(A)
-
-        JOBVS = val_to_int_ptr(ord("V"))
-        SORT = val_to_int_ptr(ord("N"))
-        SELECT = val_to_int_ptr(0.0)
-
-        N = val_to_int_ptr(_N)
-        LDA = val_to_int_ptr(_N)
-        SDIM = val_to_int_ptr(_N)
-        W = np.empty(_N, dtype=dtype)
-        _LDVS = _N
-        LDVS = val_to_int_ptr(_N)
-        VS = np.empty((_LDVS, _N), dtype=dtype)
-        LWORK = val_to_int_ptr(-1)
-        WORK = np.empty(1, dtype=dtype)
-        RWORK = np.empty(_N, dtype=w_type)
-        BWORK = val_to_int_ptr(1)
-        INFO = val_to_int_ptr(1)
-
-        # workspace query
-        numba_cgees(
-            JOBVS,
-            SORT,
-            SELECT,
-            N,
-            A_copy.view(w_type).ctypes,
-            LDA,
-            SDIM,
-            W.view(w_type).ctypes,
-            VS.view(w_type).ctypes,
-            LDVS,
-            WORK.view(w_type).ctypes,
-            LWORK,
-            RWORK.ctypes,
-            BWORK,
-            INFO,
-        )
-
-        WS_SIZE = np.int32(WORK[0].real)
-        LWORK = val_to_int_ptr(WS_SIZE)
-        WORK = np.empty(WS_SIZE, dtype=dtype)
-
-        # Actual work
-        numba_cgees(
-            JOBVS,
-            SORT,
-            SELECT,
-            N,
-            A_copy.view(w_type).ctypes,
-            LDA,
-            SDIM,
-            W.view(w_type).ctypes,
-            VS.view(w_type).ctypes,
-            LDVS,
-            WORK.view(w_type).ctypes,
-            LWORK,
-            RWORK.ctypes,
-            BWORK,
-            INFO,
-        )
-
-        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
-
-        return A_copy, VS.T
-
-    if isinstance(A.dtype, types.scalars.Complex):
-        return complex_schur_impl
-    else:
-        return real_schur_impl
-
-
-def full_return_qz(A, B, output):
-    pass
-
-
-@overload(full_return_qz)
-def full_return_qz_impl(A, B, output):
-    ensure_lapack()
-
-    _check_scipy_linalg_matrix(A, "qz")
-    _check_scipy_linalg_matrix(B, "qz")
-
-    dtype = A.dtype
-    w_type = _get_underlying_float(dtype)
-
-    numba_rgges = _LAPACK().numba_rgges(dtype)
-    numba_cgges = _LAPACK().numba_cgges(dtype)
-
-    def real_full_return_qz_impl(A, B, output):
-        """
-        schur() implementation for real arrays. Unlike the Scipy function, this has 5 returns, including the
-        generalized eigenvalues (alpha, beta), because these are required by ordqz.
-        """
-        _M, _N = np.int32(A.shape[-2:])
-        if A.shape[-2] != _N:
-            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
-        if B.shape[-2] != _N:
-            raise linalg.LinAlgError("Last 2 dimensions of B must be square")
-
-        _check_finite_matrix(A)
-        _check_finite_matrix(B)
-
-        A_copy = _copy_to_fortran_order(A)
-        B_copy = _copy_to_fortran_order(B)
-
-        JOBVSL = val_to_int_ptr(ord("V"))
-        JOBVSR = val_to_int_ptr(ord("V"))
-        SORT = val_to_int_ptr(ord("N"))
-        SELCTG = val_to_int_ptr(1)
-
-        N = val_to_int_ptr(_N)
-        LDA = val_to_int_ptr(_N)
-        LDB = val_to_int_ptr(_N)
-        SDIM = val_to_int_ptr(0)
-
-        ALPHAR = np.empty(_N, dtype=dtype)  # out
-        ALPHAI = np.empty(_N, dtype=dtype)  # out
-        BETA = np.empty(_N, dtype=dtype)  # out
-
-        _LDVSL = _N
-        _LDVSR = _N
-        LDVSL = val_to_int_ptr(_LDVSL)
-        VSL = np.empty((_LDVSL, _N), dtype=dtype)  # out
-        LDVSR = val_to_int_ptr(_LDVSR)
-        VSR = np.empty((_LDVSR, _N), dtype=dtype)  # out
-
-        WORK = np.empty((1,), dtype=dtype)  # out
-        LWORK = val_to_int_ptr(-1)
-        BWORK = val_to_int_ptr(1)
-        INFO = val_to_int_ptr(1)
-
-        # workspace query
-        numba_rgges(
-            JOBVSL,
-            JOBVSR,
-            SORT,
-            SELCTG,
-            N,
-            A_copy.ctypes,
-            LDA,
-            B_copy.ctypes,
-            LDB,
-            SDIM,
-            ALPHAR.ctypes,
-            ALPHAI.ctypes,
-            BETA.ctypes,
-            VSL.ctypes,
-            LDVSL,
-            VSR.ctypes,
-            LDVSR,
-            WORK.ctypes,
-            LWORK,
-            BWORK,
-            INFO,
-        )
-
-        WS_SIZE = np.int32(WORK[0].real)
-        LWORK = val_to_int_ptr(WS_SIZE)
-        WORK = np.empty(WS_SIZE, dtype=dtype)
-
-        # Actual work
-        numba_rgges(
-            JOBVSL,
-            JOBVSR,
-            SORT,
-            SELCTG,
-            N,
-            A_copy.ctypes,
-            LDA,
-            B_copy.ctypes,
-            LDB,
-            SDIM,
-            ALPHAR.ctypes,
-            ALPHAI.ctypes,
-            BETA.ctypes,
-            VSL.ctypes,
-            LDVSL,
-            VSR.ctypes,
-            LDVSR,
-            WORK.ctypes,
-            LWORK,
-            BWORK,
-            INFO,
-        )
-
-        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
-        ALPHA = ALPHAR + ALPHAI * 1j
-
-        return A_copy, B_copy, ALPHA, BETA, VSL.T, VSR.T
-
-    def complex_full_return_qz_impl(A, B, output):
-        """
-        qz decomposition for complex arrays. Unlike the Scipy function, this has 5 returns, including the
-        generalized eigenvalues (alpha, beta), because these are required by ordqz.
-        """
-
-        _M, _N = np.int32(A.shape[-2:])
-        if A.shape[-2] != _N:
-            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
-        if B.shape[-2] != _N:
-            raise linalg.LinAlgError("Last 2 dimensions of B must be square")
-
-        _check_finite_matrix(A)
-        _check_finite_matrix(B)
-
-        A_copy = _copy_to_fortran_order(A)
-        B_copy = _copy_to_fortran_order(B)
-
-        JOBVSL = val_to_int_ptr(ord("V"))
-        JOBVSR = val_to_int_ptr(ord("V"))
-        SORT = val_to_int_ptr(ord("N"))
-        SELCTG = val_to_int_ptr(1)
-
-        N = val_to_int_ptr(_N)
-        LDA = val_to_int_ptr(_N)
-        LDB = val_to_int_ptr(_N)
-        SDIM = val_to_int_ptr(0)
-
-        ALPHA = np.empty(_N, dtype=dtype)  # out
-        BETA = np.empty(_N, dtype=dtype)  # out
-        LDVSL = val_to_int_ptr(_N)
-        VSL = np.empty((_N, _N), dtype=dtype)  # out
-        LDVSR = val_to_int_ptr(_N)
-        VSR = np.empty((_N, _N), dtype=dtype)  # out
-
-        WORK = np.empty((1,), dtype=dtype)  # out
-        LWORK = val_to_int_ptr(-1)
-        RWORK = np.empty(8 * _N, dtype=w_type)
-        BWORK = val_to_int_ptr(1)
-        INFO = val_to_int_ptr(1)
-
-        # workspace query
-        numba_cgges(
-            JOBVSL,
-            JOBVSR,
-            SORT,
-            SELCTG,
-            N,
-            A_copy.view(w_type).ctypes,
-            LDA,
-            B_copy.view(w_type).ctypes,
-            LDB,
-            SDIM,
-            ALPHA.view(w_type).ctypes,
-            BETA.view(w_type).ctypes,
-            VSL.view(w_type).ctypes,
-            LDVSL,
-            VSR.view(w_type).ctypes,
-            LDVSR,
-            WORK.view(w_type).ctypes,
-            LWORK,
-            RWORK.ctypes,
-            BWORK,
-            INFO,
-        )
-
-        WS_SIZE = np.int32(WORK[0].real)
-        LWORK = val_to_int_ptr(WS_SIZE)
-        WORK = np.empty(WS_SIZE, dtype=dtype)
-
-        # Actual work
-        numba_cgges(
-            JOBVSL,
-            JOBVSR,
-            SORT,
-            SELCTG,
-            N,
-            A_copy.view(w_type).ctypes,
-            LDA,
-            B_copy.view(w_type).ctypes,
-            LDB,
-            SDIM,
-            ALPHA.view(w_type).ctypes,
-            BETA.view(w_type).ctypes,
-            VSL.view(w_type).ctypes,
-            LDVSL,
-            VSR.view(w_type).ctypes,
-            LDVSR,
-            WORK.view(w_type).ctypes,
-            LWORK,
-            RWORK.ctypes,
-            BWORK,
-            INFO,
-        )
-
-        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
-
-        return A_copy, B_copy, ALPHA, BETA, VSL.T, VSR.T
-
-    if isinstance(A.dtype, types.scalars.Complex):
-        return complex_full_return_qz_impl
-    else:
-        return real_full_return_qz_impl
-
-
-@overload(scipy.linalg.qz)
-def qz_impl(A, B, output):
-    """
-    scipy.linalg.qz overload. Wraps full_return_qz and returns only A, B, Q ,Z to match the scipy signature.
-    """
-    ensure_lapack()
-
-    _check_scipy_linalg_matrix(A, "qz")
-    _check_scipy_linalg_matrix(B, "qz")
-
-    def real_qz_impl(A, B, output):
-        A, B, ALPHA, BETA, VSL, VSR = full_return_qz(A, B, output)
-
-        return A, B, VSL, VSR
-
-    def complex_qz_impl(A, B, output):
-        A, B, ALPHA, BETA, VSL, VSR = full_return_qz(A, B, output)
-        return A, B, VSL, VSR
-
-    if isinstance(A.dtype, types.scalars.Complex):
-        return complex_qz_impl
-    else:
-        return real_qz_impl
-
-
-@overload(scipy.linalg.ordqz)
-def ordqz_impl(A, B, sort, output):
-    ensure_lapack()
-
-    _check_scipy_linalg_matrix(A, "ordqz")
-    _check_scipy_linalg_matrix(B, "ordqz")
-
-    dtype = A.dtype
-    w_type = _get_underlying_float(dtype)
-
-    numba_rtgsen = _LAPACK().numba_rtgsen(dtype)
-    numba_ctgsen = _LAPACK().numba_ctgsen(dtype)
-
-    def real_ordqz_impl(A, B, sort, output):
-        _M, _N = np.int32(A.shape[-2:])
-        if A.shape[-2] != _N:
-            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
-        if B.shape[-2] != _N:
-            raise linalg.LinAlgError("Last 2 dimensions of B must be square")
-
-        _check_finite_matrix(A)
-        _check_finite_matrix(B)
-
-        if sort not in ["lhp", "rhp", "iuc", "ouc"]:
-            raise ValueError('Argument "sort" should be one of: "lhp", "rhp", "iuc", "ouc"')
-
-        A_copy = _copy_to_fortran_order(A)
-        B_copy = _copy_to_fortran_order(B)
-
-        AA, BB, ALPHA, BETA, Q, Z = full_return_qz(A_copy, B_copy, output)
-
-        if sort == "lhp":
-            SELECT = _lhp(ALPHA, BETA)
-        elif sort == "rhp":
-            SELECT = _rhp(ALPHA, BETA)
-        elif sort == "iuc":
-            SELECT = _iuc(ALPHA, BETA)
-        elif sort == "ouc":
-            SELECT = _ouc(ALPHA, BETA)
-
-        IJOB = val_to_int_ptr(0)
-        WANTQ = val_to_int_ptr(1)
-        WANTZ = val_to_int_ptr(1)
-        N = val_to_int_ptr(_N)
-        LDA = val_to_int_ptr(_M)
-        LDB = val_to_int_ptr(_M)
-
-        ALPHAR = np.empty(_N, dtype=dtype)
-        ALPHAI = np.empty(_N, dtype=dtype)
-
-        LDQ = val_to_int_ptr(Q.shape[0])
-        LDZ = val_to_int_ptr(Z.shape[0])
-        M = val_to_int_ptr(_M)
-        PL = np.empty(1, dtype=dtype)
-        PR = np.empty(1, dtype=dtype)
-        DIF = np.empty(2, dtype=dtype)
-        WORK = np.empty(1, dtype=dtype)
-        LWORK = val_to_int_ptr(-1)
-        IWORK = np.empty(1, dtype=np.int32)
-        LIWORK = val_to_int_ptr(-1)
-        INFO = val_to_int_ptr(1)
-
-        # workspace query
-        numba_rtgsen(
-            IJOB,
-            WANTQ,
-            WANTZ,
-            SELECT.ctypes,
-            N,
-            AA.ctypes,
-            LDA,
-            BB.ctypes,
-            LDB,
-            ALPHAR.ctypes,
-            ALPHAI.ctypes,
-            BETA.ctypes,
-            Q.ctypes,
-            LDQ,
-            Z.ctypes,
-            LDZ,
-            M,
-            PL.ctypes,
-            PR.ctypes,
-            DIF.ctypes,
-            WORK.ctypes,
-            LWORK,
-            IWORK.ctypes,
-            LIWORK,
-            INFO,
-        )
-
-        WS_SIZE = np.int32(WORK[0].real)
-        IW_SIZE = np.int32(IWORK[0].real)
-        LWORK = val_to_int_ptr(WS_SIZE)
-        LIWORK = val_to_int_ptr(IW_SIZE)
-        WORK = np.empty(WS_SIZE, dtype=dtype)
-        IWORK = np.empty(IW_SIZE, dtype=np.int32)
-
-        numba_rtgsen(
-            IJOB,
-            WANTQ,
-            WANTZ,
-            SELECT.ctypes,
-            N,
-            AA.ctypes,
-            LDA,
-            BB.ctypes,
-            LDB,
-            ALPHAR.ctypes,
-            ALPHAI.ctypes,
-            BETA.ctypes,
-            Q.ctypes,
-            LDQ,
-            Z.ctypes,
-            LDZ,
-            M,
-            PL.ctypes,
-            PR.ctypes,
-            DIF.ctypes,
-            WORK.ctypes,
-            LWORK,
-            IWORK.ctypes,
-            LIWORK,
-            INFO,
-        )
-
-        # if np.any(ALPHAI) and output == 'complex':
-        #     raise ValueError("ordqz() argument must not cause a domain change.")
-        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
-        ALPHA = ALPHAR + 1j * ALPHAI
-        return AA, BB, ALPHA, BETA, Q, Z
-
-    def complex_ordqz_impl(A, B, sort, output):
-        _M, _N = np.int32(A.shape[-2:])
-        if A.shape[-2] != _N:
-            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
-        if B.shape[-2] != _N:
-            raise linalg.LinAlgError("Last 2 dimensions of B must be square")
-
-        _check_finite_matrix(A)
-        _check_finite_matrix(B)
-
-        if sort not in ["lhp", "rhp", "iuc", "ouc"]:
-            raise ValueError('Argument "sort" should be one of: "lhp", "rhp", "iuc", "ouc"')
-
-        A_copy = _copy_to_fortran_order(A)
-        B_copy = _copy_to_fortran_order(B)
-
-        AA, BB, ALPHA, BETA, Q, Z = full_return_qz(A_copy, B_copy, output)
-
-        if sort == "lhp":
-            SELECT = _lhp(ALPHA, BETA)
-        elif sort == "rhp":
-            SELECT = _rhp(ALPHA, BETA)
-        elif sort == "iuc":
-            SELECT = _iuc(ALPHA, BETA)
-        elif sort == "ouc":
-            SELECT = _ouc(ALPHA, BETA)
-
-        IJOB = val_to_int_ptr(0)
-        WANTQ = val_to_int_ptr(1)
-        WANTZ = val_to_int_ptr(1)
-        N = val_to_int_ptr(_N)
-        LDA = val_to_int_ptr(_M)
-        LDB = val_to_int_ptr(_M)
-
-        LDQ = val_to_int_ptr(Q.shape[0])
-        LDZ = val_to_int_ptr(Z.shape[0])
-        M = val_to_int_ptr(_M)
-        PL = np.empty(1, dtype=w_type)
-        PR = np.empty(1, dtype=w_type)
-        DIF = np.empty(2, dtype=w_type)
-        WORK = np.empty(1, dtype=dtype)
-        LWORK = val_to_int_ptr(-1)
-        IWORK = np.empty(1, dtype=np.int32)
-        LIWORK = val_to_int_ptr(-1)
-        INFO = val_to_int_ptr(1)
-
-        # workspace query
-        numba_ctgsen(
-            IJOB,
-            WANTQ,
-            WANTZ,
-            SELECT.ctypes,
-            N,
-            AA.view(w_type).ctypes,
-            LDA,
-            BB.view(w_type).ctypes,
-            LDB,
-            ALPHA.view(w_type).ctypes,
-            BETA.view(w_type).ctypes,
-            Q.view(w_type).ctypes,
-            LDQ,
-            Z.view(w_type).ctypes,
-            LDZ,
-            M,
-            PL.ctypes,
-            PR.ctypes,
-            DIF.ctypes,
-            WORK.view(w_type).ctypes,
-            LWORK,
-            IWORK.ctypes,
-            LIWORK,
-            INFO,
-        )
-
-        WS_SIZE = np.int32(WORK[0].real)
-        IW_SIZE = np.int32(IWORK[0].real)
-        LWORK = val_to_int_ptr(WS_SIZE)
-        LIWORK = val_to_int_ptr(IW_SIZE)
-        WORK = np.empty(WS_SIZE, dtype=dtype)
-        IWORK = np.empty(IW_SIZE, dtype=np.int32)
-
-        numba_ctgsen(
-            IJOB,
-            WANTQ,
-            WANTZ,
-            SELECT.ctypes,
-            N,
-            AA.view(w_type).ctypes,
-            LDA,
-            BB.view(w_type).ctypes,
-            LDB,
-            ALPHA.view(w_type).ctypes,
-            BETA.view(w_type).ctypes,
-            Q.view(w_type).ctypes,
-            LDQ,
-            Z.view(w_type).ctypes,
-            LDZ,
-            M,
-            PL.ctypes,
-            PR.ctypes,
-            DIF.ctypes,
-            WORK.view(w_type).ctypes,
-            LWORK,
-            IWORK.ctypes,
-            LIWORK,
-            INFO,
-        )
-
-        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
-
-        return AA, BB, ALPHA, BETA, Q, Z
-
-    if isinstance(A.dtype, types.scalars.Complex):
-        return complex_ordqz_impl
-    else:
-        return real_ordqz_impl
-
-
-@overload(scipy.linalg.solve_continuous_lyapunov)
-def solve_continuous_lyapunov_impl(A, Q):
-    ensure_lapack()
-
-    _check_scipy_linalg_matrix(A, "solve_continuous_lyapunov")
-    _check_scipy_linalg_matrix(Q, "solve_continuous_lyapunov")
-
-    dtype = A.dtype
-    w_type = _get_underlying_float(dtype)
-
-    numba_xtrsyl = _LAPACK().numba_xtrsyl(dtype)
-
-    def _solve_cont_lyapunov_impl(A, Q):
-        _M, _N = np.int32(A.shape)
-        _NQ = np.int32(Q.shape[-1])
-
-        if _N != _NQ:
-            raise linalg.LinAlgError("Matrices A and Q must have the same shape")
-
-        if _M != _N:
-            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
-        if Q.shape[-2] != _NQ:
-            raise linalg.LinAlgError("Last 2 dimensions of Q must be square")
-
-        _check_finite_matrix(A)
-        _check_finite_matrix(Q)
-
-        is_complex = np.iscomplexobj(A) | np.iscomplexobj(Q)
-        dtype_letter = "C" if is_complex else "T"
-        output = "complex" if is_complex else "real"
-
-        A_copy = _copy_to_fortran_order(A)
-        Q_copy = _copy_to_fortran_order(Q)
-
-        R, U = linalg.schur(A_copy, output=output)
-
-        # Construct f = u'*q*u
-        F = U.conj().T.dot(Q_copy.dot(U))
-
-        TRANA = val_to_int_ptr(ord("N"))
-        TRANB = val_to_int_ptr(ord(dtype_letter))
-        ISGN = val_to_int_ptr(1)
-
-        M = val_to_int_ptr(_N)
-        N = val_to_int_ptr(_N)
-        AA = _copy_to_fortran_order(R)
-        LDA = val_to_int_ptr(_N)
-        B = _copy_to_fortran_order(R)
-        LDB = val_to_int_ptr(_N)
-        C = _copy_to_fortran_order(F)
-        LDC = val_to_int_ptr(_N)
-
-        # TODO: There is a little bit of overhead here, can I figure out how to assign a
-        #  float or double pointer, depending on the case?
-        SCALE = np.array(1.0, dtype=w_type)
-        INFO = val_to_int_ptr(1)
-
-        numba_xtrsyl(
-            TRANA,
-            TRANB,
-            ISGN,
-            M,
-            N,
-            AA.view(w_type).ctypes,
-            LDA,
-            B.view(w_type).ctypes,
-            LDB,
-            C.view(w_type).ctypes,
-            LDC,
-            SCALE.ctypes,
-            INFO,
-        )
-
-        C *= SCALE
-        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
-        X = U.dot(C).dot(U.conj().T)
-
-        return X
-
-    return _solve_cont_lyapunov_impl
-
-
-@overload(scipy.linalg.solve_discrete_lyapunov)
-def solve_discrete_lyapunov_impl(A, Q, method="auto"):
-    ensure_lapack()
-
-    _check_scipy_linalg_matrix(A, "solve_continuous_lyapunov")
-    _check_scipy_linalg_matrix(Q, "solve_continuous_lyapunov")
-
-    dtype = A.dtype
-    w_type = _get_underlying_float(dtype)
-
-    def impl(A, Q, method="auto"):
-        _M, _N = np.int32(A.shape)
-
-        if method == "auto":
-            if _M < 10:
-                method = "direct"
-            else:
-                method = "bilinear"
-
-        if method == "direct":
-            X = direct_lyapunov_solution(A, Q)
-
-        if method == "bilinear":
-            eye = np.eye(_M)
-            AH = A.conj().transpose()
-            AHI_inv = np.linalg.inv(AH + eye)
-            B = np.dot(AH - eye, AHI_inv)
-            C = 2 * np.dot(np.dot(np.linalg.inv(A + eye), Q), AHI_inv)
-            X = linalg.solve_continuous_lyapunov(B.conj().transpose(), -C)
-
-        return X
-
-    return impl
+import numpy as np
+import scipy
+from numba.core import types
+from numba.extending import overload
+from numba.np.linalg import (
+    _check_finite_matrix,
+    _copy_to_fortran_order,
+    _handle_err_maybe_convergence_problem,
+    ensure_lapack,
+)
+from scipy import linalg
+
+from gEconpy.numba_tools.intrinsics import int_ptr_to_val, val_to_int_ptr
+from gEconpy.numba_tools.LAPACK import _LAPACK
+from gEconpy.numba_tools.utilities import (
+    _check_scipy_linalg_matrix,
+    _get_underlying_float,
+    _iuc,
+    _lhp,
+    _ouc,
+    _rhp,
+    direct_lyapunov_solution,
+)
+
+
+@overload(scipy.linalg.solve_triangular)
+def solve_triangular_impl(A, B, trans=0, lower=False, unit_diagonal=False):
+    ensure_lapack()
+
+    _check_scipy_linalg_matrix(A, "solve_triangular")
+    _check_scipy_linalg_matrix(B, "solve_triangular")
+
+    dtype = A.dtype
+    w_type = _get_underlying_float(dtype)
+
+    numba_trtrs = _LAPACK().numba_xtrtrs(dtype)
+
+    def impl(A, B, trans=0, lower=False, unit_diagonal=False):
+        _N = np.int32(A.shape[-1])
+        if A.shape[-2] != _N:
+            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
+
+        if A.shape[0] != B.shape[0]:
+            raise linalg.LinAlgError("Dimensions of A and B do not conform")
+
+        A_copy = _copy_to_fortran_order(A)
+        B_copy = _copy_to_fortran_order(B)
+
+        # if isinstance(trans, str):
+        # if trans not in ['N', 'C', 'T']:
+        #     raise ValueError('Parameter "trans" should be one of N, C, T or 0, 1, 2')
+        # transval = ord(trans)
+
+        # else:
+        if trans not in [0, 1, 2]:
+            raise ValueError('Parameter "trans" should be one of N, C, T or 0, 1, 2')
+        if trans == 0:
+            transval = ord("N")
+        elif trans == 1:
+            transval = ord("T")
+        else:
+            transval = ord("C")
+
+        UPLO = val_to_int_ptr(ord("L") if lower else ord("U"))
+        TRANS = val_to_int_ptr(transval)
+        DIAG = val_to_int_ptr(ord("U") if unit_diagonal else ord("N"))
+        N = val_to_int_ptr(_N)
+        NRHS = val_to_int_ptr(B.shape[1])
+        LDA = val_to_int_ptr(_N)
+        LDB = val_to_int_ptr(_N)
+        INFO = val_to_int_ptr(0)
+
+        numba_trtrs(
+            UPLO,
+            TRANS,
+            DIAG,
+            N,
+            NRHS,
+            A_copy.view(w_type).ctypes,
+            LDA,
+            B_copy.view(w_type).ctypes,
+            LDB,
+            INFO,
+        )
+
+        return B_copy
+
+    return impl
+
+
+@overload(scipy.linalg.schur)
+def schur_impl(A, output):
+    ensure_lapack()
+
+    _check_scipy_linalg_matrix(A, "schur")
+
+    dtype = A.dtype
+    w_type = _get_underlying_float(dtype)
+
+    numba_rgees = _LAPACK().numba_rgees(dtype)
+    numba_cgees = _LAPACK().numba_cgees(dtype)
+
+    def real_schur_impl(A, output):
+        """
+        schur() implementation for real arrays
+        """
+        _N = np.int32(A.shape[-1])
+        if A.shape[-2] != _N:
+            msg = "Last 2 dimensions of the array must be square"
+            raise linalg.LinAlgError(msg)
+
+        _check_finite_matrix(A)
+        A_copy = _copy_to_fortran_order(A)
+
+        JOBVS = val_to_int_ptr(ord("V"))
+        SORT = val_to_int_ptr(ord("N"))
+        SELECT = val_to_int_ptr(0.0)
+
+        N = val_to_int_ptr(_N)
+        LDA = val_to_int_ptr(_N)
+        SDIM = val_to_int_ptr(_N)
+        WR = np.empty(_N, dtype=dtype)
+        WI = np.empty(_N, dtype=dtype)
+        _LDVS = _N
+        LDVS = val_to_int_ptr(_N)
+        VS = np.empty((_LDVS, _N), dtype=dtype)
+        LWORK = val_to_int_ptr(-1)
+        WORK = np.empty(1, dtype=dtype)
+        BWORK = val_to_int_ptr(1)
+        INFO = val_to_int_ptr(1)
+
+        # workspace query
+        numba_rgees(
+            JOBVS,
+            SORT,
+            SELECT,
+            N,
+            A_copy.ctypes,
+            LDA,
+            SDIM,
+            WR.ctypes,
+            WI.ctypes,
+            VS.ctypes,
+            LDVS,
+            WORK.ctypes,
+            LWORK,
+            BWORK,
+            INFO,
+        )
+        WS_SIZE = np.int32(WORK[0].real)
+        LWORK = val_to_int_ptr(WS_SIZE)
+        WORK = np.empty(WS_SIZE, dtype=dtype)
+
+        # Actual work
+        numba_rgees(
+            JOBVS,
+            SORT,
+            SELECT,
+            N,
+            A_copy.ctypes,
+            LDA,
+            SDIM,
+            WR.ctypes,
+            WI.ctypes,
+            VS.ctypes,
+            LDVS,
+            WORK.ctypes,
+            LWORK,
+            BWORK,
+            INFO,
+        )
+
+        # if np.any(WI) and output == 'complex':
+        #     raise ValueError("schur() argument must not cause a domain change.")
+        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
+
+        return A_copy, VS.T
+
+    def complex_schur_impl(A, output):
+        """
+        schur() implementation for complex arrays
+        """
+
+        _N = np.int32(A.shape[-1])
+        if A.shape[-2] != _N:
+            msg = "Last 2 dimensions of the array must be square"
+            raise linalg.LinAlgError(msg)
+
+        _check_finite_matrix(A)
+        A_copy = _copy_to_fortran_order(A)
+
+        JOBVS = val_to_int_ptr(ord("V"))
+        SORT = val_to_int_ptr(ord("N"))
+        SELECT = val_to_int_ptr(0.0)
+
+        N = val_to_int_ptr(_N)
+        LDA = val_to_int_ptr(_N)
+        SDIM = val_to_int_ptr(_N)
+        W = np.empty(_N, dtype=dtype)
+        _LDVS = _N
+        LDVS = val_to_int_ptr(_N)
+        VS = np.empty((_LDVS, _N), dtype=dtype)
+        LWORK = val_to_int_ptr(-1)
+        WORK = np.empty(1, dtype=dtype)
+        RWORK = np.empty(_N, dtype=w_type)
+        BWORK = val_to_int_ptr(1)
+        INFO = val_to_int_ptr(1)
+
+        # workspace query
+        numba_cgees(
+            JOBVS,
+            SORT,
+            SELECT,
+            N,
+            A_copy.view(w_type).ctypes,
+            LDA,
+            SDIM,
+            W.view(w_type).ctypes,
+            VS.view(w_type).ctypes,
+            LDVS,
+            WORK.view(w_type).ctypes,
+            LWORK,
+            RWORK.ctypes,
+            BWORK,
+            INFO,
+        )
+
+        WS_SIZE = np.int32(WORK[0].real)
+        LWORK = val_to_int_ptr(WS_SIZE)
+        WORK = np.empty(WS_SIZE, dtype=dtype)
+
+        # Actual work
+        numba_cgees(
+            JOBVS,
+            SORT,
+            SELECT,
+            N,
+            A_copy.view(w_type).ctypes,
+            LDA,
+            SDIM,
+            W.view(w_type).ctypes,
+            VS.view(w_type).ctypes,
+            LDVS,
+            WORK.view(w_type).ctypes,
+            LWORK,
+            RWORK.ctypes,
+            BWORK,
+            INFO,
+        )
+
+        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
+
+        return A_copy, VS.T
+
+    if isinstance(A.dtype, types.scalars.Complex):
+        return complex_schur_impl
+    else:
+        return real_schur_impl
+
+
+def full_return_qz(A, B, output):
+    pass
+
+
+@overload(full_return_qz)
+def full_return_qz_impl(A, B, output):
+    ensure_lapack()
+
+    _check_scipy_linalg_matrix(A, "qz")
+    _check_scipy_linalg_matrix(B, "qz")
+
+    dtype = A.dtype
+    w_type = _get_underlying_float(dtype)
+
+    numba_rgges = _LAPACK().numba_rgges(dtype)
+    numba_cgges = _LAPACK().numba_cgges(dtype)
+
+    def real_full_return_qz_impl(A, B, output):
+        """
+        schur() implementation for real arrays. Unlike the Scipy function, this has 5 returns, including the
+        generalized eigenvalues (alpha, beta), because these are required by ordqz.
+        """
+        _M, _N = np.int32(A.shape[-2:])
+        if A.shape[-2] != _N:
+            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
+        if B.shape[-2] != _N:
+            raise linalg.LinAlgError("Last 2 dimensions of B must be square")
+
+        _check_finite_matrix(A)
+        _check_finite_matrix(B)
+
+        A_copy = _copy_to_fortran_order(A)
+        B_copy = _copy_to_fortran_order(B)
+
+        JOBVSL = val_to_int_ptr(ord("V"))
+        JOBVSR = val_to_int_ptr(ord("V"))
+        SORT = val_to_int_ptr(ord("N"))
+        SELCTG = val_to_int_ptr(1)
+
+        N = val_to_int_ptr(_N)
+        LDA = val_to_int_ptr(_N)
+        LDB = val_to_int_ptr(_N)
+        SDIM = val_to_int_ptr(0)
+
+        ALPHAR = np.empty(_N, dtype=dtype)  # out
+        ALPHAI = np.empty(_N, dtype=dtype)  # out
+        BETA = np.empty(_N, dtype=dtype)  # out
+
+        _LDVSL = _N
+        _LDVSR = _N
+        LDVSL = val_to_int_ptr(_LDVSL)
+        VSL = np.empty((_LDVSL, _N), dtype=dtype)  # out
+        LDVSR = val_to_int_ptr(_LDVSR)
+        VSR = np.empty((_LDVSR, _N), dtype=dtype)  # out
+
+        WORK = np.empty((1,), dtype=dtype)  # out
+        LWORK = val_to_int_ptr(-1)
+        BWORK = val_to_int_ptr(1)
+        INFO = val_to_int_ptr(1)
+
+        # workspace query
+        numba_rgges(
+            JOBVSL,
+            JOBVSR,
+            SORT,
+            SELCTG,
+            N,
+            A_copy.ctypes,
+            LDA,
+            B_copy.ctypes,
+            LDB,
+            SDIM,
+            ALPHAR.ctypes,
+            ALPHAI.ctypes,
+            BETA.ctypes,
+            VSL.ctypes,
+            LDVSL,
+            VSR.ctypes,
+            LDVSR,
+            WORK.ctypes,
+            LWORK,
+            BWORK,
+            INFO,
+        )
+
+        WS_SIZE = np.int32(WORK[0].real)
+        LWORK = val_to_int_ptr(WS_SIZE)
+        WORK = np.empty(WS_SIZE, dtype=dtype)
+
+        # Actual work
+        numba_rgges(
+            JOBVSL,
+            JOBVSR,
+            SORT,
+            SELCTG,
+            N,
+            A_copy.ctypes,
+            LDA,
+            B_copy.ctypes,
+            LDB,
+            SDIM,
+            ALPHAR.ctypes,
+            ALPHAI.ctypes,
+            BETA.ctypes,
+            VSL.ctypes,
+            LDVSL,
+            VSR.ctypes,
+            LDVSR,
+            WORK.ctypes,
+            LWORK,
+            BWORK,
+            INFO,
+        )
+
+        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
+        ALPHA = ALPHAR + ALPHAI * 1j
+
+        return A_copy, B_copy, ALPHA, BETA, VSL.T, VSR.T
+
+    def complex_full_return_qz_impl(A, B, output):
+        """
+        qz decomposition for complex arrays. Unlike the Scipy function, this has 5 returns, including the
+        generalized eigenvalues (alpha, beta), because these are required by ordqz.
+        """
+
+        _M, _N = np.int32(A.shape[-2:])
+        if A.shape[-2] != _N:
+            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
+        if B.shape[-2] != _N:
+            raise linalg.LinAlgError("Last 2 dimensions of B must be square")
+
+        _check_finite_matrix(A)
+        _check_finite_matrix(B)
+
+        A_copy = _copy_to_fortran_order(A)
+        B_copy = _copy_to_fortran_order(B)
+
+        JOBVSL = val_to_int_ptr(ord("V"))
+        JOBVSR = val_to_int_ptr(ord("V"))
+        SORT = val_to_int_ptr(ord("N"))
+        SELCTG = val_to_int_ptr(1)
+
+        N = val_to_int_ptr(_N)
+        LDA = val_to_int_ptr(_N)
+        LDB = val_to_int_ptr(_N)
+        SDIM = val_to_int_ptr(0)
+
+        ALPHA = np.empty(_N, dtype=dtype)  # out
+        BETA = np.empty(_N, dtype=dtype)  # out
+        LDVSL = val_to_int_ptr(_N)
+        VSL = np.empty((_N, _N), dtype=dtype)  # out
+        LDVSR = val_to_int_ptr(_N)
+        VSR = np.empty((_N, _N), dtype=dtype)  # out
+
+        WORK = np.empty((1,), dtype=dtype)  # out
+        LWORK = val_to_int_ptr(-1)
+        RWORK = np.empty(8 * _N, dtype=w_type)
+        BWORK = val_to_int_ptr(1)
+        INFO = val_to_int_ptr(1)
+
+        # workspace query
+        numba_cgges(
+            JOBVSL,
+            JOBVSR,
+            SORT,
+            SELCTG,
+            N,
+            A_copy.view(w_type).ctypes,
+            LDA,
+            B_copy.view(w_type).ctypes,
+            LDB,
+            SDIM,
+            ALPHA.view(w_type).ctypes,
+            BETA.view(w_type).ctypes,
+            VSL.view(w_type).ctypes,
+            LDVSL,
+            VSR.view(w_type).ctypes,
+            LDVSR,
+            WORK.view(w_type).ctypes,
+            LWORK,
+            RWORK.ctypes,
+            BWORK,
+            INFO,
+        )
+
+        WS_SIZE = np.int32(WORK[0].real)
+        LWORK = val_to_int_ptr(WS_SIZE)
+        WORK = np.empty(WS_SIZE, dtype=dtype)
+
+        # Actual work
+        numba_cgges(
+            JOBVSL,
+            JOBVSR,
+            SORT,
+            SELCTG,
+            N,
+            A_copy.view(w_type).ctypes,
+            LDA,
+            B_copy.view(w_type).ctypes,
+            LDB,
+            SDIM,
+            ALPHA.view(w_type).ctypes,
+            BETA.view(w_type).ctypes,
+            VSL.view(w_type).ctypes,
+            LDVSL,
+            VSR.view(w_type).ctypes,
+            LDVSR,
+            WORK.view(w_type).ctypes,
+            LWORK,
+            RWORK.ctypes,
+            BWORK,
+            INFO,
+        )
+
+        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
+
+        return A_copy, B_copy, ALPHA, BETA, VSL.T, VSR.T
+
+    if isinstance(A.dtype, types.scalars.Complex):
+        return complex_full_return_qz_impl
+    else:
+        return real_full_return_qz_impl
+
+
+@overload(scipy.linalg.qz)
+def qz_impl(A, B, output):
+    """
+    scipy.linalg.qz overload. Wraps full_return_qz and returns only A, B, Q ,Z to match the scipy signature.
+    """
+    ensure_lapack()
+
+    _check_scipy_linalg_matrix(A, "qz")
+    _check_scipy_linalg_matrix(B, "qz")
+
+    def real_qz_impl(A, B, output):
+        A, B, ALPHA, BETA, VSL, VSR = full_return_qz(A, B, output)
+
+        return A, B, VSL, VSR
+
+    def complex_qz_impl(A, B, output):
+        A, B, ALPHA, BETA, VSL, VSR = full_return_qz(A, B, output)
+        return A, B, VSL, VSR
+
+    if isinstance(A.dtype, types.scalars.Complex):
+        return complex_qz_impl
+    else:
+        return real_qz_impl
+
+
+@overload(scipy.linalg.ordqz)
+def ordqz_impl(A, B, sort, output):
+    ensure_lapack()
+
+    _check_scipy_linalg_matrix(A, "ordqz")
+    _check_scipy_linalg_matrix(B, "ordqz")
+
+    dtype = A.dtype
+    w_type = _get_underlying_float(dtype)
+
+    numba_rtgsen = _LAPACK().numba_rtgsen(dtype)
+    numba_ctgsen = _LAPACK().numba_ctgsen(dtype)
+
+    def real_ordqz_impl(A, B, sort, output):
+        _M, _N = np.int32(A.shape[-2:])
+        if A.shape[-2] != _N:
+            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
+        if B.shape[-2] != _N:
+            raise linalg.LinAlgError("Last 2 dimensions of B must be square")
+
+        _check_finite_matrix(A)
+        _check_finite_matrix(B)
+
+        if sort not in ["lhp", "rhp", "iuc", "ouc"]:
+            raise ValueError('Argument "sort" should be one of: "lhp", "rhp", "iuc", "ouc"')
+
+        A_copy = _copy_to_fortran_order(A)
+        B_copy = _copy_to_fortran_order(B)
+
+        AA, BB, ALPHA, BETA, Q, Z = full_return_qz(A_copy, B_copy, output)
+
+        if sort == "lhp":
+            SELECT = _lhp(ALPHA, BETA)
+        elif sort == "rhp":
+            SELECT = _rhp(ALPHA, BETA)
+        elif sort == "iuc":
+            SELECT = _iuc(ALPHA, BETA)
+        elif sort == "ouc":
+            SELECT = _ouc(ALPHA, BETA)
+
+        IJOB = val_to_int_ptr(0)
+        WANTQ = val_to_int_ptr(1)
+        WANTZ = val_to_int_ptr(1)
+        N = val_to_int_ptr(_N)
+        LDA = val_to_int_ptr(_M)
+        LDB = val_to_int_ptr(_M)
+
+        ALPHAR = np.empty(_N, dtype=dtype)
+        ALPHAI = np.empty(_N, dtype=dtype)
+
+        LDQ = val_to_int_ptr(Q.shape[0])
+        LDZ = val_to_int_ptr(Z.shape[0])
+        M = val_to_int_ptr(_M)
+        PL = np.empty(1, dtype=dtype)
+        PR = np.empty(1, dtype=dtype)
+        DIF = np.empty(2, dtype=dtype)
+        WORK = np.empty(1, dtype=dtype)
+        LWORK = val_to_int_ptr(-1)
+        IWORK = np.empty(1, dtype=np.int32)
+        LIWORK = val_to_int_ptr(-1)
+        INFO = val_to_int_ptr(1)
+
+        # workspace query
+        numba_rtgsen(
+            IJOB,
+            WANTQ,
+            WANTZ,
+            SELECT.ctypes,
+            N,
+            AA.ctypes,
+            LDA,
+            BB.ctypes,
+            LDB,
+            ALPHAR.ctypes,
+            ALPHAI.ctypes,
+            BETA.ctypes,
+            Q.ctypes,
+            LDQ,
+            Z.ctypes,
+            LDZ,
+            M,
+            PL.ctypes,
+            PR.ctypes,
+            DIF.ctypes,
+            WORK.ctypes,
+            LWORK,
+            IWORK.ctypes,
+            LIWORK,
+            INFO,
+        )
+
+        WS_SIZE = np.int32(WORK[0].real)
+        IW_SIZE = np.int32(IWORK[0].real)
+        LWORK = val_to_int_ptr(WS_SIZE)
+        LIWORK = val_to_int_ptr(IW_SIZE)
+        WORK = np.empty(WS_SIZE, dtype=dtype)
+        IWORK = np.empty(IW_SIZE, dtype=np.int32)
+
+        numba_rtgsen(
+            IJOB,
+            WANTQ,
+            WANTZ,
+            SELECT.ctypes,
+            N,
+            AA.ctypes,
+            LDA,
+            BB.ctypes,
+            LDB,
+            ALPHAR.ctypes,
+            ALPHAI.ctypes,
+            BETA.ctypes,
+            Q.ctypes,
+            LDQ,
+            Z.ctypes,
+            LDZ,
+            M,
+            PL.ctypes,
+            PR.ctypes,
+            DIF.ctypes,
+            WORK.ctypes,
+            LWORK,
+            IWORK.ctypes,
+            LIWORK,
+            INFO,
+        )
+
+        # if np.any(ALPHAI) and output == 'complex':
+        #     raise ValueError("ordqz() argument must not cause a domain change.")
+        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
+        ALPHA = ALPHAR + 1j * ALPHAI
+        return AA, BB, ALPHA, BETA, Q, Z
+
+    def complex_ordqz_impl(A, B, sort, output):
+        _M, _N = np.int32(A.shape[-2:])
+        if A.shape[-2] != _N:
+            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
+        if B.shape[-2] != _N:
+            raise linalg.LinAlgError("Last 2 dimensions of B must be square")
+
+        _check_finite_matrix(A)
+        _check_finite_matrix(B)
+
+        if sort not in ["lhp", "rhp", "iuc", "ouc"]:
+            raise ValueError('Argument "sort" should be one of: "lhp", "rhp", "iuc", "ouc"')
+
+        A_copy = _copy_to_fortran_order(A)
+        B_copy = _copy_to_fortran_order(B)
+
+        AA, BB, ALPHA, BETA, Q, Z = full_return_qz(A_copy, B_copy, output)
+
+        if sort == "lhp":
+            SELECT = _lhp(ALPHA, BETA)
+        elif sort == "rhp":
+            SELECT = _rhp(ALPHA, BETA)
+        elif sort == "iuc":
+            SELECT = _iuc(ALPHA, BETA)
+        elif sort == "ouc":
+            SELECT = _ouc(ALPHA, BETA)
+
+        IJOB = val_to_int_ptr(0)
+        WANTQ = val_to_int_ptr(1)
+        WANTZ = val_to_int_ptr(1)
+        N = val_to_int_ptr(_N)
+        LDA = val_to_int_ptr(_M)
+        LDB = val_to_int_ptr(_M)
+
+        LDQ = val_to_int_ptr(Q.shape[0])
+        LDZ = val_to_int_ptr(Z.shape[0])
+        M = val_to_int_ptr(_M)
+        PL = np.empty(1, dtype=w_type)
+        PR = np.empty(1, dtype=w_type)
+        DIF = np.empty(2, dtype=w_type)
+        WORK = np.empty(1, dtype=dtype)
+        LWORK = val_to_int_ptr(-1)
+        IWORK = np.empty(1, dtype=np.int32)
+        LIWORK = val_to_int_ptr(-1)
+        INFO = val_to_int_ptr(1)
+
+        # workspace query
+        numba_ctgsen(
+            IJOB,
+            WANTQ,
+            WANTZ,
+            SELECT.ctypes,
+            N,
+            AA.view(w_type).ctypes,
+            LDA,
+            BB.view(w_type).ctypes,
+            LDB,
+            ALPHA.view(w_type).ctypes,
+            BETA.view(w_type).ctypes,
+            Q.view(w_type).ctypes,
+            LDQ,
+            Z.view(w_type).ctypes,
+            LDZ,
+            M,
+            PL.ctypes,
+            PR.ctypes,
+            DIF.ctypes,
+            WORK.view(w_type).ctypes,
+            LWORK,
+            IWORK.ctypes,
+            LIWORK,
+            INFO,
+        )
+
+        WS_SIZE = np.int32(WORK[0].real)
+        IW_SIZE = np.int32(IWORK[0].real)
+        LWORK = val_to_int_ptr(WS_SIZE)
+        LIWORK = val_to_int_ptr(IW_SIZE)
+        WORK = np.empty(WS_SIZE, dtype=dtype)
+        IWORK = np.empty(IW_SIZE, dtype=np.int32)
+
+        numba_ctgsen(
+            IJOB,
+            WANTQ,
+            WANTZ,
+            SELECT.ctypes,
+            N,
+            AA.view(w_type).ctypes,
+            LDA,
+            BB.view(w_type).ctypes,
+            LDB,
+            ALPHA.view(w_type).ctypes,
+            BETA.view(w_type).ctypes,
+            Q.view(w_type).ctypes,
+            LDQ,
+            Z.view(w_type).ctypes,
+            LDZ,
+            M,
+            PL.ctypes,
+            PR.ctypes,
+            DIF.ctypes,
+            WORK.view(w_type).ctypes,
+            LWORK,
+            IWORK.ctypes,
+            LIWORK,
+            INFO,
+        )
+
+        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
+
+        return AA, BB, ALPHA, BETA, Q, Z
+
+    if isinstance(A.dtype, types.scalars.Complex):
+        return complex_ordqz_impl
+    else:
+        return real_ordqz_impl
+
+
+@overload(scipy.linalg.solve_continuous_lyapunov)
+def solve_continuous_lyapunov_impl(A, Q):
+    ensure_lapack()
+
+    _check_scipy_linalg_matrix(A, "solve_continuous_lyapunov")
+    _check_scipy_linalg_matrix(Q, "solve_continuous_lyapunov")
+
+    dtype = A.dtype
+    w_type = _get_underlying_float(dtype)
+
+    numba_xtrsyl = _LAPACK().numba_xtrsyl(dtype)
+
+    def _solve_cont_lyapunov_impl(A, Q):
+        _M, _N = np.int32(A.shape)
+        _NQ = np.int32(Q.shape[-1])
+
+        if _N != _NQ:
+            raise linalg.LinAlgError("Matrices A and Q must have the same shape")
+
+        if _M != _N:
+            raise linalg.LinAlgError("Last 2 dimensions of A must be square")
+        if Q.shape[-2] != _NQ:
+            raise linalg.LinAlgError("Last 2 dimensions of Q must be square")
+
+        _check_finite_matrix(A)
+        _check_finite_matrix(Q)
+
+        is_complex = np.iscomplexobj(A) | np.iscomplexobj(Q)
+        dtype_letter = "C" if is_complex else "T"
+        output = "complex" if is_complex else "real"
+
+        A_copy = _copy_to_fortran_order(A)
+        Q_copy = _copy_to_fortran_order(Q)
+
+        R, U = linalg.schur(A_copy, output=output)
+
+        # Construct f = u'*q*u
+        F = U.conj().T.dot(Q_copy.dot(U))
+
+        TRANA = val_to_int_ptr(ord("N"))
+        TRANB = val_to_int_ptr(ord(dtype_letter))
+        ISGN = val_to_int_ptr(1)
+
+        M = val_to_int_ptr(_N)
+        N = val_to_int_ptr(_N)
+        AA = _copy_to_fortran_order(R)
+        LDA = val_to_int_ptr(_N)
+        B = _copy_to_fortran_order(R)
+        LDB = val_to_int_ptr(_N)
+        C = _copy_to_fortran_order(F)
+        LDC = val_to_int_ptr(_N)
+
+        # TODO: There is a little bit of overhead here, can I figure out how to assign a
+        #  float or double pointer, depending on the case?
+        SCALE = np.array(1.0, dtype=w_type)
+        INFO = val_to_int_ptr(1)
+
+        numba_xtrsyl(
+            TRANA,
+            TRANB,
+            ISGN,
+            M,
+            N,
+            AA.view(w_type).ctypes,
+            LDA,
+            B.view(w_type).ctypes,
+            LDB,
+            C.view(w_type).ctypes,
+            LDC,
+            SCALE.ctypes,
+            INFO,
+        )
+
+        C *= SCALE
+        _handle_err_maybe_convergence_problem(int_ptr_to_val(INFO))
+        X = U.dot(C).dot(U.conj().T)
+
+        return X
+
+    return _solve_cont_lyapunov_impl
+
+
+@overload(scipy.linalg.solve_discrete_lyapunov)
+def solve_discrete_lyapunov_impl(A, Q, method="auto"):
+    ensure_lapack()
+
+    _check_scipy_linalg_matrix(A, "solve_continuous_lyapunov")
+    _check_scipy_linalg_matrix(Q, "solve_continuous_lyapunov")
+
+    dtype = A.dtype
+    w_type = _get_underlying_float(dtype)
+
+    def impl(A, Q, method="auto"):
+        _M, _N = np.int32(A.shape)
+
+        if method == "auto":
+            if _M < 10:
+                method = "direct"
+            else:
+                method = "bilinear"
+
+        if method == "direct":
+            X = direct_lyapunov_solution(A, Q)
+
+        if method == "bilinear":
+            eye = np.eye(_M)
+            AH = A.conj().transpose()
+            AHI_inv = np.linalg.inv(AH + eye)
+            B = np.dot(AH - eye, AHI_inv)
+            C = 2 * np.dot(np.dot(np.linalg.inv(A + eye), Q), AHI_inv)
+            X = linalg.solve_continuous_lyapunov(B.conj().transpose(), -C)
+
+        return X
+
+    return impl
```

### Comparing `gEconpy-1.1.0/gEconpy/parser/constants.py` & `gEconpy-1.2.0/gEconpy/parser/constants.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,77 +1,77 @@
-import re
-from enum import Enum
-
-import sympy as sp
-from sympy.abc import _clash1, _clash2
-
-from gEconpy.shared.utilities import IterEnum
-
-LOCAL_DICT = {}
-for letter in _clash1.keys():
-    LOCAL_DICT[letter] = sp.Symbol(letter)
-for letter in _clash2.keys():
-    LOCAL_DICT[letter] = sp.Symbol(letter)
-
-OPERATORS = re.escape("+-*/^=();:")
-
-BLOCK_START_TOKEN = "{"
-BLOCK_END_TOKEN = "};"
-LAG_TOKEN = "[-1]"
-LEAD_TOKEN = "[1]"
-SS_TOKEN = "[ss]"
-EXPECTATION_TOKEN = "E[]"
-CALIBRATING_EQ_TOKEN = "->"
-
-
-class SPECIAL_BLOCK_NAMES(Enum, metaclass=IterEnum):
-    OPTIONS = "OPTIONS"
-    TRYREDUCE = "TRYREDUCE"
-    ASSUMPTIONS = "ASSUMPTIONS"
-
-
-class STEADY_STATE_NAMES(Enum, metaclass=IterEnum):
-    STEADY_STATE = "STEADY_STATE"
-    SS = "SS"
-    STEADYSTATE = "STEADYSTATE"
-    STEADY = "STEADY"
-
-
-class BLOCK_COMPONENTS(Enum, metaclass=IterEnum):
-    DEFINITIONS = "DEFINITIONS"
-    CONTROLS = "CONTROLS"
-    OBJECTIVE = "OBJECTIVE"
-    CONSTRAINTS = "CONSTRAINTS"
-    IDENTITIES = "IDENTITIES"
-    SHOCKS = "SHOCKS"
-    CALIBRATION = "CALIBRATION"
-
-
-TIME_INDEX_DICT = {"ss": "ss", "t": 0, "tL1": -1, "t1": 1}
-
-SYMPY_ASSUMPTIONS = [
-    "finite",
-    "infinite",
-    "even",
-    "odd",
-    "prime",
-    "composite",
-    "positive",
-    "negative",
-    "zero",
-    "nonzero",
-    "nonpositive",
-    "nonnegative",
-    "integer",
-    "rational",
-    "irrational",
-    "real",
-    "extended real",
-    "hermitian",
-    "complex",
-    "imaginary",
-    "antihermitian",
-    "algebraic",
-    "transcendental",
-]
-
-DEFAULT_ASSUMPTIONS = {"real": True}
+import re
+from enum import Enum
+
+import sympy as sp
+from sympy.abc import _clash1, _clash2
+
+from gEconpy.shared.utilities import IterEnum
+
+LOCAL_DICT = {}
+for letter in _clash1.keys():
+    LOCAL_DICT[letter] = sp.Symbol(letter)
+for letter in _clash2.keys():
+    LOCAL_DICT[letter] = sp.Symbol(letter)
+
+OPERATORS = re.escape("+-*/^=();:")
+
+BLOCK_START_TOKEN = "{"
+BLOCK_END_TOKEN = "};"
+LAG_TOKEN = "[-1]"
+LEAD_TOKEN = "[1]"
+SS_TOKEN = "[ss]"
+EXPECTATION_TOKEN = "E[]"
+CALIBRATING_EQ_TOKEN = "->"
+
+
+class SPECIAL_BLOCK_NAMES(Enum, metaclass=IterEnum):
+    OPTIONS = "OPTIONS"
+    TRYREDUCE = "TRYREDUCE"
+    ASSUMPTIONS = "ASSUMPTIONS"
+
+
+class STEADY_STATE_NAMES(Enum, metaclass=IterEnum):
+    STEADY_STATE = "STEADY_STATE"
+    SS = "SS"
+    STEADYSTATE = "STEADYSTATE"
+    STEADY = "STEADY"
+
+
+class BLOCK_COMPONENTS(Enum, metaclass=IterEnum):
+    DEFINITIONS = "DEFINITIONS"
+    CONTROLS = "CONTROLS"
+    OBJECTIVE = "OBJECTIVE"
+    CONSTRAINTS = "CONSTRAINTS"
+    IDENTITIES = "IDENTITIES"
+    SHOCKS = "SHOCKS"
+    CALIBRATION = "CALIBRATION"
+
+
+TIME_INDEX_DICT = {"ss": "ss", "t": 0, "tL1": -1, "t1": 1}
+
+SYMPY_ASSUMPTIONS = [
+    "finite",
+    "infinite",
+    "even",
+    "odd",
+    "prime",
+    "composite",
+    "positive",
+    "negative",
+    "zero",
+    "nonzero",
+    "nonpositive",
+    "nonnegative",
+    "integer",
+    "rational",
+    "irrational",
+    "real",
+    "extended real",
+    "hermitian",
+    "complex",
+    "imaginary",
+    "antihermitian",
+    "algebraic",
+    "transcendental",
+]
+
+DEFAULT_ASSUMPTIONS = {"real": True}
```

### Comparing `gEconpy-1.1.0/gEconpy/parser/gEcon_parser.py` & `gEconpy-1.2.0/gEconpy/parser/gEcon_parser.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,351 +1,351 @@
-import re
-from collections import defaultdict
-from typing import Dict, List, Optional, Tuple
-
-import pyparsing as pp
-from sympy.core.assumptions import _assume_rules
-
-from gEconpy.exceptions.exceptions import GCNSyntaxError
-from gEconpy.parser.constants import (
-    DEFAULT_ASSUMPTIONS,
-    SPECIAL_BLOCK_NAMES,
-    SYMPY_ASSUMPTIONS,
-)
-from gEconpy.parser.parse_equations import rebuild_eqs_from_parser_output
-from gEconpy.parser.parse_plaintext import (
-    add_spaces_around_operators,
-    delete_block,
-    extract_distributions,
-    remove_comments,
-    remove_extra_spaces,
-    remove_newlines_and_tabs,
-)
-from gEconpy.parser.validation import (
-    block_is_empty,
-    find_typos_and_guesses,
-    validate_key,
-)
-from gEconpy.shared.utilities import flatten_list
-
-
-def block_to_clean_list(block: str) -> List[str]:
-    """
-    Processes a block of text by removing certain characters, and then splitting it into a list of strings.
-
-    Parameters
-    ----------
-    block : str
-        The block of text to process.
-
-    Returns
-    -------
-    List[str]
-        The processed list of strings.
-    """
-
-    block = re.sub("[{};]", "", block)
-    block = remove_extra_spaces(block).strip()
-    block = [x.replace(",", "").strip() for x in block.split()]
-
-    return block
-
-
-def extract_assumption_sub_blocks(block_str) -> Dict[str, List[str]]:
-    """
-    Extracts the special "Assumptions" block from the GCN file. Saves each user-provided assumption to a dictionary,
-    along with all variables associated to that assumption.
-
-    Parameters
-    ----------
-    block : List[str]
-        The block of text to process.
-
-    Returns
-    -------
-    Dict[str, List[str]]
-        A dictionary containing assumptions and variables, with the assumption names as keys and associated variables
-        as values.
-    """
-    LBRACE, RBRACE, SEMI, COMMA = map(pp.Suppress, "{};,")
-    BLOCK_END = RBRACE + SEMI
-    header = pp.Keyword("assumptions")
-    VARIABLE = pp.Word(pp.alphas, pp.alphanums + "_" + "[]").set_name("variable")
-    PARAM = pp.Word(pp.alphas, pp.alphanums + "_").set_name("parameter")
-    BLOCK_NAME = pp.Word(pp.alphas, pp.alphanums + "_")
-
-    VAR_LIST = pp.delimitedList((VARIABLE | PARAM), delim=",").set_name("var_list")
-    VAR_LINE = pp.Group(VAR_LIST + SEMI).set_name("variable_list")
-
-    ANYTHING = pp.Group(pp.Regex("[^{};]+") + SEMI).set_name("generic_line")
-
-    LINE = VAR_LINE | ANYTHING
-
-    SUBBLOCK = pp.Forward()
-    SUBBLOCK << pp.Dict(
-        pp.Group((BLOCK_NAME + pp.Group(LBRACE + (LINE | SUBBLOCK) + BLOCK_END)) | LINE)
-    )
-
-    LAYERED_BLOCK = pp.Forward()
-    LAYERED_BLOCK << pp.Dict(
-        pp.Group(header + LBRACE + pp.OneOrMore(LAYERED_BLOCK | SUBBLOCK) + BLOCK_END)
-    )
-
-    return LAYERED_BLOCK.parse_string(block_str).as_dict()["assumptions"]
-
-
-def validate_assumptions(block_dict: Dict[str, List[str]]) -> None:
-    """
-    Verify that all keys extracted from the assumption block are valid sympy assumptions.
-
-    Parameters
-    ----------
-    block_dict: dict
-        Dictionary of assumption: variable list key-value pairs, extracted from the GCN file by the
-        extract_assumption_sub_block function.
-
-    Returns
-    -------
-    None
-    """
-
-    for assumption in block_dict.keys():
-        if assumption not in SYMPY_ASSUMPTIONS:
-            best_guess, maybe_typo = find_typos_and_guesses([assumption], SYMPY_ASSUMPTIONS)
-            message = f'Assumption "{assumption}" is not a valid Sympy assumption.'
-            if best_guess is not None:
-                message += f' Did you mean "{best_guess}"?'
-            raise ValueError(message)
-
-
-def create_assumption_kwargs(assumption_dicts: Dict[str, List[str]]) -> Dict[str, Dict[str, bool]]:
-    """
-    Extracts assumption flags from `assumption_dicts` and returns them in a dictionary keyed by variable names.
-
-    Parameters
-    ----------
-    assumption_dicts : Dict[str, List[str]]
-        A dictionary containing assumptions and variables, with the assumption names as keys and associated variables
-        as values.
-
-    Returns
-    -------
-    Dict[str, Dict[str, bool]]
-        A dictionary of flags and values keyed by variable names.
-    """
-
-    assumption_kwargs = defaultdict(lambda: DEFAULT_ASSUMPTIONS.copy())
-    user_assumptions = defaultdict(dict)
-
-    # Maintain two dicts in first pass: one with user values + defaults, and one with just user values
-    # The user assumption dict will be used as the source of truth in the 2nd pass to resolve conflicts with defaults
-    for assumption, variable_list in assumption_dicts.items():
-        for var in flatten_list(variable_list):
-            base_var = re.sub(r"\[\]", "", var)
-            user_assumptions[base_var][assumption] = True
-            assumption_kwargs[base_var][assumption] = True
-
-    all_variables = set(flatten_list(list(assumption_dicts.values())))
-
-    for var in all_variables:
-        base_var = re.sub(r"\[\]", "", var)
-
-        # Check default assumptions against the user assumptions
-        for k, v in DEFAULT_ASSUMPTIONS.items():
-            implications = dict(_assume_rules.full_implications[(k, v)])
-            for user_k, user_v in user_assumptions[base_var].items():
-                # Assumptions agree, move along
-                if ((user_k == k) and (user_v == v)) or (user_k not in implications.keys()):
-                    continue
-
-                # Assumptions disagree -- delete the default
-                if implications[user_k] != user_v:
-                    del assumption_kwargs[base_var][k]
-
-    return assumption_kwargs
-
-
-def preprocess_gcn(gcn_raw: str) -> Tuple[str, Dict[str, str]]:
-    """
-    Preprocesses `gcn_raw` and returns the result.
-
-    Parameters
-    ----------
-    gcn_raw : str
-        Raw model file returned by function `load_gcn`.
-
-    Returns
-    -------
-    Tuple[str, Dict[str, str]]
-        Model file with basic preprocessing and prior distributions, respectively.
-    """
-
-    gcn_processed = remove_comments(gcn_raw)
-    gcn_processed, prior_dict = extract_distributions(gcn_processed)
-    gcn_processed = remove_newlines_and_tabs(gcn_processed)
-    gcn_processed = add_spaces_around_operators(gcn_processed)
-
-    return gcn_processed, prior_dict
-
-
-def parse_options_flags(options: str) -> Optional[Dict[str, bool]]:
-    """
-    Extracts flags and values from `options`.
-
-    Parameters
-    ----------
-    options : str
-        Text from the "options" block of a model file.
-
-    Returns
-    -------
-    Optional[Dict[str, bool]]
-        A dictionary of flags and values if they exist, or None if no options were found.
-
-    Notes
-    -----
-    Currently nothing is done with these values, and this step is primarily to ensure backwards compatibility with
-    .GCN files written for the gEcon R package.
-    """
-
-    result = dict()
-    options = re.sub("[{}]", "", options)
-    options = [line.strip() for line in options.split(";") if len(line.strip()) > 0]
-
-    if len(options) == 0:
-        return result
-
-    for option in options:
-        flag, value = option.split("=")
-        value = value.replace(";", "").strip()
-        value = True if value.lower() == "true" else False if value.lower() == "false" else value
-
-        result[flag.strip()] = value
-
-    return result
-
-
-def extract_special_block(text: str, block_name: str) -> Dict[str, List[str]]:
-    """
-    Parameters
-    ----------
-    text: str
-        Plaintext representation of a block form a GCN file. Should already be preprocessed by the
-        preprocess_gcn function.
-    block_name: str
-        Name of the block, used as the key in the block dictionary.
-
-    Returns
-    -------
-    block_dict: dict
-        A dictionary with the name as the key and the contents of the block as the values. The contents are split into
-        a list of strings, with each item in the list as a single line from the GCN file. Empty lines are discarded.
-    """
-    result = {
-        block_name: defaultdict(lambda: DEFAULT_ASSUMPTIONS)
-        if block_name == "assumptions"
-        else None
-    }
-
-    if block_name not in text:
-        return result
-
-    block = re.search(block_name + " {.*?" + "};", text)[0]
-    block = block.replace(block_name, "")
-
-    if block_is_empty(block):
-        return result
-
-    elif block_name == "options":
-        block = parse_options_flags(block)
-
-    elif block_name == "tryreduce":
-        block = block_to_clean_list(block)
-
-    elif block_name == "assumptions":
-        block = extract_assumption_sub_blocks(text)
-        validate_assumptions(block)
-        block = create_assumption_kwargs(block)
-
-    result[block_name] = block
-
-    return result
-
-
-def split_gcn_into_block_dictionary(text: str) -> Dict[str, str]:
-    """
-    Split the preprocessed GCN text by block and stores the results in a dictionary.
-
-    Parameters
-    ----------
-    text : str
-        Text of a model file after text processing using the preprocess_gcn function. I.e., comments are expected to be
-        removed, all tokens are splittable on single white spaces, and blocks are wrapped by { };
-
-    Returns
-    -------
-    Dict[str, str]
-        A "block dictionary" with key, value pairs of block_name:block_text. Special blocks are processed first
-        (currently "options" and "tryreduce"), then deleted. Normal model blocks are assumed to follow a standard format
-        of block NAME { component_1 { Equations }; component_2 { ... }; };
-
-    TODO: Add checks that model blocks follow the correct format and fail more helpfully.
-    """
-    results = dict()
-
-    for name in SPECIAL_BLOCK_NAMES:
-        name = name.lower()
-        result = extract_special_block(text, name)
-        results.update(result)
-        text = delete_block(text, name)
-
-    if "assumptions" not in results:
-        results["assumptions"] = defaultdict(lambda x: DEFAULT_ASSUMPTIONS)
-
-    gcn_blocks = [block for block in text.split("block") if len(block) > 0]
-    for block in gcn_blocks:
-        tokens = block.strip().split()
-        name = tokens[0]
-        results[name] = " ".join(tokens[1:])
-
-    return results
-
-
-def parsed_block_to_dict(block: str) -> Dict[str, List[List[str]]]:
-    """
-    Extracts the block components and equations from a pre-processed model block.
-
-    Parameters
-    ----------
-    block: str
-        Pre-processed text of the standard model block format.
-
-    Returns
-    -------
-    Dict[str, List[List[str]]]
-        A defaultdict of lists, containing lists of equation tokens. Keys are the block components found
-        in the block string. Equations are represented as lists of tokens, while sub-blocks are lists of equation lists.
-
-    Example:
-    >> Input: {definition { u[] = log ( C[] ) + log( L[] ); }; objective { U[] = u[] + beta * E[][U[1]] ;} };
-    >> Output: dict("definition" = ["u[]", "=", "log", "(", "C[]", ")", "+", "log", "(", "L[]", ")", ";"],
-                    "objective" = ["U[]", "=", "u[]", "+", "beta", "*", "E[][U[1]]", ";"])
-    """
-    block_dict = defaultdict(list)
-    parsed_block = pp.nestedExpr("{", "};").parseString(block).asList()[0]
-    current_key = parsed_block[0]
-
-    if isinstance(current_key, list):
-        # block[0] is an equation, should not be possible
-        raise GCNSyntaxError(block_name=block, key=current_key)
-
-    validate_key(key=current_key, block_name=block)
-
-    for element in parsed_block[1:]:
-        if isinstance(element, str):
-            current_key = element
-            validate_key(key=current_key, block_name=block)
-        else:
-            equations_list = rebuild_eqs_from_parser_output(element)
-            block_dict[current_key].extend(equations_list)
-
-    return block_dict
+import re
+from collections import defaultdict
+from typing import Dict, List, Optional, Tuple
+
+import pyparsing as pp
+from sympy.core.assumptions import _assume_rules
+
+from gEconpy.exceptions.exceptions import GCNSyntaxError
+from gEconpy.parser.constants import (
+    DEFAULT_ASSUMPTIONS,
+    SPECIAL_BLOCK_NAMES,
+    SYMPY_ASSUMPTIONS,
+)
+from gEconpy.parser.parse_equations import rebuild_eqs_from_parser_output
+from gEconpy.parser.parse_plaintext import (
+    add_spaces_around_operators,
+    delete_block,
+    extract_distributions,
+    remove_comments,
+    remove_extra_spaces,
+    remove_newlines_and_tabs,
+)
+from gEconpy.parser.validation import (
+    block_is_empty,
+    find_typos_and_guesses,
+    validate_key,
+)
+from gEconpy.shared.utilities import flatten_list
+
+
+def block_to_clean_list(block: str) -> List[str]:
+    """
+    Processes a block of text by removing certain characters, and then splitting it into a list of strings.
+
+    Parameters
+    ----------
+    block : str
+        The block of text to process.
+
+    Returns
+    -------
+    List[str]
+        The processed list of strings.
+    """
+
+    block = re.sub("[{};]", "", block)
+    block = remove_extra_spaces(block).strip()
+    block = [x.replace(",", "").strip() for x in block.split()]
+
+    return block
+
+
+def extract_assumption_sub_blocks(block_str) -> Dict[str, List[str]]:
+    """
+    Extracts the special "Assumptions" block from the GCN file. Saves each user-provided assumption to a dictionary,
+    along with all variables associated to that assumption.
+
+    Parameters
+    ----------
+    block : List[str]
+        The block of text to process.
+
+    Returns
+    -------
+    Dict[str, List[str]]
+        A dictionary containing assumptions and variables, with the assumption names as keys and associated variables
+        as values.
+    """
+    LBRACE, RBRACE, SEMI, COMMA = map(pp.Suppress, "{};,")
+    BLOCK_END = RBRACE + SEMI
+    header = pp.Keyword("assumptions")
+    VARIABLE = pp.Word(pp.alphas, pp.alphanums + "_" + "[]").set_name("variable")
+    PARAM = pp.Word(pp.alphas, pp.alphanums + "_").set_name("parameter")
+    BLOCK_NAME = pp.Word(pp.alphas, pp.alphanums + "_")
+
+    VAR_LIST = pp.delimitedList((VARIABLE | PARAM), delim=",").set_name("var_list")
+    VAR_LINE = pp.Group(VAR_LIST + SEMI).set_name("variable_list")
+
+    ANYTHING = pp.Group(pp.Regex("[^{};]+") + SEMI).set_name("generic_line")
+
+    LINE = VAR_LINE | ANYTHING
+
+    SUBBLOCK = pp.Forward()
+    SUBBLOCK << pp.Dict(
+        pp.Group((BLOCK_NAME + pp.Group(LBRACE + (LINE | SUBBLOCK) + BLOCK_END)) | LINE)
+    )
+
+    LAYERED_BLOCK = pp.Forward()
+    LAYERED_BLOCK << pp.Dict(
+        pp.Group(header + LBRACE + pp.OneOrMore(LAYERED_BLOCK | SUBBLOCK) + BLOCK_END)
+    )
+
+    return LAYERED_BLOCK.parse_string(block_str).as_dict()["assumptions"]
+
+
+def validate_assumptions(block_dict: Dict[str, List[str]]) -> None:
+    """
+    Verify that all keys extracted from the assumption block are valid sympy assumptions.
+
+    Parameters
+    ----------
+    block_dict: dict
+        Dictionary of assumption: variable list key-value pairs, extracted from the GCN file by the
+        extract_assumption_sub_block function.
+
+    Returns
+    -------
+    None
+    """
+
+    for assumption in block_dict.keys():
+        if assumption not in SYMPY_ASSUMPTIONS:
+            best_guess, maybe_typo = find_typos_and_guesses([assumption], SYMPY_ASSUMPTIONS)
+            message = f'Assumption "{assumption}" is not a valid Sympy assumption.'
+            if best_guess is not None:
+                message += f' Did you mean "{best_guess}"?'
+            raise ValueError(message)
+
+
+def create_assumption_kwargs(assumption_dicts: Dict[str, List[str]]) -> Dict[str, Dict[str, bool]]:
+    """
+    Extracts assumption flags from `assumption_dicts` and returns them in a dictionary keyed by variable names.
+
+    Parameters
+    ----------
+    assumption_dicts : Dict[str, List[str]]
+        A dictionary containing assumptions and variables, with the assumption names as keys and associated variables
+        as values.
+
+    Returns
+    -------
+    Dict[str, Dict[str, bool]]
+        A dictionary of flags and values keyed by variable names.
+    """
+
+    assumption_kwargs = defaultdict(lambda: DEFAULT_ASSUMPTIONS.copy())
+    user_assumptions = defaultdict(dict)
+
+    # Maintain two dicts in first pass: one with user values + defaults, and one with just user values
+    # The user assumption dict will be used as the source of truth in the 2nd pass to resolve conflicts with defaults
+    for assumption, variable_list in assumption_dicts.items():
+        for var in flatten_list(variable_list):
+            base_var = re.sub(r"\[\]", "", var)
+            user_assumptions[base_var][assumption] = True
+            assumption_kwargs[base_var][assumption] = True
+
+    all_variables = set(flatten_list(list(assumption_dicts.values())))
+
+    for var in all_variables:
+        base_var = re.sub(r"\[\]", "", var)
+
+        # Check default assumptions against the user assumptions
+        for k, v in DEFAULT_ASSUMPTIONS.items():
+            implications = dict(_assume_rules.full_implications[(k, v)])
+            for user_k, user_v in user_assumptions[base_var].items():
+                # Assumptions agree, move along
+                if ((user_k == k) and (user_v == v)) or (user_k not in implications.keys()):
+                    continue
+
+                # Assumptions disagree -- delete the default
+                if implications[user_k] != user_v:
+                    del assumption_kwargs[base_var][k]
+
+    return assumption_kwargs
+
+
+def preprocess_gcn(gcn_raw: str) -> Tuple[str, Dict[str, str]]:
+    """
+    Preprocesses `gcn_raw` and returns the result.
+
+    Parameters
+    ----------
+    gcn_raw : str
+        Raw model file returned by function `load_gcn`.
+
+    Returns
+    -------
+    Tuple[str, Dict[str, str]]
+        Model file with basic preprocessing and prior distributions, respectively.
+    """
+
+    gcn_processed = remove_comments(gcn_raw)
+    gcn_processed, prior_dict = extract_distributions(gcn_processed)
+    gcn_processed = remove_newlines_and_tabs(gcn_processed)
+    gcn_processed = add_spaces_around_operators(gcn_processed)
+
+    return gcn_processed, prior_dict
+
+
+def parse_options_flags(options: str) -> Optional[Dict[str, bool]]:
+    """
+    Extracts flags and values from `options`.
+
+    Parameters
+    ----------
+    options : str
+        Text from the "options" block of a model file.
+
+    Returns
+    -------
+    Optional[Dict[str, bool]]
+        A dictionary of flags and values if they exist, or None if no options were found.
+
+    Notes
+    -----
+    Currently nothing is done with these values, and this step is primarily to ensure backwards compatibility with
+    .GCN files written for the gEcon R package.
+    """
+
+    result = dict()
+    options = re.sub("[{}]", "", options)
+    options = [line.strip() for line in options.split(";") if len(line.strip()) > 0]
+
+    if len(options) == 0:
+        return result
+
+    for option in options:
+        flag, value = option.split("=")
+        value = value.replace(";", "").strip()
+        value = True if value.lower() == "true" else False if value.lower() == "false" else value
+
+        result[flag.strip()] = value
+
+    return result
+
+
+def extract_special_block(text: str, block_name: str) -> Dict[str, List[str]]:
+    """
+    Parameters
+    ----------
+    text: str
+        Plaintext representation of a block form a GCN file. Should already be preprocessed by the
+        preprocess_gcn function.
+    block_name: str
+        Name of the block, used as the key in the block dictionary.
+
+    Returns
+    -------
+    block_dict: dict
+        A dictionary with the name as the key and the contents of the block as the values. The contents are split into
+        a list of strings, with each item in the list as a single line from the GCN file. Empty lines are discarded.
+    """
+    result = {
+        block_name: defaultdict(lambda: DEFAULT_ASSUMPTIONS)
+        if block_name == "assumptions"
+        else None
+    }
+
+    if block_name not in text:
+        return result
+
+    block = re.search(block_name + " {.*?" + "};", text)[0]
+    block = block.replace(block_name, "")
+
+    if block_is_empty(block):
+        return result
+
+    elif block_name == "options":
+        block = parse_options_flags(block)
+
+    elif block_name == "tryreduce":
+        block = block_to_clean_list(block)
+
+    elif block_name == "assumptions":
+        block = extract_assumption_sub_blocks(text)
+        validate_assumptions(block)
+        block = create_assumption_kwargs(block)
+
+    result[block_name] = block
+
+    return result
+
+
+def split_gcn_into_block_dictionary(text: str) -> Dict[str, str]:
+    """
+    Split the preprocessed GCN text by block and stores the results in a dictionary.
+
+    Parameters
+    ----------
+    text : str
+        Text of a model file after text processing using the preprocess_gcn function. I.e., comments are expected to be
+        removed, all tokens are splittable on single white spaces, and blocks are wrapped by { };
+
+    Returns
+    -------
+    Dict[str, str]
+        A "block dictionary" with key, value pairs of block_name:block_text. Special blocks are processed first
+        (currently "options" and "tryreduce"), then deleted. Normal model blocks are assumed to follow a standard format
+        of block NAME { component_1 { Equations }; component_2 { ... }; };
+
+    TODO: Add checks that model blocks follow the correct format and fail more helpfully.
+    """
+    results = dict()
+
+    for name in SPECIAL_BLOCK_NAMES:
+        name = name.lower()
+        result = extract_special_block(text, name)
+        results.update(result)
+        text = delete_block(text, name)
+
+    if "assumptions" not in results:
+        results["assumptions"] = defaultdict(lambda x: DEFAULT_ASSUMPTIONS)
+
+    gcn_blocks = [block for block in text.split("block") if len(block) > 0]
+    for block in gcn_blocks:
+        tokens = block.strip().split()
+        name = tokens[0]
+        results[name] = " ".join(tokens[1:])
+
+    return results
+
+
+def parsed_block_to_dict(block: str) -> Dict[str, List[List[str]]]:
+    """
+    Extracts the block components and equations from a pre-processed model block.
+
+    Parameters
+    ----------
+    block: str
+        Pre-processed text of the standard model block format.
+
+    Returns
+    -------
+    Dict[str, List[List[str]]]
+        A defaultdict of lists, containing lists of equation tokens. Keys are the block components found
+        in the block string. Equations are represented as lists of tokens, while sub-blocks are lists of equation lists.
+
+    Example:
+    >> Input: {definition { u[] = log ( C[] ) + log( L[] ); }; objective { U[] = u[] + beta * E[][U[1]] ;} };
+    >> Output: dict("definition" = ["u[]", "=", "log", "(", "C[]", ")", "+", "log", "(", "L[]", ")", ";"],
+                    "objective" = ["U[]", "=", "u[]", "+", "beta", "*", "E[][U[1]]", ";"])
+    """
+    block_dict = defaultdict(list)
+    parsed_block = pp.nestedExpr("{", "};").parseString(block).asList()[0]
+    current_key = parsed_block[0]
+
+    if isinstance(current_key, list):
+        # block[0] is an equation, should not be possible
+        raise GCNSyntaxError(block_name=block, key=current_key)
+
+    validate_key(key=current_key, block_name=block)
+
+    for element in parsed_block[1:]:
+        if isinstance(element, str):
+            current_key = element
+            validate_key(key=current_key, block_name=block)
+        else:
+            equations_list = rebuild_eqs_from_parser_output(element)
+            block_dict[current_key].extend(equations_list)
+
+    return block_dict
```

### Comparing `gEconpy-1.1.0/gEconpy/parser/parse_distributions.py` & `gEconpy-1.2.0/gEconpy/parser/parse_distributions.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,1528 +1,1528 @@
-import re
-from abc import ABC, abstractmethod
-from functools import partial, reduce
-from typing import Any, Callable, Dict, List, Optional, Tuple, Union
-from warnings import warn
-
-import numpy as np
-from scipy import optimize
-from scipy.stats import (
-    beta,
-    gamma,
-    halfnorm,
-    invgamma,
-    norm,
-    rv_continuous,
-    truncnorm,
-    uniform,
-)
-from scipy.stats._distn_infrastructure import rv_frozen
-
-from gEconpy.classes.containers import SymbolDictionary
-from gEconpy.exceptions.exceptions import (
-    DistributionOverDefinedException,
-    IgnoredCloseMatchWarning,
-    InsufficientDegreesOfFreedomException,
-    InvalidDistributionException,
-    InvalidParameterException,
-    MultipleParameterDefinitionException,
-    RepeatedParameterException,
-    UnusedParameterWarning,
-)
-from gEconpy.parser.validation import find_typos_and_guesses
-from gEconpy.shared.utilities import is_number
-
-CANON_NAMES = [
-    "normal",
-    "truncnorm",
-    "halfnormal",
-    "gamma",
-    "inv_gamma",
-    "uniform",
-    "beta",
-]
-NAME_TO_DIST_SCIPY_FUNC = dict(
-    zip(CANON_NAMES, [norm, truncnorm, halfnorm, gamma, invgamma, uniform, beta])
-)
-
-NORMAL_ALIASES = ["norm", "normal", "n"]
-TRUNCNORM_ALIASES = ["truncnorm"]
-HALFNORMAL_ALIASES = ["halfnorm", "hn", "halfnormal"]
-GAMMA_ALIASES = ["gamma", "g"]
-INVERSE_GAMMA_ALIASES = [
-    "invgamma",
-    "ig",
-    "inversegamma",
-    "invg",
-    "inverseg",
-    "inv_gamma",
-    "ing_g",
-    "inverse_g",
-    "i_g",
-]
-UNIFORM_ALIASES = ["u", "uniform", "uni", "unif"]
-BETA_ALIASES = ["beta", "b"]
-
-# Moment parameter names
-MEAN_ALIASES = ["mean"]
-STD_ALIASES = ["std", "sd"]
-MOMENTS = MEAN_ALIASES + STD_ALIASES
-
-# Shared parameter names
-LOWER_BOUND_ALIASES = ["low", "lower", "lower_bound", "min"]
-UPPER_BOUND_ALIASES = ["high", "upper", "upper_bound", "max"]
-
-# Distribution specific parameter names
-NORMAL_LOC_ALIASES = ["mu", "loc"]
-NORMAL_SCALE_ALIASES = ["sigma", "tau", "precision", "scale"]
-
-INV_GAMMA_SHAPE_ALIASES = ["a", "alpha", "shape"]
-INV_GAMMA_SCALE_ALIASES = ["b", "beta", "scale"]
-
-BETA_SHAPE_ALIASES_1 = ["a", "alpha"]
-BETA_SHAPE_ALIASES_2 = ["b", "beta"]
-
-GAMMA_SHAPE_ALIASES = ["a", "alpha", "k", "shape"]
-GAMMA_SCALE_ALIASES = ["b", "beta", "theta", "scale"]
-
-DIST_ALIAS_LIST = [
-    NORMAL_ALIASES,
-    TRUNCNORM_ALIASES,
-    HALFNORMAL_ALIASES,
-    GAMMA_ALIASES,
-    INVERSE_GAMMA_ALIASES,
-    UNIFORM_ALIASES,
-    BETA_ALIASES,
-]
-
-
-class CompositeDistribution:
-    def __init__(self, dist, **parameters):
-        defined_params = {
-            param: value for param, value in parameters.items() if isinstance(value, (int, float))
-        }
-
-        self.rv_params = {
-            param: value for param, value in parameters.items() if isinstance(value, rv_frozen)
-        }
-        self.d = partial(dist, **defined_params)
-
-    def rvs(self, size=None, random_state=None):
-        sample_params = {
-            param: value.rvs(size=size, random_state=random_state)
-            for param, value in self.rv_params.items()
-        }
-        d = self.d(**sample_params)
-        return d.rvs(random_state=random_state)
-
-    def _unpack_pdf_dict(self, point_dict):
-        param_dict = {
-            param: value for param, value in point_dict.items() if param in self.rv_params.keys()
-        }
-        assert set(param_dict.keys()).union(set(self.rv_params.keys())) == set(
-            self.rv_params.keys()
-        )
-
-        point_dict = {
-            param: value for param, value in point_dict.items() if param not in param_dict.keys()
-        }
-        assert len(point_dict.keys()) == 1
-
-        point_val = list(point_dict.values())[0]
-
-        return param_dict, point_val
-
-    def pdf(self, point_dict):
-        pdf = 1
-
-        param_dict, point_val = self._unpack_pdf_dict(point_dict)
-
-        for param, value in param_dict.items():
-            pdf *= self.rv_params[param].pdf(value)
-
-        d = self.d(**param_dict)
-        pdf *= d.pdf(point_val)
-
-        return pdf
-
-    def logpdf(self, point_dict):
-        log_pdf = 0
-
-        param_dict, point_val = self._unpack_pdf_dict(point_dict)
-
-        for param, value in param_dict.items():
-            log_pdf += self.rv_params[param].logpdf(value)
-
-        d = self.d(**param_dict)
-        log_pdf += d.logpdf(point_val)
-
-        return log_pdf
-
-    def conditional_rvs(self, point_dicts):
-        n_samples = len(point_dicts)
-        samples = np.zeros(n_samples)
-        for idx in range(n_samples):
-            sample_params = {param: value.rvs() for param, value in self.rv_params.items()}
-            sample_params.update(point_dicts[idx])
-            samples[idx] = self.d(**sample_params).rvs()
-
-        return samples
-
-
-class BaseDistributionParser(ABC):
-    @abstractmethod
-    def __init__(
-        self,
-        variable_name: str,
-        d_name: str,
-        loc_param_name: Optional[str],
-        scale_param_name: str,
-        shape_param_name: Optional[str],
-        upper_bound_param_name: Optional[str],
-        lower_bound_param_name: Optional[str],
-        n_params: int,
-        all_valid_parameters: List[str],
-    ):
-
-        self.variable_name = variable_name
-        self.d_name = d_name
-        self.loc_param_name = loc_param_name
-        self.scale_param_name = scale_param_name
-        self.shape_param_name = shape_param_name
-        self.upper_bound_param_name = upper_bound_param_name
-        self.lower_bound_param_name = lower_bound_param_name
-        self.n_params = n_params
-        self.all_valid_parameters = all_valid_parameters
-
-        self.used_parameters = []
-        self.mean_constraint = None
-        self.std_constraint = None
-
-    @abstractmethod
-    def build_distribution(self, param_dict: Dict[str, str]) -> rv_continuous:
-        raise NotImplementedError
-
-    @abstractmethod
-    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
-        raise NotImplementedError
-
-    def _parse_parameter(
-        self,
-        param_dict: Dict[str, str],
-        aliases: List[str],
-        canon_name: str,
-        additional_transformation: Callable = lambda name, value: value,
-    ):
-
-        param_names = list(param_dict.keys())
-        param_name = self._parse_parameter_candidates(canon_name, param_names, aliases)
-        if param_name is None:
-            return {}
-
-        self.used_parameters.append(param_name)
-
-        value = float(param_dict[param_name])
-        value = additional_transformation(param_name, value)
-
-        return {canon_name: value}
-
-    def _verify_distribution_parameterization(self, param_dict):
-        n_constraints = sum([self.mean_constraint is not None, self.std_constraint is not None])
-        if n_constraints > self.n_params:
-            raise InsufficientDegreesOfFreedomException(self.variable_name, self.d_name)
-
-        declared_params = list(param_dict.keys())
-        boundary_params = [self.lower_bound_param_name, self.upper_bound_param_name]
-        declared_params = [param for param in declared_params if param not in boundary_params]
-
-        n_params_passed = len(declared_params)
-        if (n_constraints + n_params_passed) > self.n_params:
-            raise DistributionOverDefinedException(
-                self.variable_name,
-                self.d_name,
-                self.n_params,
-                n_params_passed,
-                n_constraints,
-            )
-
-        if self._has_std_constraint():
-            if self.std_constraint <= 0:
-                raise InvalidParameterException(
-                    self.variable_name,
-                    self.d_name,
-                    self.std_constraint,
-                    self.std_constraint,
-                    f"{self.std_constraint} > 0",
-                )
-
-        if self.scale_param_name in declared_params and param_dict[self.scale_param_name] <= 0:
-            raise InvalidParameterException(
-                self.variable_name,
-                self.d_name,
-                self.scale_param_name,
-                self.scale_param_name,
-                f"{self.scale_param_name} > 0",
-            )
-
-    @abstractmethod
-    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
-        raise NotImplementedError
-
-    def _parse_mean_constraint(self, param_dict: Dict[str, str]) -> Dict:
-        aliases = MEAN_ALIASES
-        param_names = list(param_dict.keys())
-        mean_param = self._parse_parameter_candidates("mean", param_names, aliases)
-
-        if mean_param is None:
-            return {}
-
-        self.used_parameters.append(mean_param)
-        value = float(param_dict[mean_param])
-        self.mean_constraint = value
-        return {}
-
-    def _parse_std_constraint(self, param_dict: Dict[str, str]) -> Dict:
-        aliases = STD_ALIASES
-        param_names = list(param_dict.keys())
-        std_param = self._parse_parameter_candidates("mean", param_names, aliases)
-
-        if std_param is None:
-            return {}
-
-        self.used_parameters.append(std_param)
-        value = float(param_dict[std_param])
-        self.std_constraint = value
-        return {}
-
-    def _has_mean_constraint(self):
-        return self.mean_constraint is not None
-
-    def _has_std_constraint(self):
-        return self.std_constraint is not None
-
-    def _parse_parameter_candidates(
-        self, canon_param_name: str, param_names: List[str], aliases: List[str]
-    ) -> Optional[str]:
-        candidates = list(set(param_names).intersection(set(aliases)))
-        if len(candidates) == 0:
-            invalid_param_names = list(set(param_names) - set(self.all_valid_parameters))
-
-            if len(invalid_param_names) == 0:
-                return None
-
-            best_guess, maybe_typo = find_typos_and_guesses(invalid_param_names, aliases)
-
-            if best_guess is not None and maybe_typo is not None:
-                warn(
-                    f'Found a partial name match: "{maybe_typo}" for "{best_guess}" while parsing the {self.d_name} '
-                    f'associated with "{self.variable_name}". Please verify whether the distribution is correctly '
-                    f"specified in the GCN file.",
-                    category=IgnoredCloseMatchWarning,
-                )
-
-            return None
-
-        if len(candidates) > 1:
-            raise MultipleParameterDefinitionException(
-                self.variable_name, self.d_name, canon_param_name, candidates
-            )
-
-        return list(candidates)[0]
-
-    def _warn_about_unused_parameters(self, param_dict: Dict[str, str]) -> None:
-        used_parameters = self.used_parameters
-        all_params = list(param_dict.keys())
-
-        unused_parameters = list(set(all_params) - set(used_parameters))
-        n_params = len(unused_parameters)
-        if n_params > 0:
-            message = (
-                f'After parsing {self.d_name} distribution associated with "{self.variable_name}", the '
-                f"following parameters remained unused: "
-            )
-            if n_params == 1:
-                message += unused_parameters[0] + "."
-            else:
-                message += ", ".join(unused_parameters[:-1]) + f", and {unused_parameters[-1]}."
-            message += " Please verify whether these parameters are needed, and adjust the GCN file accordingly."
-
-            warn(message, category=UnusedParameterWarning)
-
-
-# TODO: Split into NormalDistributionParser and TruncatedNormalDistributionParser?
-class NormalDistributionParser(BaseDistributionParser):
-    def __init__(self, variable_name: str):
-        super().__init__(
-            variable_name=variable_name,
-            d_name="normal",
-            loc_param_name="loc",
-            scale_param_name="scale",
-            shape_param_name=None,
-            lower_bound_param_name="a",
-            upper_bound_param_name="b",
-            n_params=2,
-            all_valid_parameters=NORMAL_LOC_ALIASES + NORMAL_SCALE_ALIASES + MOMENTS,
-        )
-
-    def build_distribution(
-        self, param_dict: Dict[str, str], package="scipy", model=None
-    ) -> rv_continuous:
-        parsed_param_dict = self._parse_parameters(param_dict)
-
-        self._warn_about_unused_parameters(param_dict)
-        self._verify_distribution_parameterization(parsed_param_dict)
-
-        if package == "scipy":
-            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
-            parameters = list(parsed_param_dict.keys())
-
-            if (
-                self.lower_bound_param_name not in parameters
-                and self.upper_bound_param_name not in parameters
-            ):
-                return norm(**parsed_param_dict)
-            else:
-                return truncnorm(**parsed_param_dict)
-
-    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
-        parsed_param_dict = {}
-
-        def tau_to_sigma(name, value):
-            return 1 / value if name in ["tau", "precision"] else value
-
-        parse_loc_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.loc_param_name,
-            aliases=NORMAL_LOC_ALIASES,
-        )
-        parse_scale_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.scale_param_name,
-            aliases=NORMAL_SCALE_ALIASES,
-            additional_transformation=tau_to_sigma,
-        )
-        parse_lower_bound = partial(
-            self._parse_parameter,
-            canon_name=self.lower_bound_param_name,
-            aliases=LOWER_BOUND_ALIASES,
-        )
-        parse_upper_bound = partial(
-            self._parse_parameter,
-            canon_name=self.upper_bound_param_name,
-            aliases=UPPER_BOUND_ALIASES,
-        )
-
-        parsing_functions = [
-            self._parse_mean_constraint,
-            self._parse_std_constraint,
-            parse_loc_parameter,
-            parse_scale_parameter,
-            parse_lower_bound,
-            parse_upper_bound,
-        ]
-
-        for f in parsing_functions:
-            parsed_param_dict.update(f(param_dict))
-
-        return parsed_param_dict
-
-    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
-
-        parameters = list(param_dict.keys())
-
-        # Easiest case: No bounds
-        if (
-            self.lower_bound_param_name not in parameters
-            and self.upper_bound_param_name not in parameters
-        ):
-            if self._has_mean_constraint():
-                param_dict[self.loc_param_name] = self.mean_constraint
-            if self._has_std_constraint():
-                param_dict[self.scale_param_name] = self.std_constraint
-
-            return param_dict
-
-        # Handle cases with bounds
-        else:
-            # User might pass only one boundary, in that case set the other to infinity
-            a = (
-                param_dict[self.lower_bound_param_name]
-                if self.lower_bound_param_name in parameters
-                else -np.inf
-            )
-            b = (
-                param_dict[self.upper_bound_param_name]
-                if self.upper_bound_param_name in parameters
-                else np.inf
-            )
-
-            # Case 1: Both mean and std constraint given
-            if self._has_std_constraint() and self._has_mean_constraint():
-                # TODO: This is extremely slow when the boundary is "binding" (i.e. the mean or std is close to it),
-                #  so the loc or scale needs to become very large to meet the moment constraint. Can it be replaced
-                #  with an approximation?
-                def moment_errors(x, target_mean, target_std, a, b):
-                    loc_approx, scale_approx = x
-
-                    alpha = (a - loc_approx) / scale_approx
-                    beta = (b - loc_approx) / scale_approx
-                    d = truncnorm(loc=loc_approx, scale=scale_approx, a=alpha, b=beta)
-
-                    error_loc = target_mean - d.mean()
-                    error_std = target_std - d.std()
-
-                    error_vec = np.array([error_loc, error_std])
-
-                    return (error_vec**2).mean()
-
-                mean, std = self.mean_constraint, self.std_constraint
-
-                result = optimize.minimize(
-                    moment_errors,
-                    x0=[mean, std],
-                    args=(mean, std, a, b),
-                    bounds=[(None, None), (0, None)],
-                    method="Nelder-Mead",
-                    options={"maxiter": 1000},
-                )
-
-                if not result.success and result.fun > 1e-5:
-                    print(result)
-                    raise ValueError
-
-                loc, scale = result.x
-                param_dict[self.loc_param_name] = loc
-                param_dict[self.scale_param_name] = scale
-
-            # Case 2: Mean constraint and scale parameter
-            elif self._has_mean_constraint():
-                mean = self.mean_constraint
-                scale = param_dict[self.scale_param_name]
-
-                def match_mean(loc, target_mean, scale, a, b):
-                    alpha = (a - loc) / scale
-                    beta = (b - loc) / scale
-
-                    return truncnorm(loc=loc, scale=scale, a=alpha, b=beta).mean() - target_mean
-
-                loc = optimize.root_scalar(
-                    match_mean,
-                    args=(mean, scale, a, b),
-                    bracket=[-100, 100],
-                    method="brenth",
-                ).root
-
-                param_dict[self.loc_param_name] = loc
-
-            # Case 3: Scale constraint and loc parameter
-            elif self._has_std_constraint():
-                std = self.std_constraint
-                loc = param_dict[self.loc_param_name]
-
-                def match_std(scale, target_std, loc, a, b):
-                    alpha = (a - loc) / scale
-                    beta = (b - loc) / scale
-
-                    return truncnorm(loc=loc, scale=scale, a=alpha, b=beta).std() - target_std
-
-                scale = optimize.root_scalar(
-                    match_std,
-                    args=(std, loc, a, b),
-                    bracket=[1e-4, 100],
-                    method="brenth",
-                ).root
-
-                param_dict[self.scale_param_name] = scale
-
-            # Case 4: a, b, loc, and scale are all provided
-            else:
-                loc = param_dict[self.loc_param_name]
-                scale = param_dict[self.scale_param_name]
-
-            # Clean up: compute the adjusted bounds
-            param_dict[self.lower_bound_param_name] = (a - loc) / scale
-            param_dict[self.upper_bound_param_name] = (b - loc) / scale
-
-        return param_dict
-
-
-class HalfNormalDistributionParser(BaseDistributionParser):
-    def __init__(self, variable_name: str):
-        super().__init__(
-            variable_name=variable_name,
-            d_name="halfnormal",
-            loc_param_name="loc",
-            scale_param_name="scale",
-            shape_param_name=None,
-            upper_bound_param_name=None,
-            lower_bound_param_name=None,
-            n_params=2,
-            all_valid_parameters=NORMAL_SCALE_ALIASES + MOMENTS + ["loc"],
-        )
-
-    def build_distribution(
-        self, param_dict: Dict[str, str], package="scipy", model=None
-    ) -> rv_continuous:
-        parsed_param_dict = self._parse_parameters(param_dict)
-        self._warn_about_unused_parameters(param_dict)
-        self._verify_distribution_parameterization(parsed_param_dict)
-
-        if package == "scipy":
-            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
-
-            return halfnorm(**parsed_param_dict)
-
-    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
-        parsed_param_dict = {}
-
-        def tau_to_sigma(name, value):
-            return 1 / value if name in ["tau", "precision"] else value
-
-        parse_loc_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.loc_param_name,
-            aliases=[self.loc_param_name],
-        )
-        parse_scale_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.scale_param_name,
-            aliases=NORMAL_SCALE_ALIASES,
-            additional_transformation=tau_to_sigma,
-        )
-
-        parsing_functions = [
-            self._parse_mean_constraint,
-            self._parse_std_constraint,
-            parse_loc_parameter,
-            parse_scale_parameter,
-        ]
-
-        for f in parsing_functions:
-            parsed_param_dict.update(f(param_dict))
-
-        return parsed_param_dict
-
-    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
-        if self._has_mean_constraint() and self._has_std_constraint():
-            mean, std = self.mean_constraint, self.std_constraint
-
-            loc, scale = match_first_two_moments(
-                target_mean=mean, target_std=std, dist_object=halfnorm
-            )
-
-            param_dict[self.loc_param_name] = loc
-            param_dict[self.scale_param_name] = scale
-
-        elif self._has_mean_constraint():
-            mu = self.mean_constraint
-            param_dict[self.scale_param_name] = mu * np.sqrt(np.pi / 2)
-
-        elif self._has_std_constraint():
-            std = self.std_constraint
-            param_dict[self.scale_param_name] = std / np.sqrt(1 - 2 / np.pi)
-
-        return param_dict
-
-
-class UniformDistributionParser(BaseDistributionParser):
-    def __init__(self, variable_name: str):
-        super().__init__(
-            variable_name=variable_name,
-            d_name="uniform",
-            loc_param_name="loc",
-            scale_param_name="scale",
-            shape_param_name=None,
-            lower_bound_param_name="a",
-            upper_bound_param_name="b",
-            n_params=2,
-            all_valid_parameters=["loc", "scale"]
-            + LOWER_BOUND_ALIASES
-            + UPPER_BOUND_ALIASES
-            + MOMENTS,
-        )
-
-    def build_distribution(
-        self, param_dict: Dict[str, str], package="scipy", model=None
-    ) -> rv_continuous:
-        parsed_param_dict = self._parse_parameters(param_dict)
-        self._warn_about_unused_parameters(param_dict)
-        self._verify_distribution_parameterization(parsed_param_dict)
-
-        if package == "scipy":
-            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
-
-            return uniform(**parsed_param_dict)
-
-    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
-        parse_loc_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.loc_param_name,
-            aliases=[self.loc_param_name],
-        )
-        parse_scale_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.scale_param_name,
-            aliases=[self.scale_param_name],
-        )
-        parse_lower_bound = partial(
-            self._parse_parameter,
-            canon_name=self.lower_bound_param_name,
-            aliases=LOWER_BOUND_ALIASES,
-        )
-        parse_upper_bound = partial(
-            self._parse_parameter,
-            canon_name=self.upper_bound_param_name,
-            aliases=UPPER_BOUND_ALIASES,
-        )
-
-        parsing_functions = [
-            self._parse_mean_constraint,
-            self._parse_std_constraint,
-            parse_loc_parameter,
-            parse_scale_parameter,
-            parse_lower_bound,
-            parse_upper_bound,
-        ]
-
-        parsed_param_dict = {}
-        for f in parsing_functions:
-            parsed_param_dict.update(f(param_dict))
-
-        return parsed_param_dict
-
-    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
-
-        parameters = list(param_dict.keys())
-
-        # Case 1: Two moment constraints
-        if self._has_mean_constraint() and self._has_std_constraint():
-            mean, std = self.mean_constraint, self.std_constraint
-            b = np.sqrt(3) * std + mean
-            a = 2 * mean - b
-            param_dict[self.loc_param_name] = a
-            param_dict[self.scale_param_name] = b - a
-
-        # Case 2: Mean condition only
-        elif self._has_mean_constraint():
-            mean = self.mean_constraint
-
-            if self.loc_param_name in parameters:
-                a = param_dict[self.loc_param_name]
-                b = 2 * mean - a
-                param_dict[self.scale_param_name] = b - a
-
-            if self.scale_param_name in parameters:
-                scale = param_dict[self.scale_param_name]
-                param_dict[self.loc_param_name] = mean - 0.5 * scale
-
-        # Case 3: Std condition only
-        elif self._has_std_constraint():
-            std = self.std_constraint
-
-            if self.loc_param_name in parameters:
-                a = param_dict[self.loc_param_name]
-                b = np.sqrt(12) * std + a
-                param_dict[self.scale_param_name] = b - a
-
-            elif self.scale_param_name in parameters:
-                # TODO: Determine if this case is plausible, doesn't seem so, because sigma = 12 ** (-1/2) * scale
-                raise ValueError("Scale and Std are not enough to identify a Uniform distribution!")
-
-        # Case 4: User passed the bounds directly, convert to loc and scale then delete
-        else:
-            if self.lower_bound_param_name in parameters:
-                param_dict[self.loc_param_name] = param_dict[self.lower_bound_param_name]
-                del param_dict[self.lower_bound_param_name]
-            if self.upper_bound_param_name in parameters:
-                b = param_dict[self.upper_bound_param_name]
-                a = param_dict[self.loc_param_name]
-                param_dict[self.scale_param_name] = b - a
-
-                del param_dict[self.upper_bound_param_name]
-
-        return param_dict
-
-
-class InverseGammaDistributionParser(BaseDistributionParser):
-    def __init__(self, variable_name: str):
-        super().__init__(
-            variable_name=variable_name,
-            d_name="inv_gamma",
-            loc_param_name="loc",
-            scale_param_name="scale",
-            shape_param_name="a",
-            upper_bound_param_name=None,
-            lower_bound_param_name=None,
-            n_params=3,
-            all_valid_parameters=INV_GAMMA_SHAPE_ALIASES + INV_GAMMA_SCALE_ALIASES + ["loc"],
-        )
-
-    def build_distribution(
-        self, param_dict: Dict[str, str], package="scipy", model=None
-    ) -> rv_continuous:
-        parsed_param_dict = self._parse_parameters(param_dict)
-        self._warn_about_unused_parameters(param_dict)
-        self._verify_distribution_parameterization(parsed_param_dict)
-
-        if package == "scipy":
-            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
-            return invgamma(**parsed_param_dict)
-
-    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
-        parse_loc_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.loc_param_name,
-            aliases=[self.loc_param_name],
-        )
-        parse_scale_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.scale_param_name,
-            aliases=INV_GAMMA_SCALE_ALIASES,
-        )
-        parse_shape_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.shape_param_name,
-            aliases=INV_GAMMA_SHAPE_ALIASES,
-        )
-
-        parsing_functions = [
-            self._parse_mean_constraint,
-            self._parse_std_constraint,
-            parse_loc_parameter,
-            parse_scale_parameter,
-            parse_shape_parameter,
-        ]
-
-        parsed_param_dict = {}
-        for f in parsing_functions:
-            parsed_param_dict.update(f(param_dict))
-
-        return parsed_param_dict
-
-    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
-        # TODO: What should be done with loc?
-        parameters = list(param_dict.keys())
-
-        user_passed_loc = self.loc_param_name in parameters
-        user_passed_scale = self.scale_param_name in parameters
-        user_passed_shape = self.shape_param_name in parameters
-
-        # Case 1: Two Constraints
-        if self._has_mean_constraint() and self._has_std_constraint():
-            mean, std = self.mean_constraint, self.std_constraint
-
-            param_dict[self.shape_param_name] = a = (mean / std) ** 2 + 2
-            param_dict[self.scale_param_name] = mean * (a - 1)
-
-            return param_dict
-
-        # Case 2: Mean constraint only
-        if self._has_mean_constraint():
-            mu = self.mean_constraint
-
-            if user_passed_shape:
-                a = param_dict[self.shape_param_name]
-                param_dict[self.scale_param_name] = mu * (a - 1)
-
-            elif user_passed_scale:
-                b = param_dict[self.scale_param_name]
-                param_dict[self.shape_param_name] = (b + mu) / mu
-
-        # Case 3: Std constraint only
-        elif self._has_std_constraint():
-            std = self.std_constraint
-            if user_passed_shape:
-                a = param_dict[self.shape_param_name]
-                param_dict[self.scale_param_name] = std * (a - 1) * np.sqrt(a - 2)
-
-            elif user_passed_scale:
-                b = param_dict[self.scale_param_name]
-
-                def solve_for_shape(a, target_std, b):
-                    return b**2 / (a - 1) ** 2 / (a - 2) - target_std**2
-
-                param_dict[self.shape_param_name] = optimize.root_scalar(
-                    solve_for_shape,
-                    args=(std, b),
-                    bracket=[2 + 1e-4, 100],
-                    method="brenth",
-                ).root
-        return param_dict
-
-
-class BetaDistributionParser(BaseDistributionParser):
-    def __init__(self, variable_name: str):
-        super().__init__(
-            variable_name=variable_name,
-            d_name="beta",
-            loc_param_name="loc",
-            scale_param_name="scale",
-            shape_param_name=None,
-            upper_bound_param_name=None,
-            lower_bound_param_name=None,
-            n_params=2,
-            all_valid_parameters=BETA_SHAPE_ALIASES_1
-            + BETA_SHAPE_ALIASES_2
-            + MOMENTS
-            + ["loc", "scale"],
-        )
-
-        self.shape_param_name_1 = "a"
-        self.shape_param_name_2 = "b"
-
-    def build_distribution(
-        self, param_dict: Dict[str, str], package="scipy", model=None
-    ) -> rv_continuous:
-        parsed_param_dict = self._parse_parameters(param_dict)
-        self._warn_about_unused_parameters(param_dict)
-        self._verify_distribution_parameterization(parsed_param_dict)
-
-        if package == "scipy":
-            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
-            return beta(**parsed_param_dict)
-
-    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
-
-        parse_loc_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.loc_param_name,
-            aliases=[self.loc_param_name],
-        )
-        parse_scale_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.scale_param_name,
-            aliases=[self.scale_param_name],
-        )
-        parse_shape_parameter_1 = partial(
-            self._parse_parameter,
-            canon_name=self.shape_param_name_1,
-            aliases=BETA_SHAPE_ALIASES_1,
-        )
-        parse_shape_parameter_2 = partial(
-            self._parse_parameter,
-            canon_name=self.shape_param_name_2,
-            aliases=BETA_SHAPE_ALIASES_2,
-        )
-
-        parsing_functions = [
-            self._parse_mean_constraint,
-            self._parse_std_constraint,
-            parse_loc_parameter,
-            parse_scale_parameter,
-            parse_shape_parameter_1,
-            parse_shape_parameter_2,
-        ]
-
-        parsed_param_dict = {}
-        for f in parsing_functions:
-            parsed_param_dict.update(f(param_dict))
-
-        return parsed_param_dict
-
-    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
-        parameters = list(param_dict.keys())
-        used_parameters = self.used_parameters
-
-        user_passed_alpha = self.shape_param_name_1 in parameters
-        user_passed_beta = self.shape_param_name_2 in parameters
-
-        # Case 1: Two moment constraints
-        if self._has_mean_constraint() and self._has_std_constraint():
-            mean, std = self.mean_constraint, self.std_constraint
-            if (mean > 1) or (mean < 0):
-                raise InvalidParameterException(
-                    self.variable_name, self.d_name, "mean", "mean", "0 < mean < 1"
-                )
-
-            if std <= 0:
-                used_name = list(set(used_parameters).intersection(set(STD_ALIASES)))[0]
-                raise InvalidParameterException(
-                    self.variable_name, self.d_name, "mean", used_name, "sd > 0"
-                )
-
-            if ((1 - mean) ** 2 * mean) < (std**2):
-                used_name = list(set(used_parameters).intersection(set(STD_ALIASES)))[0]
-                raise InvalidParameterException(
-                    self.variable_name,
-                    self.d_name,
-                    "mean, std",
-                    f"mean, {used_name}",
-                    "((1 - mean) ** 2 * mean) < (std ** 2)",
-                )
-
-            x = (1 - mean) / mean
-            param_dict[self.shape_param_name_1] = alpha = x / (std**2 * (x + 1) ** 3) - (
-                1 / (x + 1)
-            )
-            param_dict[self.shape_param_name_2] = alpha * x
-
-        # # Case 2: No moment constraints
-        # elif user_passed_alpha and user_passed_beta:
-        #     return param_dict
-
-        # Case 3: Mean constraint and one shape parameter
-        elif self._has_mean_constraint():
-            mean = self.mean_constraint
-            if user_passed_alpha:
-                a = param_dict[self.shape_param_name_1]
-                param_dict[self.shape_param_name_2] = mean * (a - 1)
-
-            elif user_passed_beta:
-                b = param_dict[self.shape_param_name_2]
-                param_dict[self.shape_param_name_1] = (b + mean) / mean
-
-        # Case 4: Std constraints and one shape parameter
-        elif self._has_std_constraint():
-            std = self.std_constraint
-
-            if user_passed_alpha:
-
-                def solve_for_beta(b, a, std):
-                    return std**2 - a * b / (a + b) ** 2 / (a + b + 1)
-
-                a = param_dict[self.shape_param_name_1]
-                b = optimize.root_scalar(
-                    solve_for_beta, args=(a, std), bracket=[1.001, 100], method="brenth"
-                ).root
-
-                param_dict[self.shape_param_name_2] = b
-
-            elif user_passed_beta:
-
-                def solve_for_alpha(a, b, std):
-                    return std**2 - a * b / (a + b) ** 2 / (a + b + 1)
-
-                b = param_dict[self.shape_param_name_2]
-                a = optimize.root_scalar(
-                    solve_for_alpha,
-                    args=(b, std),
-                    bracket=[1.001, 100],
-                    method="brenth",
-                ).root
-                param_dict[self.shape_param_name_1] = a
-
-        return param_dict
-
-
-class GammaDistributionParser(BaseDistributionParser):
-    def __init__(self, variable_name: str):
-        super().__init__(
-            variable_name=variable_name,
-            d_name="gamma",
-            loc_param_name="loc",
-            scale_param_name="scale",
-            shape_param_name="a",
-            lower_bound_param_name=None,
-            upper_bound_param_name=None,
-            n_params=3,
-            all_valid_parameters=GAMMA_SHAPE_ALIASES + GAMMA_SCALE_ALIASES + MOMENTS + ["loc"],
-        )
-
-    def build_distribution(
-        self, param_dict: Dict[str, str], package="scipy", model=None
-    ) -> rv_continuous:
-        parsed_param_dict = self._parse_parameters(param_dict)
-        self._warn_about_unused_parameters(param_dict)
-        self._verify_distribution_parameterization(parsed_param_dict)
-
-        if package == "scipy":
-            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
-            return gamma(**parsed_param_dict)
-
-    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
-        parse_loc_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.loc_param_name,
-            aliases=[self.loc_param_name],
-        )
-        parse_scale_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.scale_param_name,
-            aliases=GAMMA_SCALE_ALIASES,
-        )
-        parse_shape_parameter = partial(
-            self._parse_parameter,
-            canon_name=self.shape_param_name,
-            aliases=GAMMA_SHAPE_ALIASES,
-        )
-
-        parsing_functions = [
-            self._parse_mean_constraint,
-            self._parse_std_constraint,
-            parse_loc_parameter,
-            parse_scale_parameter,
-            parse_shape_parameter,
-        ]
-
-        parsed_param_dict = {}
-        for f in parsing_functions:
-            parsed_param_dict.update(f(param_dict))
-
-        return parsed_param_dict
-
-    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
-        parameters = list(param_dict.keys())
-
-        user_passed_scale = self.scale_param_name in parameters
-        user_passed_shape = self.shape_param_name in parameters
-
-        if self._has_mean_constraint() and self._has_std_constraint():
-            mean, std = self.mean_constraint, self.std_constraint
-            if mean < 0:
-                raise InvalidParameterException(
-                    self.variable_name, self.d_name, "mean", "mean", "mean >= 0"
-                )
-            if std <= 0:
-                raise InvalidParameterException(
-                    self.variable_name, self.d_name, "std", "std", "std >= 0"
-                )
-
-            param_dict[self.shape_param_name] = a = (mean / std) ** 2
-            param_dict[self.scale_param_name] = mean / a
-
-        elif self._has_mean_constraint():
-            mean = self.mean_constraint
-
-            if user_passed_scale:
-                b = param_dict[self.scale_param_name]
-                param_dict[self.shape_param_name] = mean / b
-
-            elif user_passed_shape:
-                a = param_dict[self.shape_param_name]
-                param_dict[self.scale_param_name] = mean / a
-
-        elif self._has_std_constraint():
-            std = self.std_constraint
-
-            if user_passed_scale:
-                b = param_dict[self.scale_param_name]
-                param_dict[self.shape_param_name] = (std / b) ** 2
-
-            elif user_passed_shape:
-                a = param_dict[self.shape_param_name]
-                param_dict[self.scale_param_name] = std / np.sqrt(a)
-
-        return param_dict
-
-
-def match_first_two_moments(
-    target_mean: float, target_std: float, dist_object: rv_continuous
-) -> Tuple[float, float]:
-    def moment_errors(
-        x, target_mean: float, target_std: float, dist_object: rv_continuous
-    ) -> float:
-        loc_approx, scale_approx = x
-
-        d = dist_object(loc=loc_approx, scale=scale_approx)
-
-        error_loc = target_mean - d.mean()
-        error_std = target_std - d.std()
-
-        error_vec = np.array([error_loc, error_std])
-
-        return (error_vec**2).mean()
-
-    result = optimize.minimize(
-        moment_errors,
-        x0=[target_mean, target_std],
-        args=(target_mean, target_std, dist_object),
-        bounds=[(None, None), (0, None)],
-        method="powell",
-        options={"maxiter": 100},
-    )
-
-    if not result.success and result.fun > 1e-5:
-        print(result)
-        raise ValueError
-
-    loc, scale = result.x
-    return loc, scale
-
-
-def build_alias_to_canon_dict(alias_list, cannon_names) -> Dict[str, str]:
-    alias_to_canon_dict = {}
-    aliases = reduce(lambda a, b: a + b, alias_list)
-
-    for alias in aliases:
-        for group, canon_name in zip(alias_list, cannon_names):
-            if alias in group:
-                alias_to_canon_dict[alias] = canon_name
-                break
-    return alias_to_canon_dict
-
-
-def preprocess_distribution_string(variable_name: str, d_string: str) -> Tuple[str, Dict[str, str]]:
-    """
-    Parameters
-    ----------
-    variable_name: str
-        A string representing the model parameter associated with this probability distribution.
-    d_string: str
-        A string representing a probability distribution, extracted from a GCN file by the gEcon_parser.preprocess_gcn
-        function.
-
-    Returns
-    -------
-    Tuple of (str, dict), containing the model parameter name associated with the distribution, and a dictionary of the
-    distribution parameters (e.g. loc, scale, shape, and bounds).
-    """
-    name_to_canon_dict = build_alias_to_canon_dict(DIST_ALIAS_LIST, CANON_NAMES)
-
-    digit_pattern = r" ?\d*\.?\d* ?"
-    general_pattern = rf" ?[\w\.]* ?"
-
-    # The not last args have a comma, while the last arg does not.
-    dist_name_pattern = r"(\w+)"
-    not_last_arg_pattern = rf"(\w+ ?={general_pattern}, ?)"
-    last_arg_pattern = rf"(\w+ ?={general_pattern})"
-    valid_pattern = rf"{dist_name_pattern}\({not_last_arg_pattern}*?{last_arg_pattern}\),?$"
-
-    # TODO: sort out where the typo is and tell the user.
-    if re.search(valid_pattern, d_string) is None:
-        raise InvalidDistributionException(variable_name, d_string)
-
-    d_name, params_string = d_string.split("(")
-    d_name = d_name.lower()
-
-    if d_name not in name_to_canon_dict.keys():
-        raise InvalidDistributionException(variable_name, d_string)
-
-    params = [x.strip() for x in params_string.replace(")", "").split(",")]
-    params = [x for x in params if len(x) > 0]
-
-    new_params = []
-    for p in params:
-        chunks = p.split("=")
-        new_p = "=".join([chunks[0].lower(), chunks[1]])
-        new_params.append(new_p)
-
-    params = new_params
-
-    param_dict = {}
-    for param in params:
-        key, value = (x.strip() for x in param.split("="))
-        if key in param_dict.keys():
-            raise RepeatedParameterException(variable_name, d_name, key)
-
-        param_dict[key] = value
-
-    return name_to_canon_dict[d_name], param_dict
-
-
-def preprocess_prior_dict(
-    raw_prior_dict: Dict[str, str]
-) -> Tuple[List[str], List[str], List[Dict[str, str]]]:
-    """
-
-    Parameters
-    ----------
-    raw_prior_dict: dict
-        Dictionary of variable name: raw distribution string pairs.
-
-    Returns
-    -------
-    list of (variable_name, distribution_name, prior_dict) tuples.
-        The prior_dict of each variable has parameter names as the keys and parameter values as the values.
-        Values are still represented as strings, since we still need to check for compound distributions at a later
-        stage.
-    """
-
-    variable_names = []
-    d_names = []
-    param_dicts = []
-    for variable_name, d_string in raw_prior_dict.items():
-        d_name, param_dict = preprocess_distribution_string(variable_name, d_string)
-        variable_names.append(variable_name)
-        d_names.append(d_name)
-        param_dicts.append(param_dict)
-
-    return variable_names, d_names, param_dicts
-
-
-def distribution_factory(
-    variable_name: str,
-    d_name: str,
-    param_dict: Dict[str, str],
-    package: str = "scipy",
-    model=None,
-) -> rv_continuous:
-    """
-    Parameters
-    ----------
-    variable_name: str
-        name of the variable with which this distribution is associated
-    d_name: str
-        plaintext name of the distribution to parameterize, from the CANNON_NAMES list.
-    param_dict: dict
-        a dictionary of parameter: value pairs, or parameter: string pairs in the case of composite distributions
-    package: str
-        package of the distribution function to parameterize
-
-    Returns
-    -------
-    d: rv_frozen
-        a scipy distribution object object
-    """
-
-    if package not in ["scipy"]:
-        raise NotImplementedError
-
-    parser = None
-
-    if d_name == "normal":
-        parser = NormalDistributionParser(variable_name=variable_name)
-
-    elif d_name == "halfnormal":
-        parser = HalfNormalDistributionParser(variable_name=variable_name)
-
-    elif d_name == "inv_gamma":
-        parser = InverseGammaDistributionParser(variable_name=variable_name)
-
-    elif d_name == "beta":
-        parser = BetaDistributionParser(variable_name=variable_name)
-
-    elif d_name == "gamma":
-        parser = GammaDistributionParser(variable_name=variable_name)
-
-    elif d_name == "uniform":
-        parser = UniformDistributionParser(variable_name=variable_name)
-
-    if parser is None:
-        print(d_name)
-        raise ValueError("How did you even get here?")
-
-    d = parser.build_distribution(param_dict, package=package, model=model)
-    return d
-
-
-def rename_dict_keys_with_value_transform(
-    d: Dict,
-    to_rename: List[str],
-    new_key: str,
-    variable_name: str,
-    d_name: str,
-    transformation: Callable = lambda name, value: value,
-) -> Dict[str, Any]:
-    result = {}
-    matches = [key for key in d.keys() if key in set(to_rename)]
-    if len(matches) > 1:
-        raise MultipleParameterDefinitionException(variable_name, d_name, new_key, matches)
-
-    for key, value in d.items():
-        if key in to_rename:
-            result[new_key] = transformation(key, value)
-        else:
-            result[key] = value
-
-    return result
-
-
-def param_values_to_floats(param_dict: Dict):
-    for param, param_value in param_dict.items():
-        if isinstance(param_value, str):
-            if is_number(param_value):
-                param_dict[param] = float(param_value)
-
-    return param_dict
-
-
-def split_out_composite_distributions(
-    variable_names: List[str], d_names: List[str], param_dicts: List[Dict[str, str]]
-) -> Tuple[Dict[str, Tuple[str, Dict[str, str]]], Dict[str, Tuple[str, Dict[str, str]]]]:
-    basic_distributions = {}
-    composite_distributions = {}
-
-    for variable_name, d_name, param_dict in zip(variable_names, d_names, param_dicts):
-        if all([is_number(x) for x in param_dict.values()]):
-            basic_distributions[variable_name] = (d_name, param_dict)
-        else:
-            composite_distributions[variable_name] = (d_name, param_dict)
-
-    return basic_distributions, composite_distributions
-
-
-def fetch_rv_params(param_dict, model):
-    return_dict = {}
-    for k, v in param_dict.items():
-        if isinstance(v, (float, int)):
-            return_dict[k] = v
-        elif isinstance(v, str):
-            return_dict[k] = model[v]
-        else:
-            raise ValueError(f"Found an illegal key:value pair in prior param dict, {k}:{v}")
-
-    return return_dict
-
-
-def composite_distribution_factory(
-    variable_name, d_name, param_dict, package="scipy", model=None
-) -> Union[CompositeDistribution, None]:
-    """
-    Parameters
-    ----------
-    variable_name: str
-        Name of the variable the distribution is associated with
-    d_name: str
-        Name of the distribution, one of CANNON_NAMES
-    param_dict: dict
-        Dictionary of parameter name, parameter value pairs. Parameter values should be either scipy rv_frozen objects
-        or strings that can be converted to floats.
-    package: str
-        Which package to use to create the distributions. Currently "scipy".
-
-    Returns
-    -------
-    d: CompositeDistribution
-         A wrapper around a set of scipy distributions with three methods: .rvs(), .pdf(), and .logpdf()
-
-    TODO: This function is a huge mess of if-else statements. All of this should maybe be put into the parser classes
-        to take advantage of all the parameter checking that happens there. Consider this temporary.
-
-    TODO: Currently no checks are done on the support of the parameter to ensure it matches parameter requirements
-        e.g. a > 0, b > 0 in the beta distribution.
-
-    TODO: It might be possible to do moment matching in some limited sense. Currently the initial value for the
-        parameter distributions is thrown away, could use this value to moment match? Maybe not worth it.
-    """
-
-    def tau_to_scale(key, value):
-        if key in {"tau", "precision"}:
-            return 1 / value
-        return value
-
-    if package == "scipy":
-        base_d = NAME_TO_DIST_SCIPY_FUNC[d_name]
-    else:
-        raise NotImplementedError('Only package = "scipy"  is supported.')
-
-    param_dict = param_values_to_floats(param_dict)
-
-    # validate parameters by simple rename, error on more complicated setups (no moment constraints!)
-    if d_name == "normal":
-        has_upper_bound = any([x in set(param_dict.keys()) for x in UPPER_BOUND_ALIASES])
-        has_lower_bound = any([x in set(param_dict.keys()) for x in LOWER_BOUND_ALIASES])
-
-        if (has_upper_bound or has_lower_bound) and package == "scipy":
-            warn(
-                'Moment conditions are not supported for compound distributions, and parameters "mean" and "std" will'
-                'be interpreted as "loc" and "scale". Since you have passed boundaries, the first and second moments'
-                "of the truncated normal distribution will not coincide with the loc and scale parameters.",
-                IgnoredCloseMatchWarning,
-            )
-
-        param_dict = rename_dict_keys_with_value_transform(
-            param_dict, NORMAL_LOC_ALIASES + MEAN_ALIASES, "loc", variable_name, d_name
-        )
-        param_dict = rename_dict_keys_with_value_transform(
-            param_dict,
-            NORMAL_SCALE_ALIASES + STD_ALIASES,
-            "scale",
-            variable_name,
-            d_name,
-            transformation=tau_to_scale,
-        )
-
-        param_dict = rename_dict_keys_with_value_transform(
-            param_dict, LOWER_BOUND_ALIASES, "a", variable_name, d_name
-        )
-
-        param_dict = rename_dict_keys_with_value_transform(
-            param_dict, UPPER_BOUND_ALIASES, "b", variable_name, d_name
-        )
-
-    elif d_name == "halfnormal" and package == "scipy":
-        if any([x in set(param_dict.keys()) for x in MEAN_ALIASES]):
-            warn(
-                "Moment conditions are not supported for compound distributions. If you pass a random variable as a "
-                "parameter value, do not pass in mean or std.",
-                IgnoredCloseMatchWarning,
-            )
-
-        param_dict = rename_dict_keys_with_value_transform(
-            param_dict,
-            NORMAL_SCALE_ALIASES,
-            "scale",
-            variable_name,
-            d_name,
-            transformation=tau_to_scale,
-        )
-
-    elif d_name == "inv_gamma":
-        if any([x in set(param_dict.keys()) for x in MOMENTS]) and package == "scipy":
-            warn(
-                "Moment conditions are not supported for compound distributions. If you pass a random variable as a "
-                "parameter value, do not pass in mean or std.",
-                IgnoredCloseMatchWarning,
-            )
-
-        param_dict = rename_dict_keys_with_value_transform(
-            param_dict, INV_GAMMA_SHAPE_ALIASES, "a", variable_name, d_name
-        )
-
-        param_dict = rename_dict_keys_with_value_transform(
-            param_dict, INV_GAMMA_SCALE_ALIASES, "scale", variable_name, d_name
-        )
-
-    elif d_name == "beta":
-        if any([x in set(param_dict.keys()) for x in MOMENTS]) and package == "scipy":
-            warn(
-                "Moment conditions are not supported for compound distributions. If you pass a random variable as a "
-                "parameter value, do not pass in mean or std. These conditions will be ignored, and this may cause an"
-                "an error to be raised when instantiating the distribution.",
-                IgnoredCloseMatchWarning,
-            )
-
-            param_dict = rename_dict_keys_with_value_transform(
-                param_dict, BETA_SHAPE_ALIASES_1, "a", variable_name, d_name
-            )
-
-            param_dict = rename_dict_keys_with_value_transform(
-                param_dict, BETA_SHAPE_ALIASES_2, "b", variable_name, d_name
-            )
-
-    elif d_name == "gamma":
-        if any([x in set(param_dict.keys()) for x in MOMENTS]) and package == "scipy":
-            warn(
-                "Moment conditions are not supported for compound distributions. If you pass a random variable as a "
-                "parameter value, do not pass in mean or std. These conditions will be ignored, and this may cause an"
-                "an error to be raised when instantiating the distribution.",
-                IgnoredCloseMatchWarning,
-            )
-
-            param_dict = rename_dict_keys_with_value_transform(
-                param_dict, BETA_SHAPE_ALIASES_1, "a", variable_name, d_name
-            )
-
-            param_dict = rename_dict_keys_with_value_transform(
-                param_dict, BETA_SHAPE_ALIASES_2, "b", variable_name, d_name
-            )
-
-    if package == "scipy":
-        d = CompositeDistribution(base_d, **param_dict)
-        return d
-
-
-def create_prior_distribution_dictionary(raw_prior_dict: Dict[str, str]) -> Dict[str, Any]:
-    variable_names, d_names, param_dicts = preprocess_prior_dict(raw_prior_dict)
-    basic_distributions, compound_distributions = split_out_composite_distributions(
-        variable_names, d_names, param_dicts
-    )
-    prior_dict = SymbolDictionary()
-
-    for variable_name, (d_name, param_dict) in basic_distributions.items():
-        d = distribution_factory(variable_name=variable_name, d_name=d_name, param_dict=param_dict)
-        prior_dict[variable_name] = d
-
-    for variable_name, (d_name, param_dict) in compound_distributions.items():
-        rvs_used_in_d = []
-        for param, value in param_dict.items():
-            if value in prior_dict.keys():
-                param_dict[param] = prior_dict[value]
-                rvs_used_in_d.append(value)
-
-        d = composite_distribution_factory(variable_name, d_name, param_dict)
-        prior_dict[variable_name] = d
-        for rv in rvs_used_in_d:
-            del prior_dict[rv]
-
-    return prior_dict
+import re
+from abc import ABC, abstractmethod
+from functools import partial, reduce
+from typing import Any, Callable, Dict, List, Optional, Tuple, Union
+from warnings import warn
+
+import numpy as np
+from scipy import optimize
+from scipy.stats import (
+    beta,
+    gamma,
+    halfnorm,
+    invgamma,
+    norm,
+    rv_continuous,
+    truncnorm,
+    uniform,
+)
+from scipy.stats._distn_infrastructure import rv_frozen
+
+from gEconpy.classes.containers import SymbolDictionary
+from gEconpy.exceptions.exceptions import (
+    DistributionOverDefinedException,
+    IgnoredCloseMatchWarning,
+    InsufficientDegreesOfFreedomException,
+    InvalidDistributionException,
+    InvalidParameterException,
+    MultipleParameterDefinitionException,
+    RepeatedParameterException,
+    UnusedParameterWarning,
+)
+from gEconpy.parser.validation import find_typos_and_guesses
+from gEconpy.shared.utilities import is_number
+
+CANON_NAMES = [
+    "normal",
+    "truncnorm",
+    "halfnormal",
+    "gamma",
+    "inv_gamma",
+    "uniform",
+    "beta",
+]
+NAME_TO_DIST_SCIPY_FUNC = dict(
+    zip(CANON_NAMES, [norm, truncnorm, halfnorm, gamma, invgamma, uniform, beta])
+)
+
+NORMAL_ALIASES = ["norm", "normal", "n"]
+TRUNCNORM_ALIASES = ["truncnorm"]
+HALFNORMAL_ALIASES = ["halfnorm", "hn", "halfnormal"]
+GAMMA_ALIASES = ["gamma", "g"]
+INVERSE_GAMMA_ALIASES = [
+    "invgamma",
+    "ig",
+    "inversegamma",
+    "invg",
+    "inverseg",
+    "inv_gamma",
+    "ing_g",
+    "inverse_g",
+    "i_g",
+]
+UNIFORM_ALIASES = ["u", "uniform", "uni", "unif"]
+BETA_ALIASES = ["beta", "b"]
+
+# Moment parameter names
+MEAN_ALIASES = ["mean"]
+STD_ALIASES = ["std", "sd"]
+MOMENTS = MEAN_ALIASES + STD_ALIASES
+
+# Shared parameter names
+LOWER_BOUND_ALIASES = ["low", "lower", "lower_bound", "min"]
+UPPER_BOUND_ALIASES = ["high", "upper", "upper_bound", "max"]
+
+# Distribution specific parameter names
+NORMAL_LOC_ALIASES = ["mu", "loc"]
+NORMAL_SCALE_ALIASES = ["sigma", "tau", "precision", "scale"]
+
+INV_GAMMA_SHAPE_ALIASES = ["a", "alpha", "shape"]
+INV_GAMMA_SCALE_ALIASES = ["b", "beta", "scale"]
+
+BETA_SHAPE_ALIASES_1 = ["a", "alpha"]
+BETA_SHAPE_ALIASES_2 = ["b", "beta"]
+
+GAMMA_SHAPE_ALIASES = ["a", "alpha", "k", "shape"]
+GAMMA_SCALE_ALIASES = ["b", "beta", "theta", "scale"]
+
+DIST_ALIAS_LIST = [
+    NORMAL_ALIASES,
+    TRUNCNORM_ALIASES,
+    HALFNORMAL_ALIASES,
+    GAMMA_ALIASES,
+    INVERSE_GAMMA_ALIASES,
+    UNIFORM_ALIASES,
+    BETA_ALIASES,
+]
+
+
+class CompositeDistribution:
+    def __init__(self, dist, **parameters):
+        defined_params = {
+            param: value for param, value in parameters.items() if isinstance(value, (int, float))
+        }
+
+        self.rv_params = {
+            param: value for param, value in parameters.items() if isinstance(value, rv_frozen)
+        }
+        self.d = partial(dist, **defined_params)
+
+    def rvs(self, size=None, random_state=None):
+        sample_params = {
+            param: value.rvs(size=size, random_state=random_state)
+            for param, value in self.rv_params.items()
+        }
+        d = self.d(**sample_params)
+        return d.rvs(random_state=random_state)
+
+    def _unpack_pdf_dict(self, point_dict):
+        param_dict = {
+            param: value for param, value in point_dict.items() if param in self.rv_params.keys()
+        }
+        assert set(param_dict.keys()).union(set(self.rv_params.keys())) == set(
+            self.rv_params.keys()
+        )
+
+        point_dict = {
+            param: value for param, value in point_dict.items() if param not in param_dict.keys()
+        }
+        assert len(point_dict.keys()) == 1
+
+        point_val = list(point_dict.values())[0]
+
+        return param_dict, point_val
+
+    def pdf(self, point_dict):
+        pdf = 1
+
+        param_dict, point_val = self._unpack_pdf_dict(point_dict)
+
+        for param, value in param_dict.items():
+            pdf *= self.rv_params[param].pdf(value)
+
+        d = self.d(**param_dict)
+        pdf *= d.pdf(point_val)
+
+        return pdf
+
+    def logpdf(self, point_dict):
+        log_pdf = 0
+
+        param_dict, point_val = self._unpack_pdf_dict(point_dict)
+
+        for param, value in param_dict.items():
+            log_pdf += self.rv_params[param].logpdf(value)
+
+        d = self.d(**param_dict)
+        log_pdf += d.logpdf(point_val)
+
+        return log_pdf
+
+    def conditional_rvs(self, point_dicts):
+        n_samples = len(point_dicts)
+        samples = np.zeros(n_samples)
+        for idx in range(n_samples):
+            sample_params = {param: value.rvs() for param, value in self.rv_params.items()}
+            sample_params.update(point_dicts[idx])
+            samples[idx] = self.d(**sample_params).rvs()
+
+        return samples
+
+
+class BaseDistributionParser(ABC):
+    @abstractmethod
+    def __init__(
+        self,
+        variable_name: str,
+        d_name: str,
+        loc_param_name: Optional[str],
+        scale_param_name: str,
+        shape_param_name: Optional[str],
+        upper_bound_param_name: Optional[str],
+        lower_bound_param_name: Optional[str],
+        n_params: int,
+        all_valid_parameters: List[str],
+    ):
+
+        self.variable_name = variable_name
+        self.d_name = d_name
+        self.loc_param_name = loc_param_name
+        self.scale_param_name = scale_param_name
+        self.shape_param_name = shape_param_name
+        self.upper_bound_param_name = upper_bound_param_name
+        self.lower_bound_param_name = lower_bound_param_name
+        self.n_params = n_params
+        self.all_valid_parameters = all_valid_parameters
+
+        self.used_parameters = []
+        self.mean_constraint = None
+        self.std_constraint = None
+
+    @abstractmethod
+    def build_distribution(self, param_dict: Dict[str, str]) -> rv_continuous:
+        raise NotImplementedError
+
+    @abstractmethod
+    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
+        raise NotImplementedError
+
+    def _parse_parameter(
+        self,
+        param_dict: Dict[str, str],
+        aliases: List[str],
+        canon_name: str,
+        additional_transformation: Callable = lambda name, value: value,
+    ):
+
+        param_names = list(param_dict.keys())
+        param_name = self._parse_parameter_candidates(canon_name, param_names, aliases)
+        if param_name is None:
+            return {}
+
+        self.used_parameters.append(param_name)
+
+        value = float(param_dict[param_name])
+        value = additional_transformation(param_name, value)
+
+        return {canon_name: value}
+
+    def _verify_distribution_parameterization(self, param_dict):
+        n_constraints = sum([self.mean_constraint is not None, self.std_constraint is not None])
+        if n_constraints > self.n_params:
+            raise InsufficientDegreesOfFreedomException(self.variable_name, self.d_name)
+
+        declared_params = list(param_dict.keys())
+        boundary_params = [self.lower_bound_param_name, self.upper_bound_param_name]
+        declared_params = [param for param in declared_params if param not in boundary_params]
+
+        n_params_passed = len(declared_params)
+        if (n_constraints + n_params_passed) > self.n_params:
+            raise DistributionOverDefinedException(
+                self.variable_name,
+                self.d_name,
+                self.n_params,
+                n_params_passed,
+                n_constraints,
+            )
+
+        if self._has_std_constraint():
+            if self.std_constraint <= 0:
+                raise InvalidParameterException(
+                    self.variable_name,
+                    self.d_name,
+                    self.std_constraint,
+                    self.std_constraint,
+                    f"{self.std_constraint} > 0",
+                )
+
+        if self.scale_param_name in declared_params and param_dict[self.scale_param_name] <= 0:
+            raise InvalidParameterException(
+                self.variable_name,
+                self.d_name,
+                self.scale_param_name,
+                self.scale_param_name,
+                f"{self.scale_param_name} > 0",
+            )
+
+    @abstractmethod
+    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
+        raise NotImplementedError
+
+    def _parse_mean_constraint(self, param_dict: Dict[str, str]) -> Dict:
+        aliases = MEAN_ALIASES
+        param_names = list(param_dict.keys())
+        mean_param = self._parse_parameter_candidates("mean", param_names, aliases)
+
+        if mean_param is None:
+            return {}
+
+        self.used_parameters.append(mean_param)
+        value = float(param_dict[mean_param])
+        self.mean_constraint = value
+        return {}
+
+    def _parse_std_constraint(self, param_dict: Dict[str, str]) -> Dict:
+        aliases = STD_ALIASES
+        param_names = list(param_dict.keys())
+        std_param = self._parse_parameter_candidates("mean", param_names, aliases)
+
+        if std_param is None:
+            return {}
+
+        self.used_parameters.append(std_param)
+        value = float(param_dict[std_param])
+        self.std_constraint = value
+        return {}
+
+    def _has_mean_constraint(self):
+        return self.mean_constraint is not None
+
+    def _has_std_constraint(self):
+        return self.std_constraint is not None
+
+    def _parse_parameter_candidates(
+        self, canon_param_name: str, param_names: List[str], aliases: List[str]
+    ) -> Optional[str]:
+        candidates = list(set(param_names).intersection(set(aliases)))
+        if len(candidates) == 0:
+            invalid_param_names = list(set(param_names) - set(self.all_valid_parameters))
+
+            if len(invalid_param_names) == 0:
+                return None
+
+            best_guess, maybe_typo = find_typos_and_guesses(invalid_param_names, aliases)
+
+            if best_guess is not None and maybe_typo is not None:
+                warn(
+                    f'Found a partial name match: "{maybe_typo}" for "{best_guess}" while parsing the {self.d_name} '
+                    f'associated with "{self.variable_name}". Please verify whether the distribution is correctly '
+                    f"specified in the GCN file.",
+                    category=IgnoredCloseMatchWarning,
+                )
+
+            return None
+
+        if len(candidates) > 1:
+            raise MultipleParameterDefinitionException(
+                self.variable_name, self.d_name, canon_param_name, candidates
+            )
+
+        return list(candidates)[0]
+
+    def _warn_about_unused_parameters(self, param_dict: Dict[str, str]) -> None:
+        used_parameters = self.used_parameters
+        all_params = list(param_dict.keys())
+
+        unused_parameters = list(set(all_params) - set(used_parameters))
+        n_params = len(unused_parameters)
+        if n_params > 0:
+            message = (
+                f'After parsing {self.d_name} distribution associated with "{self.variable_name}", the '
+                f"following parameters remained unused: "
+            )
+            if n_params == 1:
+                message += unused_parameters[0] + "."
+            else:
+                message += ", ".join(unused_parameters[:-1]) + f", and {unused_parameters[-1]}."
+            message += " Please verify whether these parameters are needed, and adjust the GCN file accordingly."
+
+            warn(message, category=UnusedParameterWarning)
+
+
+# TODO: Split into NormalDistributionParser and TruncatedNormalDistributionParser?
+class NormalDistributionParser(BaseDistributionParser):
+    def __init__(self, variable_name: str):
+        super().__init__(
+            variable_name=variable_name,
+            d_name="normal",
+            loc_param_name="loc",
+            scale_param_name="scale",
+            shape_param_name=None,
+            lower_bound_param_name="a",
+            upper_bound_param_name="b",
+            n_params=2,
+            all_valid_parameters=NORMAL_LOC_ALIASES + NORMAL_SCALE_ALIASES + MOMENTS,
+        )
+
+    def build_distribution(
+        self, param_dict: Dict[str, str], package="scipy", model=None
+    ) -> rv_continuous:
+        parsed_param_dict = self._parse_parameters(param_dict)
+
+        self._warn_about_unused_parameters(param_dict)
+        self._verify_distribution_parameterization(parsed_param_dict)
+
+        if package == "scipy":
+            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
+            parameters = list(parsed_param_dict.keys())
+
+            if (
+                self.lower_bound_param_name not in parameters
+                and self.upper_bound_param_name not in parameters
+            ):
+                return norm(**parsed_param_dict)
+            else:
+                return truncnorm(**parsed_param_dict)
+
+    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
+        parsed_param_dict = {}
+
+        def tau_to_sigma(name, value):
+            return 1 / value if name in ["tau", "precision"] else value
+
+        parse_loc_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.loc_param_name,
+            aliases=NORMAL_LOC_ALIASES,
+        )
+        parse_scale_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.scale_param_name,
+            aliases=NORMAL_SCALE_ALIASES,
+            additional_transformation=tau_to_sigma,
+        )
+        parse_lower_bound = partial(
+            self._parse_parameter,
+            canon_name=self.lower_bound_param_name,
+            aliases=LOWER_BOUND_ALIASES,
+        )
+        parse_upper_bound = partial(
+            self._parse_parameter,
+            canon_name=self.upper_bound_param_name,
+            aliases=UPPER_BOUND_ALIASES,
+        )
+
+        parsing_functions = [
+            self._parse_mean_constraint,
+            self._parse_std_constraint,
+            parse_loc_parameter,
+            parse_scale_parameter,
+            parse_lower_bound,
+            parse_upper_bound,
+        ]
+
+        for f in parsing_functions:
+            parsed_param_dict.update(f(param_dict))
+
+        return parsed_param_dict
+
+    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
+
+        parameters = list(param_dict.keys())
+
+        # Easiest case: No bounds
+        if (
+            self.lower_bound_param_name not in parameters
+            and self.upper_bound_param_name not in parameters
+        ):
+            if self._has_mean_constraint():
+                param_dict[self.loc_param_name] = self.mean_constraint
+            if self._has_std_constraint():
+                param_dict[self.scale_param_name] = self.std_constraint
+
+            return param_dict
+
+        # Handle cases with bounds
+        else:
+            # User might pass only one boundary, in that case set the other to infinity
+            a = (
+                param_dict[self.lower_bound_param_name]
+                if self.lower_bound_param_name in parameters
+                else -np.inf
+            )
+            b = (
+                param_dict[self.upper_bound_param_name]
+                if self.upper_bound_param_name in parameters
+                else np.inf
+            )
+
+            # Case 1: Both mean and std constraint given
+            if self._has_std_constraint() and self._has_mean_constraint():
+                # TODO: This is extremely slow when the boundary is "binding" (i.e. the mean or std is close to it),
+                #  so the loc or scale needs to become very large to meet the moment constraint. Can it be replaced
+                #  with an approximation?
+                def moment_errors(x, target_mean, target_std, a, b):
+                    loc_approx, scale_approx = x
+
+                    alpha = (a - loc_approx) / scale_approx
+                    beta = (b - loc_approx) / scale_approx
+                    d = truncnorm(loc=loc_approx, scale=scale_approx, a=alpha, b=beta)
+
+                    error_loc = target_mean - d.mean()
+                    error_std = target_std - d.std()
+
+                    error_vec = np.array([error_loc, error_std])
+
+                    return (error_vec**2).mean()
+
+                mean, std = self.mean_constraint, self.std_constraint
+
+                result = optimize.minimize(
+                    moment_errors,
+                    x0=[mean, std],
+                    args=(mean, std, a, b),
+                    bounds=[(None, None), (0, None)],
+                    method="Nelder-Mead",
+                    options={"maxiter": 1000},
+                )
+
+                if not result.success and result.fun > 1e-5:
+                    print(result)
+                    raise ValueError
+
+                loc, scale = result.x
+                param_dict[self.loc_param_name] = loc
+                param_dict[self.scale_param_name] = scale
+
+            # Case 2: Mean constraint and scale parameter
+            elif self._has_mean_constraint():
+                mean = self.mean_constraint
+                scale = param_dict[self.scale_param_name]
+
+                def match_mean(loc, target_mean, scale, a, b):
+                    alpha = (a - loc) / scale
+                    beta = (b - loc) / scale
+
+                    return truncnorm(loc=loc, scale=scale, a=alpha, b=beta).mean() - target_mean
+
+                loc = optimize.root_scalar(
+                    match_mean,
+                    args=(mean, scale, a, b),
+                    bracket=[-100, 100],
+                    method="brenth",
+                ).root
+
+                param_dict[self.loc_param_name] = loc
+
+            # Case 3: Scale constraint and loc parameter
+            elif self._has_std_constraint():
+                std = self.std_constraint
+                loc = param_dict[self.loc_param_name]
+
+                def match_std(scale, target_std, loc, a, b):
+                    alpha = (a - loc) / scale
+                    beta = (b - loc) / scale
+
+                    return truncnorm(loc=loc, scale=scale, a=alpha, b=beta).std() - target_std
+
+                scale = optimize.root_scalar(
+                    match_std,
+                    args=(std, loc, a, b),
+                    bracket=[1e-4, 100],
+                    method="brenth",
+                ).root
+
+                param_dict[self.scale_param_name] = scale
+
+            # Case 4: a, b, loc, and scale are all provided
+            else:
+                loc = param_dict[self.loc_param_name]
+                scale = param_dict[self.scale_param_name]
+
+            # Clean up: compute the adjusted bounds
+            param_dict[self.lower_bound_param_name] = (a - loc) / scale
+            param_dict[self.upper_bound_param_name] = (b - loc) / scale
+
+        return param_dict
+
+
+class HalfNormalDistributionParser(BaseDistributionParser):
+    def __init__(self, variable_name: str):
+        super().__init__(
+            variable_name=variable_name,
+            d_name="halfnormal",
+            loc_param_name="loc",
+            scale_param_name="scale",
+            shape_param_name=None,
+            upper_bound_param_name=None,
+            lower_bound_param_name=None,
+            n_params=2,
+            all_valid_parameters=NORMAL_SCALE_ALIASES + MOMENTS + ["loc"],
+        )
+
+    def build_distribution(
+        self, param_dict: Dict[str, str], package="scipy", model=None
+    ) -> rv_continuous:
+        parsed_param_dict = self._parse_parameters(param_dict)
+        self._warn_about_unused_parameters(param_dict)
+        self._verify_distribution_parameterization(parsed_param_dict)
+
+        if package == "scipy":
+            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
+
+            return halfnorm(**parsed_param_dict)
+
+    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
+        parsed_param_dict = {}
+
+        def tau_to_sigma(name, value):
+            return 1 / value if name in ["tau", "precision"] else value
+
+        parse_loc_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.loc_param_name,
+            aliases=[self.loc_param_name],
+        )
+        parse_scale_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.scale_param_name,
+            aliases=NORMAL_SCALE_ALIASES,
+            additional_transformation=tau_to_sigma,
+        )
+
+        parsing_functions = [
+            self._parse_mean_constraint,
+            self._parse_std_constraint,
+            parse_loc_parameter,
+            parse_scale_parameter,
+        ]
+
+        for f in parsing_functions:
+            parsed_param_dict.update(f(param_dict))
+
+        return parsed_param_dict
+
+    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
+        if self._has_mean_constraint() and self._has_std_constraint():
+            mean, std = self.mean_constraint, self.std_constraint
+
+            loc, scale = match_first_two_moments(
+                target_mean=mean, target_std=std, dist_object=halfnorm
+            )
+
+            param_dict[self.loc_param_name] = loc
+            param_dict[self.scale_param_name] = scale
+
+        elif self._has_mean_constraint():
+            mu = self.mean_constraint
+            param_dict[self.scale_param_name] = mu * np.sqrt(np.pi / 2)
+
+        elif self._has_std_constraint():
+            std = self.std_constraint
+            param_dict[self.scale_param_name] = std / np.sqrt(1 - 2 / np.pi)
+
+        return param_dict
+
+
+class UniformDistributionParser(BaseDistributionParser):
+    def __init__(self, variable_name: str):
+        super().__init__(
+            variable_name=variable_name,
+            d_name="uniform",
+            loc_param_name="loc",
+            scale_param_name="scale",
+            shape_param_name=None,
+            lower_bound_param_name="a",
+            upper_bound_param_name="b",
+            n_params=2,
+            all_valid_parameters=["loc", "scale"]
+            + LOWER_BOUND_ALIASES
+            + UPPER_BOUND_ALIASES
+            + MOMENTS,
+        )
+
+    def build_distribution(
+        self, param_dict: Dict[str, str], package="scipy", model=None
+    ) -> rv_continuous:
+        parsed_param_dict = self._parse_parameters(param_dict)
+        self._warn_about_unused_parameters(param_dict)
+        self._verify_distribution_parameterization(parsed_param_dict)
+
+        if package == "scipy":
+            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
+
+            return uniform(**parsed_param_dict)
+
+    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
+        parse_loc_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.loc_param_name,
+            aliases=[self.loc_param_name],
+        )
+        parse_scale_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.scale_param_name,
+            aliases=[self.scale_param_name],
+        )
+        parse_lower_bound = partial(
+            self._parse_parameter,
+            canon_name=self.lower_bound_param_name,
+            aliases=LOWER_BOUND_ALIASES,
+        )
+        parse_upper_bound = partial(
+            self._parse_parameter,
+            canon_name=self.upper_bound_param_name,
+            aliases=UPPER_BOUND_ALIASES,
+        )
+
+        parsing_functions = [
+            self._parse_mean_constraint,
+            self._parse_std_constraint,
+            parse_loc_parameter,
+            parse_scale_parameter,
+            parse_lower_bound,
+            parse_upper_bound,
+        ]
+
+        parsed_param_dict = {}
+        for f in parsing_functions:
+            parsed_param_dict.update(f(param_dict))
+
+        return parsed_param_dict
+
+    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
+
+        parameters = list(param_dict.keys())
+
+        # Case 1: Two moment constraints
+        if self._has_mean_constraint() and self._has_std_constraint():
+            mean, std = self.mean_constraint, self.std_constraint
+            b = np.sqrt(3) * std + mean
+            a = 2 * mean - b
+            param_dict[self.loc_param_name] = a
+            param_dict[self.scale_param_name] = b - a
+
+        # Case 2: Mean condition only
+        elif self._has_mean_constraint():
+            mean = self.mean_constraint
+
+            if self.loc_param_name in parameters:
+                a = param_dict[self.loc_param_name]
+                b = 2 * mean - a
+                param_dict[self.scale_param_name] = b - a
+
+            if self.scale_param_name in parameters:
+                scale = param_dict[self.scale_param_name]
+                param_dict[self.loc_param_name] = mean - 0.5 * scale
+
+        # Case 3: Std condition only
+        elif self._has_std_constraint():
+            std = self.std_constraint
+
+            if self.loc_param_name in parameters:
+                a = param_dict[self.loc_param_name]
+                b = np.sqrt(12) * std + a
+                param_dict[self.scale_param_name] = b - a
+
+            elif self.scale_param_name in parameters:
+                # TODO: Determine if this case is plausible, doesn't seem so, because sigma = 12 ** (-1/2) * scale
+                raise ValueError("Scale and Std are not enough to identify a Uniform distribution!")
+
+        # Case 4: User passed the bounds directly, convert to loc and scale then delete
+        else:
+            if self.lower_bound_param_name in parameters:
+                param_dict[self.loc_param_name] = param_dict[self.lower_bound_param_name]
+                del param_dict[self.lower_bound_param_name]
+            if self.upper_bound_param_name in parameters:
+                b = param_dict[self.upper_bound_param_name]
+                a = param_dict[self.loc_param_name]
+                param_dict[self.scale_param_name] = b - a
+
+                del param_dict[self.upper_bound_param_name]
+
+        return param_dict
+
+
+class InverseGammaDistributionParser(BaseDistributionParser):
+    def __init__(self, variable_name: str):
+        super().__init__(
+            variable_name=variable_name,
+            d_name="inv_gamma",
+            loc_param_name="loc",
+            scale_param_name="scale",
+            shape_param_name="a",
+            upper_bound_param_name=None,
+            lower_bound_param_name=None,
+            n_params=3,
+            all_valid_parameters=INV_GAMMA_SHAPE_ALIASES + INV_GAMMA_SCALE_ALIASES + ["loc"],
+        )
+
+    def build_distribution(
+        self, param_dict: Dict[str, str], package="scipy", model=None
+    ) -> rv_continuous:
+        parsed_param_dict = self._parse_parameters(param_dict)
+        self._warn_about_unused_parameters(param_dict)
+        self._verify_distribution_parameterization(parsed_param_dict)
+
+        if package == "scipy":
+            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
+            return invgamma(**parsed_param_dict)
+
+    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
+        parse_loc_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.loc_param_name,
+            aliases=[self.loc_param_name],
+        )
+        parse_scale_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.scale_param_name,
+            aliases=INV_GAMMA_SCALE_ALIASES,
+        )
+        parse_shape_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.shape_param_name,
+            aliases=INV_GAMMA_SHAPE_ALIASES,
+        )
+
+        parsing_functions = [
+            self._parse_mean_constraint,
+            self._parse_std_constraint,
+            parse_loc_parameter,
+            parse_scale_parameter,
+            parse_shape_parameter,
+        ]
+
+        parsed_param_dict = {}
+        for f in parsing_functions:
+            parsed_param_dict.update(f(param_dict))
+
+        return parsed_param_dict
+
+    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
+        # TODO: What should be done with loc?
+        parameters = list(param_dict.keys())
+
+        user_passed_loc = self.loc_param_name in parameters
+        user_passed_scale = self.scale_param_name in parameters
+        user_passed_shape = self.shape_param_name in parameters
+
+        # Case 1: Two Constraints
+        if self._has_mean_constraint() and self._has_std_constraint():
+            mean, std = self.mean_constraint, self.std_constraint
+
+            param_dict[self.shape_param_name] = a = (mean / std) ** 2 + 2
+            param_dict[self.scale_param_name] = mean * (a - 1)
+
+            return param_dict
+
+        # Case 2: Mean constraint only
+        if self._has_mean_constraint():
+            mu = self.mean_constraint
+
+            if user_passed_shape:
+                a = param_dict[self.shape_param_name]
+                param_dict[self.scale_param_name] = mu * (a - 1)
+
+            elif user_passed_scale:
+                b = param_dict[self.scale_param_name]
+                param_dict[self.shape_param_name] = (b + mu) / mu
+
+        # Case 3: Std constraint only
+        elif self._has_std_constraint():
+            std = self.std_constraint
+            if user_passed_shape:
+                a = param_dict[self.shape_param_name]
+                param_dict[self.scale_param_name] = std * (a - 1) * np.sqrt(a - 2)
+
+            elif user_passed_scale:
+                b = param_dict[self.scale_param_name]
+
+                def solve_for_shape(a, target_std, b):
+                    return b**2 / (a - 1) ** 2 / (a - 2) - target_std**2
+
+                param_dict[self.shape_param_name] = optimize.root_scalar(
+                    solve_for_shape,
+                    args=(std, b),
+                    bracket=[2 + 1e-4, 100],
+                    method="brenth",
+                ).root
+        return param_dict
+
+
+class BetaDistributionParser(BaseDistributionParser):
+    def __init__(self, variable_name: str):
+        super().__init__(
+            variable_name=variable_name,
+            d_name="beta",
+            loc_param_name="loc",
+            scale_param_name="scale",
+            shape_param_name=None,
+            upper_bound_param_name=None,
+            lower_bound_param_name=None,
+            n_params=2,
+            all_valid_parameters=BETA_SHAPE_ALIASES_1
+            + BETA_SHAPE_ALIASES_2
+            + MOMENTS
+            + ["loc", "scale"],
+        )
+
+        self.shape_param_name_1 = "a"
+        self.shape_param_name_2 = "b"
+
+    def build_distribution(
+        self, param_dict: Dict[str, str], package="scipy", model=None
+    ) -> rv_continuous:
+        parsed_param_dict = self._parse_parameters(param_dict)
+        self._warn_about_unused_parameters(param_dict)
+        self._verify_distribution_parameterization(parsed_param_dict)
+
+        if package == "scipy":
+            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
+            return beta(**parsed_param_dict)
+
+    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
+
+        parse_loc_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.loc_param_name,
+            aliases=[self.loc_param_name],
+        )
+        parse_scale_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.scale_param_name,
+            aliases=[self.scale_param_name],
+        )
+        parse_shape_parameter_1 = partial(
+            self._parse_parameter,
+            canon_name=self.shape_param_name_1,
+            aliases=BETA_SHAPE_ALIASES_1,
+        )
+        parse_shape_parameter_2 = partial(
+            self._parse_parameter,
+            canon_name=self.shape_param_name_2,
+            aliases=BETA_SHAPE_ALIASES_2,
+        )
+
+        parsing_functions = [
+            self._parse_mean_constraint,
+            self._parse_std_constraint,
+            parse_loc_parameter,
+            parse_scale_parameter,
+            parse_shape_parameter_1,
+            parse_shape_parameter_2,
+        ]
+
+        parsed_param_dict = {}
+        for f in parsing_functions:
+            parsed_param_dict.update(f(param_dict))
+
+        return parsed_param_dict
+
+    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
+        parameters = list(param_dict.keys())
+        used_parameters = self.used_parameters
+
+        user_passed_alpha = self.shape_param_name_1 in parameters
+        user_passed_beta = self.shape_param_name_2 in parameters
+
+        # Case 1: Two moment constraints
+        if self._has_mean_constraint() and self._has_std_constraint():
+            mean, std = self.mean_constraint, self.std_constraint
+            if (mean > 1) or (mean < 0):
+                raise InvalidParameterException(
+                    self.variable_name, self.d_name, "mean", "mean", "0 < mean < 1"
+                )
+
+            if std <= 0:
+                used_name = list(set(used_parameters).intersection(set(STD_ALIASES)))[0]
+                raise InvalidParameterException(
+                    self.variable_name, self.d_name, "mean", used_name, "sd > 0"
+                )
+
+            if ((1 - mean) ** 2 * mean) < (std**2):
+                used_name = list(set(used_parameters).intersection(set(STD_ALIASES)))[0]
+                raise InvalidParameterException(
+                    self.variable_name,
+                    self.d_name,
+                    "mean, std",
+                    f"mean, {used_name}",
+                    "((1 - mean) ** 2 * mean) < (std ** 2)",
+                )
+
+            x = (1 - mean) / mean
+            param_dict[self.shape_param_name_1] = alpha = x / (std**2 * (x + 1) ** 3) - (
+                1 / (x + 1)
+            )
+            param_dict[self.shape_param_name_2] = alpha * x
+
+        # # Case 2: No moment constraints
+        # elif user_passed_alpha and user_passed_beta:
+        #     return param_dict
+
+        # Case 3: Mean constraint and one shape parameter
+        elif self._has_mean_constraint():
+            mean = self.mean_constraint
+            if user_passed_alpha:
+                a = param_dict[self.shape_param_name_1]
+                param_dict[self.shape_param_name_2] = mean * (a - 1)
+
+            elif user_passed_beta:
+                b = param_dict[self.shape_param_name_2]
+                param_dict[self.shape_param_name_1] = (b + mean) / mean
+
+        # Case 4: Std constraints and one shape parameter
+        elif self._has_std_constraint():
+            std = self.std_constraint
+
+            if user_passed_alpha:
+
+                def solve_for_beta(b, a, std):
+                    return std**2 - a * b / (a + b) ** 2 / (a + b + 1)
+
+                a = param_dict[self.shape_param_name_1]
+                b = optimize.root_scalar(
+                    solve_for_beta, args=(a, std), bracket=[1.001, 100], method="brenth"
+                ).root
+
+                param_dict[self.shape_param_name_2] = b
+
+            elif user_passed_beta:
+
+                def solve_for_alpha(a, b, std):
+                    return std**2 - a * b / (a + b) ** 2 / (a + b + 1)
+
+                b = param_dict[self.shape_param_name_2]
+                a = optimize.root_scalar(
+                    solve_for_alpha,
+                    args=(b, std),
+                    bracket=[1.001, 100],
+                    method="brenth",
+                ).root
+                param_dict[self.shape_param_name_1] = a
+
+        return param_dict
+
+
+class GammaDistributionParser(BaseDistributionParser):
+    def __init__(self, variable_name: str):
+        super().__init__(
+            variable_name=variable_name,
+            d_name="gamma",
+            loc_param_name="loc",
+            scale_param_name="scale",
+            shape_param_name="a",
+            lower_bound_param_name=None,
+            upper_bound_param_name=None,
+            n_params=3,
+            all_valid_parameters=GAMMA_SHAPE_ALIASES + GAMMA_SCALE_ALIASES + MOMENTS + ["loc"],
+        )
+
+    def build_distribution(
+        self, param_dict: Dict[str, str], package="scipy", model=None
+    ) -> rv_continuous:
+        parsed_param_dict = self._parse_parameters(param_dict)
+        self._warn_about_unused_parameters(param_dict)
+        self._verify_distribution_parameterization(parsed_param_dict)
+
+        if package == "scipy":
+            parsed_param_dict = self._postprocess_parameters(parsed_param_dict)
+            return gamma(**parsed_param_dict)
+
+    def _parse_parameters(self, param_dict: Dict[str, str]) -> Dict[str, float]:
+        parse_loc_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.loc_param_name,
+            aliases=[self.loc_param_name],
+        )
+        parse_scale_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.scale_param_name,
+            aliases=GAMMA_SCALE_ALIASES,
+        )
+        parse_shape_parameter = partial(
+            self._parse_parameter,
+            canon_name=self.shape_param_name,
+            aliases=GAMMA_SHAPE_ALIASES,
+        )
+
+        parsing_functions = [
+            self._parse_mean_constraint,
+            self._parse_std_constraint,
+            parse_loc_parameter,
+            parse_scale_parameter,
+            parse_shape_parameter,
+        ]
+
+        parsed_param_dict = {}
+        for f in parsing_functions:
+            parsed_param_dict.update(f(param_dict))
+
+        return parsed_param_dict
+
+    def _postprocess_parameters(self, param_dict: Dict[str, float]) -> Dict[str, float]:
+        parameters = list(param_dict.keys())
+
+        user_passed_scale = self.scale_param_name in parameters
+        user_passed_shape = self.shape_param_name in parameters
+
+        if self._has_mean_constraint() and self._has_std_constraint():
+            mean, std = self.mean_constraint, self.std_constraint
+            if mean < 0:
+                raise InvalidParameterException(
+                    self.variable_name, self.d_name, "mean", "mean", "mean >= 0"
+                )
+            if std <= 0:
+                raise InvalidParameterException(
+                    self.variable_name, self.d_name, "std", "std", "std >= 0"
+                )
+
+            param_dict[self.shape_param_name] = a = (mean / std) ** 2
+            param_dict[self.scale_param_name] = mean / a
+
+        elif self._has_mean_constraint():
+            mean = self.mean_constraint
+
+            if user_passed_scale:
+                b = param_dict[self.scale_param_name]
+                param_dict[self.shape_param_name] = mean / b
+
+            elif user_passed_shape:
+                a = param_dict[self.shape_param_name]
+                param_dict[self.scale_param_name] = mean / a
+
+        elif self._has_std_constraint():
+            std = self.std_constraint
+
+            if user_passed_scale:
+                b = param_dict[self.scale_param_name]
+                param_dict[self.shape_param_name] = (std / b) ** 2
+
+            elif user_passed_shape:
+                a = param_dict[self.shape_param_name]
+                param_dict[self.scale_param_name] = std / np.sqrt(a)
+
+        return param_dict
+
+
+def match_first_two_moments(
+    target_mean: float, target_std: float, dist_object: rv_continuous
+) -> Tuple[float, float]:
+    def moment_errors(
+        x, target_mean: float, target_std: float, dist_object: rv_continuous
+    ) -> float:
+        loc_approx, scale_approx = x
+
+        d = dist_object(loc=loc_approx, scale=scale_approx)
+
+        error_loc = target_mean - d.mean()
+        error_std = target_std - d.std()
+
+        error_vec = np.array([error_loc, error_std])
+
+        return (error_vec**2).mean()
+
+    result = optimize.minimize(
+        moment_errors,
+        x0=[target_mean, target_std],
+        args=(target_mean, target_std, dist_object),
+        bounds=[(None, None), (0, None)],
+        method="powell",
+        options={"maxiter": 100},
+    )
+
+    if not result.success and result.fun > 1e-5:
+        print(result)
+        raise ValueError
+
+    loc, scale = result.x
+    return loc, scale
+
+
+def build_alias_to_canon_dict(alias_list, cannon_names) -> Dict[str, str]:
+    alias_to_canon_dict = {}
+    aliases = reduce(lambda a, b: a + b, alias_list)
+
+    for alias in aliases:
+        for group, canon_name in zip(alias_list, cannon_names):
+            if alias in group:
+                alias_to_canon_dict[alias] = canon_name
+                break
+    return alias_to_canon_dict
+
+
+def preprocess_distribution_string(variable_name: str, d_string: str) -> Tuple[str, Dict[str, str]]:
+    """
+    Parameters
+    ----------
+    variable_name: str
+        A string representing the model parameter associated with this probability distribution.
+    d_string: str
+        A string representing a probability distribution, extracted from a GCN file by the gEcon_parser.preprocess_gcn
+        function.
+
+    Returns
+    -------
+    Tuple of (str, dict), containing the model parameter name associated with the distribution, and a dictionary of the
+    distribution parameters (e.g. loc, scale, shape, and bounds).
+    """
+    name_to_canon_dict = build_alias_to_canon_dict(DIST_ALIAS_LIST, CANON_NAMES)
+
+    digit_pattern = r" ?\d*\.?\d* ?"
+    general_pattern = rf" ?[\w\.]* ?"
+
+    # The not last args have a comma, while the last arg does not.
+    dist_name_pattern = r"(\w+)"
+    not_last_arg_pattern = rf"(\w+ ?={general_pattern}, ?)"
+    last_arg_pattern = rf"(\w+ ?={general_pattern})"
+    valid_pattern = rf"{dist_name_pattern}\({not_last_arg_pattern}*?{last_arg_pattern}\),?$"
+
+    # TODO: sort out where the typo is and tell the user.
+    if re.search(valid_pattern, d_string) is None:
+        raise InvalidDistributionException(variable_name, d_string)
+
+    d_name, params_string = d_string.split("(")
+    d_name = d_name.lower()
+
+    if d_name not in name_to_canon_dict.keys():
+        raise InvalidDistributionException(variable_name, d_string)
+
+    params = [x.strip() for x in params_string.replace(")", "").split(",")]
+    params = [x for x in params if len(x) > 0]
+
+    new_params = []
+    for p in params:
+        chunks = p.split("=")
+        new_p = "=".join([chunks[0].lower(), chunks[1]])
+        new_params.append(new_p)
+
+    params = new_params
+
+    param_dict = {}
+    for param in params:
+        key, value = (x.strip() for x in param.split("="))
+        if key in param_dict.keys():
+            raise RepeatedParameterException(variable_name, d_name, key)
+
+        param_dict[key] = value
+
+    return name_to_canon_dict[d_name], param_dict
+
+
+def preprocess_prior_dict(
+    raw_prior_dict: Dict[str, str]
+) -> Tuple[List[str], List[str], List[Dict[str, str]]]:
+    """
+
+    Parameters
+    ----------
+    raw_prior_dict: dict
+        Dictionary of variable name: raw distribution string pairs.
+
+    Returns
+    -------
+    list of (variable_name, distribution_name, prior_dict) tuples.
+        The prior_dict of each variable has parameter names as the keys and parameter values as the values.
+        Values are still represented as strings, since we still need to check for compound distributions at a later
+        stage.
+    """
+
+    variable_names = []
+    d_names = []
+    param_dicts = []
+    for variable_name, d_string in raw_prior_dict.items():
+        d_name, param_dict = preprocess_distribution_string(variable_name, d_string)
+        variable_names.append(variable_name)
+        d_names.append(d_name)
+        param_dicts.append(param_dict)
+
+    return variable_names, d_names, param_dicts
+
+
+def distribution_factory(
+    variable_name: str,
+    d_name: str,
+    param_dict: Dict[str, str],
+    package: str = "scipy",
+    model=None,
+) -> rv_continuous:
+    """
+    Parameters
+    ----------
+    variable_name: str
+        name of the variable with which this distribution is associated
+    d_name: str
+        plaintext name of the distribution to parameterize, from the CANNON_NAMES list.
+    param_dict: dict
+        a dictionary of parameter: value pairs, or parameter: string pairs in the case of composite distributions
+    package: str
+        package of the distribution function to parameterize
+
+    Returns
+    -------
+    d: rv_frozen
+        a scipy distribution object object
+    """
+
+    if package not in ["scipy"]:
+        raise NotImplementedError
+
+    parser = None
+
+    if d_name == "normal":
+        parser = NormalDistributionParser(variable_name=variable_name)
+
+    elif d_name == "halfnormal":
+        parser = HalfNormalDistributionParser(variable_name=variable_name)
+
+    elif d_name == "inv_gamma":
+        parser = InverseGammaDistributionParser(variable_name=variable_name)
+
+    elif d_name == "beta":
+        parser = BetaDistributionParser(variable_name=variable_name)
+
+    elif d_name == "gamma":
+        parser = GammaDistributionParser(variable_name=variable_name)
+
+    elif d_name == "uniform":
+        parser = UniformDistributionParser(variable_name=variable_name)
+
+    if parser is None:
+        print(d_name)
+        raise ValueError("How did you even get here?")
+
+    d = parser.build_distribution(param_dict, package=package, model=model)
+    return d
+
+
+def rename_dict_keys_with_value_transform(
+    d: Dict,
+    to_rename: List[str],
+    new_key: str,
+    variable_name: str,
+    d_name: str,
+    transformation: Callable = lambda name, value: value,
+) -> Dict[str, Any]:
+    result = {}
+    matches = [key for key in d.keys() if key in set(to_rename)]
+    if len(matches) > 1:
+        raise MultipleParameterDefinitionException(variable_name, d_name, new_key, matches)
+
+    for key, value in d.items():
+        if key in to_rename:
+            result[new_key] = transformation(key, value)
+        else:
+            result[key] = value
+
+    return result
+
+
+def param_values_to_floats(param_dict: Dict):
+    for param, param_value in param_dict.items():
+        if isinstance(param_value, str):
+            if is_number(param_value):
+                param_dict[param] = float(param_value)
+
+    return param_dict
+
+
+def split_out_composite_distributions(
+    variable_names: List[str], d_names: List[str], param_dicts: List[Dict[str, str]]
+) -> Tuple[Dict[str, Tuple[str, Dict[str, str]]], Dict[str, Tuple[str, Dict[str, str]]]]:
+    basic_distributions = {}
+    composite_distributions = {}
+
+    for variable_name, d_name, param_dict in zip(variable_names, d_names, param_dicts):
+        if all([is_number(x) for x in param_dict.values()]):
+            basic_distributions[variable_name] = (d_name, param_dict)
+        else:
+            composite_distributions[variable_name] = (d_name, param_dict)
+
+    return basic_distributions, composite_distributions
+
+
+def fetch_rv_params(param_dict, model):
+    return_dict = {}
+    for k, v in param_dict.items():
+        if isinstance(v, (float, int)):
+            return_dict[k] = v
+        elif isinstance(v, str):
+            return_dict[k] = model[v]
+        else:
+            raise ValueError(f"Found an illegal key:value pair in prior param dict, {k}:{v}")
+
+    return return_dict
+
+
+def composite_distribution_factory(
+    variable_name, d_name, param_dict, package="scipy", model=None
+) -> Union[CompositeDistribution, None]:
+    """
+    Parameters
+    ----------
+    variable_name: str
+        Name of the variable the distribution is associated with
+    d_name: str
+        Name of the distribution, one of CANNON_NAMES
+    param_dict: dict
+        Dictionary of parameter name, parameter value pairs. Parameter values should be either scipy rv_frozen objects
+        or strings that can be converted to floats.
+    package: str
+        Which package to use to create the distributions. Currently "scipy".
+
+    Returns
+    -------
+    d: CompositeDistribution
+         A wrapper around a set of scipy distributions with three methods: .rvs(), .pdf(), and .logpdf()
+
+    TODO: This function is a huge mess of if-else statements. All of this should maybe be put into the parser classes
+        to take advantage of all the parameter checking that happens there. Consider this temporary.
+
+    TODO: Currently no checks are done on the support of the parameter to ensure it matches parameter requirements
+        e.g. a > 0, b > 0 in the beta distribution.
+
+    TODO: It might be possible to do moment matching in some limited sense. Currently the initial value for the
+        parameter distributions is thrown away, could use this value to moment match? Maybe not worth it.
+    """
+
+    def tau_to_scale(key, value):
+        if key in {"tau", "precision"}:
+            return 1 / value
+        return value
+
+    if package == "scipy":
+        base_d = NAME_TO_DIST_SCIPY_FUNC[d_name]
+    else:
+        raise NotImplementedError('Only package = "scipy"  is supported.')
+
+    param_dict = param_values_to_floats(param_dict)
+
+    # validate parameters by simple rename, error on more complicated setups (no moment constraints!)
+    if d_name == "normal":
+        has_upper_bound = any([x in set(param_dict.keys()) for x in UPPER_BOUND_ALIASES])
+        has_lower_bound = any([x in set(param_dict.keys()) for x in LOWER_BOUND_ALIASES])
+
+        if (has_upper_bound or has_lower_bound) and package == "scipy":
+            warn(
+                'Moment conditions are not supported for compound distributions, and parameters "mean" and "std" will'
+                'be interpreted as "loc" and "scale". Since you have passed boundaries, the first and second moments'
+                "of the truncated normal distribution will not coincide with the loc and scale parameters.",
+                IgnoredCloseMatchWarning,
+            )
+
+        param_dict = rename_dict_keys_with_value_transform(
+            param_dict, NORMAL_LOC_ALIASES + MEAN_ALIASES, "loc", variable_name, d_name
+        )
+        param_dict = rename_dict_keys_with_value_transform(
+            param_dict,
+            NORMAL_SCALE_ALIASES + STD_ALIASES,
+            "scale",
+            variable_name,
+            d_name,
+            transformation=tau_to_scale,
+        )
+
+        param_dict = rename_dict_keys_with_value_transform(
+            param_dict, LOWER_BOUND_ALIASES, "a", variable_name, d_name
+        )
+
+        param_dict = rename_dict_keys_with_value_transform(
+            param_dict, UPPER_BOUND_ALIASES, "b", variable_name, d_name
+        )
+
+    elif d_name == "halfnormal" and package == "scipy":
+        if any([x in set(param_dict.keys()) for x in MEAN_ALIASES]):
+            warn(
+                "Moment conditions are not supported for compound distributions. If you pass a random variable as a "
+                "parameter value, do not pass in mean or std.",
+                IgnoredCloseMatchWarning,
+            )
+
+        param_dict = rename_dict_keys_with_value_transform(
+            param_dict,
+            NORMAL_SCALE_ALIASES,
+            "scale",
+            variable_name,
+            d_name,
+            transformation=tau_to_scale,
+        )
+
+    elif d_name == "inv_gamma":
+        if any([x in set(param_dict.keys()) for x in MOMENTS]) and package == "scipy":
+            warn(
+                "Moment conditions are not supported for compound distributions. If you pass a random variable as a "
+                "parameter value, do not pass in mean or std.",
+                IgnoredCloseMatchWarning,
+            )
+
+        param_dict = rename_dict_keys_with_value_transform(
+            param_dict, INV_GAMMA_SHAPE_ALIASES, "a", variable_name, d_name
+        )
+
+        param_dict = rename_dict_keys_with_value_transform(
+            param_dict, INV_GAMMA_SCALE_ALIASES, "scale", variable_name, d_name
+        )
+
+    elif d_name == "beta":
+        if any([x in set(param_dict.keys()) for x in MOMENTS]) and package == "scipy":
+            warn(
+                "Moment conditions are not supported for compound distributions. If you pass a random variable as a "
+                "parameter value, do not pass in mean or std. These conditions will be ignored, and this may cause an"
+                "an error to be raised when instantiating the distribution.",
+                IgnoredCloseMatchWarning,
+            )
+
+            param_dict = rename_dict_keys_with_value_transform(
+                param_dict, BETA_SHAPE_ALIASES_1, "a", variable_name, d_name
+            )
+
+            param_dict = rename_dict_keys_with_value_transform(
+                param_dict, BETA_SHAPE_ALIASES_2, "b", variable_name, d_name
+            )
+
+    elif d_name == "gamma":
+        if any([x in set(param_dict.keys()) for x in MOMENTS]) and package == "scipy":
+            warn(
+                "Moment conditions are not supported for compound distributions. If you pass a random variable as a "
+                "parameter value, do not pass in mean or std. These conditions will be ignored, and this may cause an"
+                "an error to be raised when instantiating the distribution.",
+                IgnoredCloseMatchWarning,
+            )
+
+            param_dict = rename_dict_keys_with_value_transform(
+                param_dict, BETA_SHAPE_ALIASES_1, "a", variable_name, d_name
+            )
+
+            param_dict = rename_dict_keys_with_value_transform(
+                param_dict, BETA_SHAPE_ALIASES_2, "b", variable_name, d_name
+            )
+
+    if package == "scipy":
+        d = CompositeDistribution(base_d, **param_dict)
+        return d
+
+
+def create_prior_distribution_dictionary(raw_prior_dict: Dict[str, str]) -> Dict[str, Any]:
+    variable_names, d_names, param_dicts = preprocess_prior_dict(raw_prior_dict)
+    basic_distributions, compound_distributions = split_out_composite_distributions(
+        variable_names, d_names, param_dicts
+    )
+    prior_dict = SymbolDictionary()
+
+    for variable_name, (d_name, param_dict) in basic_distributions.items():
+        d = distribution_factory(variable_name=variable_name, d_name=d_name, param_dict=param_dict)
+        prior_dict[variable_name] = d
+
+    for variable_name, (d_name, param_dict) in compound_distributions.items():
+        rvs_used_in_d = []
+        for param, value in param_dict.items():
+            if value in prior_dict.keys():
+                param_dict[param] = prior_dict[value]
+                rvs_used_in_d.append(value)
+
+        d = composite_distribution_factory(variable_name, d_name, param_dict)
+        prior_dict[variable_name] = d
+        for rv in rvs_used_in_d:
+            del prior_dict[rv]
+
+    return prior_dict
```

### Comparing `gEconpy-1.1.0/gEconpy/parser/parse_equations.py` & `gEconpy-1.2.0/gEconpy/parser/parse_equations.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,419 +1,421 @@
-import re
-from collections import defaultdict
-from typing import Dict, List, Optional
-
-import sympy as sp
-
-from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
-from gEconpy.parser.constants import CALIBRATING_EQ_TOKEN, LOCAL_DICT, TIME_INDEX_DICT
-
-
-def rebuild_eqs_from_parser_output(parser_output: List[str]) -> List[List[str]]:
-    """
-    Takes the output of pyparsing applied to the text of a model block and returns a list of lists of tokens
-    that represent equations.
-
-    The heavy lifting of parsing the model blocks is done by the pyparsing, which returns a list of tokens found
-    between the BLOCK_START_TOKEN and BLOCK_END_TOKEN. These lists need to be further decomposed into equations,
-    which is done using the LINE_TERMINATOR_TOKEN, ';'. The result is a list of lists, with each list containing the
-    tokens for a single model equation.
-
-    The function consumes the LINE_TERMINATOR_TOKENs in the process, and the returned lists are assumed to contain only
-    variables, parameters, and valid mathematical operators.
-
-    Parameters
-    ----------
-    parser_output: list of str
-        Output of pyparsing applied to the text of a model block.
-
-    Returns
-    -------
-    eqs: list of lists of str
-        A list of lists, with each list containing the tokens for a single model equation.
-
-    Examples
-    --------
-    ..code-block:: text
-        rebuild_eqs_from_parser_output([A[], +, B[], +, C[], ;, Y[], =, L[], ^, alpha, *, K[-1], ^, (, 1, -, alpha, ), ;]) [[A[], +, B[], +, C[]],
-         [Y[], =, L[], ^, alpha, *, K[-1], ^, (, 1, -, alpha, )]
-    """
-
-    eqs = []
-    eq = []
-    for element in parser_output:
-        # TODO: Why are commas appearing in the control list after parsing? This is a hack fix.
-        element = element.replace(",", "").strip()
-        if len(element) > 0:
-            eq.append(element)
-        if ";" in element:
-            # ; signals end of line, needs to be removed now
-            eq[-1] = eq[-1].replace(";", "").strip()
-            if len(eq[-1]) == 0:
-                del eq[-1]
-
-            eqs.append(eq)
-            eq = []
-    return eqs
-
-
-def token_classifier(token: str) -> str:
-    """
-    Classify tokens as variables, parameters, operators, or special.
-
-    Parameters
-    ----------
-    token: str
-        Token from a gEcon model equation
-
-    Returns
-    -------
-    classification: str
-        Classification of the model token. One of ['operator', 'variable', 'number', 'lagrange_definition', 'calibration_definition', 'parameter']
-
-    Notes
-    -----
-    There are two special tokens:
-        1. The colon ":", that signifies that the variable after it should be the name of the lagrange multiplier
-         associated with the equation.
-        2. The arrow "->", that defines an calibrating equation for a parameter.
-    """
-
-    if token in ["E[]", "log", "exp"] or token in "+-*/^=()[]":
-        return "operator"
-    if "[" in token and "]" in token:
-        return "variable"
-    if all(s in "0123456789." for s in token):
-        return "number"
-    if token == ":":
-        return "lagrange_definition"
-    if token == "->":
-        return "calibration_definition"
-
-    return "parameter"
-
-
-def has_num_index(token: str) -> bool:
-    """
-    Check if a token has a numerical lag/lead index.
-
-    Lag and lead indices are of the form X[1] or X[-1], this function checks if such an index exists. Split on the
-    opening square bracket first to be robust against variables with numbers in their names, i.e. alpha_1[1].
-
-    Parameters
-    ----------
-    token : str
-        A plaintext model variable.
-
-    Returns
-    -------
-    bool
-        Whether the variable has a numerical lag/lead index.
-    """
-
-    numbers = list("0123456789")
-    index_part = token.split("[")[-1]
-    return any([n in index_part for n in numbers])
-
-
-def extract_time_index(token: str) -> str:
-    """
-    Extract a time index from a model variable.
-
-    Parameters
-    ----------
-    token : str
-        A string representing a model variable.
-
-    Returns
-    -------
-    time_index : str
-        A time-index string.
-
-    Examples
-    --------
-    >>> extract_time_index("alpha[1]")
-    't1'
-    >>> extract_time_index("alpha[-1]")
-    'tL1'
-    >>> extract_time_index("alpha[ss]")
-    'ss'
-    >>> extract_time_index("alpha")
-    't'
-    """
-
-    if has_num_index(token) and "-" not in token:
-        lead = re.findall(r"\d+", token)[0]
-        time_index = "t" + lead
-    elif has_num_index(token) and "-" in token:
-        lag = re.findall(r"\d+", token)[0]
-        time_index = "tL" + lag
-    elif "[ss]" in token:
-        time_index = "ss"
-    else:
-        time_index = "t"
-
-    return time_index
-
-
-def remove_timing_information(token: str) -> str:
-    """
-    Remove timing information from a model variable token.
-
-    A variable's timing information is contained in the square brackets next to it. This needs to be removed and
-    replaced with some plaintext before it can be passed to the SymPy parser.
-
-    Parameters
-    ----------
-    token : str
-        A string representing a model variable.
-
-    Returns
-    -------
-    str
-        The same token with the timing information removed.
-    """
-    token = re.sub(r"\[.*?\]", "", token)
-    return token.strip()
-
-
-def convert_to_python_operator(token: str) -> str:
-    """
-    Convert a GCN operation token to a python-compatible one.
-
-    Parameters
-    ----------
-    token : str
-        A string representing a mathematical operation.
-
-    Returns
-    ---------
-    str
-        A string representing the same operation in python syntax.
-
-    Notes
-    ----------
-    The syntax of a gEcon GCN file is slightly different from what SymPy expects, this function resolves the
-    differences. In particular:
-        1. Exponents are marked with a caret "^" in the GCN file, and must be converted to python's **
-        2. SymPy's parse_expr cannot handle equalities, but can handle a list of equations. Equalities are thus
-           converted to two separate equations then set as equal later.
-        3. Remove the expectation operator completely
-        4. Replace square brackets with parenthesis
-
-    TODO: Implement an expectation operation in Sympy that can inserted here.
-    """
-
-    if token == "^":
-        return "**"
-    if token == "=":
-        return ","
-    if token == "E[]":
-        return ""
-    if token == "[":
-        return "("
-    if token == "]":
-        return ")"
-
-    return token
-
-
-def rename_time_indexes(eq: sp.Eq) -> sp.Eq:
-    """
-    Rename time indices of the form "tL1" or "t1" to the normal form "t", "t+1", or "t-1".
-
-    Parameters
-    ----------
-    eq : sp.Eq
-        A sympy equation representing a model equation.
-
-    Returns
-    -------
-    sp.Eq
-        The same equation, with time indices renamed.
-
-    Notes
-    -----
-    The function assumes the index is always at the end of the variable name.
-    """
-
-    ret_eq = eq.copy()
-    for atom in ret_eq.atoms():
-        if isinstance(atom, sp.core.Symbol):
-            if re.search(r"tL?\d+", atom.name) is not None:
-                name_tokens = atom.name.split("_")
-                index_token = name_tokens[-1]
-                operator = "-" if "L" in index_token else "+"
-                number = re.search(r"\d+", index_token)[0]
-                new_index = "".join(["_{t", operator, number, "}"])
-
-                var_name = "_".join(s for s in name_tokens[:-1])
-                atom.name = var_name + new_index
-
-    return ret_eq
-
-
-def convert_symbols_to_time_symbols(eq: sp.Eq) -> sp.Eq:
-    """
-    Convert SymPy symbols to time aware symbols.
-
-    Parameters
-    ----------
-    eq : SymPy.Eq
-        Sympy representation of a model equation.
-
-    Returns
-    -------
-    Sympy.Eq
-        The same equation with Symbols replaced with TimeAwareSymbols
-
-    Notes
-    -----
-    Despite having time indexes, SymPy symbols are not "time aware". This function replaces all sp.Symbols with
-    TimeAwareSymbols, which are extended to include a time index.
-    """
-
-    sub_dict = {}
-    var_list = [variable for variable in eq.atoms() if isinstance(variable, sp.Symbol)]
-
-    for variable in var_list:
-        var_name = variable.name
-        if re.search(r"_\{?t[-+ ]?\d*\}?$", var_name) is not None:
-            name_tokens = var_name.split("_")
-            name_part = "_".join(s for s in name_tokens[:-1])
-            time_part = name_tokens[-1]
-
-            time_part = re.sub(r"[\{\}t]", "", time_part)
-            if len(time_part) == 0:
-                time_index = 0
-            else:
-                time_index = int(time_part)
-            time_var = TimeAwareSymbol(name_part, time_index)
-            sub_dict[variable] = time_var
-        elif "_ss" in var_name:
-            base_name = var_name.replace("_ss", "")
-            time_var = TimeAwareSymbol(base_name, 0)
-            time_var = time_var.to_ss()
-            sub_dict[variable] = time_var
-
-    return eq.subs(sub_dict)
-
-
-def single_symbol_to_sympy(variable: str, assumptions: Optional[Dict] = None) -> TimeAwareSymbol:
-    """
-    Convert a single gEcon style variable (e.g. X[], or X[-1]) to a Time-Aware Sympy symbol. If it seems to be a
-    parameter (no []), it returns a standard Sympy symbol instead.
-
-    Parameters
-    ----------
-    variable : str
-        A gEcon variable or parameter.
-    assumptions : Optional[Dict]
-        Assumptions for the symbol.
-
-    Returns
-    -------
-    TimeAwareSymbol
-        The same variable.
-    """
-    if assumptions is None:
-        assumptions = defaultdict(dict)
-
-    if "[" not in variable and "]" not in variable:
-        return sp.Symbol(variable, **assumptions[variable])
-
-    variable_name, time_part = variable.split("[")
-    time_part = time_part.replace("]", "")
-    if time_part == "ss":
-        return TimeAwareSymbol(variable_name, 0).to_ss()
-    else:
-        time_index = int(time_part) if time_part != "" else 0
-        return TimeAwareSymbol(variable_name, time_index, **assumptions[variable_name])
-
-
-def build_sympy_equations(eqs: List[List[str]], assumptions: Optional[Dict] = None) -> List[sp.Eq]:
-
-    """
-    Convert processed list of equation tokens to sympy equations.
-
-    Parameters
-    ----------
-    eqs : list of lists of str
-        A list of list of equation tokens associated with a model
-    assumptions : dict, optional
-        A dictionary of assumptions for each variable or parameter. Keys are variable names, values are dictionaries
-        of assumptions.
-
-    Returns
-    -------
-    list of sp.Eq
-        A list of SymPy equations
-
-    Notes
-    -----
-    To convert SymPy Symbols to the TimeAwareSymbol class, variables are re-named with a time index.
-    Time-aware symbols are extended to include a time index.
-
-    TODO: Improve error handling before parse_expr is called
-
-    Examples
-    --------
-    .. code-block:: py
-        eqs = [['Y', '=', 'C', '+', 'I']]
-        assumptions = {'Y': {'real': True}, 'C': {'real': True}, 'I': {'real': True}}
-        build_sympy_equations(eqs, assumptions)
-    """
-
-    if assumptions is None:
-        assumptions = defaultdict(defaultdict)
-
-    eqs_processed = []
-    for eq in eqs:
-        eq_str = ""
-        calibrating_parameter = None
-        sub_dict = LOCAL_DICT.copy()
-
-        if CALIBRATING_EQ_TOKEN in eq:
-            arrow_idx = eq.index(CALIBRATING_EQ_TOKEN)
-            calibrating_parameter = eq[arrow_idx + 1]
-            eq = eq[:arrow_idx]
-
-        for token in eq:
-            token_type = token_classifier(token)
-            token = token.strip()
-            if token_type == "variable":
-                time_index = extract_time_index(token)
-                token_base = remove_timing_information(token)
-                token = token_base + "_" + time_index
-
-                symbol = TimeAwareSymbol(
-                    token_base, TIME_INDEX_DICT[time_index], **assumptions[token_base]
-                )
-                sub_dict[token] = symbol
-
-            elif token_type == "parameter":
-                symbol = sp.Symbol(token, **assumptions[token])
-                sub_dict[token] = symbol
-
-            elif token_type == "operator":
-                token = convert_to_python_operator(token)
-
-            eq_str += token
-
-        try:
-            eq_sympy = sp.parse_expr(eq_str, evaluate=False, local_dict=sub_dict)
-        except Exception as e:
-            print(f"Error encountered while parsing {eq_str}")
-            print(e)
-            raise e
-
-        eq_sympy = sp.Eq(*eq_sympy)
-        if calibrating_parameter is not None:
-            param = sp.Symbol(calibrating_parameter, **assumptions[calibrating_parameter])
-            eq_sympy = sp.Eq(param, eq_sympy.lhs - eq_sympy.rhs)
-
-        # eq_sympy = rename_time_indexes(eq_sympy)
-        # eq_sympy = convert_symbols_to_time_symbols(eq_sympy)
-
-        eqs_processed.append(eq_sympy)
-
-    return eqs_processed
+import re
+from collections import defaultdict
+from typing import Dict, List, Optional
+
+import sympy as sp
+
+from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
+from gEconpy.parser.constants import CALIBRATING_EQ_TOKEN, LOCAL_DICT, TIME_INDEX_DICT
+
+
+def rebuild_eqs_from_parser_output(parser_output: List[str]) -> List[List[str]]:
+    """
+    Takes the output of pyparsing applied to the text of a model block and returns a list of lists of tokens
+    that represent equations.
+
+    The heavy lifting of parsing the model blocks is done by the pyparsing, which returns a list of tokens found
+    between the BLOCK_START_TOKEN and BLOCK_END_TOKEN. These lists need to be further decomposed into equations,
+    which is done using the LINE_TERMINATOR_TOKEN, ';'. The result is a list of lists, with each list containing the
+    tokens for a single model equation.
+
+    The function consumes the LINE_TERMINATOR_TOKENs in the process, and the returned lists are assumed to contain only
+    variables, parameters, and valid mathematical operators.
+
+    Parameters
+    ----------
+    parser_output: list of str
+        Output of pyparsing applied to the text of a model block.
+
+    Returns
+    -------
+    eqs: list of lists of str
+        A list of lists, with each list containing the tokens for a single model equation.
+
+    Examples
+    --------
+    ..code-block:: text
+        rebuild_eqs_from_parser_output([A[], +, B[], +, C[], ;, Y[], =, L[], ^, alpha, *, K[-1], ^, (, 1, -, alpha, ), ;]) [[A[], +, B[], +, C[]],
+         [Y[], =, L[], ^, alpha, *, K[-1], ^, (, 1, -, alpha, )]
+    """
+
+    eqs = []
+    eq = []
+    for element in parser_output:
+        # TODO: Why are commas appearing in the control list after parsing? This is a hack fix.
+        element = element.replace(",", "").strip()
+        if len(element) > 0:
+            eq.append(element)
+        if ";" in element:
+            # ; signals end of line, needs to be removed now
+            eq[-1] = eq[-1].replace(";", "").strip()
+            if len(eq[-1]) == 0:
+                del eq[-1]
+
+            eqs.append(eq)
+            eq = []
+    return eqs
+
+
+def token_classifier(token: str) -> str:
+    """
+    Classify tokens as variables, parameters, operators, or special.
+
+    Parameters
+    ----------
+    token: str
+        Token from a gEcon model equation
+
+    Returns
+    -------
+    classification: str
+        Classification of the model token. One of ['operator', 'variable', 'number', 'lagrange_definition', 'calibration_definition', 'parameter']
+
+    Notes
+    -----
+    There are two special tokens:
+        1. The colon ":", that signifies that the variable after it should be the name of the lagrange multiplier
+         associated with the equation.
+        2. The arrow "->", that defines an calibrating equation for a parameter.
+    """
+
+    if token in ["E[]", "log", "exp"] or token in "+-*/^=()[]":
+        return "operator"
+    if "[" in token and "]" in token:
+        return "variable"
+    if all(s in "0123456789." for s in token):
+        return "number"
+    if token == ":":
+        return "lagrange_definition"
+    if token == "->":
+        return "calibration_definition"
+
+    return "parameter"
+
+
+def has_num_index(token: str) -> bool:
+    """
+    Check if a token has a numerical lag/lead index.
+
+    Lag and lead indices are of the form X[1] or X[-1], this function checks if such an index exists. Split on the
+    opening square bracket first to be robust against variables with numbers in their names, i.e. alpha_1[1].
+
+    Parameters
+    ----------
+    token : str
+        A plaintext model variable.
+
+    Returns
+    -------
+    bool
+        Whether the variable has a numerical lag/lead index.
+    """
+
+    numbers = list("0123456789")
+    index_part = token.split("[")[-1]
+    return any([n in index_part for n in numbers])
+
+
+def extract_time_index(token: str) -> str:
+    """
+    Extract a time index from a model variable.
+
+    Parameters
+    ----------
+    token : str
+        A string representing a model variable.
+
+    Returns
+    -------
+    time_index : str
+        A time-index string.
+
+    Examples
+    --------
+    >>> extract_time_index("alpha[1]")
+    >>> # Out: 't1'
+    >>> extract_time_index("alpha[-1]")
+    >>> # Out: 'tL1'
+    >>> extract_time_index("alpha[ss]")
+    >>> # Out: 'ss'
+    >>> extract_time_index("alpha")
+    >>> # Out: 't'
+    """
+
+    if has_num_index(token) and "-" not in token:
+        lead = re.findall(r"\d+", token)[0]
+        time_index = "t" + lead
+    elif has_num_index(token) and "-" in token:
+        lag = re.findall(r"\d+", token)[0]
+        time_index = "tL" + lag
+    elif "[ss]" in token:
+        time_index = "ss"
+    else:
+        time_index = "t"
+
+    return time_index
+
+
+def remove_timing_information(token: str) -> str:
+    """
+    Remove timing information from a model variable token.
+
+    A variable's timing information is contained in the square brackets next to it. This needs to be removed and
+    replaced with some plaintext before it can be passed to the SymPy parser.
+
+    Parameters
+    ----------
+    token : str
+        A string representing a model variable.
+
+    Returns
+    -------
+    str
+        The same token with the timing information removed.
+    """
+    token = re.sub(r"\[.*?\]", "", token)
+    return token.strip()
+
+
+def convert_to_python_operator(token: str) -> str:
+    """
+    Convert a GCN operation token to a python-compatible one.
+
+    Parameters
+    ----------
+    token : str
+        A string representing a mathematical operation.
+
+    Returns
+    ---------
+    str
+        A string representing the same operation in python syntax.
+
+    Notes
+    ----------
+    The syntax of a gEcon GCN file is slightly different from what SymPy expects, this function resolves the
+    differences. In particular:
+        1. Exponents are marked with a caret "^" in the GCN file, and must be converted to python's **
+        2. SymPy's parse_expr cannot handle equalities, but can handle a list of equations. Equalities are thus
+           converted to two separate equations then set as equal later.
+        3. Remove the expectation operator completely
+        4. Replace square brackets with parenthesis
+
+    TODO: Implement an expectation operation in Sympy that can inserted here.
+    """
+
+    if token == "^":
+        return "**"
+    if token == "=":
+        return ","
+    if token == "E[]":
+        return ""
+    if token == "[":
+        return "("
+    if token == "]":
+        return ")"
+
+    return token
+
+
+def rename_time_indexes(eq: sp.Eq) -> sp.Eq:
+    """
+    Rename time indices of the form "tL1" or "t1" to the normal form "t", "t+1", or "t-1".
+
+    Parameters
+    ----------
+    eq : sp.Eq
+        A sympy equation representing a model equation.
+
+    Returns
+    -------
+    sp.Eq
+        The same equation, with time indices renamed.
+
+    Notes
+    -----
+    The function assumes the index is always at the end of the variable name.
+    """
+
+    ret_eq = eq.copy()
+    for atom in ret_eq.atoms():
+        if isinstance(atom, sp.core.Symbol):
+            if re.search(r"tL?\d+", atom.name) is not None:
+                name_tokens = atom.name.split("_")
+                index_token = name_tokens[-1]
+                operator = "-" if "L" in index_token else "+"
+                number = re.search(r"\d+", index_token)[0]
+                new_index = "".join(["_{t", operator, number, "}"])
+
+                var_name = "_".join(s for s in name_tokens[:-1])
+                atom.name = var_name + new_index
+
+    return ret_eq
+
+
+def convert_symbols_to_time_symbols(eq: sp.Eq) -> sp.Eq:
+    """
+    Convert SymPy symbols to time aware symbols.
+
+    Parameters
+    ----------
+    eq : SymPy.Eq
+        Sympy representation of a model equation.
+
+    Returns
+    -------
+    Sympy.Eq
+        The same equation with Symbols replaced with TimeAwareSymbols
+
+    Notes
+    -----
+    Despite having time indexes, SymPy symbols are not "time aware". This function replaces all sp.Symbols with
+    TimeAwareSymbols, which are extended to include a time index.
+    """
+
+    sub_dict = {}
+    var_list = [variable for variable in eq.atoms() if isinstance(variable, sp.Symbol)]
+
+    for variable in var_list:
+        var_name = variable.name
+        if re.search(r"_\{?t[-+ ]?\d*\}?$", var_name) is not None:
+            name_tokens = var_name.split("_")
+            name_part = "_".join(s for s in name_tokens[:-1])
+            time_part = name_tokens[-1]
+
+            time_part = re.sub(r"[\{\}t]", "", time_part)
+            if len(time_part) == 0:
+                time_index = 0
+            else:
+                time_index = int(time_part)
+            time_var = TimeAwareSymbol(name_part, time_index)
+            sub_dict[variable] = time_var
+        elif "_ss" in var_name:
+            base_name = var_name.replace("_ss", "")
+            time_var = TimeAwareSymbol(base_name, 0)
+            time_var = time_var.to_ss()
+            sub_dict[variable] = time_var
+
+    return eq.subs(sub_dict)
+
+
+def single_symbol_to_sympy(variable: str, assumptions: Optional[Dict] = None) -> TimeAwareSymbol:
+    """
+    Convert a single gEcon style variable (e.g. X[], or X[-1]) to a Time-Aware Sympy symbol. If it seems to be a
+    parameter (no []), it returns a standard Sympy symbol instead.
+
+    Parameters
+    ----------
+    variable : str
+        A gEcon variable or parameter.
+    assumptions : Optional[Dict]
+        Assumptions for the symbol.
+
+    Returns
+    -------
+    TimeAwareSymbol
+        The same variable.
+    """
+    if assumptions is None:
+        assumptions = defaultdict(dict)
+
+    if "[" not in variable and "]" not in variable:
+        return sp.Symbol(variable, **assumptions[variable])
+
+    variable_name, time_part = variable.split("[")
+    time_part = time_part.replace("]", "")
+    if time_part == "ss":
+        return TimeAwareSymbol(variable_name, 0).to_ss()
+    else:
+        time_index = int(time_part) if time_part != "" else 0
+        return TimeAwareSymbol(variable_name, time_index, **assumptions[variable_name])
+
+
+def build_sympy_equations(eqs: List[List[str]], assumptions: Optional[Dict] = None) -> List[sp.Eq]:
+
+    """
+    Convert processed list of equation tokens to sympy equations.
+
+    Parameters
+    ----------
+    eqs : list of lists of str
+        A list of list of equation tokens associated with a model
+    assumptions : dict, optional
+        A dictionary of assumptions for each variable or parameter. Keys are variable names, values are dictionaries
+        of assumptions.
+
+    Returns
+    -------
+    list of tuple, (sp.Eq, bool)
+        A list of SymPy equations, along with a flag indicting whether the equation is a calibrating equation
+
+    Notes
+    -----
+    To convert SymPy Symbols to the TimeAwareSymbol class, variables are re-named with a time index.
+    Time-aware symbols are extended to include a time index.
+
+    TODO: Improve error handling before parse_expr is called
+
+    Examples
+    --------
+    .. code-block:: py
+        eqs = [['Y', '=', 'C', '+', 'I']]
+        assumptions = {'Y': {'real': True}, 'C': {'real': True}, 'I': {'real': True}}
+        build_sympy_equations(eqs, assumptions)
+    """
+
+    if assumptions is None:
+        assumptions = defaultdict(defaultdict)
+
+    eqs_processed = []
+    for eq in eqs:
+        eq_str = ""
+        calibrating_parameter = None
+        sub_dict = LOCAL_DICT.copy()
+        flags = {"is_calibrating": False}
+
+        if CALIBRATING_EQ_TOKEN in eq:
+            arrow_idx = eq.index(CALIBRATING_EQ_TOKEN)
+            calibrating_parameter = eq[arrow_idx + 1]
+            eq = eq[:arrow_idx]
+
+        for token in eq:
+            token_type = token_classifier(token)
+            token = token.strip()
+            if token_type == "variable":
+                time_index = extract_time_index(token)
+                token_base = remove_timing_information(token)
+                token = token_base + "_" + time_index
+
+                symbol = TimeAwareSymbol(
+                    token_base, TIME_INDEX_DICT[time_index], **assumptions[token_base]
+                )
+                sub_dict[token] = symbol
+
+            elif token_type == "parameter":
+                symbol = sp.Symbol(token, **assumptions[token])
+                sub_dict[token] = symbol
+
+            elif token_type == "operator":
+                token = convert_to_python_operator(token)
+
+            eq_str += token
+
+        try:
+            eq_sympy = sp.parse_expr(eq_str, evaluate=False, local_dict=sub_dict)
+        except Exception as e:
+            print(f"Error encountered while parsing {eq_str}")
+            print(e)
+            raise e
+
+        eq_sympy = sp.Eq(*eq_sympy)
+        flags["is_calibrating"] = calibrating_parameter is not None
+        if flags["is_calibrating"]:
+            param = sp.Symbol(calibrating_parameter, **assumptions[calibrating_parameter])
+            eq_sympy = sp.Eq(param, eq_sympy.lhs - eq_sympy.rhs)
+
+        # eq_sympy = rename_time_indexes(eq_sympy)
+        # eq_sympy = convert_symbols_to_time_symbols(eq_sympy)
+
+        eqs_processed.append((eq_sympy, flags))
+
+    return eqs_processed
```

### Comparing `gEconpy-1.1.0/gEconpy/parser/parse_plaintext.py` & `gEconpy-1.2.0/gEconpy/parser/parse_plaintext.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,286 +1,286 @@
-import re
-from typing import Dict, Tuple
-
-from gEconpy.exceptions.exceptions import (
-    DistributionParsingError,
-    MissingParameterValueException,
-)
-from gEconpy.parser.constants import (
-    BLOCK_END_TOKEN,
-    CALIBRATING_EQ_TOKEN,
-    EXPECTATION_TOKEN,
-    LAG_TOKEN,
-    LEAD_TOKEN,
-    OPERATORS,
-    SS_TOKEN,
-)
-
-
-def remove_extra_spaces(text: str) -> str:
-    """
-    Remove multiple spaces and return the resulting string.
-
-    Parameters
-    ----------
-    text : str
-        A string to remove extra spaces from.
-
-    Returns
-    -------
-    str
-        A string with multiple spaces replaced by single spaces.
-
-    Notes
-    -----
-    This function is used to replace multiple spaces ('   ') with single spaces (' ') that result from removing special
-    characters during preprocessing (tabs, newlines, etc).
-    """
-
-    out_text = re.sub(" +", " ", text)
-    return out_text.strip()
-
-
-def remove_newlines_and_tabs(text: str) -> str:
-    """
-    Remove newline and tab characters from a string.
-
-    Parameters
-    ----------
-    text : str
-        A string to remove newline and tab characters from.
-
-    Returns
-    -------
-    str
-        A string with newline and tab characters removed.
-    """
-
-    out_text = text.replace("\n", " ")
-    out_text = out_text.replace("\t", " ")
-    out_text = remove_extra_spaces(out_text)
-
-    return out_text
-
-
-def remove_comments(text: str) -> str:
-    """
-    Remove comments from a string.
-
-    Parameters
-    ----------
-    text : str
-        A string representing a GCN model file with comments to remove.
-
-    Returns
-    -------
-    str
-        The input string with comments removed.
-
-    Notes
-    -----
-    The GCN model language allows for comments using the # prefix, either on their own line or following an equation
-    in-line. This function strips these comments out.
-    """
-
-    lines = text.split("\n")
-    lines = [line.strip() for line in lines if len(line.strip()) > 0]
-    output = []
-
-    for line in lines:
-        if line[0] != "#":
-            if "#" in line:
-                hash_idx = line.find("#")
-                output.append(line[:hash_idx])
-            else:
-                output.append(line)
-
-    return "\n".join(output)
-
-
-def extract_distributions(text: str) -> Tuple[str, Dict[str, str]]:
-    """
-    Extract prior distributions from a GCN model file and return the "clean" model file and a dictionary of the form
-    parameter:distribution.
-
-    Parameters
-    ----------
-    text : str
-        Raw model file return by function `load_gcn`.
-
-    Returns
-    -------
-    str
-        Model file with prior distribution information removed.
-    Dict[str, str]
-        Dictionary of the form parameter:distribution.
-
-    Examples
-    --------
-    .. code-block:: py
-         extract_distributions('alpha ~ Beta(mean=0.5, sd=0.1) = 0.55;')
-        # ('alpha = 0.55;', {"alpha": "Beta(mean=0.5, sd=0.1)"})
-    """
-
-    lines = text.split("\n")
-    output = []
-    prior_dict = {}
-
-    for line in lines:
-        if "~" in line:
-            param_name, other = line.split("~")
-
-            # This is a shock definition, there won't be an "=" after the distribution
-            if "[]" in param_name:
-
-                dist_info = other.strip().replace(";", "")
-                new_line = param_name.strip() + ";"
-
-            # This is a parameter definition, but it might be missing a default value
-            else:
-                # Extract the distribution declaration
-                *dist_info, param_value = other.split("=")
-                dist_info = "=".join(dist_info)
-
-                # This should only happen in the user didn't give a default value
-                if ")" in param_value:
-                    raise MissingParameterValueException(param_name)
-
-                new_line = f"{param_name.strip()} = {param_value.strip()}"
-            output.append(new_line)
-            prior_dict[param_name.strip()] = dist_info.strip()
-        else:
-            if line.count("=") > 1:
-                raise DistributionParsingError(line)
-
-            output.append(line)
-
-    output = "\n".join(output).strip()
-
-    return output, prior_dict
-
-
-def add_spaces_around_expectations(text: str) -> str:
-    """
-    Insert spaces around expectation tokens and the square brackets that define what is in the expectation.
-
-    Parameters
-    ----------
-    text : str
-        A raw model file as plaintext.
-
-    Returns
-    -------
-    str
-        A raw model as plaintext.
-
-    Examples
-    --------
-    .. code-block:: python
-        add_spaces_around_expectations("E[][u[] + beta * U[1]];")
-        # Output: "E[] [ u[] + beta * U[1] ];"
-
-    """
-
-    # Only add white space to the left of the expectation token so we can look for [[ when splitting the square brackets
-    out_text = re.sub(f"(\\b{re.escape(EXPECTATION_TOKEN)})", r" \g<0>", text)
-    out_text = re.sub(r"(?<=\])[\[\]]|(?<!(\[|\]))\]", r" \g<0> ", out_text)
-
-    return out_text
-
-
-def repair_special_tokens(text: str) -> str:
-    """
-    Repair the lag, lead, ss, and calibrating_eq tokens needed to mark variables in a later processing step.
-
-    Parameters
-    ----------
-    text : str
-        A raw model file as plaintext.
-
-    Returns
-    -------
-    str
-        A raw model file as plaintext.
-
-    Examples
-    --------
-    .. code-block:: python
-        repair_special_tokens(" u[ -1 ]")
-        # Output: "u[-1]"
-    """
-
-    out_text = re.sub(r"\[ *\- *1 *\]", LAG_TOKEN, text)
-    out_text = re.sub(r"\[ *1 *\]", LEAD_TOKEN, out_text)
-    out_text = re.sub(r"\[ *ss * \]", SS_TOKEN, out_text)
-    out_text = re.sub(r" * - * > *", f" {CALIBRATING_EQ_TOKEN} ", out_text)
-    out_text = re.sub("} ;", BLOCK_END_TOKEN, out_text)
-
-    return out_text
-
-
-def add_spaces_around_operators(text: str) -> str:
-    """
-    Insert spaces around math operators.
-
-    Parameters
-    ----------
-    text : str
-        Raw text model file, including special model syntax and mathematical equations.
-
-    Returns
-    -------
-    text : str
-        Same text, with spaces added around math operators.
-
-    Notes
-    -----
-    To convert the model into a series of tokens that can be processed, space is added between math operators,
-    defined in the OPERATORS global as '+-*/^=();:'. Mathematical "sentences" should then be of the form
-    {Y[] = a + X[] ; };, which can be parsed in a later step.
-
-    Several errors are introduced by simply adding spaces around operators: lagged variables tokens, written as X[-1],
-    are mutilated to X[ - 1], lead tokens are mutilated to X[  1  ], steady_state tokens become [ ss],
-    the calibrating equation assignment operator "->" becomes " - >", and the "end of block" token, "};"
-    is mutilated to "} ;". These errors are corrected by the repair_special_tokens function.
-    """
-
-    out_text = re.sub(f"[{OPERATORS}]", r" \g<0> ", text)
-    out_text = add_spaces_around_expectations(out_text)
-    out_text = remove_extra_spaces(out_text)
-    out_text = repair_special_tokens(out_text)
-
-    return out_text
-
-
-def delete_block(text: str, block_name: str) -> str:
-    """
-    Delete a block from a model text file.
-
-    Parameters
-    ----------
-    text: str
-        Raw model file as text.
-    block_name: str
-        Block name to delete.
-
-    Returns
-    -------
-    str
-        Model file without the selected block.
-
-    Special blocks "options" and "tryreduce" follow a special format. These blocks are pre-processed separately from
-    the rest of the model blocks. This is a helper function to delete these blocks from the raw text after they have
-    been processed, making assumptions about structure of the remaining blocks uniform.
-
-    Examples
-    --------
-    >>> delete_block("options { }; tryreduce { };", "tryreduce")
-    'options { };'
-    """
-
-    if block_name not in text:
-        return text
-    elif block_name == "assumptions":
-        return re.sub(block_name + " {.*?}; };", "", text).strip()
-    else:
-        return re.sub(block_name + " {.*?};", "", text).strip()
+import re
+from typing import Dict, Tuple
+
+from gEconpy.exceptions.exceptions import (
+    DistributionParsingError,
+    MissingParameterValueException,
+)
+from gEconpy.parser.constants import (
+    BLOCK_END_TOKEN,
+    CALIBRATING_EQ_TOKEN,
+    EXPECTATION_TOKEN,
+    LAG_TOKEN,
+    LEAD_TOKEN,
+    OPERATORS,
+    SS_TOKEN,
+)
+
+
+def remove_extra_spaces(text: str) -> str:
+    """
+    Remove multiple spaces and return the resulting string.
+
+    Parameters
+    ----------
+    text : str
+        A string to remove extra spaces from.
+
+    Returns
+    -------
+    str
+        A string with multiple spaces replaced by single spaces.
+
+    Notes
+    -----
+    This function is used to replace multiple spaces ('   ') with single spaces (' ') that result from removing special
+    characters during preprocessing (tabs, newlines, etc).
+    """
+
+    out_text = re.sub(" +", " ", text)
+    return out_text.strip()
+
+
+def remove_newlines_and_tabs(text: str) -> str:
+    """
+    Remove newline and tab characters from a string.
+
+    Parameters
+    ----------
+    text : str
+        A string to remove newline and tab characters from.
+
+    Returns
+    -------
+    str
+        A string with newline and tab characters removed.
+    """
+
+    out_text = text.replace("\n", " ")
+    out_text = out_text.replace("\t", " ")
+    out_text = remove_extra_spaces(out_text)
+
+    return out_text
+
+
+def remove_comments(text: str) -> str:
+    """
+    Remove comments from a string.
+
+    Parameters
+    ----------
+    text : str
+        A string representing a GCN model file with comments to remove.
+
+    Returns
+    -------
+    str
+        The input string with comments removed.
+
+    Notes
+    -----
+    The GCN model language allows for comments using the # prefix, either on their own line or following an equation
+    in-line. This function strips these comments out.
+    """
+
+    lines = text.split("\n")
+    lines = [line.strip() for line in lines if len(line.strip()) > 0]
+    output = []
+
+    for line in lines:
+        if line[0] != "#":
+            if "#" in line:
+                hash_idx = line.find("#")
+                output.append(line[:hash_idx])
+            else:
+                output.append(line)
+
+    return "\n".join(output)
+
+
+def extract_distributions(text: str) -> Tuple[str, Dict[str, str]]:
+    """
+    Extract prior distributions from a GCN model file and return the "clean" model file and a dictionary of the form
+    parameter:distribution.
+
+    Parameters
+    ----------
+    text : str
+        Raw model file return by function `load_gcn`.
+
+    Returns
+    -------
+    str
+        Model file with prior distribution information removed.
+    Dict[str, str]
+        Dictionary of the form parameter:distribution.
+
+    Examples
+    --------
+    .. code-block:: py
+         extract_distributions('alpha ~ Beta(mean=0.5, sd=0.1) = 0.55;')
+        # ('alpha = 0.55;', {"alpha": "Beta(mean=0.5, sd=0.1)"})
+    """
+
+    lines = text.split("\n")
+    output = []
+    prior_dict = {}
+
+    for line in lines:
+        if "~" in line:
+            param_name, other = line.split("~")
+
+            # This is a shock definition, there won't be an "=" after the distribution
+            if "[]" in param_name:
+
+                dist_info = other.strip().replace(";", "")
+                new_line = param_name.strip() + ";"
+
+            # This is a parameter definition, but it might be missing a default value
+            else:
+                # Extract the distribution declaration
+                *dist_info, param_value = other.split("=")
+                dist_info = "=".join(dist_info)
+
+                # This should only happen in the user didn't give a default value
+                if ")" in param_value:
+                    raise MissingParameterValueException(param_name)
+
+                new_line = f"{param_name.strip()} = {param_value.strip()}"
+            output.append(new_line)
+            prior_dict[param_name.strip()] = dist_info.strip()
+        else:
+            if line.count("=") > 1:
+                raise DistributionParsingError(line)
+
+            output.append(line)
+
+    output = "\n".join(output).strip()
+
+    return output, prior_dict
+
+
+def add_spaces_around_expectations(text: str) -> str:
+    """
+    Insert spaces around expectation tokens and the square brackets that define what is in the expectation.
+
+    Parameters
+    ----------
+    text : str
+        A raw model file as plaintext.
+
+    Returns
+    -------
+    str
+        A raw model as plaintext.
+
+    Examples
+    --------
+    .. code-block:: python
+        add_spaces_around_expectations("E[][u[] + beta * U[1]];")
+        # Output: "E[] [ u[] + beta * U[1] ];"
+
+    """
+
+    # Only add white space to the left of the expectation token so we can look for [[ when splitting the square brackets
+    out_text = re.sub(f"(\\b{re.escape(EXPECTATION_TOKEN)})", r" \g<0>", text)
+    out_text = re.sub(r"(?<=\])[\[\]]|(?<!(\[|\]))\]", r" \g<0> ", out_text)
+
+    return out_text
+
+
+def repair_special_tokens(text: str) -> str:
+    """
+    Repair the lag, lead, ss, and calibrating_eq tokens needed to mark variables in a later processing step.
+
+    Parameters
+    ----------
+    text : str
+        A raw model file as plaintext.
+
+    Returns
+    -------
+    str
+        A raw model file as plaintext.
+
+    Examples
+    --------
+    .. code-block:: python
+        repair_special_tokens(" u[ -1 ]")
+        # Output: "u[-1]"
+    """
+
+    out_text = re.sub(r"\[ *\- *1 *\]", LAG_TOKEN, text)
+    out_text = re.sub(r"\[ *1 *\]", LEAD_TOKEN, out_text)
+    out_text = re.sub(r"\[ *ss * \]", SS_TOKEN, out_text)
+    out_text = re.sub(r" * - * > *", f" {CALIBRATING_EQ_TOKEN} ", out_text)
+    out_text = re.sub("} ;", BLOCK_END_TOKEN, out_text)
+
+    return out_text
+
+
+def add_spaces_around_operators(text: str) -> str:
+    """
+    Insert spaces around math operators.
+
+    Parameters
+    ----------
+    text : str
+        Raw text model file, including special model syntax and mathematical equations.
+
+    Returns
+    -------
+    text : str
+        Same text, with spaces added around math operators.
+
+    Notes
+    -----
+    To convert the model into a series of tokens that can be processed, space is added between math operators,
+    defined in the OPERATORS global as '+-*/^=();:'. Mathematical "sentences" should then be of the form
+    {Y[] = a + X[] ; };, which can be parsed in a later step.
+
+    Several errors are introduced by simply adding spaces around operators: lagged variables tokens, written as X[-1],
+    are mutilated to X[ - 1], lead tokens are mutilated to X[  1  ], steady_state tokens become [ ss],
+    the calibrating equation assignment operator "->" becomes " - >", and the "end of block" token, "};"
+    is mutilated to "} ;". These errors are corrected by the repair_special_tokens function.
+    """
+
+    out_text = re.sub(f"[{OPERATORS}]", r" \g<0> ", text)
+    out_text = add_spaces_around_expectations(out_text)
+    out_text = remove_extra_spaces(out_text)
+    out_text = repair_special_tokens(out_text)
+
+    return out_text
+
+
+def delete_block(text: str, block_name: str) -> str:
+    """
+    Delete a block from a model text file.
+
+    Parameters
+    ----------
+    text: str
+        Raw model file as text.
+    block_name: str
+        Block name to delete.
+
+    Returns
+    -------
+    str
+        Model file without the selected block.
+
+    Special blocks "options" and "tryreduce" follow a special format. These blocks are pre-processed separately from
+    the rest of the model blocks. This is a helper function to delete these blocks from the raw text after they have
+    been processed, making assumptions about structure of the remaining blocks uniform.
+
+    Examples
+    --------
+    >>> delete_block("options { }; tryreduce { };", "tryreduce")
+    'options { };'
+    """
+
+    if block_name not in text:
+        return text
+    elif block_name == "assumptions":
+        return re.sub(block_name + " {.*?}; };", "", text).strip()
+    else:
+        return re.sub(block_name + " {.*?};", "", text).strip()
```

### Comparing `gEconpy-1.1.0/gEconpy/parser/validation.py` & `gEconpy-1.2.0/gEconpy/parser/validation.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,139 +1,139 @@
-from typing import List, Tuple, Union
-
-from gEconpy.exceptions.exceptions import InvalidComponentNameException
-from gEconpy.parser.constants import BLOCK_COMPONENTS
-
-
-def block_is_empty(block: str) -> bool:
-    """
-    Check whether a model block is empty, i.e. contains no flags, variables, or equations.
-
-    Parameters
-    ----------
-    block : str
-        Raw text of a model block.
-
-    Returns
-    -------
-    bool
-        Whether the block is empty or not.
-    """
-
-    return block.strip() == "{ };"
-
-
-def validate_key(key: str, block_name: str) -> None:
-    """
-    Check that the component name matches something in the list of valid block components.
-
-    The R implementation of gEcon only allows the names in BLOCK_COMPONENTS to be used inside model blocks.
-    This function checks that a component name matches something in that list, and raises an error if not.
-
-    Parameters
-    ----------
-    block_name : str
-        The name of the block.
-    key : str
-        A block sub-component name.
-
-    Returns
-    -------
-    None
-
-    Raises
-    ------
-    InvalidComponentNameException
-        If the component name is invalid.
-
-    # TODO: Allow arbitrary component names? Is there any need to?
-    """
-    if key.upper() not in BLOCK_COMPONENTS:
-        valid_names = ", ".join(BLOCK_COMPONENTS)
-        error = f"Valid sub-block names are: {valid_names}\n"
-        error += f"Found: {key} in block {block_name}"
-
-        raise InvalidComponentNameException(
-            component_name=key, block_name=block_name, message=error
-        )
-
-
-def jaccard_distance(s: str, d: str) -> float:
-    """
-    Calculate the Jaccard distance between two strings.
-
-    The Jaccard distance is defined as the size of the intersection of two sets divided by the size of their union.
-    For example, the Jaccard distance between the sets {"C", "A", "T"} and {"C", "U", "T"} is 1/2 because the
-    intersection of these two sets is {"C", "T"} (of size 2) and the union is {"C", "A", "T", "U"} (of size 4).
-    Therefore, the Jaccard distance is 2/4 = 1/2.
-
-    Parameters
-    ----------
-    s : str
-        The first string.
-    d : str
-        The second string.
-
-    Returns
-    -------
-    float
-        The Jaccard distance between the two strings.
-    """
-
-    s = set(s)
-    d = set(d)
-    union = len(s.union(d))
-    intersection = len(s.intersection(d))
-
-    return intersection / union
-
-
-def elementwise_jaccard_distance(s: str, l: List[str]) -> List[float]:
-    """
-    Calculate the Jaccard distance between a string and each element in a list of strings.
-
-    Parameters
-    ----------
-    s : str
-        The string to compare against.
-    l : list of str
-        The list of strings to compare to `s`.
-
-    Returns
-    -------
-    list of float
-        A list of the Jaccard distances between `s` and each element in `l`.
-    """
-    return [jaccard_distance(s, element) for element in l]
-
-
-def find_typos_and_guesses(
-    user_inputs: List[str], valid_inputs: List[str], match_threshold: float = 0.8
-) -> Tuple[Union[str, None], Union[str, None]]:
-    """
-    Find the best matching suggestion from a list of valid inputs for a list of invalid user inputs.
-
-    Parameters
-    ----------
-    user_inputs : list of str
-        The list of invalid user inputs.
-    valid_inputs : list of str
-        The list of valid inputs.
-    match_threshold : float, optional
-        The minimum Jaccard distance required to consider a user input a typo. Default is 0.8.
-
-    Returns
-    -------
-    tuple of (str or None, str or None)
-        A tuple containing the best matching valid input and the user input that may be a typo, if they are above the
-        match threshold. If no user input is above the threshold, both elements of the tuple will be None.
-
-    TODO: Tune match_threshold
-    """
-
-    best_guess = max(valid_inputs, key=lambda x: elementwise_jaccard_distance(x, user_inputs))
-    maybe_typo = max(user_inputs, key=lambda x: elementwise_jaccard_distance(x, valid_inputs))
-
-    if jaccard_distance(best_guess, maybe_typo) < match_threshold:
-        return None, None
-
-    return best_guess, maybe_typo
+from typing import List, Tuple, Union
+
+from gEconpy.exceptions.exceptions import InvalidComponentNameException
+from gEconpy.parser.constants import BLOCK_COMPONENTS
+
+
+def block_is_empty(block: str) -> bool:
+    """
+    Check whether a model block is empty, i.e. contains no flags, variables, or equations.
+
+    Parameters
+    ----------
+    block : str
+        Raw text of a model block.
+
+    Returns
+    -------
+    bool
+        Whether the block is empty or not.
+    """
+
+    return block.strip() == "{ };"
+
+
+def validate_key(key: str, block_name: str) -> None:
+    """
+    Check that the component name matches something in the list of valid block components.
+
+    The R implementation of gEcon only allows the names in BLOCK_COMPONENTS to be used inside model blocks.
+    This function checks that a component name matches something in that list, and raises an error if not.
+
+    Parameters
+    ----------
+    block_name : str
+        The name of the block.
+    key : str
+        A block sub-component name.
+
+    Returns
+    -------
+    None
+
+    Raises
+    ------
+    InvalidComponentNameException
+        If the component name is invalid.
+
+    # TODO: Allow arbitrary component names? Is there any need to?
+    """
+    if key.upper() not in BLOCK_COMPONENTS:
+        valid_names = ", ".join(BLOCK_COMPONENTS)
+        error = f"Valid sub-block names are: {valid_names}\n"
+        error += f"Found: {key} in block {block_name}"
+
+        raise InvalidComponentNameException(
+            component_name=key, block_name=block_name, message=error
+        )
+
+
+def jaccard_distance(s: str, d: str) -> float:
+    """
+    Calculate the Jaccard distance between two strings.
+
+    The Jaccard distance is defined as the size of the intersection of two sets divided by the size of their union.
+    For example, the Jaccard distance between the sets {"C", "A", "T"} and {"C", "U", "T"} is 1/2 because the
+    intersection of these two sets is {"C", "T"} (of size 2) and the union is {"C", "A", "T", "U"} (of size 4).
+    Therefore, the Jaccard distance is 2/4 = 1/2.
+
+    Parameters
+    ----------
+    s : str
+        The first string.
+    d : str
+        The second string.
+
+    Returns
+    -------
+    float
+        The Jaccard distance between the two strings.
+    """
+
+    s = set(s)
+    d = set(d)
+    union = len(s.union(d))
+    intersection = len(s.intersection(d))
+
+    return intersection / union
+
+
+def elementwise_jaccard_distance(s: str, l: List[str]) -> List[float]:
+    """
+    Calculate the Jaccard distance between a string and each element in a list of strings.
+
+    Parameters
+    ----------
+    s : str
+        The string to compare against.
+    l : list of str
+        The list of strings to compare to `s`.
+
+    Returns
+    -------
+    list of float
+        A list of the Jaccard distances between `s` and each element in `l`.
+    """
+    return [jaccard_distance(s, element) for element in l]
+
+
+def find_typos_and_guesses(
+    user_inputs: List[str], valid_inputs: List[str], match_threshold: float = 0.8
+) -> Tuple[Union[str, None], Union[str, None]]:
+    """
+    Find the best matching suggestion from a list of valid inputs for a list of invalid user inputs.
+
+    Parameters
+    ----------
+    user_inputs : list of str
+        The list of invalid user inputs.
+    valid_inputs : list of str
+        The list of valid inputs.
+    match_threshold : float, optional
+        The minimum Jaccard distance required to consider a user input a typo. Default is 0.8.
+
+    Returns
+    -------
+    tuple of (str or None, str or None)
+        A tuple containing the best matching valid input and the user input that may be a typo, if they are above the
+        match threshold. If no user input is above the threshold, both elements of the tuple will be None.
+
+    TODO: Tune match_threshold
+    """
+
+    best_guess = max(valid_inputs, key=lambda x: elementwise_jaccard_distance(x, user_inputs))
+    maybe_typo = max(user_inputs, key=lambda x: elementwise_jaccard_distance(x, valid_inputs))
+
+    if jaccard_distance(best_guess, maybe_typo) < match_threshold:
+        return None, None
+
+    return best_guess, maybe_typo
```

### Comparing `gEconpy-1.1.0/gEconpy/plotting/plotting.py` & `gEconpy-1.2.0/gEconpy/plotting/plotting.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,919 +1,919 @@
-from itertools import combinations_with_replacement
-from typing import Any, Dict, List, Optional, Tuple, Union
-
-import matplotlib
-import matplotlib.pyplot as plt
-import numpy as np
-import pandas as pd
-import xarray as xr
-from matplotlib.colors import Colormap
-from matplotlib.figure import Figure
-from matplotlib.gridspec import GridSpec
-from matplotlib.ticker import ScalarFormatter
-from scipy import stats
-
-
-class ScalarFormatterForceFormat(ScalarFormatter):
-    """
-    A ScalarFormatter that forces a specific format for the tick labels.
-    """
-
-    def _set_format(self, vmin, vmax):
-        """
-        Set the format for the tick labels.
-
-        Parameters
-        ----------
-        vmin : float
-            The minimum value of the data.
-        vmax : float
-            The maximum value of the data.
-        """
-        self.format = "%1.1f"
-
-
-def prepare_gridspec_figure(n_cols: int, n_plots: int) -> Tuple[GridSpec, List]:
-    """
-     Prepare a figure with a grid of subplots. Centers the last row of plots if the number of plots is not square.
-
-     Parameters
-     ----------
-     n_cols : int
-         The number of columns in the grid.
-     n_plots : int
-         The number of subplots in the grid.
-
-     Returns
-     -------
-     GridSpec
-         A matplotlib GridSpec object representing the layout of the grid.
-    list of tuple(slice, slice)
-         A list of tuples of slices representing the indices of the grid cells to be used for each subplot.
-    """
-
-    remainder = n_plots % n_cols
-    has_remainder = remainder > 0
-    n_rows = n_plots // n_cols + 1
-
-    gs = GridSpec(2 * n_rows, 2 * n_cols)
-    plot_locs = []
-
-    for i in range(n_rows - int(has_remainder)):
-        for j in range(n_cols):
-            plot_locs.append((slice(i * 2, (i + 1) * 2), slice(j * 2, (j + 1) * 2)))
-
-    if has_remainder:
-        last_row = slice((n_rows - 1) * 2, n_rows * 2)
-        left_pad = int(n_cols - remainder)
-        for j in range(remainder):
-            col_slice = slice(left_pad + j * 2, left_pad + (j + 1) * 2)
-            plot_locs.append((last_row, col_slice))
-
-    return gs, plot_locs
-
-
-def _plot_single_variable(data, ax, ci=None, cmap=None, fill_color="tab:blue"):
-    """
-    Plot the mean and optionally a confidence interval for a single variable.
-
-    Parameters
-    ----------
-    data : pd.DataFrame
-        A DataFrame with one or more columns containing the data to plot.
-    ax : Matplotlib Axes
-        The Axes object to plot on.
-    ci : float, optional
-        The confidence interval to plot, between 0 and 1. If not provided, only the mean will be plotted.
-    cmap : str or Colormap, optional
-        The color map to use for the data.
-    fill_color : str, optional
-        The color to use to fill the confidence interval.
-
-    Returns
-    -------
-    None
-    """
-
-    if ci is None:
-        data.plot(ax=ax, legend=False, cmap=cmap)
-
-    else:
-        q_low, q_high = ((1 - ci) / 2), 1 - ((1 - ci) / 2)
-        ci_bounds = data.quantile([q_low, q_high], axis=1).T
-
-        data.mean(axis=1).plot(ax=ax, legend=False, cmap=cmap)
-        ci_bounds.plot(ax=ax, ls="--", lw=0.5, color="k", legend=False)
-        ax.fill_between(
-            ci_bounds.index,
-            y1=ci_bounds.iloc[:, 0],
-            y2=ci_bounds.iloc[:, 1],
-            color=fill_color,
-            alpha=0.25,
-        )
-
-
-def plot_simulation(
-    simulation: pd.DataFrame,
-    vars_to_plot: Optional[List[str]] = None,
-    ci: Optional[float] = None,
-    n_cols: Optional[int] = None,
-    cmap: Optional[Union[str, Colormap]] = None,
-    fill_color: Optional[str] = None,
-    figsize: Tuple[int, int] = (12, 8),
-    dpi: int = 100,
-) -> plt.Figure:
-    """
-    Plot a simulation of multiple variables.
-
-    Parameters
-    ----------
-    simulation : pd.DataFrame
-        A DataFrame with one or more columns containing the data to plot. The columns should be the variables to plot
-        and the index should be the time.
-    vars_to_plot : list of str, optional
-        A list of the variables to plot. If not provided, all variables in the simulation DataFrame will be plotted.
-    ci : float, optional
-        The confidence interval to plot, between 0 and 1. If not provided, only the mean will be plotted.
-    n_cols : int, optional
-        The number of columns of plots to show. If not provided, the minimum of (4, number of columns in df) will be
-        used.
-    cmap : str or Colormap, optional
-        The color map to use for the data.
-    fill_color : str, optional
-        The color to use to fill the confidence interval.
-    figsize : tuple of int
-        The size of the figure in inches. Default is (12, 8).
-    dpi : int
-        The resolution of the figure in dots per inch. Default is 100.
-
-    Returns
-    -------
-    Figure
-        The Matplotlib Figure object containing the plots.
-    """
-
-    if vars_to_plot is None:
-        vars_to_plot = simulation.index
-    n_plots = len(vars_to_plot)
-    n_cols = min(4, n_plots) if n_cols is None else n_cols
-
-    gs, plot_locs = prepare_gridspec_figure(n_cols, n_plots)
-    fig = plt.figure(figsize=figsize, dpi=dpi)
-
-    for idx, variable in enumerate(vars_to_plot):
-        axis = fig.add_subplot(gs[plot_locs[idx]])
-
-        _plot_single_variable(
-            simulation.loc[variable].unstack(1),
-            ci=ci,
-            ax=axis,
-            cmap=cmap,
-            fill_color=fill_color,
-        )
-
-        axis.set(title=variable)
-        [spine.set_visible(False) for spine in axis.spines.values()]
-        axis.grid(ls="--", lw=0.5)
-
-    fig.tight_layout()
-    return fig
-
-
-def plot_irf(
-    irf: pd.DataFrame,
-    vars_to_plot: Optional[List[str]] = None,
-    shocks_to_plot: Optional[List[str]] = None,
-    n_cols: Optional[int] = None,
-    legend: bool = False,
-    cmap: Optional[Union[str, Colormap]] = None,
-    legend_kwargs: Optional[Dict] = None,
-    figsize: Tuple[int, int] = (14, 10),
-    dpi: int = 100,
-) -> plt.Figure:
-    """
-    Plot the impulse response functions for a set of variables.
-
-    Parameters
-    ----------
-    irf : pd.DataFrame
-        A DataFrame with the impulse response functions. The index should contain the variables to plot, and the columns
-        should contain the shocks, with a multi-index for the period and shock type.
-    vars_to_plot : list of str, optional
-        A list of variables to plot. If not provided, all variables in the DataFrame will be plotted.
-    shocks_to_plot : list of str, optional
-        A list of shocks to plot. If not provided, all shocks in the DataFrame will be plotted.
-    n_cols : int, optional
-        The number of columns to use in the plot grid. If not provided, the number of columns will be determined
-        automatically based on the number of variables to plot.
-    legend : bool, optional
-        Whether to show a legend with the shocks.
-    cmap : str or Colormap, optional
-        The color map to use for the impulse response functions.
-    legend_kwargs : dict, optional
-        Keyword arguments to pass to `matplotlib.figure.Figure.legend()`.
-    figsize : tuple, optional
-        The size of the figure in inches.
-    dpi : int, optional
-        The DPI of the figure.
-
-    Returns
-    -------
-    matplotlib.figure.Figure
-        The figure object.
-    """
-
-    n_plots = len(vars_to_plot)
-    n_cols = min(4, n_plots) if n_cols is None else n_cols
-
-    gs, plot_locs = prepare_gridspec_figure(n_cols, n_plots)
-    fig = plt.figure(figsize=figsize, dpi=dpi)
-
-    for idx, variable in enumerate(vars_to_plot):
-        axis = fig.add_subplot(gs[plot_locs[idx]])
-
-        _plot_single_variable(
-            irf.loc[variable, pd.IndexSlice[:, shocks_to_plot]].unstack(1),
-            ax=axis,
-            cmap=cmap,
-        )
-
-        axis.set(title=variable)
-        [spine.set_visible(False) for spine in axis.spines.values()]
-        axis.grid(ls="--", lw=0.5)
-
-    fig.tight_layout()
-
-    if legend:
-        if legend_kwargs is None:
-            legend_kwargs = {
-                "ncol": min(4, len(shocks_to_plot)),
-                "loc": "center",
-                "bbox_to_anchor": (0.5, 1.05),
-                "bbox_transform": fig.transFigure,
-            }
-
-        fig.axes[0].legend(**legend_kwargs)
-
-    return fig
-
-
-def plot_prior_solvability(
-    data: pd.DataFrame,
-    n_samples: int = 1_000,
-    seed: Optional[int] = None,
-    plotting_subset: Optional[List[str]] = None,
-):
-    """
-    Plot the results of sampling from the prior distributions of a GCN and attempting to fit a DSGE model.
-
-    This function produces a grid of plots that show the distribution of parameter values where model fitting was successful
-    or where it failed. Each plot on the grid shows the distribution of one parameter against another, with successful
-    fits plotted in blue and failed fits plotted in red.
-
-    Parameters
-    ----------
-    data : pd.DataFrame
-        A DataFrame containing the results of sampling from the prior distributions and attempting to fit a model.
-    n_samples : int, optional
-        The number of samples to draw from the prior distributions.
-    seed : int, optional
-        The seed to use for the random number generator.
-    plotting_subset : List[str], optional
-        A list of parameter names to include in the plots. If not provided, all parameters will be plotted.
-
-    Returns
-    -------
-    fig : Matplotlib Figure
-        The Figure object containing the plots
-
-    Notes
-    ----------
-    - Parameters will be sampled from prior distributions defined in the GCN.
-    - The following failure modes are considered:
-        - Steady state: The steady state of the model could not be calculated.
-        - Perturbation: The perturbation of the model failed.
-        - Blanchard-Kahn: The Blanchard-Kahn condition was not satisfied.
-        - Deterministic norm: Residuals of the deterministic part of the solution matrix were not zero.
-        - Stochastic norm: Residuals of the stochastic part of the solution matrix were not zero.
-    """
-
-    plot_data = data.copy()
-    failure_step = plot_data["failure_step"].copy()
-    plot_data.drop(columns=["failure_step"], inplace=True)
-
-    color_dict = {
-        "steady_state": "tab:red",
-        "perturbation": "tab:orange",
-        "blanchard-kahn": "tab:green",
-        "deterministic_norm": "tab:purple",
-        "stochastic_norm": "tab:pink",
-    }
-
-    constant_cols = plot_data.var() < 1e-18
-
-    plot_data = plot_data.loc[:, ~constant_cols].copy()
-    params = plot_data.columns
-    n_params = len(params) if plotting_subset is None else len(plotting_subset)
-
-    plot_data["success"] = failure_step.isna()
-    fig, axes = plt.subplots(n_params, n_params, figsize=(16, 16), dpi=100)
-
-    if plotting_subset is None:
-        param_pairs = list(combinations_with_replacement(params, 2))
-    else:
-        param_pairs = list(combinations_with_replacement(plotting_subset, 2))
-
-    plot_grid = np.arange(1, n_params**2 + 1).reshape((n_params, n_params))
-    plot_grid[np.tril_indices(n_params, k=-1)] = 0
-
-    plot_idxs = np.where(plot_grid)
-    blank_idxs = np.where(plot_grid == 0)
-
-    for col, row in zip(*blank_idxs):
-        axes[row][col].set_visible(False)
-
-    for col, row, pair in zip(*plot_idxs, param_pairs):
-        param_1, param_2 = pair
-        axis = axes[row][col]
-        if param_1 == param_2:
-
-            X_sorted = plot_data[param_1].sort_values()
-            X_success = X_sorted[plot_data["success"]]
-            X_failure = X_sorted[~plot_data["success"]]
-
-            n_success = X_success.shape[0]
-            n_failure = X_failure.shape[0]
-
-            if n_success > 0:
-                success_grid = np.linspace(X_success.min() * 0.9, X_success.max() * 1.1, 100)
-                d_success = stats.gaussian_kde(X_success)
-                axis.plot(success_grid, d_success.pdf(success_grid), color="tab:blue")
-                axis.fill_between(
-                    x=success_grid,
-                    y1=d_success.pdf(success_grid),
-                    y2=0,
-                    color="tab:blue",
-                    alpha=0.25,
-                )
-
-            if n_failure > 0:
-                failure_grid = np.linspace(X_failure.min() * 0.9, X_failure.max() * 1.1, 100)
-                d_failure = stats.gaussian_kde(X_failure)
-                axis.plot(failure_grid, d_failure.pdf(failure_grid), color="tab:red")
-                axis.fill_between(
-                    x=failure_grid,
-                    y1=d_failure.pdf(failure_grid),
-                    y2=0,
-                    color="tab:red",
-                    alpha=0.25,
-                )
-
-        else:
-            axis.scatter(
-                plot_data.loc[plot_data.success, param_1],
-                plot_data.loc[plot_data.success, param_2],
-                c="tab:blue",
-                s=10,
-                label="Model Successfully Fit",
-            )
-            why_failed = failure_step[~plot_data.success]
-            for reason in why_failed.unique():
-                reason_mask = why_failed == reason
-                axis.scatter(
-                    plot_data.loc[~plot_data.success, param_1][reason_mask],
-                    plot_data.loc[~plot_data.success, param_2][reason_mask],
-                    c=color_dict[reason],
-                    s=10,
-                    label=f"{reason.title()} Failed",
-                )
-
-        if col == 0:
-            axis.set_ylabel(param_2)
-        if row == n_params - 1:
-            axis.set_xlabel(param_1)
-
-        [spine.set_visible(False) for spine in axis.spines.values()]
-        axis.grid(ls="--", lw=0.5)
-
-    axes[1][0].legend(
-        loc="center",
-        bbox_to_anchor=(0.5, 0.91),
-        bbox_transform=fig.transFigure,
-        ncol=2,
-        fontsize=8,
-        frameon=False,
-    )
-    fig.suptitle("Model Solution Results by Parameter Values", y=0.95)
-    return fig
-
-
-def plot_eigenvalues(model: Any, figsize: Tuple[float, float] = None, dpi: int = None):
-    """
-    Plot the eigenvalues of the model solution, along with a unit circle. Eigenvalues with modulus greater than 1 are
-    shown in red, while those with modulus less than 1 are shown in blue. Eigenvalues greater than 10 in modulus
-    are not drawn.
-
-    Parameters
-    ----------
-    model : gEconModel
-        The model to plot the eigenvalues of.
-    figsize : Tuple[float, float], optional
-        The size of the figure to create.
-    dpi : int, optional
-        The resolution of the figure to create.
-
-    Returns
-    -------
-    Matplotlib Figure
-        The figure object containing the plot.
-    """
-
-    if figsize is None:
-        figsize = (5, 5)
-    if dpi is None:
-        dpi = 100
-
-    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
-    data = model.check_bk_condition(verbose=False)
-    n_infinity = (data.Modulus > 10).sum()
-
-    data = data[data.Modulus < 10]
-
-    x_circle = np.linspace(-2 * np.pi, 2 * np.pi, 1000)
-
-    ax.plot(np.cos(x_circle), np.sin(x_circle), color="k", lw=1)
-    ax.set_aspect("equal")
-    colors = ["tab:red" if x > 1.0 else "tab:blue" for x in data.Modulus]
-    ax.scatter(data.Real, data.Imaginary, color=colors, s=50, lw=1, edgecolor="k")
-    [spine.set_visible(False) for spine in ax.spines.values()]
-    ax.grid(ls="--", lw=0.5)
-    ax.set_title(
-        f"Eigenvalues of Model Solution\n{n_infinity} Eigenvalues with Infinity Modulus not shown."
-    )
-    return fig
-
-
-def plot_covariance_matrix(
-    data: pd.DataFrame,
-    vars_to_plot: Optional[List[str]] = None,
-    cbarlabel: str = "Covariance",
-    figsize: Tuple[float, float] = (8, 8),
-    dpi: int = 100,
-    cbar_kw: Optional[Dict] = None,
-    cmap: str = "YlGn",
-    annotation_fontsize: int = 8,
-) -> plt.Figure:
-    """
-    Plots a heatmap of the covariance matrix of the input data.
-
-    Parameters
-    ----------
-    data : pd.DataFrame
-        A square DataFrame, representing a covariance matrix. The index and the columns should both have the same
-        values.
-    vars_to_plot : list of str, optional
-        A list of strings containing the names of the variables to plot. If not provided, all variables in the input data
-        will be plotted.
-    cbarlabel : str, optional
-        The label for the colorbar.
-    figsize : tuple of float, optional
-        The size of the figure to create, in inches.
-    dpi : int, optional
-        The dots per inch of the figure.
-    cbar_kw : dict, optional
-        A dictionary of keyword arguments to pass to the colorbar.
-    cmap : str, optional
-        The color map to use for the heatmap.
-    annotation_fontsize : int, optional
-        The font size for the annotation in the heatmap cells.
-
-    Returns
-    -------
-    matplotlib.figure.Figure
-        A figure containing the heatmap.
-    """
-
-    if vars_to_plot is None:
-        vars_to_plot = data.columns
-
-    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
-    im, cbar = plot_heatmap(
-        data.loc[vars_to_plot, vars_to_plot],
-        ax=ax,
-        cbar_kw=cbar_kw,
-        cmap=cmap,
-        cbarlabel=cbarlabel,
-    )
-    annotate_heatmap(im, valfmt="{x:.2f}", fontsize=annotation_fontsize)
-
-    fig.tight_layout()
-    return fig
-
-
-def plot_heatmap(
-    data: pd.DataFrame,
-    ax: Optional[Any] = None,
-    cbar_kw: Optional[dict] = None,
-    cbarlabel: Optional[str] = "",
-    **kwargs,
-):
-    """
-    Create a heatmap from a pandas dataframe.
-
-    Parameters
-    ----------
-    data: Dataframe
-        A pandas dataframe to plat
-    ax: matplotlib.axes.ax, Optional
-        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If
-        not provided, use current axes or create a new one.
-    cbar_kw: Dict, Optional
-        A dictionary with arguments to `matplotlib.Figure.colorbar`.
-    cbarlabel: str, Optional
-        The label for the colorbar.  Optional.
-    **kwargs
-        All other arguments are forwarded to `imshow`.
-    """
-
-    if not ax:
-        ax = plt.gca()
-
-    if not cbar_kw:
-        cbar_kw = {}
-
-    # Plot the heatmap
-    im = ax.imshow(data, **kwargs)
-
-    n_rows, n_columns = data.shape
-
-    # Create colorbar
-    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)
-    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va="bottom")
-
-    # Show all ticks and label them with the respective list entries.
-    ax.set(
-        xticks=np.arange(n_rows),
-        xticklabels=data.columns,
-        yticks=np.arange(n_columns),
-        yticklabels=data.index,
-    )
-
-    # Let the horizontal axes labeling appear on top.
-    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)
-
-    # Turn spines off and create white grid.
-    ax.spines[:].set_visible(False)
-
-    ax.set_xticks(np.arange(data.shape[1] + 1) - 0.5, minor=True)
-    ax.set_yticks(np.arange(data.shape[0] + 1) - 0.5, minor=True)
-    ax.grid(which="minor", color="w", linestyle="-", linewidth=3)
-    ax.tick_params(which="minor", bottom=False, left=False)
-
-    return im, cbar
-
-
-def annotate_heatmap(
-    im,
-    data=None,
-    valfmt="{x:.2f}",
-    textcolors=("black", "white"),
-    threshold=None,
-    **textkw,
-):
-    """
-    A function to annotate a heatmap.
-
-    Parameters
-    ----------
-    im
-        The AxesImage to be labeled.
-    data
-        Data used to annotate.  If None, the image's data is used.  Optional.
-    valfmt
-        The format of the annotations inside the heatmap.  This should either
-        use the string format method, e.g. "$ {x:.2f}", or be a
-        `matplotlib.ticker.Formatter`.  Optional.
-    textcolors
-        A pair of colors.  The first is used for values below a threshold,
-        the second for those above.  Optional.
-    threshold
-        Value in data units according to which the colors from textcolors are
-        applied.  If None (the default) uses the middle of the colormap as
-        separation.  Optional.
-    **kwargs
-        All other arguments are forwarded to each call to `text` used to create
-        the text labels.
-    """
-
-    if not isinstance(data, (list, np.ndarray)):
-        data = im.get_array()
-
-    # Normalize the threshold to the images color range.
-    if threshold is not None:
-        threshold = im.norm(threshold)
-    else:
-        threshold = im.norm(data.max()) / 2.0
-
-    # Set default alignment to center, but allow it to be
-    # overwritten by textkw.
-    kw = dict(horizontalalignment="center", verticalalignment="center")
-    kw.update(textkw)
-
-    # Get the formatter in case a string is supplied
-    if isinstance(valfmt, str):
-        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)
-
-    # Loop over the data and create a `Text` for each "pixel".
-    # Change the text's color depending on the data.
-    texts = []
-    for i in range(data.shape[0]):
-        for j in range(data.shape[1]):
-            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])
-            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)
-            texts.append(text)
-
-    return texts
-
-
-def plot_acf(
-    acorr_matrix: pd.DataFrame,
-    vars_to_plot: Optional[List[str]] = None,
-    figsize: Optional[Tuple[int, int]] = (14, 4),
-    dpi: Optional[int] = 100,
-    n_cols: Optional[int] = 4,
-) -> plt.Figure:
-    """
-    Plot the autocorrelation function for a set of variables.
-
-    Parameters
-    ----------
-    acorr_matrix: pandas.DataFrame
-        Matrix of autocorrelation values. Rows represent variables and columns represent lags.
-    vars_to_plot: list of str, optional
-        List of variables to plot. If not provided, all variables in `acorr_matrix` will be plotted.
-    figsize: tuple, optional
-        Figure size in inches.
-    dpi: int, optional
-        Figure resolution in dots per inch.
-    n_cols: int, optional
-        Number of columns in the subplot grid.
-
-    Returns
-    -------
-    matplotlib.figure.Figure
-        Figure object containing the plots.
-    """
-
-    if vars_to_plot is None:
-        vars_to_plot = acorr_matrix.index
-
-    n_plots = len(vars_to_plot)
-    n_cols = min(n_cols, n_plots)
-
-    fig = plt.figure(figsize=figsize, dpi=dpi)
-    gc, plot_locs = prepare_gridspec_figure(n_cols=n_cols, n_plots=n_plots)
-
-    x_values = acorr_matrix.columns
-
-    for variable, plot_loc in zip(vars_to_plot, plot_locs):
-        axis = fig.add_subplot(gc[plot_loc])
-        axis.scatter(x_values, acorr_matrix.loc[variable, :])
-        axis.vlines(x_values, 0, acorr_matrix.loc[variable, :])
-
-        [spine.set_visible(False) for spine in axis.spines.values()]
-        axis.grid(ls="--", lw=0.5)
-        axis.set(title=variable)
-
-    fig.tight_layout()
-    return fig
-
-
-def plot_corner(
-    idata: Any,
-    var_names: Optional[List[str]] = None,
-    figsize: Tuple[int, int] = (14, 14),
-    dpi: int = 144,
-    hist_bins: int = 200,
-    rug_bins: int = 50,
-    rug_levels: int = 6,
-    fontsize: int = 8,
-    show_marginal_modes: bool = True,
-) -> None:
-    """
-    Produces a corner plot, also known as a scatterplot matrix, of the posterior distributions of a set of variables.
-    Each panel of the plot shows the two-dimensional distribution of two of the variables, with the remaining variables
-    marginalized out. The diagonal panels show the one-dimensional distribution of each variable.
-
-    Parameters
-    ----------
-    idata : arviz.InferenceData
-        An arviz idata object with a posterior group.
-    var_names : List[str], optional
-        A list of strings specifying the variables to plot. If not provided, all variables in `idata` will be plotted.
-    figsize : Tuple[int, int], optional
-        The size of the figure in inches. Default is (14, 14).
-    dpi : int, optional
-        The resolution of the figure in dots per inch. Default is 144.
-    hist_bins : int, optional
-        The number of bins to use for the histograms on the diagonal panels. Default is 200.
-    rug_bins : int, optional
-        The number of bins to use for the histograms on the off-diagonal panels. Default is 50.
-    rug_levels : int, optional
-        The number of contour levels to use for the histograms on the off-diagonal panels. Default is 6.
-    fontsize : int, optional
-        The font size for the axis labels and ticks.
-    show_marginal_modes : bool, optional
-        Whether or not to show the modes of the marginal distributions. Default is True.
-
-    Returns
-    ----------
-    matplotlib.figure.Figure
-        Figure object containing the plots.
-    """
-
-    if not hasattr(idata, "posterior"):
-        raise ValueError("Argument idata should be an arviz idata object with a posterior group")
-    var_names = var_names or list(idata.posterior.data_vars)
-    k_params = len(var_names)
-
-    fig, ax = plt.subplots(k_params, k_params, figsize=figsize, dpi=dpi)
-
-    for i, axis in enumerate(fig.axes):
-        row = i // k_params
-        col = i % k_params
-
-        axis.ticklabel_format(axis="both", style="sci")
-        axis.yaxis.major.formatter.set_powerlimits((-2, 2))
-        axis.yaxis.offsetText.set_fontsize(fontsize)
-        axis.xaxis.major.formatter.set_powerlimits((-2, 2))
-        axis.xaxis.offsetText.set_fontsize(fontsize)
-        if col <= row:
-            if col == row:
-                v = var_names[col]
-                axis.hist(
-                    idata.posterior[v].values.ravel(),
-                    bins=hist_bins,
-                    histtype="step",
-                    density=True,
-                )
-                axis.set_yticklabels([])
-                axis.set_title(v, fontsize=fontsize)
-                axis.tick_params(
-                    axis="both",
-                    left=False,
-                    bottom=row == (k_params - 1),
-                    labelsize=fontsize,
-                )
-                if row != (k_params - 1):
-                    axis.set_xticklabels([])
-                    axis.tick_params(axis="x", which="both", bottom=False)
-
-            else:
-                x = var_names[col]
-                y = var_names[row]
-
-                data_x = idata.posterior[x].values.ravel()
-                data_y = idata.posterior[y].values.ravel()
-
-                # x_hist, edges = np.histogram(data_x, bins=hist_bins)
-                # x_mode = edges[np.argmax(x_hist)]
-                #
-                # y_hist, edges = np.histogram(data_y, bins=hist_bins)
-                # y_mode = edges[np.argmax(y_hist)]
-
-                H, y_edges, x_edges = np.histogram2d(data_y, data_x, bins=rug_bins)
-
-                ymax_idx, xmax_idx = np.where(H == H.max())
-                x_mode = x_edges[xmax_idx]
-                y_mode = y_edges[ymax_idx]
-                if len(x_mode) > 1:
-                    x_mode = x_mode[0]
-                if len(y_mode) > 1:
-                    y_mode = y_mode[0]
-
-                axis.contourf(x_edges[1:], y_edges[1:], H, cmap="Blues", levels=rug_levels)
-
-                if show_marginal_modes:
-                    axis.axvline(x_mode, ls="--", lw=0.5, color="k")
-                    axis.axhline(y_mode, ls="--", lw=0.5, color="k")
-                    axis.scatter(x_mode, y_mode, color="k", marker="s", s=20)
-
-                if col == 0:
-                    axis.set_ylabel(y, fontsize=fontsize)
-                else:
-                    axis.set_yticklabels([])
-                    axis.tick_params(axis="y", which="both", left=False)
-
-                if row != (k_params - 1):
-                    axis.set_xticklabels([])
-                    axis.tick_params(axis="x", which="both", bottom=False)
-                else:
-                    axis.set_xlabel(x, fontsize=fontsize)
-
-                axis.tick_params(axis="both", which="both", labelsize=fontsize)
-        else:
-            axis.set(xticks=[], yticks=[], xlabel="", ylabel="")
-            axis.set_visible(False)
-
-    fig.tight_layout(h_pad=0.1, w_pad=0.5)
-    plt.show()
-
-
-def plot_kalman_filter(
-    idata: xr.Dataset,
-    data: pd.DataFrame,
-    kalman_output: str = "predicted",
-    n_cols: Optional[int] = None,
-    vars_to_plot: Optional[List[str]] = None,
-    fig: Optional[Figure] = None,
-    figsize: Tuple[int, int] = (14, 6),
-    dpi: int = 144,
-    cmap: Optional[str] = None,
-):
-
-    """
-    Plot Kalman filter, prediction or smoothed series for variables in idata.
-
-    Parameters
-    ----------
-    idata : xarray.Dataset
-        Dataset with Kalman filter variables.
-    data : pandas.DataFrame
-        DataFrame with original time series data.
-    kalman_output : str, optional
-        String indicating whether to plot filtered, predicted, or smoothed series.
-        Must be one of 'filtered', 'predicted', or 'smoothed'.
-    n_cols : int, optional
-        Number of columns in the plot.
-    vars_to_plot : list of str, optional
-        List of variable names to plot.
-    fig : matplotlib.figure.Figure, optional
-        Matplotlib Figure object to plot on.
-    figsize : tuple of int, optional
-        Figure size in inches.
-    dpi : int, optional
-        Figure DPI.
-    cmap : str, optional
-        Colormap name.
-
-    Returns
-    -------
-    matplotlib.figure.Figure
-        Matplotlib Figure object with the plot.
-    """
-
-    if kalman_output.lower() not in ["filtered", "predicted", "smoothed"]:
-        raise ValueError(
-            f'kalman_output must be one of "filtered", "predicted", "smoothed". Found {kalman_output}.'
-        )
-    kalman_output = kalman_output.capitalize()
-
-    if fig is None:
-        fig = plt.figure(figsize=figsize, dpi=dpi)
-
-    if vars_to_plot is None:
-        vars_to_plot = idata.coords["variable"].values
-
-    n_plots = len(vars_to_plot)
-    n_cols = min(4, n_plots) if n_cols is None else n_cols
-
-    gs, plot_locs = prepare_gridspec_figure(n_cols, n_plots)
-    time_idx = idata.coords["time"]
-    time_slice = (
-        slice(None, None, None) if kalman_output.lower() == "predicted" else slice(1, None, None)
-    )
-
-    for idx, variable in enumerate(vars_to_plot):
-        axis = fig.add_subplot(gs[plot_locs[idx]])
-
-        mu = idata[f"{kalman_output}_State"].dropna(dim="time").sel(variable=variable)
-
-        q05, q50, q95 = mu.quantile([0.05, 0.5, 0.95], dim="sample")
-
-        sigma = (
-            idata[f"{kalman_output}_Cov"]
-            .dropna(dim="time")
-            .sel(variable=variable, variable2=variable)
-        )
-        s05, s95 = sigma.quantile([0.05, 0.95], dim="sample")
-
-        top_ci = mu + 1.98 * np.sqrt(s05 + 1e-6)
-        bot_ci = mu - 1.98 * np.sqrt(s95 + 1e-6)
-
-        axis.plot(time_idx[time_slice], q50.values, color="tab:red")
-        axis.fill_between(time_idx[time_slice], q05, q95, color="tab:blue", alpha=1)
-        axis.fill_between(
-            time_idx[time_slice],
-            top_ci.max(dim=["sample"]),
-            bot_ci.min(dim=["sample"]),
-            color="0.5",
-            alpha=0.5,
-        )
-
-        if variable in data.columns:
-            data[variable].plot(ax=axis, color="k", ls="--", lw=2)
-
-        axis.set(title=variable, xlabel=None, ylabel="% Deviation from SS")
-        axis.tick_params(axis="x", rotation=45)
-
-    fig.tight_layout()
+from itertools import combinations_with_replacement
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+import matplotlib
+import matplotlib.pyplot as plt
+import numpy as np
+import pandas as pd
+import xarray as xr
+from matplotlib.colors import Colormap
+from matplotlib.figure import Figure
+from matplotlib.gridspec import GridSpec
+from matplotlib.ticker import ScalarFormatter
+from scipy import stats
+
+
+class ScalarFormatterForceFormat(ScalarFormatter):
+    """
+    A ScalarFormatter that forces a specific format for the tick labels.
+    """
+
+    def _set_format(self, vmin, vmax):
+        """
+        Set the format for the tick labels.
+
+        Parameters
+        ----------
+        vmin : float
+            The minimum value of the data.
+        vmax : float
+            The maximum value of the data.
+        """
+        self.format = "%1.1f"
+
+
+def prepare_gridspec_figure(n_cols: int, n_plots: int) -> Tuple[GridSpec, List]:
+    """
+     Prepare a figure with a grid of subplots. Centers the last row of plots if the number of plots is not square.
+
+     Parameters
+     ----------
+     n_cols : int
+         The number of columns in the grid.
+     n_plots : int
+         The number of subplots in the grid.
+
+     Returns
+     -------
+     GridSpec
+         A matplotlib GridSpec object representing the layout of the grid.
+    list of tuple(slice, slice)
+         A list of tuples of slices representing the indices of the grid cells to be used for each subplot.
+    """
+
+    remainder = n_plots % n_cols
+    has_remainder = remainder > 0
+    n_rows = n_plots // n_cols + 1
+
+    gs = GridSpec(2 * n_rows, 2 * n_cols)
+    plot_locs = []
+
+    for i in range(n_rows - int(has_remainder)):
+        for j in range(n_cols):
+            plot_locs.append((slice(i * 2, (i + 1) * 2), slice(j * 2, (j + 1) * 2)))
+
+    if has_remainder:
+        last_row = slice((n_rows - 1) * 2, n_rows * 2)
+        left_pad = int(n_cols - remainder)
+        for j in range(remainder):
+            col_slice = slice(left_pad + j * 2, left_pad + (j + 1) * 2)
+            plot_locs.append((last_row, col_slice))
+
+    return gs, plot_locs
+
+
+def _plot_single_variable(data, ax, ci=None, cmap=None, fill_color="tab:blue"):
+    """
+    Plot the mean and optionally a confidence interval for a single variable.
+
+    Parameters
+    ----------
+    data : pd.DataFrame
+        A DataFrame with one or more columns containing the data to plot.
+    ax : Matplotlib Axes
+        The Axes object to plot on.
+    ci : float, optional
+        The confidence interval to plot, between 0 and 1. If not provided, only the mean will be plotted.
+    cmap : str or Colormap, optional
+        The color map to use for the data.
+    fill_color : str, optional
+        The color to use to fill the confidence interval.
+
+    Returns
+    -------
+    None
+    """
+
+    if ci is None:
+        data.plot(ax=ax, legend=False, cmap=cmap)
+
+    else:
+        q_low, q_high = ((1 - ci) / 2), 1 - ((1 - ci) / 2)
+        ci_bounds = data.quantile([q_low, q_high], axis=1).T
+
+        data.mean(axis=1).plot(ax=ax, legend=False, cmap=cmap)
+        ci_bounds.plot(ax=ax, ls="--", lw=0.5, color="k", legend=False)
+        ax.fill_between(
+            ci_bounds.index,
+            y1=ci_bounds.iloc[:, 0],
+            y2=ci_bounds.iloc[:, 1],
+            color=fill_color,
+            alpha=0.25,
+        )
+
+
+def plot_simulation(
+    simulation: pd.DataFrame,
+    vars_to_plot: Optional[List[str]] = None,
+    ci: Optional[float] = None,
+    n_cols: Optional[int] = None,
+    cmap: Optional[Union[str, Colormap]] = None,
+    fill_color: Optional[str] = None,
+    figsize: Tuple[int, int] = (12, 8),
+    dpi: int = 100,
+) -> plt.Figure:
+    """
+    Plot a simulation of multiple variables.
+
+    Parameters
+    ----------
+    simulation : pd.DataFrame
+        A DataFrame with one or more columns containing the data to plot. The columns should be the variables to plot
+        and the index should be the time.
+    vars_to_plot : list of str, optional
+        A list of the variables to plot. If not provided, all variables in the simulation DataFrame will be plotted.
+    ci : float, optional
+        The confidence interval to plot, between 0 and 1. If not provided, only the mean will be plotted.
+    n_cols : int, optional
+        The number of columns of plots to show. If not provided, the minimum of (4, number of columns in df) will be
+        used.
+    cmap : str or Colormap, optional
+        The color map to use for the data.
+    fill_color : str, optional
+        The color to use to fill the confidence interval.
+    figsize : tuple of int
+        The size of the figure in inches. Default is (12, 8).
+    dpi : int
+        The resolution of the figure in dots per inch. Default is 100.
+
+    Returns
+    -------
+    Figure
+        The Matplotlib Figure object containing the plots.
+    """
+
+    if vars_to_plot is None:
+        vars_to_plot = simulation.index
+    n_plots = len(vars_to_plot)
+    n_cols = min(4, n_plots) if n_cols is None else n_cols
+
+    gs, plot_locs = prepare_gridspec_figure(n_cols, n_plots)
+    fig = plt.figure(figsize=figsize, dpi=dpi)
+
+    for idx, variable in enumerate(vars_to_plot):
+        axis = fig.add_subplot(gs[plot_locs[idx]])
+
+        _plot_single_variable(
+            simulation.loc[variable].unstack(1),
+            ci=ci,
+            ax=axis,
+            cmap=cmap,
+            fill_color=fill_color,
+        )
+
+        axis.set(title=variable)
+        [spine.set_visible(False) for spine in axis.spines.values()]
+        axis.grid(ls="--", lw=0.5)
+
+    fig.tight_layout()
+    return fig
+
+
+def plot_irf(
+    irf: pd.DataFrame,
+    vars_to_plot: Optional[List[str]] = None,
+    shocks_to_plot: Optional[List[str]] = None,
+    n_cols: Optional[int] = None,
+    legend: bool = False,
+    cmap: Optional[Union[str, Colormap]] = None,
+    legend_kwargs: Optional[Dict] = None,
+    figsize: Tuple[int, int] = (14, 10),
+    dpi: int = 100,
+) -> plt.Figure:
+    """
+    Plot the impulse response functions for a set of variables.
+
+    Parameters
+    ----------
+    irf : pd.DataFrame
+        A DataFrame with the impulse response functions. The index should contain the variables to plot, and the columns
+        should contain the shocks, with a multi-index for the period and shock type.
+    vars_to_plot : list of str, optional
+        A list of variables to plot. If not provided, all variables in the DataFrame will be plotted.
+    shocks_to_plot : list of str, optional
+        A list of shocks to plot. If not provided, all shocks in the DataFrame will be plotted.
+    n_cols : int, optional
+        The number of columns to use in the plot grid. If not provided, the number of columns will be determined
+        automatically based on the number of variables to plot.
+    legend : bool, optional
+        Whether to show a legend with the shocks.
+    cmap : str or Colormap, optional
+        The color map to use for the impulse response functions.
+    legend_kwargs : dict, optional
+        Keyword arguments to pass to `matplotlib.figure.Figure.legend()`.
+    figsize : tuple, optional
+        The size of the figure in inches.
+    dpi : int, optional
+        The DPI of the figure.
+
+    Returns
+    -------
+    matplotlib.figure.Figure
+        The figure object.
+    """
+
+    n_plots = len(vars_to_plot)
+    n_cols = min(4, n_plots) if n_cols is None else n_cols
+
+    gs, plot_locs = prepare_gridspec_figure(n_cols, n_plots)
+    fig = plt.figure(figsize=figsize, dpi=dpi)
+
+    for idx, variable in enumerate(vars_to_plot):
+        axis = fig.add_subplot(gs[plot_locs[idx]])
+
+        _plot_single_variable(
+            irf.loc[variable, pd.IndexSlice[:, shocks_to_plot]].unstack(1),
+            ax=axis,
+            cmap=cmap,
+        )
+
+        axis.set(title=variable)
+        [spine.set_visible(False) for spine in axis.spines.values()]
+        axis.grid(ls="--", lw=0.5)
+
+    fig.tight_layout()
+
+    if legend:
+        if legend_kwargs is None:
+            legend_kwargs = {
+                "ncol": min(4, len(shocks_to_plot)),
+                "loc": "center",
+                "bbox_to_anchor": (0.5, 1.05),
+                "bbox_transform": fig.transFigure,
+            }
+
+        fig.axes[0].legend(**legend_kwargs)
+
+    return fig
+
+
+def plot_prior_solvability(
+    data: pd.DataFrame,
+    n_samples: int = 1_000,
+    seed: Optional[int] = None,
+    plotting_subset: Optional[List[str]] = None,
+):
+    """
+    Plot the results of sampling from the prior distributions of a GCN and attempting to fit a DSGE model.
+
+    This function produces a grid of plots that show the distribution of parameter values where model fitting was successful
+    or where it failed. Each plot on the grid shows the distribution of one parameter against another, with successful
+    fits plotted in blue and failed fits plotted in red.
+
+    Parameters
+    ----------
+    data : pd.DataFrame
+        A DataFrame containing the results of sampling from the prior distributions and attempting to fit a model.
+    n_samples : int, optional
+        The number of samples to draw from the prior distributions.
+    seed : int, optional
+        The seed to use for the random number generator.
+    plotting_subset : List[str], optional
+        A list of parameter names to include in the plots. If not provided, all parameters will be plotted.
+
+    Returns
+    -------
+    fig : Matplotlib Figure
+        The Figure object containing the plots
+
+    Notes
+    ----------
+    - Parameters will be sampled from prior distributions defined in the GCN.
+    - The following failure modes are considered:
+        - Steady state: The steady state of the model could not be calculated.
+        - Perturbation: The perturbation of the model failed.
+        - Blanchard-Kahn: The Blanchard-Kahn condition was not satisfied.
+        - Deterministic norm: Residuals of the deterministic part of the solution matrix were not zero.
+        - Stochastic norm: Residuals of the stochastic part of the solution matrix were not zero.
+    """
+
+    plot_data = data.copy()
+    failure_step = plot_data["failure_step"].copy()
+    plot_data.drop(columns=["failure_step"], inplace=True)
+
+    color_dict = {
+        "steady_state": "tab:red",
+        "perturbation": "tab:orange",
+        "blanchard-kahn": "tab:green",
+        "deterministic_norm": "tab:purple",
+        "stochastic_norm": "tab:pink",
+    }
+
+    constant_cols = plot_data.var() < 1e-18
+
+    plot_data = plot_data.loc[:, ~constant_cols].copy()
+    params = plot_data.columns
+    n_params = len(params) if plotting_subset is None else len(plotting_subset)
+
+    plot_data["success"] = failure_step.isna()
+    fig, axes = plt.subplots(n_params, n_params, figsize=(16, 16), dpi=100)
+
+    if plotting_subset is None:
+        param_pairs = list(combinations_with_replacement(params, 2))
+    else:
+        param_pairs = list(combinations_with_replacement(plotting_subset, 2))
+
+    plot_grid = np.arange(1, n_params**2 + 1).reshape((n_params, n_params))
+    plot_grid[np.tril_indices(n_params, k=-1)] = 0
+
+    plot_idxs = np.where(plot_grid)
+    blank_idxs = np.where(plot_grid == 0)
+
+    for col, row in zip(*blank_idxs):
+        axes[row][col].set_visible(False)
+
+    for col, row, pair in zip(*plot_idxs, param_pairs):
+        param_1, param_2 = pair
+        axis = axes[row][col]
+        if param_1 == param_2:
+
+            X_sorted = plot_data[param_1].sort_values()
+            X_success = X_sorted[plot_data["success"]]
+            X_failure = X_sorted[~plot_data["success"]]
+
+            n_success = X_success.shape[0]
+            n_failure = X_failure.shape[0]
+
+            if n_success > 0:
+                success_grid = np.linspace(X_success.min() * 0.9, X_success.max() * 1.1, 100)
+                d_success = stats.gaussian_kde(X_success)
+                axis.plot(success_grid, d_success.pdf(success_grid), color="tab:blue")
+                axis.fill_between(
+                    x=success_grid,
+                    y1=d_success.pdf(success_grid),
+                    y2=0,
+                    color="tab:blue",
+                    alpha=0.25,
+                )
+
+            if n_failure > 0:
+                failure_grid = np.linspace(X_failure.min() * 0.9, X_failure.max() * 1.1, 100)
+                d_failure = stats.gaussian_kde(X_failure)
+                axis.plot(failure_grid, d_failure.pdf(failure_grid), color="tab:red")
+                axis.fill_between(
+                    x=failure_grid,
+                    y1=d_failure.pdf(failure_grid),
+                    y2=0,
+                    color="tab:red",
+                    alpha=0.25,
+                )
+
+        else:
+            axis.scatter(
+                plot_data.loc[plot_data.success, param_1],
+                plot_data.loc[plot_data.success, param_2],
+                c="tab:blue",
+                s=10,
+                label="Model Successfully Fit",
+            )
+            why_failed = failure_step[~plot_data.success]
+            for reason in why_failed.unique():
+                reason_mask = why_failed == reason
+                axis.scatter(
+                    plot_data.loc[~plot_data.success, param_1][reason_mask],
+                    plot_data.loc[~plot_data.success, param_2][reason_mask],
+                    c=color_dict[reason],
+                    s=10,
+                    label=f"{reason.title()} Failed",
+                )
+
+        if col == 0:
+            axis.set_ylabel(param_2)
+        if row == n_params - 1:
+            axis.set_xlabel(param_1)
+
+        [spine.set_visible(False) for spine in axis.spines.values()]
+        axis.grid(ls="--", lw=0.5)
+
+    axes[1][0].legend(
+        loc="center",
+        bbox_to_anchor=(0.5, 0.91),
+        bbox_transform=fig.transFigure,
+        ncol=2,
+        fontsize=8,
+        frameon=False,
+    )
+    fig.suptitle("Model Solution Results by Parameter Values", y=0.95)
+    return fig
+
+
+def plot_eigenvalues(model: Any, figsize: Tuple[float, float] = None, dpi: int = None):
+    """
+    Plot the eigenvalues of the model solution, along with a unit circle. Eigenvalues with modulus greater than 1 are
+    shown in red, while those with modulus less than 1 are shown in blue. Eigenvalues greater than 10 in modulus
+    are not drawn.
+
+    Parameters
+    ----------
+    model : gEconModel
+        The model to plot the eigenvalues of.
+    figsize : Tuple[float, float], optional
+        The size of the figure to create.
+    dpi : int, optional
+        The resolution of the figure to create.
+
+    Returns
+    -------
+    Matplotlib Figure
+        The figure object containing the plot.
+    """
+
+    if figsize is None:
+        figsize = (5, 5)
+    if dpi is None:
+        dpi = 100
+
+    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
+    data = model.check_bk_condition(verbose=False)
+    n_infinity = (data.Modulus > 10).sum()
+
+    data = data[data.Modulus < 10]
+
+    x_circle = np.linspace(-2 * np.pi, 2 * np.pi, 1000)
+
+    ax.plot(np.cos(x_circle), np.sin(x_circle), color="k", lw=1)
+    ax.set_aspect("equal")
+    colors = ["tab:red" if x > 1.0 else "tab:blue" for x in data.Modulus]
+    ax.scatter(data.Real, data.Imaginary, color=colors, s=50, lw=1, edgecolor="k")
+    [spine.set_visible(False) for spine in ax.spines.values()]
+    ax.grid(ls="--", lw=0.5)
+    ax.set_title(
+        f"Eigenvalues of Model Solution\n{n_infinity} Eigenvalues with Infinity Modulus not shown."
+    )
+    return fig
+
+
+def plot_covariance_matrix(
+    data: pd.DataFrame,
+    vars_to_plot: Optional[List[str]] = None,
+    cbarlabel: str = "Covariance",
+    figsize: Tuple[float, float] = (8, 8),
+    dpi: int = 100,
+    cbar_kw: Optional[Dict] = None,
+    cmap: str = "YlGn",
+    annotation_fontsize: int = 8,
+) -> plt.Figure:
+    """
+    Plots a heatmap of the covariance matrix of the input data.
+
+    Parameters
+    ----------
+    data : pd.DataFrame
+        A square DataFrame, representing a covariance matrix. The index and the columns should both have the same
+        values.
+    vars_to_plot : list of str, optional
+        A list of strings containing the names of the variables to plot. If not provided, all variables in the input data
+        will be plotted.
+    cbarlabel : str, optional
+        The label for the colorbar.
+    figsize : tuple of float, optional
+        The size of the figure to create, in inches.
+    dpi : int, optional
+        The dots per inch of the figure.
+    cbar_kw : dict, optional
+        A dictionary of keyword arguments to pass to the colorbar.
+    cmap : str, optional
+        The color map to use for the heatmap.
+    annotation_fontsize : int, optional
+        The font size for the annotation in the heatmap cells.
+
+    Returns
+    -------
+    matplotlib.figure.Figure
+        A figure containing the heatmap.
+    """
+
+    if vars_to_plot is None:
+        vars_to_plot = data.columns
+
+    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
+    im, cbar = plot_heatmap(
+        data.loc[vars_to_plot, vars_to_plot],
+        ax=ax,
+        cbar_kw=cbar_kw,
+        cmap=cmap,
+        cbarlabel=cbarlabel,
+    )
+    annotate_heatmap(im, valfmt="{x:.2f}", fontsize=annotation_fontsize)
+
+    fig.tight_layout()
+    return fig
+
+
+def plot_heatmap(
+    data: pd.DataFrame,
+    ax: Optional[Any] = None,
+    cbar_kw: Optional[dict] = None,
+    cbarlabel: Optional[str] = "",
+    **kwargs,
+):
+    """
+    Create a heatmap from a pandas dataframe.
+
+    Parameters
+    ----------
+    data: Dataframe
+        A pandas dataframe to plat
+    ax: matplotlib.axes.ax, Optional
+        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If
+        not provided, use current axes or create a new one.
+    cbar_kw: Dict, Optional
+        A dictionary with arguments to `matplotlib.Figure.colorbar`.
+    cbarlabel: str, Optional
+        The label for the colorbar.  Optional.
+    **kwargs
+        All other arguments are forwarded to `imshow`.
+    """
+
+    if not ax:
+        ax = plt.gca()
+
+    if not cbar_kw:
+        cbar_kw = {}
+
+    # Plot the heatmap
+    im = ax.imshow(data, **kwargs)
+
+    n_rows, n_columns = data.shape
+
+    # Create colorbar
+    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)
+    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va="bottom")
+
+    # Show all ticks and label them with the respective list entries.
+    ax.set(
+        xticks=np.arange(n_rows),
+        xticklabels=data.columns,
+        yticks=np.arange(n_columns),
+        yticklabels=data.index,
+    )
+
+    # Let the horizontal axes labeling appear on top.
+    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)
+
+    # Turn spines off and create white grid.
+    ax.spines[:].set_visible(False)
+
+    ax.set_xticks(np.arange(data.shape[1] + 1) - 0.5, minor=True)
+    ax.set_yticks(np.arange(data.shape[0] + 1) - 0.5, minor=True)
+    ax.grid(which="minor", color="w", linestyle="-", linewidth=3)
+    ax.tick_params(which="minor", bottom=False, left=False)
+
+    return im, cbar
+
+
+def annotate_heatmap(
+    im,
+    data=None,
+    valfmt="{x:.2f}",
+    textcolors=("black", "white"),
+    threshold=None,
+    **textkw,
+):
+    """
+    A function to annotate a heatmap.
+
+    Parameters
+    ----------
+    im
+        The AxesImage to be labeled.
+    data
+        Data used to annotate.  If None, the image's data is used.  Optional.
+    valfmt
+        The format of the annotations inside the heatmap.  This should either
+        use the string format method, e.g. "$ {x:.2f}", or be a
+        `matplotlib.ticker.Formatter`.  Optional.
+    textcolors
+        A pair of colors.  The first is used for values below a threshold,
+        the second for those above.  Optional.
+    threshold
+        Value in data units according to which the colors from textcolors are
+        applied.  If None (the default) uses the middle of the colormap as
+        separation.  Optional.
+    **kwargs
+        All other arguments are forwarded to each call to `text` used to create
+        the text labels.
+    """
+
+    if not isinstance(data, (list, np.ndarray)):
+        data = im.get_array()
+
+    # Normalize the threshold to the images color range.
+    if threshold is not None:
+        threshold = im.norm(threshold)
+    else:
+        threshold = im.norm(data.max()) / 2.0
+
+    # Set default alignment to center, but allow it to be
+    # overwritten by textkw.
+    kw = dict(horizontalalignment="center", verticalalignment="center")
+    kw.update(textkw)
+
+    # Get the formatter in case a string is supplied
+    if isinstance(valfmt, str):
+        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)
+
+    # Loop over the data and create a `Text` for each "pixel".
+    # Change the text's color depending on the data.
+    texts = []
+    for i in range(data.shape[0]):
+        for j in range(data.shape[1]):
+            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])
+            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)
+            texts.append(text)
+
+    return texts
+
+
+def plot_acf(
+    acorr_matrix: pd.DataFrame,
+    vars_to_plot: Optional[List[str]] = None,
+    figsize: Optional[Tuple[int, int]] = (14, 4),
+    dpi: Optional[int] = 100,
+    n_cols: Optional[int] = 4,
+) -> plt.Figure:
+    """
+    Plot the autocorrelation function for a set of variables.
+
+    Parameters
+    ----------
+    acorr_matrix: pandas.DataFrame
+        Matrix of autocorrelation values. Rows represent variables and columns represent lags.
+    vars_to_plot: list of str, optional
+        List of variables to plot. If not provided, all variables in `acorr_matrix` will be plotted.
+    figsize: tuple, optional
+        Figure size in inches.
+    dpi: int, optional
+        Figure resolution in dots per inch.
+    n_cols: int, optional
+        Number of columns in the subplot grid.
+
+    Returns
+    -------
+    matplotlib.figure.Figure
+        Figure object containing the plots.
+    """
+
+    if vars_to_plot is None:
+        vars_to_plot = acorr_matrix.index
+
+    n_plots = len(vars_to_plot)
+    n_cols = min(n_cols, n_plots)
+
+    fig = plt.figure(figsize=figsize, dpi=dpi)
+    gc, plot_locs = prepare_gridspec_figure(n_cols=n_cols, n_plots=n_plots)
+
+    x_values = acorr_matrix.columns
+
+    for variable, plot_loc in zip(vars_to_plot, plot_locs):
+        axis = fig.add_subplot(gc[plot_loc])
+        axis.scatter(x_values, acorr_matrix.loc[variable, :])
+        axis.vlines(x_values, 0, acorr_matrix.loc[variable, :])
+
+        [spine.set_visible(False) for spine in axis.spines.values()]
+        axis.grid(ls="--", lw=0.5)
+        axis.set(title=variable)
+
+    fig.tight_layout()
+    return fig
+
+
+def plot_corner(
+    idata: Any,
+    var_names: Optional[List[str]] = None,
+    figsize: Tuple[int, int] = (14, 14),
+    dpi: int = 144,
+    hist_bins: int = 200,
+    rug_bins: int = 50,
+    rug_levels: int = 6,
+    fontsize: int = 8,
+    show_marginal_modes: bool = True,
+) -> None:
+    """
+    Produces a corner plot, also known as a scatterplot matrix, of the posterior distributions of a set of variables.
+    Each panel of the plot shows the two-dimensional distribution of two of the variables, with the remaining variables
+    marginalized out. The diagonal panels show the one-dimensional distribution of each variable.
+
+    Parameters
+    ----------
+    idata : arviz.InferenceData
+        An arviz idata object with a posterior group.
+    var_names : List[str], optional
+        A list of strings specifying the variables to plot. If not provided, all variables in `idata` will be plotted.
+    figsize : Tuple[int, int], optional
+        The size of the figure in inches. Default is (14, 14).
+    dpi : int, optional
+        The resolution of the figure in dots per inch. Default is 144.
+    hist_bins : int, optional
+        The number of bins to use for the histograms on the diagonal panels. Default is 200.
+    rug_bins : int, optional
+        The number of bins to use for the histograms on the off-diagonal panels. Default is 50.
+    rug_levels : int, optional
+        The number of contour levels to use for the histograms on the off-diagonal panels. Default is 6.
+    fontsize : int, optional
+        The font size for the axis labels and ticks.
+    show_marginal_modes : bool, optional
+        Whether or not to show the modes of the marginal distributions. Default is True.
+
+    Returns
+    ----------
+    matplotlib.figure.Figure
+        Figure object containing the plots.
+    """
+
+    if not hasattr(idata, "posterior"):
+        raise ValueError("Argument idata should be an arviz idata object with a posterior group")
+    var_names = var_names or list(idata.posterior.data_vars)
+    k_params = len(var_names)
+
+    fig, ax = plt.subplots(k_params, k_params, figsize=figsize, dpi=dpi)
+
+    for i, axis in enumerate(fig.axes):
+        row = i // k_params
+        col = i % k_params
+
+        axis.ticklabel_format(axis="both", style="sci")
+        axis.yaxis.major.formatter.set_powerlimits((-2, 2))
+        axis.yaxis.offsetText.set_fontsize(fontsize)
+        axis.xaxis.major.formatter.set_powerlimits((-2, 2))
+        axis.xaxis.offsetText.set_fontsize(fontsize)
+        if col <= row:
+            if col == row:
+                v = var_names[col]
+                axis.hist(
+                    idata.posterior[v].values.ravel(),
+                    bins=hist_bins,
+                    histtype="step",
+                    density=True,
+                )
+                axis.set_yticklabels([])
+                axis.set_title(v, fontsize=fontsize)
+                axis.tick_params(
+                    axis="both",
+                    left=False,
+                    bottom=row == (k_params - 1),
+                    labelsize=fontsize,
+                )
+                if row != (k_params - 1):
+                    axis.set_xticklabels([])
+                    axis.tick_params(axis="x", which="both", bottom=False)
+
+            else:
+                x = var_names[col]
+                y = var_names[row]
+
+                data_x = idata.posterior[x].values.ravel()
+                data_y = idata.posterior[y].values.ravel()
+
+                # x_hist, edges = np.histogram(data_x, bins=hist_bins)
+                # x_mode = edges[np.argmax(x_hist)]
+                #
+                # y_hist, edges = np.histogram(data_y, bins=hist_bins)
+                # y_mode = edges[np.argmax(y_hist)]
+
+                H, y_edges, x_edges = np.histogram2d(data_y, data_x, bins=rug_bins)
+
+                ymax_idx, xmax_idx = np.where(H == H.max())
+                x_mode = x_edges[xmax_idx]
+                y_mode = y_edges[ymax_idx]
+                if len(x_mode) > 1:
+                    x_mode = x_mode[0]
+                if len(y_mode) > 1:
+                    y_mode = y_mode[0]
+
+                axis.contourf(x_edges[1:], y_edges[1:], H, cmap="Blues", levels=rug_levels)
+
+                if show_marginal_modes:
+                    axis.axvline(x_mode, ls="--", lw=0.5, color="k")
+                    axis.axhline(y_mode, ls="--", lw=0.5, color="k")
+                    axis.scatter(x_mode, y_mode, color="k", marker="s", s=20)
+
+                if col == 0:
+                    axis.set_ylabel(y, fontsize=fontsize)
+                else:
+                    axis.set_yticklabels([])
+                    axis.tick_params(axis="y", which="both", left=False)
+
+                if row != (k_params - 1):
+                    axis.set_xticklabels([])
+                    axis.tick_params(axis="x", which="both", bottom=False)
+                else:
+                    axis.set_xlabel(x, fontsize=fontsize)
+
+                axis.tick_params(axis="both", which="both", labelsize=fontsize)
+        else:
+            axis.set(xticks=[], yticks=[], xlabel="", ylabel="")
+            axis.set_visible(False)
+
+    fig.tight_layout(h_pad=0.1, w_pad=0.5)
+    plt.show()
+
+
+def plot_kalman_filter(
+    idata: xr.Dataset,
+    data: pd.DataFrame,
+    kalman_output: str = "predicted",
+    n_cols: Optional[int] = None,
+    vars_to_plot: Optional[List[str]] = None,
+    fig: Optional[Figure] = None,
+    figsize: Tuple[int, int] = (14, 6),
+    dpi: int = 144,
+    cmap: Optional[str] = None,
+):
+
+    """
+    Plot Kalman filter, prediction or smoothed series for variables in idata.
+
+    Parameters
+    ----------
+    idata : xarray.Dataset
+        Dataset with Kalman filter variables.
+    data : pandas.DataFrame
+        DataFrame with original time series data.
+    kalman_output : str, optional
+        String indicating whether to plot filtered, predicted, or smoothed series.
+        Must be one of 'filtered', 'predicted', or 'smoothed'.
+    n_cols : int, optional
+        Number of columns in the plot.
+    vars_to_plot : list of str, optional
+        List of variable names to plot.
+    fig : matplotlib.figure.Figure, optional
+        Matplotlib Figure object to plot on.
+    figsize : tuple of int, optional
+        Figure size in inches.
+    dpi : int, optional
+        Figure DPI.
+    cmap : str, optional
+        Colormap name.
+
+    Returns
+    -------
+    matplotlib.figure.Figure
+        Matplotlib Figure object with the plot.
+    """
+
+    if kalman_output.lower() not in ["filtered", "predicted", "smoothed"]:
+        raise ValueError(
+            f'kalman_output must be one of "filtered", "predicted", "smoothed". Found {kalman_output}.'
+        )
+    kalman_output = kalman_output.capitalize()
+
+    if fig is None:
+        fig = plt.figure(figsize=figsize, dpi=dpi)
+
+    if vars_to_plot is None:
+        vars_to_plot = idata.coords["variable"].values
+
+    n_plots = len(vars_to_plot)
+    n_cols = min(4, n_plots) if n_cols is None else n_cols
+
+    gs, plot_locs = prepare_gridspec_figure(n_cols, n_plots)
+    time_idx = idata.coords["time"]
+    time_slice = (
+        slice(None, None, None) if kalman_output.lower() == "predicted" else slice(1, None, None)
+    )
+
+    for idx, variable in enumerate(vars_to_plot):
+        axis = fig.add_subplot(gs[plot_locs[idx]])
+
+        mu = idata[f"{kalman_output}_State"].dropna(dim="time").sel(variable=variable)
+
+        q05, q50, q95 = mu.quantile([0.05, 0.5, 0.95], dim="sample")
+
+        sigma = (
+            idata[f"{kalman_output}_Cov"]
+            .dropna(dim="time")
+            .sel(variable=variable, variable2=variable)
+        )
+        s05, s95 = sigma.quantile([0.05, 0.95], dim="sample")
+
+        top_ci = mu + 1.98 * np.sqrt(s05 + 1e-6)
+        bot_ci = mu - 1.98 * np.sqrt(s95 + 1e-6)
+
+        axis.plot(time_idx[time_slice], q50.values, color="tab:red")
+        axis.fill_between(time_idx[time_slice], q05, q95, color="tab:blue", alpha=1)
+        axis.fill_between(
+            time_idx[time_slice],
+            top_ci.max(dim=["sample"]),
+            bot_ci.min(dim=["sample"]),
+            color="0.5",
+            alpha=0.5,
+        )
+
+        if variable in data.columns:
+            data[variable].plot(ax=axis, color="k", ls="--", lw=2)
+
+        axis.set(title=variable, xlabel=None, ylabel="% Deviation from SS")
+        axis.tick_params(axis="x", rotation=45)
+
+    fig.tight_layout()
```

### Comparing `gEconpy-1.1.0/gEconpy/sampling/prior_utilities.py` & `gEconpy-1.2.0/gEconpy/sampling/prior_utilities.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,272 +1,272 @@
-import numpy as np
-import pandas as pd
-import xarray as xr
-from numpy.linalg import LinAlgError
-
-from gEconpy.classes.progress_bar import ProgressBar
-from gEconpy.estimation.estimate import build_Q_and_H, build_Z_matrix
-from gEconpy.estimation.estimation_utilities import split_random_variables
-from gEconpy.estimation.kalman_filter import kalman_filter
-from gEconpy.estimation.kalman_smoother import kalman_smoother
-
-
-def prior_solvability_check(
-    model, n_samples, seed=None, param_subset=None, pert_solver="cycle_reduction"
-):
-    data = pd.DataFrame(
-        model.sample_param_dict_from_prior(n_samples, seed, param_subset, sample_shock_sigma=True)
-    )
-    progress_bar = ProgressBar(n_samples, verb="Sampling")
-
-    def check_solvable(param_dict):
-        try:
-            ss_dict, calib_dict = model.f_ss(param_dict)
-            resids = model.f_ss_resid(**ss_dict, **calib_dict, **param_dict)
-            ss_success = (np.array(resids) ** 2).sum() < 1e-8
-        except ValueError:
-            return "steady_state"
-
-        if not ss_success:
-            return "steady_state"
-
-        try:
-            max_iter = 1000
-            tol = 1e-18
-            verbose = False
-
-            A, B, C, D = model.build_perturbation_matrices(**param_dict, **ss_dict)
-            if pert_solver == "cycle_reduction":
-                solver = model.perturbation_solver.solve_policy_function_with_cycle_reduction
-                T, R, result, log_norm = solver(A, B, C, D, max_iter, tol, verbose)
-                pert_success = log_norm < 1e-8
-
-            elif pert_solver == "gensys":
-                solver = model.perturbation_solver.solve_policy_function_with_gensys
-                G_1, constant, impact, f_mat, f_wt, y_wt, gev, eu, loose = solver(
-                    A, B, C, D, tol, verbose
-                )
-                T = G_1[: model.n_variables, :][:, : model.n_variables]
-                R = impact[: model.n_variables, :]
-                pert_success = G_1 is not None
-
-            else:
-                raise NotImplementedError
-
-        except (ValueError, LinAlgError):
-            return "perturbation"
-
-        if not pert_success:
-            return "perturbation"
-
-        bk_success = model.check_bk_condition(
-            system_matrices=[A, B, C, D], verbose=False, return_value="bool"
-        )
-        if not bk_success:
-            return "blanchard-kahn"
-
-        (
-            _,
-            variables,
-            _,
-        ) = model.perturbation_solver.make_all_variable_time_combinations()
-        gEcon_matrices = model.perturbation_solver.statespace_to_gEcon_representation(
-            A, T, R, variables, tol
-        )
-        P, Q, _, _, A_prime, R_prime, S_prime = gEcon_matrices
-
-        resid_norms = model.perturbation_solver.residual_norms(
-            B, C, D, Q, P, A_prime, R_prime, S_prime
-        )
-        norm_deterministic, norm_stochastic = resid_norms
-
-        if norm_deterministic > 1e-8:
-            return "deterministic_norm"
-        if norm_stochastic > 1e-8:
-            return "stochastic_norm"
-
-        return None
-
-    param_dicts = data.T.to_dict().values()
-    results = []
-
-    # TODO: How to parallelize this? The problem is the huge model object causes massive overhead.
-    for param_dict in param_dicts:
-        progress_bar.start()
-        result = check_solvable(param_dict)
-        results.append(result)
-        progress_bar.stop()
-
-    data["failure_step"] = results
-
-    return data
-
-
-def get_initial_time_index(df):
-    t0 = df.index[0]
-    freq = df.index.inferred_freq
-    base_freq = freq.split("-")[0]
-
-    if "Q" in base_freq:
-        offset = pd.DateOffset(months=3)
-    elif "M" in base_freq:
-        offset = pd.DateOffset(months=1)
-    elif "A" in base_freq:
-        offset = pd.DateOffset(years=1)
-    else:
-        raise NotImplementedError("Data isn't one of: Quarterly, Monthly, Annual")
-
-    return t0 - offset
-
-
-def simulate_trajectories_from_prior(
-    model, n_samples=1000, n_simulations=100, simulation_length=40
-):
-    simulations = []
-    model_var_names = [x.base_name for x in model.variables]
-    shock_names = [x.name for x in model.shocks]
-
-    param_dicts = pd.DataFrame(model.sample_param_dict_from_prior(n_samples)).T.to_dict()
-    i = 0
-
-    progress_bar = ProgressBar(n_samples, "Sampling")
-    for param_dict in param_dicts.values():
-        # free_param_dict, shock_dict, obs_dict = split_random_variables(param_dict, shock_names, model_var_names)
-        model.free_param_dict.update(param_dict)
-        progress_bar.start()
-
-        try:
-            model.steady_state(verbose=False)
-            model.solve_model(verbose=False, on_failure="ignore")
-
-            data = model.simulate(
-                simulation_length=simulation_length,
-                n_simulations=n_simulations,
-                show_progress_bar=False,
-            )
-            simulaton_ids = np.arange(n_simulations).astype(int)
-
-            data = data.rename(
-                axis=1,
-                level=1,
-                mapper=dict(zip(simulaton_ids, simulaton_ids + (n_simulations * i))),
-            )
-
-            simulations.append(data)
-            i += 1
-
-        except ValueError:
-            continue
-
-        finally:
-            progress_bar.stop()
-
-    simulations = pd.concat(simulations, axis=1)
-    return simulations
-
-
-def kalman_filter_from_prior(model, data, n_samples, filter_type="univariate"):
-    observed_vars = data.columns.tolist()
-    model_var_names = [x.base_name for x in model.variables]
-    shock_names = [x.base_name for x in model.shocks]
-
-    results = []
-    param_dicts = pd.DataFrame(
-        model.sample_param_dict_from_prior(n_samples, sample_shock_sigma=True)
-    ).T.to_dict()
-
-    progress_bar = ProgressBar(n_samples, "Sampling")
-    i = 0
-
-    while i < n_samples:
-        try:
-            param_dict = param_dicts[i]
-            param_dict, shock_dict, obs_dict = split_random_variables(
-                param_dict, shock_names, observed_vars
-            )
-            model.free_param_dict.update(param_dict)
-
-            progress_bar.start()
-            model.steady_state(verbose=False)
-            model.solve_model(verbose=False, on_failure="raise")
-
-            T, R = model.T.values, model.R.values
-            Z = build_Z_matrix(observed_vars, model_var_names)
-            Q, H = build_Q_and_H(shock_dict, shock_names, observed_vars, obs_dict)
-
-            filter_results = kalman_filter(
-                data.values, T, Z, R, H, Q, a0=None, P0=None, filter_type=filter_type
-            )
-            filtered_states, _, filtered_covariances, *_ = filter_results
-
-            smoother_results = kalman_smoother(T, R, Q, filtered_states, filtered_covariances)
-            results.append(list(filter_results) + list(smoother_results))
-
-            i += 1
-            progress_bar.stop()
-        except ValueError:
-            continue
-
-    coords = {
-        "sample": np.arange(n_samples),
-        "time": data.index.values,
-        "variable": model_var_names,
-    }
-
-    pred_coords = {
-        "sample": np.arange(n_samples),
-        "time": np.r_[
-            np.array(get_initial_time_index(data), dtype="datetime64"),
-            data.index.values,
-        ],
-        "variable": model_var_names,
-    }
-
-    cov_coords = {
-        "sample": np.arange(n_samples),
-        "time": data.index.values,
-        "variable": model_var_names,
-        "variable2": model_var_names,
-    }
-
-    pred_cov_coords = {
-        "sample": np.arange(n_samples),
-        "time": np.r_[
-            np.array(get_initial_time_index(data), dtype="datetime64"),
-            data.index.values,
-        ],
-        "variable": model_var_names,
-        "variable2": model_var_names,
-    }
-
-    kf_data = xr.Dataset(
-        {
-            "Filtered_State": xr.DataArray(
-                data=np.stack([results[i][0] for i in range(n_samples)]), coords=coords
-            ),
-            "Predicted_State": xr.DataArray(
-                data=np.stack([results[i][1] for i in range(n_samples)]),
-                coords=pred_coords,
-            ),
-            "Smoothed_State": xr.DataArray(
-                data=np.stack([results[i][5] for i in range(n_samples)]), coords=coords
-            ),
-            "Filtered_Cov": xr.DataArray(
-                data=np.stack([results[i][2] for i in range(n_samples)]),
-                coords=cov_coords,
-            ),
-            "Predicted_Cov": xr.DataArray(
-                data=np.stack([results[i][3] for i in range(n_samples)]),
-                coords=pred_cov_coords,
-            ),
-            "Smoothed_Cov": xr.DataArray(
-                data=np.stack([results[i][6] for i in range(n_samples)]),
-                coords=cov_coords,
-            ),
-            "loglikelihood": xr.DataArray(
-                data=np.stack([results[i][4] for i in range(n_samples)]),
-                coords={"sample": np.arange(n_samples), "time": data.index.values},
-            ),
-        }
-    )
-
-    return kf_data
+import numpy as np
+import pandas as pd
+import xarray as xr
+from numpy.linalg import LinAlgError
+
+from gEconpy.classes.progress_bar import ProgressBar
+from gEconpy.estimation.estimate import build_Q_and_H, build_Z_matrix
+from gEconpy.estimation.estimation_utilities import split_random_variables
+from gEconpy.estimation.kalman_filter import kalman_filter
+from gEconpy.estimation.kalman_smoother import kalman_smoother
+
+
+def prior_solvability_check(
+    model, n_samples, seed=None, param_subset=None, pert_solver="cycle_reduction"
+):
+    data = pd.DataFrame(
+        model.sample_param_dict_from_prior(n_samples, seed, param_subset, sample_shock_sigma=True)
+    )
+    progress_bar = ProgressBar(n_samples, verb="Sampling")
+
+    def check_solvable(param_dict):
+        try:
+            ss_dict, calib_dict = model.f_ss(param_dict)
+            resids = model.f_ss_resid(**ss_dict, **calib_dict, **param_dict)
+            ss_success = (np.array(resids) ** 2).sum() < 1e-8
+        except ValueError:
+            return "steady_state"
+
+        if not ss_success:
+            return "steady_state"
+
+        try:
+            max_iter = 1000
+            tol = 1e-18
+            verbose = False
+
+            A, B, C, D = model.build_perturbation_matrices(**param_dict, **ss_dict)
+            if pert_solver == "cycle_reduction":
+                solver = model.perturbation_solver.solve_policy_function_with_cycle_reduction
+                T, R, result, log_norm = solver(A, B, C, D, max_iter, tol, verbose)
+                pert_success = log_norm < 1e-8
+
+            elif pert_solver == "gensys":
+                solver = model.perturbation_solver.solve_policy_function_with_gensys
+                G_1, constant, impact, f_mat, f_wt, y_wt, gev, eu, loose = solver(
+                    A, B, C, D, tol, verbose
+                )
+                T = G_1[: model.n_variables, :][:, : model.n_variables]
+                R = impact[: model.n_variables, :]
+                pert_success = G_1 is not None
+
+            else:
+                raise NotImplementedError
+
+        except (ValueError, LinAlgError):
+            return "perturbation"
+
+        if not pert_success:
+            return "perturbation"
+
+        bk_success = model.check_bk_condition(
+            system_matrices=[A, B, C, D], verbose=False, return_value="bool"
+        )
+        if not bk_success:
+            return "blanchard-kahn"
+
+        (
+            _,
+            variables,
+            _,
+        ) = model.perturbation_solver.make_all_variable_time_combinations()
+        gEcon_matrices = model.perturbation_solver.statespace_to_gEcon_representation(
+            A, T, R, variables, tol
+        )
+        P, Q, _, _, A_prime, R_prime, S_prime = gEcon_matrices
+
+        resid_norms = model.perturbation_solver.residual_norms(
+            B, C, D, Q, P, A_prime, R_prime, S_prime
+        )
+        norm_deterministic, norm_stochastic = resid_norms
+
+        if norm_deterministic > 1e-8:
+            return "deterministic_norm"
+        if norm_stochastic > 1e-8:
+            return "stochastic_norm"
+
+        return None
+
+    param_dicts = data.T.to_dict().values()
+    results = []
+
+    # TODO: How to parallelize this? The problem is the huge model object causes massive overhead.
+    for param_dict in param_dicts:
+        progress_bar.start()
+        result = check_solvable(param_dict)
+        results.append(result)
+        progress_bar.stop()
+
+    data["failure_step"] = results
+
+    return data
+
+
+def get_initial_time_index(df):
+    t0 = df.index[0]
+    freq = df.index.inferred_freq
+    base_freq = freq.split("-")[0]
+
+    if "Q" in base_freq:
+        offset = pd.DateOffset(months=3)
+    elif "M" in base_freq:
+        offset = pd.DateOffset(months=1)
+    elif "A" in base_freq:
+        offset = pd.DateOffset(years=1)
+    else:
+        raise NotImplementedError("Data isn't one of: Quarterly, Monthly, Annual")
+
+    return t0 - offset
+
+
+def simulate_trajectories_from_prior(
+    model, n_samples=1000, n_simulations=100, simulation_length=40
+):
+    simulations = []
+    model_var_names = [x.base_name for x in model.variables]
+    shock_names = [x.name for x in model.shocks]
+
+    param_dicts = pd.DataFrame(model.sample_param_dict_from_prior(n_samples)).T.to_dict()
+    i = 0
+
+    progress_bar = ProgressBar(n_samples, "Sampling")
+    for param_dict in param_dicts.values():
+        # free_param_dict, shock_dict, obs_dict = split_random_variables(param_dict, shock_names, model_var_names)
+        model.free_param_dict.update(param_dict)
+        progress_bar.start()
+
+        try:
+            model.steady_state(verbose=False)
+            model.solve_model(verbose=False, on_failure="ignore")
+
+            data = model.simulate(
+                simulation_length=simulation_length,
+                n_simulations=n_simulations,
+                show_progress_bar=False,
+            )
+            simulaton_ids = np.arange(n_simulations).astype(int)
+
+            data = data.rename(
+                axis=1,
+                level=1,
+                mapper=dict(zip(simulaton_ids, simulaton_ids + (n_simulations * i))),
+            )
+
+            simulations.append(data)
+            i += 1
+
+        except ValueError:
+            continue
+
+        finally:
+            progress_bar.stop()
+
+    simulations = pd.concat(simulations, axis=1)
+    return simulations
+
+
+def kalman_filter_from_prior(model, data, n_samples, filter_type="univariate"):
+    observed_vars = data.columns.tolist()
+    model_var_names = [x.base_name for x in model.variables]
+    shock_names = [x.base_name for x in model.shocks]
+
+    results = []
+    param_dicts = pd.DataFrame(
+        model.sample_param_dict_from_prior(n_samples, sample_shock_sigma=True)
+    ).T.to_dict()
+
+    progress_bar = ProgressBar(n_samples, "Sampling")
+    i = 0
+
+    while i < n_samples:
+        try:
+            param_dict = param_dicts[i]
+            param_dict, shock_dict, obs_dict = split_random_variables(
+                param_dict, shock_names, observed_vars
+            )
+            model.free_param_dict.update(param_dict)
+
+            progress_bar.start()
+            model.steady_state(verbose=False)
+            model.solve_model(verbose=False, on_failure="raise")
+
+            T, R = model.T.values, model.R.values
+            Z = build_Z_matrix(observed_vars, model_var_names)
+            Q, H = build_Q_and_H(shock_dict, shock_names, observed_vars, obs_dict)
+
+            filter_results = kalman_filter(
+                data.values, T, Z, R, H, Q, a0=None, P0=None, filter_type=filter_type
+            )
+            filtered_states, _, filtered_covariances, *_ = filter_results
+
+            smoother_results = kalman_smoother(T, R, Q, filtered_states, filtered_covariances)
+            results.append(list(filter_results) + list(smoother_results))
+
+            i += 1
+            progress_bar.stop()
+        except ValueError:
+            continue
+
+    coords = {
+        "sample": np.arange(n_samples),
+        "time": data.index.values,
+        "variable": model_var_names,
+    }
+
+    pred_coords = {
+        "sample": np.arange(n_samples),
+        "time": np.r_[
+            np.array(get_initial_time_index(data), dtype="datetime64"),
+            data.index.values,
+        ],
+        "variable": model_var_names,
+    }
+
+    cov_coords = {
+        "sample": np.arange(n_samples),
+        "time": data.index.values,
+        "variable": model_var_names,
+        "variable2": model_var_names,
+    }
+
+    pred_cov_coords = {
+        "sample": np.arange(n_samples),
+        "time": np.r_[
+            np.array(get_initial_time_index(data), dtype="datetime64"),
+            data.index.values,
+        ],
+        "variable": model_var_names,
+        "variable2": model_var_names,
+    }
+
+    kf_data = xr.Dataset(
+        {
+            "Filtered_State": xr.DataArray(
+                data=np.stack([results[i][0] for i in range(n_samples)]), coords=coords
+            ),
+            "Predicted_State": xr.DataArray(
+                data=np.stack([results[i][1] for i in range(n_samples)]),
+                coords=pred_coords,
+            ),
+            "Smoothed_State": xr.DataArray(
+                data=np.stack([results[i][5] for i in range(n_samples)]), coords=coords
+            ),
+            "Filtered_Cov": xr.DataArray(
+                data=np.stack([results[i][2] for i in range(n_samples)]),
+                coords=cov_coords,
+            ),
+            "Predicted_Cov": xr.DataArray(
+                data=np.stack([results[i][3] for i in range(n_samples)]),
+                coords=pred_cov_coords,
+            ),
+            "Smoothed_Cov": xr.DataArray(
+                data=np.stack([results[i][6] for i in range(n_samples)]),
+                coords=cov_coords,
+            ),
+            "loglikelihood": xr.DataArray(
+                data=np.stack([results[i][4] for i in range(n_samples)]),
+                coords={"sample": np.arange(n_samples), "time": data.index.values},
+            ),
+        }
+    )
+
+    return kf_data
```

### Comparing `gEconpy-1.1.0/gEconpy/shared/dynare_convert.py` & `gEconpy-1.2.0/gEconpy/shared/dynare_convert.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,351 +1,351 @@
-import re
-from typing import Dict, List, Tuple, Union
-
-import sympy as sp
-from sympy.abc import greeks
-
-from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
-from gEconpy.shared.utilities import make_all_var_time_combos, string_keys_to_sympy
-
-OPERATORS = list("+-/*^()=")
-
-
-def get_name(x: Union[str, sp.Symbol]) -> str:
-    """
-    This function returns the name of a string, TimeAwareSymbol, or sp.Symbol object.
-
-    Parameters
-    ----------
-    x : str, or sp.Symbol
-        The object whose name is to be returned. If str, x is directly returned.
-
-    Returns
-    -------
-    str
-        The name of the object.
-    """
-
-    if isinstance(x, str):
-        return x
-
-    elif isinstance(x, TimeAwareSymbol):
-        return x.safe_name
-
-    elif isinstance(x, sp.Symbol):
-        return x.name
-
-
-def build_hash_table(
-    items_to_hash: List[Union[str, sp.Symbol]]
-) -> Tuple[Dict[str, str], Dict[str, str]]:
-    """
-    This function builds a pair of hash tables, one mapping variable names to hash values
-    and the other mapping hash values to variable names.
-
-    To safely distinguish between numeric values, variables, parameters, and time-indices
-    when converting sympy code to a Dynare model, all variables are first hashed to
-    strictly positive int64 objects using the square of the built-in `hash` function.
-
-    Parameters
-    ----------
-    items_to_hash : str or sp.Symbol
-        A list of variables to be hashed. Can contain strings or sp.Symbol objects.
-
-    Returns
-    -------
-    tuple of (dict, dict)
-        A tuple containing two dictionaries: the first maps variable names to
-         hash values, and the second maps hash values to variable names.
-    """
-
-    var_to_hash = {}
-    hash_to_var = {}
-    name_list = [get_name(x) for x in items_to_hash]
-    for thing in sorted(name_list, key=len, reverse=True):
-        # ensure the hash value is positive so the minus sign isn't confused as part of the equation
-        hashkey = str(hash(thing) ** 2)
-        var_to_hash[thing] = hashkey
-        hash_to_var[hashkey] = thing
-
-    return var_to_hash, hash_to_var
-
-
-def substitute_equation_from_dict(eq_str: str, hash_dict: Dict[str, str]) -> str:
-    """
-    This function substitutes variables in an equation string with their corresponding values from a dictionary.
-
-    Parameters
-    ----------
-    eq_str : str
-        The equation string containing variables to be replaced.
-    hash_dict : Dict[str, str]
-        A dictionary mapping variables to their corresponding values.
-
-    Returns
-    -------
-    str
-        The equation string with the variables replaced by their values.
-    """
-    for key, value in hash_dict.items():
-        eq_str = eq_str.replace(key, value)
-    return eq_str
-
-
-def make_var_to_matlab_sub_dict(
-    var_list: List[Union[str, TimeAwareSymbol, sp.Symbol]], clash_prefix: str = "a"
-) -> Dict[Union[str, TimeAwareSymbol, sp.Symbol], str]:
-    """
-    This function builds a dictionary that maps variables to their corresponding names that
-    can be used in a Matlab script.
-
-    Parameters
-    ----------
-    var_list : List[Union[str, TimeAwareSymbol, sp.Symbol]]
-        A list of variables to be mapped. Can contain strings, TimeAwareSymbol objects,
-         or sp.Symbol objects.
-    clash_prefix : str, optional
-        A prefix to add to the names of variables that might clash with Matlab keywords
-        (e.g. greek letters). Default is 'a'.
-
-    Returns
-    -------
-    Dict[Union[str, TimeAwareSymbol, sp.Symbol], str]
-        A dictionary mapping the variables in `var_list` to their corresponding
-        names that can be used in a Matlab script.
-
-    Examples
-    --------
-    .. code-block:: py
-        make_var_to_matlab_sub_dict([sp.Symbol('beta')])
-        # {sp.Symbol('beta'): 'abeta'}
-    """
-
-    sub_dict = {}
-
-    for var in var_list:
-        if isinstance(var, str):
-            var_name = var if var.lower() not in greeks else clash_prefix + var
-        elif isinstance(var, TimeAwareSymbol):
-            var_name = (
-                var.base_name
-                if var.base_name.lower() not in greeks
-                else clash_prefix + var.base_name
-            )
-            time_index = var.safe_name.split("_")[-1]
-            var_name += f"_{time_index}"
-        elif isinstance(var, sp.Symbol):
-            var_name = var.name if var.name.lower() not in greeks else clash_prefix + var.name
-        else:
-            raise ValueError("var_list should contain only strings, symbols, or TimeAwareSymbols")
-
-        sub_dict[var] = var_name
-
-    return sub_dict
-
-
-def convert_var_timings_to_matlab(var_list: List[str]) -> List[str]:
-    """
-    This function converts the timing notation in a list of variable names to a
-    form that can be used in a Dynare mod file.
-
-    Parameters
-    ----------
-    var_list : list of str
-        A list of variable names with "mathematical" timing notation (e.g. '_t+1', '_t-1', '_t').
-
-    Returns
-    -------
-    list of str
-        A list of variable names with the timing notation converted to a
-        form that can be used in a Dynare mod file (e.g. '(1)', '(-1)', '').
-    """
-    matlab_var_list = [
-        var.replace("_t+1", "(1)").replace("_t-1", "(-1)").replace("_t", "") for var in var_list
-    ]
-
-    return matlab_var_list
-
-
-def write_lines_from_list(l: List[str], file: str, line_start: str = "", line_max: int = 50) -> str:
-    """
-    This function writes a list of items to a string, inserting line
-    breaks at a specified maximum line length.
-
-    Parameters
-    ----------
-    l : list of strings
-        A list of items to be written to the string.
-    file : str
-        A string to which the items will be appended.
-    line_start : str, optional
-        A string to be prepended to each line. Default is an empty string.
-    line_max : int, optional
-        The maximum line length. Default is 50.
-
-    Returns
-    -------
-    str
-        The modified `file` string with the items from `l` appended to it.
-    """
-
-    line = line_start
-    for item in sorted(l):
-        line += f" {item},"
-        if len(line) > line_max:
-            line = line[:-1]
-            line = line + ";\n"
-            file += line
-            line = line_start
-
-    if line != line_start:
-        line = line[:-1]
-        file += line + ";\n"
-
-    return file
-
-
-UNDER_T_PATTERN = r"_t(?=[^\w]|$)"
-
-
-def make_mod_file(model) -> str:
-    """
-    This function generates a string representation of a Dynare model file for
-    a dynamic stochastic general equilibrium (DSGE) model. For more information,
-    see [1].
-
-    Parameters
-    ----------
-    model : gEconModel
-        A gEconModel object with solved steady state.
-
-    Returns
-    -------
-    str
-        A string representation of a Dynare model file.
-
-    References
-    ----------
-    ..[1] Adjemian, Stéphane, et al. "Dynare: Reference manual, version 4." (2011).
-    """
-
-    var_list = model.variables
-    param_dict = model.free_param_dict
-    param_dict.update(model.calib_param_dict)
-
-    shocks = model.shocks
-    ss_value_dict = model.steady_state_dict
-
-    var_to_matlab = make_var_to_matlab_sub_dict(
-        make_all_var_time_combos(var_list), clash_prefix="var_"
-    )
-    par_to_matlab = make_var_to_matlab_sub_dict(param_dict.keys(), clash_prefix="param_")
-    shock_to_matlab = make_var_to_matlab_sub_dict(shocks, clash_prefix="exog_")
-
-    items_to_hash = (
-        list(var_to_matlab.keys()) + list(par_to_matlab.keys()) + list(shock_to_matlab.keys())
-    )
-
-    file = ""
-    file = write_lines_from_list(
-        [re.sub(UNDER_T_PATTERN, "", var_to_matlab[x]) for x in model.variables],
-        file,
-        line_start="var",
-    )
-    file = write_lines_from_list(
-        [re.sub(UNDER_T_PATTERN, "", x) for x in shock_to_matlab.values()],
-        file,
-        line_start="varexo",
-    )
-    file += "\n"
-    file = write_lines_from_list(par_to_matlab.values(), file, line_start="parameters")
-    file += "\n"
-
-    for model_param in sorted(param_dict.keys()):
-        matlab_param = par_to_matlab[model_param]
-        value = param_dict[model_param]
-        file += f"{matlab_param} = {value};\n"
-
-    file += "\n"
-    file += "model;\n"
-    for var, val in ss_value_dict.items():
-        if var in var_to_matlab.keys():
-            matlab_var = var_to_matlab[var]
-            file += f"#{matlab_var}_ss = {val:0.4f};\n"
-
-    for eq in model.system_equations:
-        matlab_subdict = {}
-
-        for atom in eq.atoms():
-            if not isinstance(atom, TimeAwareSymbol) and isinstance(atom, sp.core.Symbol):
-                if atom in par_to_matlab.keys():
-                    matlab_subdict[atom] = sp.Symbol(par_to_matlab[atom])
-            elif isinstance(atom, TimeAwareSymbol):
-                if atom in var_to_matlab.keys():
-                    matlab_subdict[atom] = var_to_matlab[atom]
-                elif atom in shock_to_matlab.keys():
-                    matlab_subdict[atom] = shock_to_matlab[atom]
-
-        eq_str = eq.subs(matlab_subdict)
-        eq_str = str(eq_str)
-        prepare_eq = eq_str.replace("**", "^")
-        var_to_hash, hash_to_var = build_hash_table(items_to_hash)
-
-        hash_eq = substitute_equation_from_dict(prepare_eq, var_to_hash)
-
-        for operator in OPERATORS:
-            hash_eq = hash_eq.replace(operator, " " + operator + " ")
-        hash_eq = re.sub(" +", " ", hash_eq)
-        hash_eq = hash_eq.strip()
-        final_eq = substitute_equation_from_dict(hash_eq, hash_to_var)
-
-        matlab_eq = final_eq.replace("_tp1", "(1)").replace("_tm1", "(-1)")
-        split_eq = matlab_eq.split(" ")
-
-        new_eq = []
-        for atom in split_eq:
-            if atom in par_to_matlab.keys():
-                atom = par_to_matlab[atom]
-            elif atom in var_to_matlab.keys():
-                atom = var_to_matlab[atom]
-            elif atom in shock_to_matlab.keys():
-                atom = shock_to_matlab[atom]
-
-            new_eq.append(atom)
-
-        matlab_eq = ""
-        for i, atom in enumerate(new_eq):
-            if i == 0:
-                matlab_eq += atom
-            elif i == 1 and new_eq[0] == "-":
-                matlab_eq += atom
-            else:
-                if atom in "()":
-                    matlab_eq += atom
-                elif new_eq[i - 1] in "(":
-                    matlab_eq += atom
-                else:
-                    matlab_eq += " " + atom
-        matlab_eq += " = 0;"
-        matlab_eq = re.sub(UNDER_T_PATTERN, "", matlab_eq)
-
-        file += matlab_eq + "\n"
-
-    file += "end;\n\n"
-
-    file += "initval;\n"
-    for var, val in string_keys_to_sympy(ss_value_dict).items():
-        matlab_var = var_to_matlab[var].replace("_ss", "")
-        file += f"{matlab_var} = {val:0.4f};\n"
-
-    file += "end;\n\n"
-    file += "steady;\n"
-    file += "check(qz_zero_threshold=1e-20);\n\n"
-
-    file += "shocks;\n"
-    for shock in shocks:
-        file += "var " + re.sub(UNDER_T_PATTERN, "", shock_to_matlab[shock]) + ";\n"
-        file += "stderr 0.01;\n"
-    file += "end;\n\n"
-    file += "stoch_simul(order=1, irf=100, qz_zero_threshold=1e-20);"
-
-    return file
+import re
+from typing import Dict, List, Tuple, Union
+
+import sympy as sp
+from sympy.abc import greeks
+
+from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
+from gEconpy.shared.utilities import make_all_var_time_combos, string_keys_to_sympy
+
+OPERATORS = list("+-/*^()=")
+
+
+def get_name(x: Union[str, sp.Symbol]) -> str:
+    """
+    This function returns the name of a string, TimeAwareSymbol, or sp.Symbol object.
+
+    Parameters
+    ----------
+    x : str, or sp.Symbol
+        The object whose name is to be returned. If str, x is directly returned.
+
+    Returns
+    -------
+    str
+        The name of the object.
+    """
+
+    if isinstance(x, str):
+        return x
+
+    elif isinstance(x, TimeAwareSymbol):
+        return x.safe_name
+
+    elif isinstance(x, sp.Symbol):
+        return x.name
+
+
+def build_hash_table(
+    items_to_hash: List[Union[str, sp.Symbol]]
+) -> Tuple[Dict[str, str], Dict[str, str]]:
+    """
+    This function builds a pair of hash tables, one mapping variable names to hash values
+    and the other mapping hash values to variable names.
+
+    To safely distinguish between numeric values, variables, parameters, and time-indices
+    when converting sympy code to a Dynare model, all variables are first hashed to
+    strictly positive int64 objects using the square of the built-in `hash` function.
+
+    Parameters
+    ----------
+    items_to_hash : str or sp.Symbol
+        A list of variables to be hashed. Can contain strings or sp.Symbol objects.
+
+    Returns
+    -------
+    tuple of (dict, dict)
+        A tuple containing two dictionaries: the first maps variable names to
+         hash values, and the second maps hash values to variable names.
+    """
+
+    var_to_hash = {}
+    hash_to_var = {}
+    name_list = [get_name(x) for x in items_to_hash]
+    for thing in sorted(name_list, key=len, reverse=True):
+        # ensure the hash value is positive so the minus sign isn't confused as part of the equation
+        hashkey = str(hash(thing) ** 2)
+        var_to_hash[thing] = hashkey
+        hash_to_var[hashkey] = thing
+
+    return var_to_hash, hash_to_var
+
+
+def substitute_equation_from_dict(eq_str: str, hash_dict: Dict[str, str]) -> str:
+    """
+    This function substitutes variables in an equation string with their corresponding values from a dictionary.
+
+    Parameters
+    ----------
+    eq_str : str
+        The equation string containing variables to be replaced.
+    hash_dict : Dict[str, str]
+        A dictionary mapping variables to their corresponding values.
+
+    Returns
+    -------
+    str
+        The equation string with the variables replaced by their values.
+    """
+    for key, value in hash_dict.items():
+        eq_str = eq_str.replace(key, value)
+    return eq_str
+
+
+def make_var_to_matlab_sub_dict(
+    var_list: List[Union[str, TimeAwareSymbol, sp.Symbol]], clash_prefix: str = "a"
+) -> Dict[Union[str, TimeAwareSymbol, sp.Symbol], str]:
+    """
+    This function builds a dictionary that maps variables to their corresponding names that
+    can be used in a Matlab script.
+
+    Parameters
+    ----------
+    var_list : List[Union[str, TimeAwareSymbol, sp.Symbol]]
+        A list of variables to be mapped. Can contain strings, TimeAwareSymbol objects,
+         or sp.Symbol objects.
+    clash_prefix : str, optional
+        A prefix to add to the names of variables that might clash with Matlab keywords
+        (e.g. greek letters). Default is 'a'.
+
+    Returns
+    -------
+    Dict[Union[str, TimeAwareSymbol, sp.Symbol], str]
+        A dictionary mapping the variables in `var_list` to their corresponding
+        names that can be used in a Matlab script.
+
+    Examples
+    --------
+    .. code-block:: py
+        make_var_to_matlab_sub_dict([sp.Symbol('beta')])
+        # {sp.Symbol('beta'): 'abeta'}
+    """
+
+    sub_dict = {}
+
+    for var in var_list:
+        if isinstance(var, str):
+            var_name = var if var.lower() not in greeks else clash_prefix + var
+        elif isinstance(var, TimeAwareSymbol):
+            var_name = (
+                var.base_name
+                if var.base_name.lower() not in greeks
+                else clash_prefix + var.base_name
+            )
+            time_index = var.safe_name.split("_")[-1]
+            var_name += f"_{time_index}"
+        elif isinstance(var, sp.Symbol):
+            var_name = var.name if var.name.lower() not in greeks else clash_prefix + var.name
+        else:
+            raise ValueError("var_list should contain only strings, symbols, or TimeAwareSymbols")
+
+        sub_dict[var] = var_name
+
+    return sub_dict
+
+
+def convert_var_timings_to_matlab(var_list: List[str]) -> List[str]:
+    """
+    This function converts the timing notation in a list of variable names to a
+    form that can be used in a Dynare mod file.
+
+    Parameters
+    ----------
+    var_list : list of str
+        A list of variable names with "mathematical" timing notation (e.g. '_t+1', '_t-1', '_t').
+
+    Returns
+    -------
+    list of str
+        A list of variable names with the timing notation converted to a
+        form that can be used in a Dynare mod file (e.g. '(1)', '(-1)', '').
+    """
+    matlab_var_list = [
+        var.replace("_t+1", "(1)").replace("_t-1", "(-1)").replace("_t", "") for var in var_list
+    ]
+
+    return matlab_var_list
+
+
+def write_lines_from_list(l: List[str], file: str, line_start: str = "", line_max: int = 50) -> str:
+    """
+    This function writes a list of items to a string, inserting line
+    breaks at a specified maximum line length.
+
+    Parameters
+    ----------
+    l : list of strings
+        A list of items to be written to the string.
+    file : str
+        A string to which the items will be appended.
+    line_start : str, optional
+        A string to be prepended to each line. Default is an empty string.
+    line_max : int, optional
+        The maximum line length. Default is 50.
+
+    Returns
+    -------
+    str
+        The modified `file` string with the items from `l` appended to it.
+    """
+
+    line = line_start
+    for item in sorted(l):
+        line += f" {item},"
+        if len(line) > line_max:
+            line = line[:-1]
+            line = line + ";\n"
+            file += line
+            line = line_start
+
+    if line != line_start:
+        line = line[:-1]
+        file += line + ";\n"
+
+    return file
+
+
+UNDER_T_PATTERN = r"_t(?=[^\w]|$)"
+
+
+def make_mod_file(model) -> str:
+    """
+    This function generates a string representation of a Dynare model file for
+    a dynamic stochastic general equilibrium (DSGE) model. For more information,
+    see [1].
+
+    Parameters
+    ----------
+    model : gEconModel
+        A gEconModel object with solved steady state.
+
+    Returns
+    -------
+    str
+        A string representation of a Dynare model file.
+
+    References
+    ----------
+    ..[1] Adjemian, Stéphane, et al. "Dynare: Reference manual, version 4." (2011).
+    """
+
+    var_list = model.variables
+    param_dict = model.free_param_dict
+    param_dict.update(model.calib_param_dict)
+
+    shocks = model.shocks
+    ss_value_dict = model.steady_state_dict
+
+    var_to_matlab = make_var_to_matlab_sub_dict(
+        make_all_var_time_combos(var_list), clash_prefix="var_"
+    )
+    par_to_matlab = make_var_to_matlab_sub_dict(param_dict.keys(), clash_prefix="param_")
+    shock_to_matlab = make_var_to_matlab_sub_dict(shocks, clash_prefix="exog_")
+
+    items_to_hash = (
+        list(var_to_matlab.keys()) + list(par_to_matlab.keys()) + list(shock_to_matlab.keys())
+    )
+
+    file = ""
+    file = write_lines_from_list(
+        [re.sub(UNDER_T_PATTERN, "", var_to_matlab[x]) for x in model.variables],
+        file,
+        line_start="var",
+    )
+    file = write_lines_from_list(
+        [re.sub(UNDER_T_PATTERN, "", x) for x in shock_to_matlab.values()],
+        file,
+        line_start="varexo",
+    )
+    file += "\n"
+    file = write_lines_from_list(par_to_matlab.values(), file, line_start="parameters")
+    file += "\n"
+
+    for model_param in sorted(param_dict.keys()):
+        matlab_param = par_to_matlab[model_param]
+        value = param_dict[model_param]
+        file += f"{matlab_param} = {value};\n"
+
+    file += "\n"
+    file += "model;\n"
+    for var, val in ss_value_dict.items():
+        if var in var_to_matlab.keys():
+            matlab_var = var_to_matlab[var]
+            file += f"#{matlab_var}_ss = {val:0.4f};\n"
+
+    for eq in model.system_equations:
+        matlab_subdict = {}
+
+        for atom in eq.atoms():
+            if not isinstance(atom, TimeAwareSymbol) and isinstance(atom, sp.core.Symbol):
+                if atom in par_to_matlab.keys():
+                    matlab_subdict[atom] = sp.Symbol(par_to_matlab[atom])
+            elif isinstance(atom, TimeAwareSymbol):
+                if atom in var_to_matlab.keys():
+                    matlab_subdict[atom] = var_to_matlab[atom]
+                elif atom in shock_to_matlab.keys():
+                    matlab_subdict[atom] = shock_to_matlab[atom]
+
+        eq_str = eq.subs(matlab_subdict)
+        eq_str = str(eq_str)
+        prepare_eq = eq_str.replace("**", "^")
+        var_to_hash, hash_to_var = build_hash_table(items_to_hash)
+
+        hash_eq = substitute_equation_from_dict(prepare_eq, var_to_hash)
+
+        for operator in OPERATORS:
+            hash_eq = hash_eq.replace(operator, " " + operator + " ")
+        hash_eq = re.sub(" +", " ", hash_eq)
+        hash_eq = hash_eq.strip()
+        final_eq = substitute_equation_from_dict(hash_eq, hash_to_var)
+
+        matlab_eq = final_eq.replace("_tp1", "(1)").replace("_tm1", "(-1)")
+        split_eq = matlab_eq.split(" ")
+
+        new_eq = []
+        for atom in split_eq:
+            if atom in par_to_matlab.keys():
+                atom = par_to_matlab[atom]
+            elif atom in var_to_matlab.keys():
+                atom = var_to_matlab[atom]
+            elif atom in shock_to_matlab.keys():
+                atom = shock_to_matlab[atom]
+
+            new_eq.append(atom)
+
+        matlab_eq = ""
+        for i, atom in enumerate(new_eq):
+            if i == 0:
+                matlab_eq += atom
+            elif i == 1 and new_eq[0] == "-":
+                matlab_eq += atom
+            else:
+                if atom in "()":
+                    matlab_eq += atom
+                elif new_eq[i - 1] in "(":
+                    matlab_eq += atom
+                else:
+                    matlab_eq += " " + atom
+        matlab_eq += " = 0;"
+        matlab_eq = re.sub(UNDER_T_PATTERN, "", matlab_eq)
+
+        file += matlab_eq + "\n"
+
+    file += "end;\n\n"
+
+    file += "initval;\n"
+    for var, val in string_keys_to_sympy(ss_value_dict).items():
+        matlab_var = var_to_matlab[var].replace("_ss", "")
+        file += f"{matlab_var} = {val:0.4f};\n"
+
+    file += "end;\n\n"
+    file += "steady;\n"
+    file += "check(qz_zero_threshold=1e-20);\n\n"
+
+    file += "shocks;\n"
+    for shock in shocks:
+        file += "var " + re.sub(UNDER_T_PATTERN, "", shock_to_matlab[shock]) + ";\n"
+        file += "stderr 0.01;\n"
+    file += "end;\n\n"
+    file += "stoch_simul(order=1, irf=100, qz_zero_threshold=1e-20);"
+
+    return file
```

### Comparing `gEconpy-1.1.0/gEconpy/shared/statsmodel_convert.py` & `gEconpy-1.2.0/gEconpy/shared/statsmodel_convert.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,677 +1,677 @@
-from typing import Callable, Dict, Optional
-
-import numpy as np
-import pandas as pd
-from statsmodels.tsa.statespace.kalman_filter import INVERT_UNIVARIATE, SOLVE_LU
-from statsmodels.tsa.statespace.mlemodel import MLEModel, _handle_args
-
-from gEconpy.classes.transformers import IdentityTransformer, PositiveTransformer
-
-
-def compile_to_statsmodels(model):
-    """
-    Compile a gEconModel object into a Statsmodels MLEModel object.
-
-    Statsmodels includes a full suite of tools for solving and fitting linear state space
-    models via Maximum Likelihood. This function takes a solved gEconpy model object
-    and uses it to implement a `statsmodels.tsa.statespace` state space model.
-
-    Parameters
-    ----------
-    model : gEconModel
-        A gEconModel object to be compiled into a Statsmodels MLEModel object.
-
-    Returns
-    -------
-    MLEModel
-        A Statsmodels MLEModel object compiled from the gEconModel object.
-
-    """
-
-    class DSGEModel(MLEModel):
-        def __init__(
-            self,
-            data: pd.DataFrame,
-            initialization: str,
-            param_start_dict: Dict[str, float],
-            shock_start_dict: Dict[str, float],
-            noise_start_dict: Optional[Dict[str, float]] = None,
-            param_transforms: Optional[Dict[str, Callable]] = None,
-            shock_transforms: Optional[Dict[str, Callable]] = None,
-            noise_transforms: Optional[Dict[str, Callable]] = None,
-            x0: Optional[np.ndarray] = None,
-            P0: Optional[np.ndarray] = None,
-            fit_MAP: bool = False,
-            **kwargs,
-        ):
-            """
-            Create a DSGEModel object for maximum-likelihood estimation, subclassed from
-            `statsmodels.tsa.statespace.MLEModel`.
-
-            Parameters
-            ----------
-            model: A DSGE model object
-                The model object to be used to create the DSGEModel
-            data: pd.DataFrame
-                A pandas DataFrame containing the data to be used for estimation
-            initialization: string
-                The type of Kalman filter initialization to use.  One of 'approximate_diffuse',
-                'stationary', 'known', 'fixed', 'diffuse' or 'none'
-            param_start_dict: dict
-                A dictionary of parameter starting values, where keys are parameter names
-                and values are floats. Parameters not included this dictionary will not be
-                estimated when `.fit()`. is called.
-            shock_start_dict: dict
-                A dictionary of shock variance starting values, where keys are shock names and values
-                are floats. All shocks not include in this dictionary will be dropped from the model
-                when `.fit()` is called.
-            noise_start_dict: dict, optional
-                A dictionary of observation noise starting values, where keys are observed state names
-                and values are floats. Default is zero for all observed variables.
-            param_transforms: dict, optional
-                A dictionary of functions to transform parameters before they are passed to the likelihood
-                function.  Keys are parameter names, values are functions. Default is the identity
-                function for all parameters.
-            shock_transforms: dict, optional
-                A dictionary of functions to transform shock variance terms before they are passed to
-                the likelihood function.  Keys are shock names, values are functions. Default is
-                the square function for all variances.
-            noise_transforms: dict, optional
-                A dictionary of functions to transform observation noise variances before they are
-                 passed to the likelihood function.  Keys are noise state names, values are
-                  functions. Default is the square function for all variannces.
-            x0: array_like, optional
-                A 1-d array of starting values for the state vector
-            P0: array_like, optional
-                A 2-d array of starting values for the state covariance matrix
-            fit_MAP: bool, optional
-                If True, fit the model in maximum a posteriori (MAP) sense rather than maximum
-                likelihood sense.  Defaults to False.
-            kwargs:
-                Additional arguments to pass to the MLEModel constructor
-            """
-            k_states = model.n_variables
-            k_observed = data.shape[1]
-            k_posdef = model.n_shocks
-
-            noise_start_dict = noise_start_dict or {}
-
-            self.model = model
-            self.data = data
-            self.fit_MAP = fit_MAP
-
-            self.shock_names = [x.base_name for x in self.model.shocks]
-            self.dsge_params = list(model.free_param_dict.keys())
-
-            param_priors = self.model.param_priors.copy()
-            shock_priors = self.model.shock_priors.copy()
-            noise_priors = self.model.observation_noise_priors.copy()
-
-            self.prior_dict = param_priors.copy()
-            self.prior_dict.update({k: d.rv_params["scale"] for k, d in shock_priors.items()})
-            self.prior_dict.update(noise_priors)
-
-            n_shocks = len(self.shock_names)
-
-            self.params_to_estimate = list(param_start_dict.keys())
-            self.shocks_to_estimate = list(shock_start_dict.keys())
-            self.noisy_states = list(noise_start_dict.keys())
-
-            self.start_dict = param_start_dict.copy()
-            self.start_dict.update(shock_start_dict)
-            self.start_dict.update(noise_start_dict)
-
-            self._validate_start_dict(param_start_dict, shock_start_dict, noise_start_dict)
-            self._build_transform_dict(param_transforms, shock_transforms, noise_transforms)
-            self._validate_priors(param_priors, shock_priors, noise_priors)
-
-            super().__init__(
-                endog=data,
-                k_states=k_states,
-                k_posdef=k_posdef,
-                initialization=initialization,
-                constant=x0,
-                initial_state_cov=P0,
-                **kwargs,
-            )
-
-            model_names = [x.base_name for x in model.variables]
-            missing_vars = [x for x in data.columns if x not in model_names]
-            if any(missing_vars):
-                msg = "Data contains the following columns not associated with variables in the model:"
-                msg += ", ".join(missing_vars)
-                raise ValueError(msg)
-
-            Z_idx = [model_names.index(x) for x in data.columns if x in model_names]
-
-            self.ssm["design"][np.arange(k_observed), Z_idx] = 1
-            self.ssm["state_cov"] = np.eye(k_posdef) * 0.1
-            self.ssm["obs_cov"] = np.zeros((k_observed, k_observed))
-
-            self.state_cov_idxs = (
-                np.arange(n_shocks, dtype="int"),
-                np.arange(n_shocks, dtype="int"),
-            )
-            self.obs_cov_idxs = np.where(np.isin(data.columns, self.noisy_states))
-
-        def _validate_start_dict(
-            self,
-            param_start_dict: Dict[str, float],
-            shock_start_dict: Dict[str, float],
-            noise_start_dict: Dict[str, float],
-        ) -> None:
-            """
-            Validate that all the parameters, shocks, and observation noises that are supposed to be
-             estimated have starting values, and that any starting values provided correspond to
-             parameters, shocks, or observation noises that exist in the model or data.
-
-            Parameters
-            ----------
-            param_start_dict: Dict[str, float]
-                A dictionary of starting values for parameters that are to be estimated.
-            shock_start_dict: Dict[str, float]
-                A dictionary of starting values for shocks that are to be estimated.
-            noise_start_dict: Dict[str, float]
-                A dictionary of starting values for observation noises that are to be estimated.
-            """
-            missing_vars = [x for x in self.params_to_estimate if x not in param_start_dict.keys()]
-            missing_shocks = [
-                x for x in self.shocks_to_estimate if x not in shock_start_dict.keys()
-            ]
-            missing_noise = [x for x in self.noisy_states if x not in noise_start_dict.keys()]
-            msg = "The following {} to be estimated were not assigned a starting value: "
-
-            if any(missing_vars):
-                raise ValueError(msg.format("parameters") + ", ".join(missing_vars))
-
-            if any(missing_shocks):
-                raise ValueError(msg.format("shocks") + ", ".join(missing_shocks))
-
-            if any(missing_noise):
-                raise ValueError(msg.format("observation noises") + ", ".join(missing_noise))
-
-            extra_vars = [
-                x for x in param_start_dict.keys() if x not in self.model.free_param_dict.keys()
-            ]
-            extra_shocks = [
-                x
-                for x in shock_start_dict.keys()
-                if x not in [x.base_name for x in self.model.shocks]
-            ]
-            extra_noise = [x for x in noise_start_dict.keys() if x not in self.data.columns]
-
-            msg = "The following {} were given starting values, but did not appear in the {}: "
-            if any(extra_vars):
-                raise ValueError(
-                    msg.format("parameters", "model definition") + ", ".join(extra_vars)
-                )
-
-            if any(extra_shocks):
-                raise ValueError(
-                    msg.format("shocks", "model definition") + ", ".join(missing_shocks)
-                )
-
-            if any(extra_noise):
-                raise ValueError(
-                    msg.format("observation noises", "data") + ", ".join(missing_noise)
-                )
-
-        def _build_transform_dict(self, param_transforms, shock_transforms, noise_transforms):
-            self.transform_dict = {}
-            for param in self.params_to_estimate:
-                if param in param_transforms.keys():
-                    self.transform_dict[param] = param_transforms[param]
-                else:
-                    print(
-                        f"Parameter {param} was not assigned a transformation, assigning IdentityTransform"
-                    )
-                    self.transform_dict[param] = IdentityTransformer()
-
-            if shock_transforms is None:
-                self.transform_dict.update(
-                    {k: PositiveTransformer() for k in self.shocks_to_estimate}
-                )
-            else:
-                for shock in self.shocks_to_estimate:
-                    if shock in shock_transforms.keys():
-                        self.transform_dict[shock] = shock_transforms[shock]
-                    else:
-                        print(
-                            f"Shock {shock} was not assigned a transformation, assigning IdentityTransform"
-                        )
-                        self.transform_dict[shock] = IdentityTransformer()
-
-            if noise_transforms is None:
-                self.transform_dict.update({k: PositiveTransformer() for k in self.noisy_states})
-            else:
-                for noise in self.noisy_states:
-                    if noise in noise_transforms.keys():
-                        self.transform_dict[noise] = noise_transforms[noise]
-                    else:
-                        print(
-                            f"Noise for state {noise} was not assigned a transformation, assigning IdentityTransform"
-                        )
-                        self.transform_dict[noise] = IdentityTransformer()
-
-        def _validate_priors(self, param_priors, shock_priors, noise_priors):
-            if not self.fit_MAP:
-                return
-
-            missing_vars = [x for x in self.params_to_estimate if x not in param_priors.keys()]
-            missing_shocks = [x for x in self.shocks_to_estimate if x not in shock_priors.keys()]
-            missing_noise = [x for x in self.noisy_states if x not in noise_priors.keys()]
-            msg = "The following {} to be estimated were not assigned a prior: "
-            if any(missing_vars):
-                raise ValueError(msg.format("parameters") + ", ".join(missing_vars))
-
-            if any(missing_shocks):
-                raise ValueError(msg.format("shocks") + ", ".join(missing_shocks))
-
-            if any(missing_noise):
-                raise ValueError(msg.format("observation noises") + ", ".join(missing_noise))
-
-        @property
-        def param_names(self):
-            shock_names = [f"sigma2.{x}" for x in self.shocks_to_estimate]
-            noise_names = [f"sigma2.{x}" for x in self.noisy_states]
-            return self.params_to_estimate + shock_names + noise_names
-
-        @property
-        def external_param_names(self):
-            return self.params_to_estimate + self.shocks_to_estimate + self.noisy_states
-
-        @property
-        def state_names(self):
-            return [x.base_name for x in self.model.variables]
-
-        @property
-        def start_params(self):
-            param_names = self.external_param_names
-            start_params = []
-
-            for name in param_names:
-                start_params.append(self.start_dict[name])
-            return np.array(start_params)
-
-        def unpack_statespace(self):
-            T = np.ascontiguousarray(self.ssm["transition"])
-            Z = np.ascontiguousarray(self.ssm["design"])
-            R = np.ascontiguousarray(self.ssm["selection"])
-            H = np.ascontiguousarray(self.ssm["obs_cov"])
-            Q = np.ascontiguousarray(self.ssm["state_cov"])
-
-            return T, Z, R, H, Q
-
-        def transform_params(self, real_line_params):
-            """
-            Take in optimizer values on R and map them into parameter space.
-
-            Example: variances must be positive, so apply x ** 2.
-            """
-            param_space_params = np.zeros_like(real_line_params)
-            for i, (name, param) in enumerate(zip(self.external_param_names, real_line_params)):
-                param_space_params[i] = self.transform_dict[name].constrain(param)
-
-            return param_space_params
-
-        def untransform_params(self, param_space_params):
-            """
-            Take in parameters living in the parameter space and apply an "inverse transform"
-            to put them back to where the optimizer's last guess was.
-
-            Example: We applied x ** 2 to ensure x is positive, apply x ** (1 / 2).
-            """
-            real_line_params = np.zeros_like(param_space_params)
-            for i, (name, param) in enumerate(zip(self.external_param_names, param_space_params)):
-                real_line_params[i] = self.transform_dict[name].unconstrain(param)
-
-            return real_line_params
-
-        def make_param_update_dict(self, params):
-            shock_names = self.shock_names
-            dsge_params = self.dsge_params
-            param_names = self.external_param_names
-
-            param_update_dict = {}
-            shock_params = []
-            observation_noise_params = []
-
-            for name, param in zip(param_names, params):
-                if name in dsge_params:
-                    param_update_dict[name] = param
-                elif name in shock_names:
-                    shock_params.append(param)
-                else:
-                    observation_noise_params.append(param)
-
-            return (
-                param_update_dict,
-                np.array(shock_params),
-                np.array(observation_noise_params),
-            )
-
-        def update(self, params, **kwargs):
-            params = super().update(params, **kwargs)
-
-            update_dict, shock_params, obs_params = self.make_param_update_dict(params)
-            # original_params = model.free_param_dict.copy()
-
-            self.model.free_param_dict.update(update_dict)
-            try:
-                self.model.steady_state(verbose=False)
-                self.model.solve_model(verbose=False)
-                pert_success = True
-            except np.linalg.LinAlgError:
-                pert_success = False
-
-            condition_satisfied = model.check_bk_condition(verbose=False, return_value="bool")
-
-            self.ssm["transition"] = self.model.T.values
-            self.ssm["selection"] = self.model.R.values
-
-            cov_idx = self.state_cov_idxs
-            self.ssm["state_cov", cov_idx, cov_idx] = shock_params
-
-            obs_idx = self.obs_cov_idxs
-            self.ssm["obs_cov", obs_idx, obs_idx] = obs_params
-
-            return pert_success & condition_satisfied
-
-        def loglike(self, params, *args, **kwargs):
-            """
-            Loglikelihood evaluation
-
-            Parameters
-            ----------
-            params : array_like
-                Array of parameters at which to evaluate the loglikelihood
-                function.
-            transformed : bool, optional
-                Whether or not `params` is already transformed. Default is True.
-            **kwargs
-                Additional keyword arguments to pass to the Kalman filter. See
-                `KalmanFilter.filter` for more details.
-
-            See Also
-            --------
-            update : modifies the internal state of the state space model to
-                     reflect new params
-
-            Notes
-            -----
-            [1]_ recommend maximizing the average likelihood to avoid scale issues;
-            this is done automatically by the base Model fit method.
-
-            References
-            ----------
-            .. [1] Koopman, Siem Jan, Neil Shephard, and Jurgen A. Doornik. 1999.
-               Statistical Algorithms for Models in State Space Using SsfPack 2.2.
-               Econometrics Journal 2 (1): 107-60. doi:10.1111/1368-423X.00023.
-            """
-            transformed, includes_fixed, complex_step, kwargs = _handle_args(
-                MLEModel._loglike_param_names,
-                MLEModel._loglike_param_defaults,
-                *args,
-                **kwargs,
-            )
-
-            params = self.handle_params(
-                params, transformed=transformed, includes_fixed=includes_fixed
-            )
-            success = self.update(
-                params,
-                transformed=transformed,
-                includes_fixed=includes_fixed,
-                complex_step=complex_step,
-            )
-
-            if complex_step:
-                kwargs["inversion_method"] = INVERT_UNIVARIATE | SOLVE_LU
-
-            if success:
-                loglike = self.ssm.loglike(complex_step=complex_step, **kwargs)
-                if self.fit_MAP:
-                    for name, param in zip(self.external_param_names, params):
-                        loglike += max(-1e6, self.prior_dict[name].logpdf(param))
-
-            else:
-                # If the parameters are invalid, return a large negative number
-                loglike = -1e6
-
-            # Koopman, Shephard, and Doornik recommend maximizing the average
-            # likelihood to avoid scale issues, but the averaging is done
-            # automatically in the base model `fit` method
-            return loglike
-
-        def loglikeobs(
-            self,
-            params,
-            transformed=True,
-            includes_fixed=False,
-            complex_step=False,
-            **kwargs,
-        ):
-            """
-            Loglikelihood evaluation
-
-            Parameters
-            ----------
-            params : array_like
-                Array of parameters at which to evaluate the loglikelihood
-                function.
-            transformed : bool, optional
-                Whether or not `params` is already transformed. Default is True.
-            **kwargs
-                Additional keyword arguments to pass to the Kalman filter. See
-                `KalmanFilter.filter` for more details.
-
-            See Also
-            --------
-            update : modifies the internal state of the Model to reflect new params
-
-            Notes
-            -----
-            [1]_ recommend maximizing the average likelihood to avoid scale issues;
-            this is done automatically by the base Model fit method.
-
-            References
-            ----------
-            .. [1] Koopman, Siem Jan, Neil Shephard, and Jurgen A. Doornik. 1999.
-               Statistical Algorithms for Models in State Space Using SsfPack 2.2.
-               Econometrics Journal 2 (1): 107-60. doi:10.1111/1368-423X.00023.
-            """
-            params = self.handle_params(
-                params, transformed=transformed, includes_fixed=includes_fixed
-            )
-
-            # If we're using complex-step differentiation, then we cannot use
-            # Cholesky factorization
-            if complex_step:
-                kwargs["inversion_method"] = INVERT_UNIVARIATE | SOLVE_LU
-
-            success = self.update(
-                params,
-                transformed=transformed,
-                includes_fixed=includes_fixed,
-                complex_step=complex_step,
-            )
-
-            if success:
-                ll_obs = self.ssm.loglikeobs(complex_step=complex_step, **kwargs)
-                if self.fit_MAP:
-                    for name, param in zip(self.external_param_names, params):
-                        ll_obs += max(-1e6, self.prior_dict[name].logpdf(param)) / self.nobs
-                return ll_obs
-
-            else:
-                # Large negative likelihood for all observations if the parameters are invalid
-                return np.full(self.endog.shape[0], -1e6)
-
-        def fit(
-            self,
-            start_params=None,
-            transformed=True,
-            includes_fixed=False,
-            cov_type=None,
-            cov_kwds=None,
-            method="lbfgs",
-            maxiter=50,
-            full_output=1,
-            disp=5,
-            callback=None,
-            return_params=False,
-            optim_score=None,
-            optim_complex_step=None,
-            optim_hessian=None,
-            flags=None,
-            low_memory=False,
-            **kwargs,
-        ):
-            """
-            Fits the model by maximum likelihood via Kalman filter.
-
-            Parameters
-            ----------
-            start_params : array_like, optional
-                Initial guess of the solution for the loglikelihood maximization.
-                If None, the default is given by Model.start_params.
-            transformed : bool, optional
-                Whether or not `start_params` is already transformed. Default is
-                True.
-            includes_fixed : bool, optional
-                If parameters were previously fixed with the `fix_params` method,
-                this argument describes whether or not `start_params` also includes
-                the fixed parameters, in addition to the free parameters. Default
-                is False.
-            cov_type : str, optional
-                The `cov_type` keyword governs the method for calculating the
-                covariance matrix of parameter estimates. Can be one of:
-
-                - 'opg' for the outer product of gradient estimator
-                - 'oim' for the observed information matrix estimator, calculated
-                  using the method of Harvey (1989)
-                - 'approx' for the observed information matrix estimator,
-                  calculated using a numerical approximation of the Hessian matrix.
-                - 'robust' for an approximate (quasi-maximum likelihood) covariance
-                  matrix that may be valid even in the presence of some
-                  misspecifications. Intermediate calculations use the 'oim'
-                  method.
-                - 'robust_approx' is the same as 'robust' except that the
-                  intermediate calculations use the 'approx' method.
-                - 'none' for no covariance matrix calculation.
-
-                Default is 'opg' unless memory conservation is used to avoid
-                computing the loglikelihood values for each observation, in which
-                case the default is 'approx'.
-            cov_kwds : dict or None, optional
-                A dictionary of arguments affecting covariance matrix computation.
-
-                **opg, oim, approx, robust, robust_approx**
-
-                - 'approx_complex_step' : bool, optional - If True, numerical
-                  approximations are computed using complex-step methods. If False,
-                  numerical approximations are computed using finite difference
-                  methods. Default is True.
-                - 'approx_centered' : bool, optional - If True, numerical
-                  approximations computed using finite difference methods use a
-                  centered approximation. Default is False.
-            method : str, optional
-                The `method` determines which solver from `scipy.optimize`
-                is used, and it can be chosen from among the following strings:
-
-                - 'newton' for Newton-Raphson
-                - 'nm' for Nelder-Mead
-                - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)
-                - 'lbfgs' for limited-memory BFGS with optional box constraints
-                - 'powell' for modified Powell's method
-                - 'cg' for conjugate gradient
-                - 'ncg' for Newton-conjugate gradient
-                - 'basinhopping' for global basin-hopping solver
-
-                The explicit arguments in `fit` are passed to the solver,
-                with the exception of the basin-hopping solver. Each
-                solver has several optional arguments that are not the same across
-                solvers. See the notes section below (or scipy.optimize) for the
-                available arguments and for the list of explicit arguments that the
-                basin-hopping solver supports.
-            maxiter : int, optional
-                The maximum number of iterations to perform.
-            full_output : bool, optional
-                Set to True to have all available output in the Results object's
-                mle_retvals attribute. The output is dependent on the solver.
-                See LikelihoodModelResults notes section for more information.
-            disp : bool, optional
-                Set to True to print convergence messages.
-            callback : callable callback(xk), optional
-                Called after each iteration, as callback(xk), where xk is the
-                current parameter vector.
-            return_params : bool, optional
-                Whether or not to return only the array of maximizing parameters.
-                Default is False.
-            optim_score : {'harvey', 'approx'} or None, optional
-                The method by which the score vector is calculated. 'harvey' uses
-                the method from Harvey (1989), 'approx' uses either finite
-                difference or complex step differentiation depending upon the
-                value of `optim_complex_step`, and None uses the built-in gradient
-                approximation of the optimizer. Default is None. This keyword is
-                only relevant if the optimization method uses the score.
-            optim_complex_step : bool, optional
-                Whether or not to use complex step differentiation when
-                approximating the score; if False, finite difference approximation
-                is used. Default is True. This keyword is only relevant if
-                `optim_score` is set to 'harvey' or 'approx'.
-            optim_hessian : {'opg','oim','approx'}, optional
-                The method by which the Hessian is numerically approximated. 'opg'
-                uses outer product of gradients, 'oim' uses the information
-                matrix formula from Harvey (1989), and 'approx' uses numerical
-                approximation. This keyword is only relevant if the
-                optimization method uses the Hessian matrix.
-            low_memory : bool, optional
-                If set to True, techniques are applied to substantially reduce
-                memory usage. If used, some features of the results object will
-                not be available (including smoothed results and in-sample
-                prediction), although out-of-sample forecasting is possible.
-                Default is False.
-            **kwargs
-                Additional keyword arguments to pass to the optimizer.
-
-            Returns
-            -------
-            results
-                Results object holding results from fitting a state space model.
-
-            See Also
-            --------
-            statsmodels.base.model.LikelihoodModel.fit
-            statsmodels.tsa.statespace.mlemodel.MLEResults
-            statsmodels.tsa.statespace.structural.UnobservedComponentsResults
-            """
-
-            # Disable complex step approximations by default
-            optim_complex_step = optim_complex_step or False
-            cov_kwds = cov_kwds or {
-                "approx_complex_step": False,
-                "approx_centered": True,
-            }
-
-            return super().fit(
-                start_params=start_params,
-                transformed=transformed,
-                includes_fixed=includes_fixed,
-                cov_type=cov_type,
-                cov_kwds=cov_kwds,
-                method=method,
-                maxiter=maxiter,
-                full_output=full_output,
-                disp=disp,
-                callback=callback,
-                return_params=return_params,
-                optim_score=optim_score,
-                optim_complex_step=optim_complex_step,
-                optim_hessian=optim_hessian,
-                flags=flags,
-                low_memory=low_memory,
-                **kwargs,
-            )
-
-    return DSGEModel
+from typing import Callable, Dict, Optional
+
+import numpy as np
+import pandas as pd
+from statsmodels.tsa.statespace.kalman_filter import INVERT_UNIVARIATE, SOLVE_LU
+from statsmodels.tsa.statespace.mlemodel import MLEModel, _handle_args
+
+from gEconpy.classes.transformers import IdentityTransformer, PositiveTransformer
+
+
+def compile_to_statsmodels(model):
+    """
+    Compile a gEconModel object into a Statsmodels MLEModel object.
+
+    Statsmodels includes a full suite of tools for solving and fitting linear state space
+    models via Maximum Likelihood. This function takes a solved gEconpy model object
+    and uses it to implement a `statsmodels.tsa.statespace` state space model.
+
+    Parameters
+    ----------
+    model : gEconModel
+        A gEconModel object to be compiled into a Statsmodels MLEModel object.
+
+    Returns
+    -------
+    MLEModel
+        A Statsmodels MLEModel object compiled from the gEconModel object.
+
+    """
+
+    class DSGEModel(MLEModel):
+        def __init__(
+            self,
+            data: pd.DataFrame,
+            initialization: str,
+            param_start_dict: Dict[str, float],
+            shock_start_dict: Dict[str, float],
+            noise_start_dict: Optional[Dict[str, float]] = None,
+            param_transforms: Optional[Dict[str, Callable]] = None,
+            shock_transforms: Optional[Dict[str, Callable]] = None,
+            noise_transforms: Optional[Dict[str, Callable]] = None,
+            x0: Optional[np.ndarray] = None,
+            P0: Optional[np.ndarray] = None,
+            fit_MAP: bool = False,
+            **kwargs,
+        ):
+            """
+            Create a DSGEModel object for maximum-likelihood estimation, subclassed from
+            `statsmodels.tsa.statespace.MLEModel`.
+
+            Parameters
+            ----------
+            model: A DSGE model object
+                The model object to be used to create the DSGEModel
+            data: pd.DataFrame
+                A pandas DataFrame containing the data to be used for estimation
+            initialization: string
+                The type of Kalman filter initialization to use.  One of 'approximate_diffuse',
+                'stationary', 'known', 'fixed', 'diffuse' or 'none'
+            param_start_dict: dict
+                A dictionary of parameter starting values, where keys are parameter names
+                and values are floats. Parameters not included this dictionary will not be
+                estimated when `.fit()`. is called.
+            shock_start_dict: dict
+                A dictionary of shock variance starting values, where keys are shock names and values
+                are floats. All shocks not include in this dictionary will be dropped from the model
+                when `.fit()` is called.
+            noise_start_dict: dict, optional
+                A dictionary of observation noise starting values, where keys are observed state names
+                and values are floats. Default is zero for all observed variables.
+            param_transforms: dict, optional
+                A dictionary of functions to transform parameters before they are passed to the likelihood
+                function.  Keys are parameter names, values are functions. Default is the identity
+                function for all parameters.
+            shock_transforms: dict, optional
+                A dictionary of functions to transform shock variance terms before they are passed to
+                the likelihood function.  Keys are shock names, values are functions. Default is
+                the square function for all variances.
+            noise_transforms: dict, optional
+                A dictionary of functions to transform observation noise variances before they are
+                 passed to the likelihood function.  Keys are noise state names, values are
+                  functions. Default is the square function for all variannces.
+            x0: array_like, optional
+                A 1-d array of starting values for the state vector
+            P0: array_like, optional
+                A 2-d array of starting values for the state covariance matrix
+            fit_MAP: bool, optional
+                If True, fit the model in maximum a posteriori (MAP) sense rather than maximum
+                likelihood sense.  Defaults to False.
+            kwargs:
+                Additional arguments to pass to the MLEModel constructor
+            """
+            k_states = model.n_variables
+            k_observed = data.shape[1]
+            k_posdef = model.n_shocks
+
+            noise_start_dict = noise_start_dict or {}
+
+            self.model = model
+            self.data = data
+            self.fit_MAP = fit_MAP
+
+            self.shock_names = [x.base_name for x in self.model.shocks]
+            self.dsge_params = list(model.free_param_dict.keys())
+
+            param_priors = self.model.param_priors.copy()
+            shock_priors = self.model.shock_priors.copy()
+            noise_priors = self.model.observation_noise_priors.copy()
+
+            self.prior_dict = param_priors.copy()
+            self.prior_dict.update({k: d.rv_params["scale"] for k, d in shock_priors.items()})
+            self.prior_dict.update(noise_priors)
+
+            n_shocks = len(self.shock_names)
+
+            self.params_to_estimate = list(param_start_dict.keys())
+            self.shocks_to_estimate = list(shock_start_dict.keys())
+            self.noisy_states = list(noise_start_dict.keys())
+
+            self.start_dict = param_start_dict.copy()
+            self.start_dict.update(shock_start_dict)
+            self.start_dict.update(noise_start_dict)
+
+            self._validate_start_dict(param_start_dict, shock_start_dict, noise_start_dict)
+            self._build_transform_dict(param_transforms, shock_transforms, noise_transforms)
+            self._validate_priors(param_priors, shock_priors, noise_priors)
+
+            super().__init__(
+                endog=data,
+                k_states=k_states,
+                k_posdef=k_posdef,
+                initialization=initialization,
+                constant=x0,
+                initial_state_cov=P0,
+                **kwargs,
+            )
+
+            model_names = [x.base_name for x in model.variables]
+            missing_vars = [x for x in data.columns if x not in model_names]
+            if any(missing_vars):
+                msg = "Data contains the following columns not associated with variables in the model:"
+                msg += ", ".join(missing_vars)
+                raise ValueError(msg)
+
+            Z_idx = [model_names.index(x) for x in data.columns if x in model_names]
+
+            self.ssm["design"][np.arange(k_observed), Z_idx] = 1
+            self.ssm["state_cov"] = np.eye(k_posdef) * 0.1
+            self.ssm["obs_cov"] = np.zeros((k_observed, k_observed))
+
+            self.state_cov_idxs = (
+                np.arange(n_shocks, dtype="int"),
+                np.arange(n_shocks, dtype="int"),
+            )
+            self.obs_cov_idxs = np.where(np.isin(data.columns, self.noisy_states))
+
+        def _validate_start_dict(
+            self,
+            param_start_dict: Dict[str, float],
+            shock_start_dict: Dict[str, float],
+            noise_start_dict: Dict[str, float],
+        ) -> None:
+            """
+            Validate that all the parameters, shocks, and observation noises that are supposed to be
+             estimated have starting values, and that any starting values provided correspond to
+             parameters, shocks, or observation noises that exist in the model or data.
+
+            Parameters
+            ----------
+            param_start_dict: Dict[str, float]
+                A dictionary of starting values for parameters that are to be estimated.
+            shock_start_dict: Dict[str, float]
+                A dictionary of starting values for shocks that are to be estimated.
+            noise_start_dict: Dict[str, float]
+                A dictionary of starting values for observation noises that are to be estimated.
+            """
+            missing_vars = [x for x in self.params_to_estimate if x not in param_start_dict.keys()]
+            missing_shocks = [
+                x for x in self.shocks_to_estimate if x not in shock_start_dict.keys()
+            ]
+            missing_noise = [x for x in self.noisy_states if x not in noise_start_dict.keys()]
+            msg = "The following {} to be estimated were not assigned a starting value: "
+
+            if any(missing_vars):
+                raise ValueError(msg.format("parameters") + ", ".join(missing_vars))
+
+            if any(missing_shocks):
+                raise ValueError(msg.format("shocks") + ", ".join(missing_shocks))
+
+            if any(missing_noise):
+                raise ValueError(msg.format("observation noises") + ", ".join(missing_noise))
+
+            extra_vars = [
+                x for x in param_start_dict.keys() if x not in self.model.free_param_dict.keys()
+            ]
+            extra_shocks = [
+                x
+                for x in shock_start_dict.keys()
+                if x not in [x.base_name for x in self.model.shocks]
+            ]
+            extra_noise = [x for x in noise_start_dict.keys() if x not in self.data.columns]
+
+            msg = "The following {} were given starting values, but did not appear in the {}: "
+            if any(extra_vars):
+                raise ValueError(
+                    msg.format("parameters", "model definition") + ", ".join(extra_vars)
+                )
+
+            if any(extra_shocks):
+                raise ValueError(
+                    msg.format("shocks", "model definition") + ", ".join(missing_shocks)
+                )
+
+            if any(extra_noise):
+                raise ValueError(
+                    msg.format("observation noises", "data") + ", ".join(missing_noise)
+                )
+
+        def _build_transform_dict(self, param_transforms, shock_transforms, noise_transforms):
+            self.transform_dict = {}
+            for param in self.params_to_estimate:
+                if param in param_transforms.keys():
+                    self.transform_dict[param] = param_transforms[param]
+                else:
+                    print(
+                        f"Parameter {param} was not assigned a transformation, assigning IdentityTransform"
+                    )
+                    self.transform_dict[param] = IdentityTransformer()
+
+            if shock_transforms is None:
+                self.transform_dict.update(
+                    {k: PositiveTransformer() for k in self.shocks_to_estimate}
+                )
+            else:
+                for shock in self.shocks_to_estimate:
+                    if shock in shock_transforms.keys():
+                        self.transform_dict[shock] = shock_transforms[shock]
+                    else:
+                        print(
+                            f"Shock {shock} was not assigned a transformation, assigning IdentityTransform"
+                        )
+                        self.transform_dict[shock] = IdentityTransformer()
+
+            if noise_transforms is None:
+                self.transform_dict.update({k: PositiveTransformer() for k in self.noisy_states})
+            else:
+                for noise in self.noisy_states:
+                    if noise in noise_transforms.keys():
+                        self.transform_dict[noise] = noise_transforms[noise]
+                    else:
+                        print(
+                            f"Noise for state {noise} was not assigned a transformation, assigning IdentityTransform"
+                        )
+                        self.transform_dict[noise] = IdentityTransformer()
+
+        def _validate_priors(self, param_priors, shock_priors, noise_priors):
+            if not self.fit_MAP:
+                return
+
+            missing_vars = [x for x in self.params_to_estimate if x not in param_priors.keys()]
+            missing_shocks = [x for x in self.shocks_to_estimate if x not in shock_priors.keys()]
+            missing_noise = [x for x in self.noisy_states if x not in noise_priors.keys()]
+            msg = "The following {} to be estimated were not assigned a prior: "
+            if any(missing_vars):
+                raise ValueError(msg.format("parameters") + ", ".join(missing_vars))
+
+            if any(missing_shocks):
+                raise ValueError(msg.format("shocks") + ", ".join(missing_shocks))
+
+            if any(missing_noise):
+                raise ValueError(msg.format("observation noises") + ", ".join(missing_noise))
+
+        @property
+        def param_names(self):
+            shock_names = [f"sigma2.{x}" for x in self.shocks_to_estimate]
+            noise_names = [f"sigma2.{x}" for x in self.noisy_states]
+            return self.params_to_estimate + shock_names + noise_names
+
+        @property
+        def external_param_names(self):
+            return self.params_to_estimate + self.shocks_to_estimate + self.noisy_states
+
+        @property
+        def state_names(self):
+            return [x.base_name for x in self.model.variables]
+
+        @property
+        def start_params(self):
+            param_names = self.external_param_names
+            start_params = []
+
+            for name in param_names:
+                start_params.append(self.start_dict[name])
+            return np.array(start_params)
+
+        def unpack_statespace(self):
+            T = np.ascontiguousarray(self.ssm["transition"])
+            Z = np.ascontiguousarray(self.ssm["design"])
+            R = np.ascontiguousarray(self.ssm["selection"])
+            H = np.ascontiguousarray(self.ssm["obs_cov"])
+            Q = np.ascontiguousarray(self.ssm["state_cov"])
+
+            return T, Z, R, H, Q
+
+        def transform_params(self, real_line_params):
+            """
+            Take in optimizer values on R and map them into parameter space.
+
+            Example: variances must be positive, so apply x ** 2.
+            """
+            param_space_params = np.zeros_like(real_line_params)
+            for i, (name, param) in enumerate(zip(self.external_param_names, real_line_params)):
+                param_space_params[i] = self.transform_dict[name].constrain(param)
+
+            return param_space_params
+
+        def untransform_params(self, param_space_params):
+            """
+            Take in parameters living in the parameter space and apply an "inverse transform"
+            to put them back to where the optimizer's last guess was.
+
+            Example: We applied x ** 2 to ensure x is positive, apply x ** (1 / 2).
+            """
+            real_line_params = np.zeros_like(param_space_params)
+            for i, (name, param) in enumerate(zip(self.external_param_names, param_space_params)):
+                real_line_params[i] = self.transform_dict[name].unconstrain(param)
+
+            return real_line_params
+
+        def make_param_update_dict(self, params):
+            shock_names = self.shock_names
+            dsge_params = self.dsge_params
+            param_names = self.external_param_names
+
+            param_update_dict = {}
+            shock_params = []
+            observation_noise_params = []
+
+            for name, param in zip(param_names, params):
+                if name in dsge_params:
+                    param_update_dict[name] = param
+                elif name in shock_names:
+                    shock_params.append(param)
+                else:
+                    observation_noise_params.append(param)
+
+            return (
+                param_update_dict,
+                np.array(shock_params),
+                np.array(observation_noise_params),
+            )
+
+        def update(self, params, **kwargs):
+            params = super().update(params, **kwargs)
+
+            update_dict, shock_params, obs_params = self.make_param_update_dict(params)
+            # original_params = model.free_param_dict.copy()
+
+            self.model.free_param_dict.update(update_dict)
+            try:
+                self.model.steady_state(verbose=False)
+                self.model.solve_model(verbose=False)
+                pert_success = True
+            except np.linalg.LinAlgError:
+                pert_success = False
+
+            condition_satisfied = model.check_bk_condition(verbose=False, return_value="bool")
+
+            self.ssm["transition"] = self.model.T.values
+            self.ssm["selection"] = self.model.R.values
+
+            cov_idx = self.state_cov_idxs
+            self.ssm["state_cov", cov_idx, cov_idx] = shock_params
+
+            obs_idx = self.obs_cov_idxs
+            self.ssm["obs_cov", obs_idx, obs_idx] = obs_params
+
+            return pert_success & condition_satisfied
+
+        def loglike(self, params, *args, **kwargs):
+            """
+            Loglikelihood evaluation
+
+            Parameters
+            ----------
+            params : array_like
+                Array of parameters at which to evaluate the loglikelihood
+                function.
+            transformed : bool, optional
+                Whether or not `params` is already transformed. Default is True.
+            **kwargs
+                Additional keyword arguments to pass to the Kalman filter. See
+                `KalmanFilter.filter` for more details.
+
+            See Also
+            --------
+            update : modifies the internal state of the state space model to
+                     reflect new params
+
+            Notes
+            -----
+            [1]_ recommend maximizing the average likelihood to avoid scale issues;
+            this is done automatically by the base Model fit method.
+
+            References
+            ----------
+            .. [1] Koopman, Siem Jan, Neil Shephard, and Jurgen A. Doornik. 1999.
+               Statistical Algorithms for Models in State Space Using SsfPack 2.2.
+               Econometrics Journal 2 (1): 107-60. doi:10.1111/1368-423X.00023.
+            """
+            transformed, includes_fixed, complex_step, kwargs = _handle_args(
+                MLEModel._loglike_param_names,
+                MLEModel._loglike_param_defaults,
+                *args,
+                **kwargs,
+            )
+
+            params = self.handle_params(
+                params, transformed=transformed, includes_fixed=includes_fixed
+            )
+            success = self.update(
+                params,
+                transformed=transformed,
+                includes_fixed=includes_fixed,
+                complex_step=complex_step,
+            )
+
+            if complex_step:
+                kwargs["inversion_method"] = INVERT_UNIVARIATE | SOLVE_LU
+
+            if success:
+                loglike = self.ssm.loglike(complex_step=complex_step, **kwargs)
+                if self.fit_MAP:
+                    for name, param in zip(self.external_param_names, params):
+                        loglike += max(-1e6, self.prior_dict[name].logpdf(param))
+
+            else:
+                # If the parameters are invalid, return a large negative number
+                loglike = -1e6
+
+            # Koopman, Shephard, and Doornik recommend maximizing the average
+            # likelihood to avoid scale issues, but the averaging is done
+            # automatically in the base model `fit` method
+            return loglike
+
+        def loglikeobs(
+            self,
+            params,
+            transformed=True,
+            includes_fixed=False,
+            complex_step=False,
+            **kwargs,
+        ):
+            """
+            Loglikelihood evaluation
+
+            Parameters
+            ----------
+            params : array_like
+                Array of parameters at which to evaluate the loglikelihood
+                function.
+            transformed : bool, optional
+                Whether or not `params` is already transformed. Default is True.
+            **kwargs
+                Additional keyword arguments to pass to the Kalman filter. See
+                `KalmanFilter.filter` for more details.
+
+            See Also
+            --------
+            update : modifies the internal state of the Model to reflect new params
+
+            Notes
+            -----
+            [1]_ recommend maximizing the average likelihood to avoid scale issues;
+            this is done automatically by the base Model fit method.
+
+            References
+            ----------
+            .. [1] Koopman, Siem Jan, Neil Shephard, and Jurgen A. Doornik. 1999.
+               Statistical Algorithms for Models in State Space Using SsfPack 2.2.
+               Econometrics Journal 2 (1): 107-60. doi:10.1111/1368-423X.00023.
+            """
+            params = self.handle_params(
+                params, transformed=transformed, includes_fixed=includes_fixed
+            )
+
+            # If we're using complex-step differentiation, then we cannot use
+            # Cholesky factorization
+            if complex_step:
+                kwargs["inversion_method"] = INVERT_UNIVARIATE | SOLVE_LU
+
+            success = self.update(
+                params,
+                transformed=transformed,
+                includes_fixed=includes_fixed,
+                complex_step=complex_step,
+            )
+
+            if success:
+                ll_obs = self.ssm.loglikeobs(complex_step=complex_step, **kwargs)
+                if self.fit_MAP:
+                    for name, param in zip(self.external_param_names, params):
+                        ll_obs += max(-1e6, self.prior_dict[name].logpdf(param)) / self.nobs
+                return ll_obs
+
+            else:
+                # Large negative likelihood for all observations if the parameters are invalid
+                return np.full(self.endog.shape[0], -1e6)
+
+        def fit(
+            self,
+            start_params=None,
+            transformed=True,
+            includes_fixed=False,
+            cov_type=None,
+            cov_kwds=None,
+            method="lbfgs",
+            maxiter=50,
+            full_output=1,
+            disp=5,
+            callback=None,
+            return_params=False,
+            optim_score=None,
+            optim_complex_step=None,
+            optim_hessian=None,
+            flags=None,
+            low_memory=False,
+            **kwargs,
+        ):
+            """
+            Fits the model by maximum likelihood via Kalman filter.
+
+            Parameters
+            ----------
+            start_params : array_like, optional
+                Initial guess of the solution for the loglikelihood maximization.
+                If None, the default is given by Model.start_params.
+            transformed : bool, optional
+                Whether or not `start_params` is already transformed. Default is
+                True.
+            includes_fixed : bool, optional
+                If parameters were previously fixed with the `fix_params` method,
+                this argument describes whether or not `start_params` also includes
+                the fixed parameters, in addition to the free parameters. Default
+                is False.
+            cov_type : str, optional
+                The `cov_type` keyword governs the method for calculating the
+                covariance matrix of parameter estimates. Can be one of:
+
+                - 'opg' for the outer product of gradient estimator
+                - 'oim' for the observed information matrix estimator, calculated
+                  using the method of Harvey (1989)
+                - 'approx' for the observed information matrix estimator,
+                  calculated using a numerical approximation of the Hessian matrix.
+                - 'robust' for an approximate (quasi-maximum likelihood) covariance
+                  matrix that may be valid even in the presence of some
+                  misspecifications. Intermediate calculations use the 'oim'
+                  method.
+                - 'robust_approx' is the same as 'robust' except that the
+                  intermediate calculations use the 'approx' method.
+                - 'none' for no covariance matrix calculation.
+
+                Default is 'opg' unless memory conservation is used to avoid
+                computing the loglikelihood values for each observation, in which
+                case the default is 'approx'.
+            cov_kwds : dict or None, optional
+                A dictionary of arguments affecting covariance matrix computation.
+
+                **opg, oim, approx, robust, robust_approx**
+
+                - 'approx_complex_step' : bool, optional - If True, numerical
+                  approximations are computed using complex-step methods. If False,
+                  numerical approximations are computed using finite difference
+                  methods. Default is True.
+                - 'approx_centered' : bool, optional - If True, numerical
+                  approximations computed using finite difference methods use a
+                  centered approximation. Default is False.
+            method : str, optional
+                The `method` determines which solver from `scipy.optimize`
+                is used, and it can be chosen from among the following strings:
+
+                - 'newton' for Newton-Raphson
+                - 'nm' for Nelder-Mead
+                - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)
+                - 'lbfgs' for limited-memory BFGS with optional box constraints
+                - 'powell' for modified Powell's method
+                - 'cg' for conjugate gradient
+                - 'ncg' for Newton-conjugate gradient
+                - 'basinhopping' for global basin-hopping solver
+
+                The explicit arguments in `fit` are passed to the solver,
+                with the exception of the basin-hopping solver. Each
+                solver has several optional arguments that are not the same across
+                solvers. See the notes section below (or scipy.optimize) for the
+                available arguments and for the list of explicit arguments that the
+                basin-hopping solver supports.
+            maxiter : int, optional
+                The maximum number of iterations to perform.
+            full_output : bool, optional
+                Set to True to have all available output in the Results object's
+                mle_retvals attribute. The output is dependent on the solver.
+                See LikelihoodModelResults notes section for more information.
+            disp : bool, optional
+                Set to True to print convergence messages.
+            callback : callable callback(xk), optional
+                Called after each iteration, as callback(xk), where xk is the
+                current parameter vector.
+            return_params : bool, optional
+                Whether or not to return only the array of maximizing parameters.
+                Default is False.
+            optim_score : {'harvey', 'approx'} or None, optional
+                The method by which the score vector is calculated. 'harvey' uses
+                the method from Harvey (1989), 'approx' uses either finite
+                difference or complex step differentiation depending upon the
+                value of `optim_complex_step`, and None uses the built-in gradient
+                approximation of the optimizer. Default is None. This keyword is
+                only relevant if the optimization method uses the score.
+            optim_complex_step : bool, optional
+                Whether or not to use complex step differentiation when
+                approximating the score; if False, finite difference approximation
+                is used. Default is True. This keyword is only relevant if
+                `optim_score` is set to 'harvey' or 'approx'.
+            optim_hessian : {'opg','oim','approx'}, optional
+                The method by which the Hessian is numerically approximated. 'opg'
+                uses outer product of gradients, 'oim' uses the information
+                matrix formula from Harvey (1989), and 'approx' uses numerical
+                approximation. This keyword is only relevant if the
+                optimization method uses the Hessian matrix.
+            low_memory : bool, optional
+                If set to True, techniques are applied to substantially reduce
+                memory usage. If used, some features of the results object will
+                not be available (including smoothed results and in-sample
+                prediction), although out-of-sample forecasting is possible.
+                Default is False.
+            **kwargs
+                Additional keyword arguments to pass to the optimizer.
+
+            Returns
+            -------
+            results
+                Results object holding results from fitting a state space model.
+
+            See Also
+            --------
+            statsmodels.base.model.LikelihoodModel.fit
+            statsmodels.tsa.statespace.mlemodel.MLEResults
+            statsmodels.tsa.statespace.structural.UnobservedComponentsResults
+            """
+
+            # Disable complex step approximations by default
+            optim_complex_step = optim_complex_step or False
+            cov_kwds = cov_kwds or {
+                "approx_complex_step": False,
+                "approx_centered": True,
+            }
+
+            return super().fit(
+                start_params=start_params,
+                transformed=transformed,
+                includes_fixed=includes_fixed,
+                cov_type=cov_type,
+                cov_kwds=cov_kwds,
+                method=method,
+                maxiter=maxiter,
+                full_output=full_output,
+                disp=disp,
+                callback=callback,
+                return_params=return_params,
+                optim_score=optim_score,
+                optim_complex_step=optim_complex_step,
+                optim_hessian=optim_hessian,
+                flags=flags,
+                low_memory=low_memory,
+                **kwargs,
+            )
+
+    return DSGEModel
```

### Comparing `gEconpy-1.1.0/gEconpy/shared/utilities.py` & `gEconpy-1.2.0/gEconpy/shared/utilities.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,403 +1,403 @@
-from collections import defaultdict
-from copy import copy
-from enum import EnumMeta
-from typing import Any, Callable, Dict, List, Union
-
-import numpy as np
-import sympy as sp
-
-from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
-
-VariableType = Union[TimeAwareSymbol, sp.Symbol]
-
-
-class IterEnum(EnumMeta):
-    def __init__(self, *args, **kwargs):
-        self.__idx = 0
-        super().__init__(*args, **kwargs)
-
-    def __contains__(self, item):
-        return item in {v.value for v in self.__members__.values()}
-
-    def __len__(self):
-        return len(self.__members__)
-
-    def __iter__(self):
-        return self
-
-    def __next__(self):
-        self.__idx += 1
-        try:
-            return list(self.__members__)[self.__idx - 1]
-        except IndexError:
-            self.__idx = 0
-            raise StopIteration
-
-
-def flatten_list(l, result_list=None):
-    if result_list is None:
-        result_list = []
-
-    if not isinstance(l, list):
-        result_list.append(l)
-        return result_list
-
-    for item in l:
-        if isinstance(item, list):
-            result_list = flatten_list(item, result_list)
-        else:
-            result_list.append(item)
-    return result_list
-
-
-SAFE_STRING_TO_INDEX_DICT = dict(ss="ss", tp1=1, tm1=-1, t=0)
-
-
-def safe_string_to_sympy(s, assumptions=None):
-    if isinstance(s, sp.Symbol):
-        return s
-
-    assumptions = assumptions or defaultdict(lambda: {})
-
-    *name, time_index_str = s.split("_")
-    if time_index_str not in [str(x) for x in SAFE_STRING_TO_INDEX_DICT.keys()]:
-        name.append(time_index_str)
-        name = "_".join(name)
-        return sp.Symbol(name, **assumptions[name])
-
-    name = "_".join(name)
-    time_index = SAFE_STRING_TO_INDEX_DICT[time_index_str]
-    symbol = TimeAwareSymbol(name, time_index, **assumptions[name])
-
-    return symbol
-
-
-def symbol_to_string(symbol: Union[str, VariableType]):
-    if isinstance(symbol, str):
-        return symbol
-    else:
-        return symbol.safe_name if isinstance(symbol, TimeAwareSymbol) else symbol.name
-
-
-def string_keys_to_sympy(d, assumptions=None):
-    result = {}
-    assumptions = assumptions or defaultdict(lambda: {})
-
-    for key, value in d.items():
-        if isinstance(key, sp.Symbol):
-            result[key] = value
-            continue
-
-        if "_" not in key:
-            result[sp.Symbol(key, **assumptions.get(key, {}))] = value
-            continue
-
-        new_key = safe_string_to_sympy(key, assumptions)
-        result[new_key] = value
-
-    return result
-
-
-def sympy_keys_to_strings(d):
-    result = {}
-    for key in d.keys():
-        result[symbol_to_string(key)] = d[key]
-
-    return result
-
-
-def set_equality_equals_zero(eq):
-    if not isinstance(eq, sp.Eq):
-        return eq
-
-    return eq.rhs - eq.lhs
-
-
-def eq_to_ss(eq):
-    var_list = [x for x in eq.atoms() if isinstance(x, TimeAwareSymbol)]
-    sub_dict = dict(zip(var_list, [x.to_ss() for x in var_list]))
-    return eq.subs(sub_dict)
-
-
-def expand_subs_for_all_times(sub_dict: Dict[TimeAwareSymbol, TimeAwareSymbol]):
-    result = {}
-    for lhs, rhs in sub_dict.items():
-        for t in [-1, 0, 1, "ss"]:
-            result[lhs.set_t(t)] = rhs.set_t(t) if isinstance(rhs, TimeAwareSymbol) else rhs
-
-    return result
-
-
-def step_equation_forward(eq):
-    to_step = []
-
-    for variable in set(eq.atoms()):
-        if hasattr(variable, "step_forward"):
-            if variable.time_index != "ss":
-                to_step.append(variable)
-
-    for variable in sorted(to_step, key=lambda x: x.time_index, reverse=True):
-        eq = eq.subs({variable: variable.step_forward()})
-
-    return eq
-
-
-def step_equation_backward(eq):
-    to_step = []
-
-    for variable in set(eq.atoms()):
-        if hasattr(variable, "step_forward"):
-            to_step.append(variable)
-
-    for variable in sorted(to_step, key=lambda x: x.time_index, reverse=False):
-        eq = eq.subs({variable: variable.step_backward()})
-
-    return eq
-
-
-def diff_through_time(eq, dx, discount_factor=1):
-    total_dydx = 0
-    next_dydx = 1
-
-    while next_dydx != 0:
-        next_dydx = eq.diff(dx)
-        eq = step_equation_forward(eq) * discount_factor
-        total_dydx += next_dydx
-
-    return total_dydx
-
-
-def substitute_all_equations(eqs, *sub_dicts):
-    if len(sub_dicts) > 1:
-        merged_dict = merge_dictionaries(*sub_dicts)
-        sub_dict = string_keys_to_sympy(merged_dict)
-    else:
-        sub_dict = string_keys_to_sympy(sub_dicts[0])
-
-    if isinstance(eqs, list):
-        return [eq.subs(sub_dict) for eq in eqs]
-    else:
-        result = {}
-        for key in eqs:
-            result[key] = (
-                eqs[key] if isinstance(eqs[key], (int, float)) else eqs[key].subs(sub_dict)
-            )
-        return result
-
-
-def is_variable(x):
-    return isinstance(x, TimeAwareSymbol)
-
-
-def is_number(x: str):
-    """
-    Parameters
-    ----------
-    x: str
-        string to test
-
-    Returns
-    -------
-    is_number: bool
-        Flag indicating whether this is a number
-
-    A small extension to the .isnumeric() string built-in method, to allow float values with "." to pass.
-    """
-
-    return all([c in set("0123456789.") for c in x])
-
-
-def sequential(x: Any, funcs: List[Callable]) -> Any:
-    """
-    Parameters
-    ----------
-    x: Any
-        A value to operate on
-    funcs: list
-        A list of functions to sequentially apply
-
-    Returns
-    -------
-    x: Any
-
-    Given a list of functions f, g, h, compute h(g(f(x)))
-    """
-
-    result = copy(x)
-    for func in funcs:
-        result = func(result)
-    return result
-
-
-def unpack_keys_and_values(d):
-    keys = list(d.keys())
-    values = list(d.values())
-
-    return keys, values
-
-
-def sympy_number_values_to_floats(d: Dict[VariableType, Any]):
-    for var, value in d.items():
-        if isinstance(value, sp.core.Number):
-            d[var] = float(value)
-    return d
-
-
-def float_values_to_sympy_float(d: Dict[VariableType, Any]):
-    for var, value in d.items():
-        if isinstance(value, (float, int)):
-            d[var] = sp.Float(value)
-
-    return d
-
-
-def sort_dictionary(d):
-    result = {}
-    sorted_keys = sorted(list(d.keys()))
-    for key in sorted_keys:
-        result[key] = d[key]
-
-    return result
-
-
-def sort_sympy_dict(d):
-    result = {}
-    sorted_keys = sorted(
-        list(d.keys()),
-        key=lambda x: x.base_name if isinstance(x, TimeAwareSymbol) else x.name,
-    )
-    for key in sorted_keys:
-        result[key] = d[key]
-
-    return result
-
-
-def select_keys(d, keys):
-    result = {}
-    for key in keys:
-        result[key] = d[key]
-    return result
-
-
-def reduce_system_via_substitution(system, sub_dict):
-    reduced_system = [eq.subs(sub_dict) for eq in system]
-    return [eq for eq in reduced_system if eq != 0]
-
-
-def merge_dictionaries(*dicts):
-    if not isinstance(dicts, (list, tuple)):
-        return dicts
-
-    result = {}
-    for d in dicts:
-        result.update(d)
-    return result
-
-
-def merge_functions(funcs, *args, **kwargs):
-    def combined_function(*args, **kwargs):
-        output = {}
-
-        for f in funcs:
-            output.update(f(*args, **kwargs))
-
-        return output
-
-    return combined_function
-
-
-def find_exp_args(eq):
-    if eq is None:
-        return None
-    comp_tree = list(sp.postorder_traversal(eq))
-    for arg in comp_tree:
-        if arg.func == sp.exp:
-            return arg
-
-
-def find_log_args(eq):
-    if eq is None:
-        return None
-
-    comp_tree = list(sp.postorder_traversal(eq))
-    for arg in comp_tree:
-        if arg.func == sp.Mul:
-            if isinstance(arg.args[0], sp.Symbol) and arg.args[1].func == sp.log:
-                return arg
-
-
-def is_log_transform_candidate(eq):
-    inside_exp = sequential(eq, [find_exp_args, find_log_args])
-    return inside_exp is not None
-
-
-def log_transform_exp_shock(eq):
-    out = (-sp.log(-eq.args[0]) + sp.log(eq.args[1])).simplify(inverse=True)
-    return out
-
-
-def expand_sub_dict_for_all_times(sub_dict):
-    result = {}
-    for k, v in sub_dict.items():
-        result[k] = v
-        result[step_equation_forward(k)] = step_equation_forward(v)
-        result[step_equation_backward(k)] = step_equation_backward(v)
-        result[eq_to_ss(k)] = eq_to_ss(v)
-
-    return result
-
-
-def make_all_var_time_combos(var_list):
-    result = []
-    for x in var_list:
-        result.extend([x.set_t(-1), x.set_t(0), x.set_t(1), x.set_t("ss")])
-
-    return result
-
-
-def add_all_variables_to_global_namespace(mod):
-    all_vars = [v for x in mod.variables for v in [x.step_backward(), x, x.step_forward()]]
-    var_dict = {}
-    for x in all_vars:
-        var_dict[x.safe_name] = x
-
-    for x in list(string_keys_to_sympy(mod.free_param_dict, mod.assumptions).keys()):
-        var_dict[x.name] = x
-
-    return var_dict
-
-
-def test_expr_is_zero(
-    eq: sp.Expr, params_to_test: List[sp.Symbol], n_tests: int = 10, tol: int = 16
-) -> bool:
-    """
-    Test if an expression is equal to zero by plugging in random values for requested symbols and evaluating. Useful
-    for complicated expressions involving exponents, where sympy's equivalence tests can fail.
-
-    Parameters
-    ----------
-    eq: sp.Expr
-        A sympy expression to be tested
-    params_to_test: list of sp.List
-        Variables to be tested
-    n_tests: int, default: 10
-        Number of tests to preform.
-    tol: int, default:16
-        Number of decimal places used to test equivlance to zero.
-
-    Returns
-    -------
-    result: bool
-        True if the expression was equal to zero in all tests, else False
-    """
-
-    n_params = len(params_to_test)
-    for _ in range(n_tests):
-        sub_dict = dict(zip(params_to_test, np.random.uniform(1e-4, 0.99, n_params)))
-        res = eq.evalf(subs=sub_dict, n=tol, chop=True)
-        for a in sp.preorder_traversal(res):
-            if isinstance(a, sp.Float):
-                res = res.subs(a, round(a, tol))
-        if res != 0:
-            return False
-    return True
+from collections import defaultdict
+from copy import copy
+from enum import EnumMeta
+from typing import Any, Callable, Dict, List, Union
+
+import numpy as np
+import sympy as sp
+
+from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
+
+VariableType = Union[TimeAwareSymbol, sp.Symbol]
+
+
+class IterEnum(EnumMeta):
+    def __init__(self, *args, **kwargs):
+        self.__idx = 0
+        super().__init__(*args, **kwargs)
+
+    def __contains__(self, item):
+        return item in {v.value for v in self.__members__.values()}
+
+    def __len__(self):
+        return len(self.__members__)
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        self.__idx += 1
+        try:
+            return list(self.__members__)[self.__idx - 1]
+        except IndexError:
+            self.__idx = 0
+            raise StopIteration
+
+
+def flatten_list(l, result_list=None):
+    if result_list is None:
+        result_list = []
+
+    if not isinstance(l, list):
+        result_list.append(l)
+        return result_list
+
+    for item in l:
+        if isinstance(item, list):
+            result_list = flatten_list(item, result_list)
+        else:
+            result_list.append(item)
+    return result_list
+
+
+SAFE_STRING_TO_INDEX_DICT = dict(ss="ss", tp1=1, tm1=-1, t=0)
+
+
+def safe_string_to_sympy(s, assumptions=None):
+    if isinstance(s, sp.Symbol):
+        return s
+
+    assumptions = assumptions or defaultdict(lambda: {})
+
+    *name, time_index_str = s.split("_")
+    if time_index_str not in [str(x) for x in SAFE_STRING_TO_INDEX_DICT.keys()]:
+        name.append(time_index_str)
+        name = "_".join(name)
+        return sp.Symbol(name, **assumptions[name])
+
+    name = "_".join(name)
+    time_index = SAFE_STRING_TO_INDEX_DICT[time_index_str]
+    symbol = TimeAwareSymbol(name, time_index, **assumptions[name])
+
+    return symbol
+
+
+def symbol_to_string(symbol: Union[str, VariableType]):
+    if isinstance(symbol, str):
+        return symbol
+    else:
+        return symbol.safe_name if isinstance(symbol, TimeAwareSymbol) else symbol.name
+
+
+def string_keys_to_sympy(d, assumptions=None):
+    result = {}
+    assumptions = assumptions or defaultdict(lambda: {})
+
+    for key, value in d.items():
+        if isinstance(key, sp.Symbol):
+            result[key] = value
+            continue
+
+        if "_" not in key:
+            result[sp.Symbol(key, **assumptions.get(key, {}))] = value
+            continue
+
+        new_key = safe_string_to_sympy(key, assumptions)
+        result[new_key] = value
+
+    return result
+
+
+def sympy_keys_to_strings(d):
+    result = {}
+    for key in d.keys():
+        result[symbol_to_string(key)] = d[key]
+
+    return result
+
+
+def set_equality_equals_zero(eq):
+    if not isinstance(eq, sp.Eq):
+        return eq
+
+    return eq.rhs - eq.lhs
+
+
+def eq_to_ss(eq):
+    var_list = [x for x in eq.atoms() if isinstance(x, TimeAwareSymbol)]
+    sub_dict = dict(zip(var_list, [x.to_ss() for x in var_list]))
+    return eq.subs(sub_dict)
+
+
+def expand_subs_for_all_times(sub_dict: Dict[TimeAwareSymbol, TimeAwareSymbol]):
+    result = {}
+    for lhs, rhs in sub_dict.items():
+        for t in [-1, 0, 1, "ss"]:
+            result[lhs.set_t(t)] = rhs.set_t(t) if isinstance(rhs, TimeAwareSymbol) else rhs
+
+    return result
+
+
+def step_equation_forward(eq):
+    to_step = []
+
+    for variable in set(eq.atoms()):
+        if hasattr(variable, "step_forward"):
+            if variable.time_index != "ss":
+                to_step.append(variable)
+
+    for variable in sorted(to_step, key=lambda x: x.time_index, reverse=True):
+        eq = eq.subs({variable: variable.step_forward()})
+
+    return eq
+
+
+def step_equation_backward(eq):
+    to_step = []
+
+    for variable in set(eq.atoms()):
+        if hasattr(variable, "step_forward"):
+            to_step.append(variable)
+
+    for variable in sorted(to_step, key=lambda x: x.time_index, reverse=False):
+        eq = eq.subs({variable: variable.step_backward()})
+
+    return eq
+
+
+def diff_through_time(eq, dx, discount_factor=1):
+    total_dydx = 0
+    next_dydx = 1
+
+    while next_dydx != 0:
+        next_dydx = eq.diff(dx)
+        eq = step_equation_forward(eq) * discount_factor
+        total_dydx += next_dydx
+
+    return total_dydx
+
+
+def substitute_all_equations(eqs, *sub_dicts):
+    if len(sub_dicts) > 1:
+        merged_dict = merge_dictionaries(*sub_dicts)
+        sub_dict = string_keys_to_sympy(merged_dict)
+    else:
+        sub_dict = string_keys_to_sympy(sub_dicts[0])
+
+    if isinstance(eqs, list):
+        return [eq.subs(sub_dict) for eq in eqs]
+    else:
+        result = {}
+        for key in eqs:
+            result[key] = (
+                eqs[key] if isinstance(eqs[key], (int, float)) else eqs[key].subs(sub_dict)
+            )
+        return result
+
+
+def is_variable(x):
+    return isinstance(x, TimeAwareSymbol)
+
+
+def is_number(x: str):
+    """
+    Parameters
+    ----------
+    x: str
+        string to test
+
+    Returns
+    -------
+    is_number: bool
+        Flag indicating whether this is a number
+
+    A small extension to the .isnumeric() string built-in method, to allow float values with "." to pass.
+    """
+
+    return all([c in set("0123456789.") for c in x])
+
+
+def sequential(x: Any, funcs: List[Callable]) -> Any:
+    """
+    Parameters
+    ----------
+    x: Any
+        A value to operate on
+    funcs: list
+        A list of functions to sequentially apply
+
+    Returns
+    -------
+    x: Any
+
+    Given a list of functions f, g, h, compute h(g(f(x)))
+    """
+
+    result = copy(x)
+    for func in funcs:
+        result = func(result)
+    return result
+
+
+def unpack_keys_and_values(d):
+    keys = list(d.keys())
+    values = list(d.values())
+
+    return keys, values
+
+
+def sympy_number_values_to_floats(d: Dict[VariableType, Any]):
+    for var, value in d.items():
+        if isinstance(value, sp.core.Number):
+            d[var] = float(value)
+    return d
+
+
+def float_values_to_sympy_float(d: Dict[VariableType, Any]):
+    for var, value in d.items():
+        if isinstance(value, (float, int)):
+            d[var] = sp.Float(value)
+
+    return d
+
+
+def sort_dictionary(d):
+    result = {}
+    sorted_keys = sorted(list(d.keys()))
+    for key in sorted_keys:
+        result[key] = d[key]
+
+    return result
+
+
+def sort_sympy_dict(d):
+    result = {}
+    sorted_keys = sorted(
+        list(d.keys()),
+        key=lambda x: x.base_name if isinstance(x, TimeAwareSymbol) else x.name,
+    )
+    for key in sorted_keys:
+        result[key] = d[key]
+
+    return result
+
+
+def select_keys(d, keys):
+    result = {}
+    for key in keys:
+        result[key] = d[key]
+    return result
+
+
+def reduce_system_via_substitution(system, sub_dict):
+    reduced_system = [eq.subs(sub_dict) for eq in system]
+    return [eq for eq in reduced_system if eq != 0]
+
+
+def merge_dictionaries(*dicts):
+    if not isinstance(dicts, (list, tuple)):
+        return dicts
+
+    result = {}
+    for d in dicts:
+        result.update(d)
+    return result
+
+
+def merge_functions(funcs, *args, **kwargs):
+    def combined_function(*args, **kwargs):
+        output = {}
+
+        for f in funcs:
+            output.update(f(*args, **kwargs))
+
+        return output
+
+    return combined_function
+
+
+def find_exp_args(eq):
+    if eq is None:
+        return None
+    comp_tree = list(sp.postorder_traversal(eq))
+    for arg in comp_tree:
+        if arg.func == sp.exp:
+            return arg
+
+
+def find_log_args(eq):
+    if eq is None:
+        return None
+
+    comp_tree = list(sp.postorder_traversal(eq))
+    for arg in comp_tree:
+        if arg.func == sp.Mul:
+            if isinstance(arg.args[0], sp.Symbol) and arg.args[1].func == sp.log:
+                return arg
+
+
+def is_log_transform_candidate(eq):
+    inside_exp = sequential(eq, [find_exp_args, find_log_args])
+    return inside_exp is not None
+
+
+def log_transform_exp_shock(eq):
+    out = (-sp.log(-eq.args[0]) + sp.log(eq.args[1])).simplify(inverse=True)
+    return out
+
+
+def expand_sub_dict_for_all_times(sub_dict):
+    result = {}
+    for k, v in sub_dict.items():
+        result[k] = v
+        result[step_equation_forward(k)] = step_equation_forward(v)
+        result[step_equation_backward(k)] = step_equation_backward(v)
+        result[eq_to_ss(k)] = eq_to_ss(v)
+
+    return result
+
+
+def make_all_var_time_combos(var_list):
+    result = []
+    for x in var_list:
+        result.extend([x.set_t(-1), x.set_t(0), x.set_t(1), x.set_t("ss")])
+
+    return result
+
+
+def add_all_variables_to_global_namespace(mod):
+    all_vars = [v for x in mod.variables for v in [x.step_backward(), x, x.step_forward()]]
+    var_dict = {}
+    for x in all_vars:
+        var_dict[x.safe_name] = x
+
+    for x in list(string_keys_to_sympy(mod.free_param_dict, mod.assumptions).keys()):
+        var_dict[x.name] = x
+
+    return var_dict
+
+
+def test_expr_is_zero(
+    eq: sp.Expr, params_to_test: List[sp.Symbol], n_tests: int = 10, tol: int = 16
+) -> bool:
+    """
+    Test if an expression is equal to zero by plugging in random values for requested symbols and evaluating. Useful
+    for complicated expressions involving exponents, where sympy's equivalence tests can fail.
+
+    Parameters
+    ----------
+    eq: sp.Expr
+        A sympy expression to be tested
+    params_to_test: list of sp.List
+        Variables to be tested
+    n_tests: int, default: 10
+        Number of tests to preform.
+    tol: int, default:16
+        Number of decimal places used to test equivlance to zero.
+
+    Returns
+    -------
+    result: bool
+        True if the expression was equal to zero in all tests, else False
+    """
+
+    n_params = len(params_to_test)
+    for _ in range(n_tests):
+        sub_dict = dict(zip(params_to_test, np.random.uniform(1e-4, 0.99, n_params)))
+        res = eq.evalf(subs=sub_dict, n=tol, chop=True)
+        for a in sp.preorder_traversal(res):
+            if isinstance(a, sp.Float):
+                res = res.subs(a, round(a, tol))
+        if res != 0:
+            return False
+    return True
```

### Comparing `gEconpy-1.1.0/gEconpy/solvers/cycle_reduction.py` & `gEconpy-1.2.0/gEconpy/solvers/cycle_reduction.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,136 +1,136 @@
-from typing import Optional, Tuple
-
-import numpy as np
-from numba import njit
-from numpy.typing import ArrayLike
-
-
-@njit(cache=True)
-def cycle_reduction(
-    A0: ArrayLike,
-    A1: ArrayLike,
-    A2: ArrayLike,
-    max_iter: int = 1000,
-    tol: float = 1e-7,
-    verbose: bool = True,
-) -> Tuple[Optional[ArrayLike], str, float]:
-
-    """
-    Solve quadratic matrix equation of the form $A0x^2 + A1x + A2 = 0$ via cycle reduction algorithm of [1].
-    Useful in the DSGE context to solve for the implicit derivative of the policy function, g, with respect to
-    state vector y.
-
-    Adapted from the Dynare file cycle_reduction.m, found at
-    https://github.com/DynareTeam/dynare/blob/master/matlab/cycle_reduction.m
-
-    Parameters
-    ----------
-    A0: Arraylike
-        Coefficient matrix associated with the constant term of the matrix quadratic equation. In DSGE models, this is
-        dF_d_t-1, the derivative of the system with respect to variables that enter as lags
-    A1: ArrayLike
-        Coefficient matrix associated with the linear term of the matrix quadratic equation. In DSGE models, this is
-        dF_d_t, the derivative of the system with respect to variables that enter at the current time
-    A2: ArrayLike
-        Coefficient matrix associated with the quadratic term of the matrix quadratic equation. In DSGE models, this is
-        dF_d_t+1, the derivative of the system with respect to variables that enter in expectation
-    max_iter: int, default: 1000
-        Maximum number of iterations to perform before giving up.
-    tol: float, default: 1e-7
-        Floating point tolerance used to detect algorithmic convergence
-    verbose: bool, default: True
-        If true, prints the sum of squared residuals that result when the system is computed used the solution.
-
-    Returns
-    -------
-
-    References
-    -------
-    ..[1] D.A. Bini, G. Latouche, B. Meini (2002), "Solving matrix polynomial equations
-          arising in queueing problems", Linear Algebra and its Applications 340, pp. 222-244
-    ..[2]
-
-    """
-    result = "Optimization successful"
-    log_norm = 0
-    X = None
-
-    A0_initial = A0.copy()
-    A1_hat = A1.copy()
-
-    if verbose:
-        A1_initial = A1.copy()
-        A2_initial = A2.copy()
-
-    n, _ = A0.shape
-    idx_0 = np.arange(n)
-    idx_1 = idx_0 + n
-
-    # Pre-allocate this so it doesn't have to be repeatedly created
-    EYE = np.eye(A1.shape[0])
-
-    for i in range(max_iter):
-        tmp = np.vstack((A0, A2)) @ np.linalg.solve(A1, EYE) @ np.hstack((A0, A2))
-
-        A1 = A1 - tmp[idx_0, :][:, idx_1] - tmp[idx_1, :][:, idx_0]
-        A0 = -tmp[idx_0, :][:, idx_0]
-        A2 = -tmp[idx_1, :][:, idx_1]
-        A1_hat = A1_hat - tmp[idx_1, :][:, idx_0]
-
-        A0_L1_norm = np.linalg.norm(A0, ord=1)
-        if A0_L1_norm < tol:
-            # Algorithm is successful when the L1 norm of A2 is sufficiently small
-            A2_L1_norm = np.linalg.norm(A2, ord=1)
-            if A2_L1_norm < tol:
-                break
-
-        elif np.isnan(A0_L1_norm) or i == (max_iter - 1):
-            # If we fail, figure out how far we got
-            if A0_L1_norm < tol:
-                result = (
-                    "Iteration on matrix A0 and A1 converged towards a solution, but A2 did not."
-                )
-                log_norm = np.log(np.linalg.norm(A2, 1))
-            else:
-                result = "Iteration on all matrices failed to converged"
-                log_norm = np.log(np.linalg.norm(A1, 1))
-
-            return X, result, log_norm
-
-    X = -np.linalg.solve(A1_hat, A0_initial)
-
-    if verbose:
-        res = A0_initial + A1_initial @ X + A2_initial @ X @ X
-        print("Solution found, sum of squared residuals: ", (res**2).sum())
-
-    return X, result, log_norm
-
-
-@njit(cache=True)
-def solve_shock_matrix(B, C, D, G_1):
-    """
-    Given the partial solution to the linear approximate policy function G_1, solve for the remaining component of the
-    policy function, R.
-
-    Parameters
-    ----------
-    B: ArrayLike
-        Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to variables that
-        are observed when decision making: those with t subscripts.
-    C: Arraylike
-        Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to variables that
-        enter in expectation when decision making: those with t+1 subscripts.
-    D: ArrayLike
-        Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to exogenous shocks.
-    G_1: ArrayLike
-        Transition matrix T in state space jargon. Gives the effect of variable values at time t on the
-        values of the variables at time t+1.
-    Returns
-    -------
-    impact: ArrayLike
-        Selection matrix R in state space jargon. Gives the effect of exogenous shocks at time t on the values of
-        system variables at time t+1.
-
-    """
-
-    return -np.linalg.solve(C @ G_1 + B, np.eye(C.shape[0])) @ D
+from typing import Optional, Tuple
+
+import numpy as np
+from numba import njit
+from numpy.typing import ArrayLike
+
+
+@njit(cache=True)
+def cycle_reduction(
+    A0: ArrayLike,
+    A1: ArrayLike,
+    A2: ArrayLike,
+    max_iter: int = 1000,
+    tol: float = 1e-7,
+    verbose: bool = True,
+) -> Tuple[Optional[ArrayLike], str, float]:
+
+    """
+    Solve quadratic matrix equation of the form $A0x^2 + A1x + A2 = 0$ via cycle reduction algorithm of [1].
+    Useful in the DSGE context to solve for the implicit derivative of the policy function, g, with respect to
+    state vector y.
+
+    Adapted from the Dynare file cycle_reduction.m, found at
+    https://github.com/DynareTeam/dynare/blob/master/matlab/cycle_reduction.m
+
+    Parameters
+    ----------
+    A0: Arraylike
+        Coefficient matrix associated with the constant term of the matrix quadratic equation. In DSGE models, this is
+        dF_d_t-1, the derivative of the system with respect to variables that enter as lags
+    A1: ArrayLike
+        Coefficient matrix associated with the linear term of the matrix quadratic equation. In DSGE models, this is
+        dF_d_t, the derivative of the system with respect to variables that enter at the current time
+    A2: ArrayLike
+        Coefficient matrix associated with the quadratic term of the matrix quadratic equation. In DSGE models, this is
+        dF_d_t+1, the derivative of the system with respect to variables that enter in expectation
+    max_iter: int, default: 1000
+        Maximum number of iterations to perform before giving up.
+    tol: float, default: 1e-7
+        Floating point tolerance used to detect algorithmic convergence
+    verbose: bool, default: True
+        If true, prints the sum of squared residuals that result when the system is computed used the solution.
+
+    Returns
+    -------
+
+    References
+    -------
+    ..[1] D.A. Bini, G. Latouche, B. Meini (2002), "Solving matrix polynomial equations
+          arising in queueing problems", Linear Algebra and its Applications 340, pp. 222-244
+    ..[2]
+
+    """
+    result = "Optimization successful"
+    log_norm = 0
+    X = None
+
+    A0_initial = A0.copy()
+    A1_hat = A1.copy()
+
+    if verbose:
+        A1_initial = A1.copy()
+        A2_initial = A2.copy()
+
+    n, _ = A0.shape
+    idx_0 = np.arange(n)
+    idx_1 = idx_0 + n
+
+    # Pre-allocate this so it doesn't have to be repeatedly created
+    EYE = np.eye(A1.shape[0])
+
+    for i in range(max_iter):
+        tmp = np.vstack((A0, A2)) @ np.linalg.solve(A1, EYE) @ np.hstack((A0, A2))
+
+        A1 = A1 - tmp[idx_0, :][:, idx_1] - tmp[idx_1, :][:, idx_0]
+        A0 = -tmp[idx_0, :][:, idx_0]
+        A2 = -tmp[idx_1, :][:, idx_1]
+        A1_hat = A1_hat - tmp[idx_1, :][:, idx_0]
+
+        A0_L1_norm = np.linalg.norm(A0, ord=1)
+        if A0_L1_norm < tol:
+            # Algorithm is successful when the L1 norm of A2 is sufficiently small
+            A2_L1_norm = np.linalg.norm(A2, ord=1)
+            if A2_L1_norm < tol:
+                break
+
+        elif np.isnan(A0_L1_norm) or i == (max_iter - 1):
+            # If we fail, figure out how far we got
+            if A0_L1_norm < tol:
+                result = (
+                    "Iteration on matrix A0 and A1 converged towards a solution, but A2 did not."
+                )
+                log_norm = np.log(np.linalg.norm(A2, 1))
+            else:
+                result = "Iteration on all matrices failed to converged"
+                log_norm = np.log(np.linalg.norm(A1, 1))
+
+            return X, result, log_norm
+
+    X = -np.linalg.solve(A1_hat, A0_initial)
+
+    if verbose:
+        res = A0_initial + A1_initial @ X + A2_initial @ X @ X
+        print("Solution found, sum of squared residuals: ", (res**2).sum())
+
+    return X, result, log_norm
+
+
+@njit(cache=True)
+def solve_shock_matrix(B, C, D, G_1):
+    """
+    Given the partial solution to the linear approximate policy function G_1, solve for the remaining component of the
+    policy function, R.
+
+    Parameters
+    ----------
+    B: ArrayLike
+        Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to variables that
+        are observed when decision making: those with t subscripts.
+    C: Arraylike
+        Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to variables that
+        enter in expectation when decision making: those with t+1 subscripts.
+    D: ArrayLike
+        Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to exogenous shocks.
+    G_1: ArrayLike
+        Transition matrix T in state space jargon. Gives the effect of variable values at time t on the
+        values of the variables at time t+1.
+    Returns
+    -------
+    impact: ArrayLike
+        Selection matrix R in state space jargon. Gives the effect of exogenous shocks at time t on the values of
+        system variables at time t+1.
+
+    """
+
+    return -np.linalg.solve(C @ G_1 + B, np.eye(C.shape[0])) @ D
```

### Comparing `gEconpy-1.1.0/gEconpy/solvers/gensys.py` & `gEconpy-1.2.0/gEconpy/solvers/gensys.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,399 +1,399 @@
-from typing import Optional, Tuple
-
-import numpy as np
-from numpy.typing import ArrayLike
-from scipy import linalg
-
-
-def qzdiv(
-    stake: float, A: ArrayLike, B: ArrayLike, Q: ArrayLike, Z: ArrayLike
-) -> Tuple[ArrayLike, ArrayLike, ArrayLike, ArrayLike]:
-    """
-    Christopher Sim's qzdiv http://sims.princeton.edu/yftp/gensys/mfiles/qzdiv.m
-    :param stake: float, largest positive value for which an eigenvalue is considered stable
-    :param A: Array, upper-triangular matrix
-    :param B: Array, upper-triangular matrix
-    :param Q: Array, matrix of left Schur vectors.
-    :param Z: Array, matrix of right Schur vectors.
-    :return: Tuple, A, B, Q, Z, sorted such that all unstable roots are placed in the lower-right corners of the
-             matrices.
-
-    Original docstring:
-    Takes U.T. matrices A, B, orthonormal matrices Q,Z, rearranges them so that all cases of abs(B(i,i)/A(i,i))>stake
-    are in lower right  corner, while preserving U.T. and orthonormal properties and Q'AZ' and Q'BZ'.
-    The columns of v are sorted correspondingly.
-
-    Additional notes:
-    Matrices A, B, Q, and Z are the output of the generalized Schur decomposition (QZ decomposition) of the system
-    matrices G0 and G1. A and B are upper triangular, with the properties Q @ A @ Z.T = G0 and Q @ B @ Z.T = G1.
-
-    TODO: scipy offers a sorted qz routine, ordqz, which automatically sorts the matrices by size of eigenvalue. This
-        seems to be what the functions qzdiv and qzswitch do, so it might be worthwhile to see if we can just use
-        ordqz instead.
-
-    TODO: Add shape information to the Typing (see PEP 646)
-    """
-
-    n, _ = A.shape
-
-    root = np.hstack([np.diag(A)[:, None], np.diag(B)[:, None]])
-    root = np.abs(root)
-    root[:, 0] = root[:, 0] - (root[:, 0] < 1e-13) * (root[:, 0] + root[:, 1])
-    root[:, 1] = root[:, 1] / root[:, 0]
-
-    for i in range(n - 1, -1, -1):
-        m = None
-        for j in range(i, -1, -1):
-            if (root[j, 1] > stake) or (root[j, 1] < -0.1):
-                m = j
-                break
-
-        if m is None:
-            return A, B, Q, Z
-
-        for k in range(m, i):
-            A, B, Q, Z = qzswitch(k, A, B, Q, Z)
-            root[k, 1], root[k + 1, 1] = root[k + 1, 1], root[k, 1]
-
-    return A, B, Q, Z
-
-
-def qzswitch(
-    i: int, A: ArrayLike, B: ArrayLike, Q: ArrayLike, Z: ArrayLike
-) -> Tuple[ArrayLike, ArrayLike, ArrayLike, ArrayLike]:
-    """
-    Christopher Sim's qzswitch,
-    :param i: int, index of matrix diagonal to switch
-    :param A: Array, upper-triangular matrix
-    :param B: Array, upper-triangular matrix
-    :param Q: Array, matrix of left Schur vectors.
-    :param Z: Array, matrix of right Schur vectors.
-    :return: Tuple of A, B, Q, Z
-
-    Original docstring:
-    Takes U.T. matrices A, B, orthonormal matrices Q,Z, interchanges diagonal elements i and i+1 of both A and B,
-    while maintaining Q'AZ' and Q'BZ' unchanged.  If diagonal elements of A and B are zero at matching positions,
-    the returned A will have zeros at both positions on the diagonal.  This is natural behavior if this routine is used
-    to drive all zeros on the diagonal of A to the lower right, but in this case the qz transformation is not unique
-    and it is not possible simply to switch the positions of the diagonal elements of both A and B.
-
-    TODO: Add shape information to the Typing (see PEP 646)
-    """
-    eps = np.spacing(1)
-
-    a = A[i, i]
-    b = A[i, i + 1]
-    c = A[i + 1, i + 1]
-    d = B[i, i]
-    e = B[i, i + 1]
-    f = B[i + 1, i + 1]
-
-    if (abs(c) < eps) & (abs(f) < eps):
-        if abs(a) < eps:
-            return A, B, Q, Z
-
-        else:
-            wz = np.c_[b, -a].T
-            wz = wz / np.sqrt(wz.conj().T @ wz)
-            wz = np.hstack([wz, np.c_[wz[1].conj().T, -wz[0].conj().T].T])
-            xy = np.eye(2)
-
-    elif (abs(a) < eps) & (abs(d) < eps):
-        if abs(c) < eps:
-            return A, B, Q, Z
-        else:
-            wz = np.eye(2)
-            xy = np.c_[c, -b].T
-            xy = xy / np.sqrt(xy @ xy.conj().T)
-            xy = np.hstack([np.c_[xy[1].conj().T, -xy[0].conj().T].T, xy])
-
-    else:
-        wz = np.c_[c * e - f * b, (c * d - f * a).conj()]
-        xy = np.c_[(b * d - e * a).conj(), (c * d - f * a).conj()]
-
-        n = np.sqrt(wz @ wz.conj().T)
-        m = np.sqrt(xy @ xy.conj().T)
-
-        if m < eps * 100:
-            return A, B, Q, Z
-
-        wz = wz / n
-        xy = xy / m
-
-        wz = np.vstack([wz, np.c_[-wz[:, 1].conj(), wz[:, 0].conj()]])
-        xy = np.vstack([xy, np.c_[-xy[:, 1].conj(), xy[:, 0].conj()]])
-
-    idx_slice = slice(i, i + 2)
-
-    A[idx_slice, :] = xy @ A[idx_slice, :]
-    B[idx_slice, :] = xy @ B[idx_slice, :]
-    Q[idx_slice, :] = xy @ Q[idx_slice, :]
-
-    A[:, idx_slice] = A[:, idx_slice] @ wz
-    B[:, idx_slice] = B[:, idx_slice] @ wz
-    Z[:, idx_slice] = Z[:, idx_slice] @ wz
-
-    return A, B, Q, Z
-
-
-def determine_n_unstable(
-    A: ArrayLike, B: ArrayLike, div: float, realsmall: float
-) -> Tuple[float, int, bool]:
-    """
-    :param A: array, upper-triangular matrix, output of QZ decomposition
-    :param B: array, upper-triangular matrix, output of QZ decomposition
-    :param div: float, largest positive value for which an eigenvalue is considered stable
-    :param realsmall: an arbitrarily small number
-    :return: Tuple
-
-    Originally part of gensys, this helper function determines how many roots of the system described by A and B are
-    unstable. Returns three values:
-        div, a float representing which roots of the system can be considered stable,
-        n_unstable, an int of how many unstable roots are in the system, and
-        zxz, a bool that signals whether the system has a unique solution.
-    """
-    n, _ = A.shape
-    n_unstable = 0
-    zxz = False
-    compute_div = div is None
-    div = 1.01 if div is None else div
-
-    for i in range(n):
-        if compute_div:
-            if abs(A[i, i]) > 0:
-                divhat = abs(B[i, i] / A[i, i])
-                if (1 + realsmall < divhat) and divhat <= div:
-                    div = 0.5 * (1 + divhat)
-        n_unstable += abs(B[i, i]) > div * abs(A[i, i])
-
-        zxz = (abs(A[i, i]) < realsmall) & (abs(B[i, i]) < realsmall)
-
-    return div, n_unstable, zxz
-
-
-def split_matrix_on_eigen_stability(A: ArrayLike, n_unstable: int) -> Tuple[ArrayLike, ArrayLike]:
-    """
-    :param A: Arrayline, array to split
-    :param n_unstable: int, number of unstable roots in the
-    :return: Tuple of (A1, A2), the A matrix split such that all stable roots are in the A1 matrix,
-             and the unstable roots are in the A2 matrix.
-
-    Originally in the gensys function, split out here for readability.
-    """
-    n, _ = A.shape
-    stable_slice = slice(None, n - n_unstable)
-    unstable_slice = slice(n - n_unstable, None)
-
-    A1 = A[stable_slice]
-    A2 = A[unstable_slice]
-
-    return A1, A2
-
-
-def build_u_v_d(eta: ArrayLike, realsmall: float):
-    """
-
-    :param eta:
-    :param realsmall:
-    :return: tuple, svd decomposition of eta plus an array of non-zero indices
-
-    Piece of gensys adapted from Matlab code, split out as a helper function.
-    """
-
-    u_eta, d_eta, v_eta = linalg.svd(eta)
-    d_eta = np.diag(d_eta)  # match matlab output of svd
-    v_eta = v_eta.conj().T  # match matlab output of svd
-
-    md = min(d_eta.shape)
-    big_ev = np.where(np.diagonal(d_eta[:md, :md] > realsmall))[0]
-
-    u_eta = u_eta[:, big_ev]
-    v_eta = v_eta[:, big_ev]
-    d_eta = d_eta[big_ev, big_ev]
-
-    if d_eta.ndim == 1:
-        d_eta = np.diag(d_eta)
-
-    return u_eta, v_eta, d_eta, big_ev
-
-
-def gensys(
-    g0: ArrayLike,
-    g1: ArrayLike,
-    c: ArrayLike,
-    psi: ArrayLike,
-    pi: ArrayLike,
-    div: Optional[float] = None,
-    tol: Optional[float] = 1e-8,
-) -> Tuple:
-    """
-    Christopher Sim's gensys, http://sims.princeton.edu/yftp/gensys/mfiles/gensys.m
-
-    Solves rational expectations equations as described in [1] by partitioning partitioning the system into stable
-    and unstable roots, then eliminating the unstable roots via QZ decomposition.
-
-    Original matlab docstring:
-    System given as g0*y(t)=g1*y(t-1)+c+psi*z(t)+pi*eta(t), with z an exogenous variable process and eta being
-    endogenously determined one-step-ahead expectational errors.  Returned system is
-        y(t)=G1*y(t-1)+C+impact*z(t)+ywt*inv(I-fmat*inv(L))*fwt*z(t+1) .
-    If z(t) is i.i.d., the last term drops out. If div is omitted from argument list, a div>1 is calculated.
-    eu(1)=1 for existence,
-    eu(2)=1 for uniqueness.
-    eu(1)=-1 for existence only with not-s.c. z;
-    eu=[-2,-2] for coincident zeros.
-
-    Parameters
-    ----------
-    g0: ArrayLike
-        Coefficient matrix of the dynamic system corresponding to the time-t variables
-    g1: ArrayLike
-        Coefficient matrix of the dynamic system corresponding to the time t-1 variables
-    c: ArrayLike
-        Vector of constant terms
-    psi: ArrayLike
-        Coefficient matrix of the dynamic system corresponding to the exogenous shock terms
-    pi: ArrayLike
-        Coefficient matrix of the dynamic system corresponding to the endogenously determined
-        expectational errors.
-    div: float
-        # TODO: WRITE ME
-    tol: float, default: 1e-8
-        Level of floating point precision
-
-    Returns
-    -------
-    G_1: ArrayLike
-        Policy function relating the current timestep to the next, transition matrix T in state space jargon.
-    C: ArrayLike
-        Array of system means, intercept vector c in state space jargon.
-    impact: ArrayLike
-        Policy function component relating exogenous shocks observed at the t to variable values in t+1, selection
-        matric R in state space jargon.
-    f_mat: ArrayLike
-        # TODO: WRITE ME
-    f_wt: ArrayLike
-        # TODO: WRITE ME
-    y_wt: ArrayLike
-        # TODO: WRITE ME
-    gev: ArrayLike
-        Generalized left and right eigenvalues generated by qz(g0, g1), sorted such that stable roots are in the
-        top-left corner
-    eu: tuple
-        Tuple of two values indicting uniqueness and determinacy of the solution.
-    loose: int
-        Number of loose endogenous variables.
-
-    References
-    -------
-    ..[1] Sims, Christopher A. "Solving linear rational expectations models." Computational economics 20.1-2 (2002): 1.
-
-    TODO: Can this be written in Numba/Aesara?
-    """
-    eu = [0, 0, 0]
-
-    n, _ = g1.shape
-    A, B, Q, Z = linalg.qz(g0, g1, "complex")
-    Q = Q.conj().T  # q is transposed relative to matlab, see scipy docs
-
-    div, n_unstable, zxz = determine_n_unstable(A, B, div, tol)
-    n_stable = n - n_unstable
-
-    if zxz:
-        eu = [-2, -2, 0]
-        return None, None, None, None, None, None, None, eu, None
-
-    A, B, Q, Z = qzdiv(div, A, B, Q, Z)
-    gev = np.c_[np.diagonal(A), np.diagonal(B)]
-
-    Q1, Q2 = split_matrix_on_eigen_stability(Q, n_unstable)
-
-    eta_wt = Q2 @ pi
-    _, n_eta = pi.shape
-
-    # No stable roots
-    if n_unstable == 0:
-        big_ev = 0
-
-        u_eta = np.zeros((0, 0))
-        d_eta = np.zeros((0, 0))
-        v_eta = np.zeros((n_eta, 0))
-
-    else:
-        u_eta, v_eta, d_eta, big_ev = build_u_v_d(eta_wt, tol)
-
-    if len(big_ev) >= n_unstable:
-        eu[0] = 1
-
-    # All stable roots
-    if n_unstable == n:
-        eta_wt_1 = np.zeros((0, n_eta))
-        u_eta_1 = np.zeros((0, 0))
-        d_eta_1 = np.zeros((0, 0))
-        v_eta_1 = np.zeros((n_eta, 0))
-
-    else:
-        eta_wt_1 = Q1 @ pi
-        u_eta_1, v_eta_1, d_eta_1, big_ev = build_u_v_d(eta_wt_1, tol)
-
-    if 0 in v_eta_1.shape:
-        unique = True
-    else:
-        loose = v_eta_1 - v_eta @ v_eta.T @ v_eta_1
-        ul, dl, vl = linalg.svd(loose)
-        if dl.ndim == 1:
-            dl = np.diag(dl)
-
-        n_loose = (np.abs(np.diagonal(dl)) > (tol * n)).sum()
-        eu[2] = n_loose
-        unique = n_loose == 0
-
-    if unique:
-        eu[1] = 1
-
-    inner_term = u_eta @ linalg.solve(d_eta, v_eta.conj().T) @ v_eta_1 @ d_eta_1 @ u_eta_1.conj().T
-
-    T_mat = np.c_[np.eye(n_stable), -inner_term.conj().T]
-    G_0 = np.r_[T_mat @ A, np.c_[np.zeros((n_unstable, n_stable)), np.eye(n_unstable)]]
-
-    G_1 = np.r_[T_mat @ B, np.zeros((n_unstable, n))]
-
-    G_0_inv = linalg.inv(G_0)
-    G_1 = G_0_inv @ G_1
-
-    idx = slice(n_stable, n)
-
-    C = np.r_[T_mat @ Q @ c, linalg.solve(A[idx, idx] - B[idx, idx], Q2) @ c]
-
-    impact = G_0_inv @ np.r_[T_mat @ Q @ psi, np.zeros((n_unstable, psi.shape[1]))]
-
-    f_mat = linalg.solve(B[idx, idx], A[idx, idx])
-    f_wt = -linalg.solve(B[idx, idx], Q2) @ psi
-    y_wt = G_0_inv[:, idx]
-
-    loose = (
-        G_0_inv
-        @ np.r_[
-            eta_wt_1 @ (np.eye(n_eta) - v_eta @ v_eta.conj().T),
-            np.zeros((n_unstable, n_eta)),
-        ]
-    )
-
-    G_1 = (Z @ G_1 @ Z.conj().T).real
-    C = (Z @ C).real
-    impact = (Z @ impact).real
-    loose = (Z @ loose).real
-    y_wt = Z @ y_wt
-
-    return G_1, C, impact, f_mat, f_wt, y_wt, gev, eu, loose
-
-
-def interpret_gensys_output(eu):
-    message = ""
-    if eu[0] == -2 and eu[1] == -2:
-        message = "Coincident zeros.  Indeterminacy and/or nonexistence."
-    elif eu[0] == -1:
-        message = f"System is indeterminate. There are {eu[2]} loose endogenous variables."
-    elif eu[1] == -1:
-        message = f"Solution exists, but it is not unique -- sunspots."
-
-    return message
+from typing import Optional, Tuple
+
+import numpy as np
+from numpy.typing import ArrayLike
+from scipy import linalg
+
+
+def qzdiv(
+    stake: float, A: ArrayLike, B: ArrayLike, Q: ArrayLike, Z: ArrayLike
+) -> Tuple[ArrayLike, ArrayLike, ArrayLike, ArrayLike]:
+    """
+    Christopher Sim's qzdiv http://sims.princeton.edu/yftp/gensys/mfiles/qzdiv.m
+    :param stake: float, largest positive value for which an eigenvalue is considered stable
+    :param A: Array, upper-triangular matrix
+    :param B: Array, upper-triangular matrix
+    :param Q: Array, matrix of left Schur vectors.
+    :param Z: Array, matrix of right Schur vectors.
+    :return: Tuple, A, B, Q, Z, sorted such that all unstable roots are placed in the lower-right corners of the
+             matrices.
+
+    Original docstring:
+    Takes U.T. matrices A, B, orthonormal matrices Q,Z, rearranges them so that all cases of abs(B(i,i)/A(i,i))>stake
+    are in lower right  corner, while preserving U.T. and orthonormal properties and Q'AZ' and Q'BZ'.
+    The columns of v are sorted correspondingly.
+
+    Additional notes:
+    Matrices A, B, Q, and Z are the output of the generalized Schur decomposition (QZ decomposition) of the system
+    matrices G0 and G1. A and B are upper triangular, with the properties Q @ A @ Z.T = G0 and Q @ B @ Z.T = G1.
+
+    TODO: scipy offers a sorted qz routine, ordqz, which automatically sorts the matrices by size of eigenvalue. This
+        seems to be what the functions qzdiv and qzswitch do, so it might be worthwhile to see if we can just use
+        ordqz instead.
+
+    TODO: Add shape information to the Typing (see PEP 646)
+    """
+
+    n, _ = A.shape
+
+    root = np.hstack([np.diag(A)[:, None], np.diag(B)[:, None]])
+    root = np.abs(root)
+    root[:, 0] = root[:, 0] - (root[:, 0] < 1e-13) * (root[:, 0] + root[:, 1])
+    root[:, 1] = root[:, 1] / root[:, 0]
+
+    for i in range(n - 1, -1, -1):
+        m = None
+        for j in range(i, -1, -1):
+            if (root[j, 1] > stake) or (root[j, 1] < -0.1):
+                m = j
+                break
+
+        if m is None:
+            return A, B, Q, Z
+
+        for k in range(m, i):
+            A, B, Q, Z = qzswitch(k, A, B, Q, Z)
+            root[k, 1], root[k + 1, 1] = root[k + 1, 1], root[k, 1]
+
+    return A, B, Q, Z
+
+
+def qzswitch(
+    i: int, A: ArrayLike, B: ArrayLike, Q: ArrayLike, Z: ArrayLike
+) -> Tuple[ArrayLike, ArrayLike, ArrayLike, ArrayLike]:
+    """
+    Christopher Sim's qzswitch,
+    :param i: int, index of matrix diagonal to switch
+    :param A: Array, upper-triangular matrix
+    :param B: Array, upper-triangular matrix
+    :param Q: Array, matrix of left Schur vectors.
+    :param Z: Array, matrix of right Schur vectors.
+    :return: Tuple of A, B, Q, Z
+
+    Original docstring:
+    Takes U.T. matrices A, B, orthonormal matrices Q,Z, interchanges diagonal elements i and i+1 of both A and B,
+    while maintaining Q'AZ' and Q'BZ' unchanged.  If diagonal elements of A and B are zero at matching positions,
+    the returned A will have zeros at both positions on the diagonal.  This is natural behavior if this routine is used
+    to drive all zeros on the diagonal of A to the lower right, but in this case the qz transformation is not unique
+    and it is not possible simply to switch the positions of the diagonal elements of both A and B.
+
+    TODO: Add shape information to the Typing (see PEP 646)
+    """
+    eps = np.spacing(1)
+
+    a = A[i, i]
+    b = A[i, i + 1]
+    c = A[i + 1, i + 1]
+    d = B[i, i]
+    e = B[i, i + 1]
+    f = B[i + 1, i + 1]
+
+    if (abs(c) < eps) & (abs(f) < eps):
+        if abs(a) < eps:
+            return A, B, Q, Z
+
+        else:
+            wz = np.c_[b, -a].T
+            wz = wz / np.sqrt(wz.conj().T @ wz)
+            wz = np.hstack([wz, np.c_[wz[1].conj().T, -wz[0].conj().T].T])
+            xy = np.eye(2)
+
+    elif (abs(a) < eps) & (abs(d) < eps):
+        if abs(c) < eps:
+            return A, B, Q, Z
+        else:
+            wz = np.eye(2)
+            xy = np.c_[c, -b].T
+            xy = xy / np.sqrt(xy @ xy.conj().T)
+            xy = np.hstack([np.c_[xy[1].conj().T, -xy[0].conj().T].T, xy])
+
+    else:
+        wz = np.c_[c * e - f * b, (c * d - f * a).conj()]
+        xy = np.c_[(b * d - e * a).conj(), (c * d - f * a).conj()]
+
+        n = np.sqrt(wz @ wz.conj().T)
+        m = np.sqrt(xy @ xy.conj().T)
+
+        if m < eps * 100:
+            return A, B, Q, Z
+
+        wz = wz / n
+        xy = xy / m
+
+        wz = np.vstack([wz, np.c_[-wz[:, 1].conj(), wz[:, 0].conj()]])
+        xy = np.vstack([xy, np.c_[-xy[:, 1].conj(), xy[:, 0].conj()]])
+
+    idx_slice = slice(i, i + 2)
+
+    A[idx_slice, :] = xy @ A[idx_slice, :]
+    B[idx_slice, :] = xy @ B[idx_slice, :]
+    Q[idx_slice, :] = xy @ Q[idx_slice, :]
+
+    A[:, idx_slice] = A[:, idx_slice] @ wz
+    B[:, idx_slice] = B[:, idx_slice] @ wz
+    Z[:, idx_slice] = Z[:, idx_slice] @ wz
+
+    return A, B, Q, Z
+
+
+def determine_n_unstable(
+    A: ArrayLike, B: ArrayLike, div: float, realsmall: float
+) -> Tuple[float, int, bool]:
+    """
+    :param A: array, upper-triangular matrix, output of QZ decomposition
+    :param B: array, upper-triangular matrix, output of QZ decomposition
+    :param div: float, largest positive value for which an eigenvalue is considered stable
+    :param realsmall: an arbitrarily small number
+    :return: Tuple
+
+    Originally part of gensys, this helper function determines how many roots of the system described by A and B are
+    unstable. Returns three values:
+        div, a float representing which roots of the system can be considered stable,
+        n_unstable, an int of how many unstable roots are in the system, and
+        zxz, a bool that signals whether the system has a unique solution.
+    """
+    n, _ = A.shape
+    n_unstable = 0
+    zxz = False
+    compute_div = div is None
+    div = 1.01 if div is None else div
+
+    for i in range(n):
+        if compute_div:
+            if abs(A[i, i]) > 0:
+                divhat = abs(B[i, i] / A[i, i])
+                if (1 + realsmall < divhat) and divhat <= div:
+                    div = 0.5 * (1 + divhat)
+        n_unstable += abs(B[i, i]) > div * abs(A[i, i])
+
+        zxz = (abs(A[i, i]) < realsmall) & (abs(B[i, i]) < realsmall)
+
+    return div, n_unstable, zxz
+
+
+def split_matrix_on_eigen_stability(A: ArrayLike, n_unstable: int) -> Tuple[ArrayLike, ArrayLike]:
+    """
+    :param A: Arrayline, array to split
+    :param n_unstable: int, number of unstable roots in the
+    :return: Tuple of (A1, A2), the A matrix split such that all stable roots are in the A1 matrix,
+             and the unstable roots are in the A2 matrix.
+
+    Originally in the gensys function, split out here for readability.
+    """
+    n, _ = A.shape
+    stable_slice = slice(None, n - n_unstable)
+    unstable_slice = slice(n - n_unstable, None)
+
+    A1 = A[stable_slice]
+    A2 = A[unstable_slice]
+
+    return A1, A2
+
+
+def build_u_v_d(eta: ArrayLike, realsmall: float):
+    """
+
+    :param eta:
+    :param realsmall:
+    :return: tuple, svd decomposition of eta plus an array of non-zero indices
+
+    Piece of gensys adapted from Matlab code, split out as a helper function.
+    """
+
+    u_eta, d_eta, v_eta = linalg.svd(eta)
+    d_eta = np.diag(d_eta)  # match matlab output of svd
+    v_eta = v_eta.conj().T  # match matlab output of svd
+
+    md = min(d_eta.shape)
+    big_ev = np.where(np.diagonal(d_eta[:md, :md] > realsmall))[0]
+
+    u_eta = u_eta[:, big_ev]
+    v_eta = v_eta[:, big_ev]
+    d_eta = d_eta[big_ev, big_ev]
+
+    if d_eta.ndim == 1:
+        d_eta = np.diag(d_eta)
+
+    return u_eta, v_eta, d_eta, big_ev
+
+
+def gensys(
+    g0: ArrayLike,
+    g1: ArrayLike,
+    c: ArrayLike,
+    psi: ArrayLike,
+    pi: ArrayLike,
+    div: Optional[float] = None,
+    tol: Optional[float] = 1e-8,
+) -> Tuple:
+    """
+    Christopher Sim's gensys, http://sims.princeton.edu/yftp/gensys/mfiles/gensys.m
+
+    Solves rational expectations equations as described in [1] by partitioning partitioning the system into stable
+    and unstable roots, then eliminating the unstable roots via QZ decomposition.
+
+    Original matlab docstring:
+    System given as g0*y(t)=g1*y(t-1)+c+psi*z(t)+pi*eta(t), with z an exogenous variable process and eta being
+    endogenously determined one-step-ahead expectational errors.  Returned system is
+        y(t)=G1*y(t-1)+C+impact*z(t)+ywt*inv(I-fmat*inv(L))*fwt*z(t+1) .
+    If z(t) is i.i.d., the last term drops out. If div is omitted from argument list, a div>1 is calculated.
+    eu(1)=1 for existence,
+    eu(2)=1 for uniqueness.
+    eu(1)=-1 for existence only with not-s.c. z;
+    eu=[-2,-2] for coincident zeros.
+
+    Parameters
+    ----------
+    g0: ArrayLike
+        Coefficient matrix of the dynamic system corresponding to the time-t variables
+    g1: ArrayLike
+        Coefficient matrix of the dynamic system corresponding to the time t-1 variables
+    c: ArrayLike
+        Vector of constant terms
+    psi: ArrayLike
+        Coefficient matrix of the dynamic system corresponding to the exogenous shock terms
+    pi: ArrayLike
+        Coefficient matrix of the dynamic system corresponding to the endogenously determined
+        expectational errors.
+    div: float
+        # TODO: WRITE ME
+    tol: float, default: 1e-8
+        Level of floating point precision
+
+    Returns
+    -------
+    G_1: ArrayLike
+        Policy function relating the current timestep to the next, transition matrix T in state space jargon.
+    C: ArrayLike
+        Array of system means, intercept vector c in state space jargon.
+    impact: ArrayLike
+        Policy function component relating exogenous shocks observed at the t to variable values in t+1, selection
+        matric R in state space jargon.
+    f_mat: ArrayLike
+        # TODO: WRITE ME
+    f_wt: ArrayLike
+        # TODO: WRITE ME
+    y_wt: ArrayLike
+        # TODO: WRITE ME
+    gev: ArrayLike
+        Generalized left and right eigenvalues generated by qz(g0, g1), sorted such that stable roots are in the
+        top-left corner
+    eu: tuple
+        Tuple of two values indicting uniqueness and determinacy of the solution.
+    loose: int
+        Number of loose endogenous variables.
+
+    References
+    -------
+    ..[1] Sims, Christopher A. "Solving linear rational expectations models." Computational economics 20.1-2 (2002): 1.
+
+    TODO: Can this be written in Numba/Aesara?
+    """
+    eu = [0, 0, 0]
+
+    n, _ = g1.shape
+    A, B, Q, Z = linalg.qz(g0, g1, "complex")
+    Q = Q.conj().T  # q is transposed relative to matlab, see scipy docs
+
+    div, n_unstable, zxz = determine_n_unstable(A, B, div, tol)
+    n_stable = n - n_unstable
+
+    if zxz:
+        eu = [-2, -2, 0]
+        return None, None, None, None, None, None, None, eu, None
+
+    A, B, Q, Z = qzdiv(div, A, B, Q, Z)
+    gev = np.c_[np.diagonal(A), np.diagonal(B)]
+
+    Q1, Q2 = split_matrix_on_eigen_stability(Q, n_unstable)
+
+    eta_wt = Q2 @ pi
+    _, n_eta = pi.shape
+
+    # No stable roots
+    if n_unstable == 0:
+        big_ev = 0
+
+        u_eta = np.zeros((0, 0))
+        d_eta = np.zeros((0, 0))
+        v_eta = np.zeros((n_eta, 0))
+
+    else:
+        u_eta, v_eta, d_eta, big_ev = build_u_v_d(eta_wt, tol)
+
+    if len(big_ev) >= n_unstable:
+        eu[0] = 1
+
+    # All stable roots
+    if n_unstable == n:
+        eta_wt_1 = np.zeros((0, n_eta))
+        u_eta_1 = np.zeros((0, 0))
+        d_eta_1 = np.zeros((0, 0))
+        v_eta_1 = np.zeros((n_eta, 0))
+
+    else:
+        eta_wt_1 = Q1 @ pi
+        u_eta_1, v_eta_1, d_eta_1, big_ev = build_u_v_d(eta_wt_1, tol)
+
+    if 0 in v_eta_1.shape:
+        unique = True
+    else:
+        loose = v_eta_1 - v_eta @ v_eta.T @ v_eta_1
+        ul, dl, vl = linalg.svd(loose)
+        if dl.ndim == 1:
+            dl = np.diag(dl)
+
+        n_loose = (np.abs(np.diagonal(dl)) > (tol * n)).sum()
+        eu[2] = n_loose
+        unique = n_loose == 0
+
+    if unique:
+        eu[1] = 1
+
+    inner_term = u_eta @ linalg.solve(d_eta, v_eta.conj().T) @ v_eta_1 @ d_eta_1 @ u_eta_1.conj().T
+
+    T_mat = np.c_[np.eye(n_stable), -inner_term.conj().T]
+    G_0 = np.r_[T_mat @ A, np.c_[np.zeros((n_unstable, n_stable)), np.eye(n_unstable)]]
+
+    G_1 = np.r_[T_mat @ B, np.zeros((n_unstable, n))]
+
+    G_0_inv = linalg.inv(G_0)
+    G_1 = G_0_inv @ G_1
+
+    idx = slice(n_stable, n)
+
+    C = np.r_[T_mat @ Q @ c, linalg.solve(A[idx, idx] - B[idx, idx], Q2) @ c]
+
+    impact = G_0_inv @ np.r_[T_mat @ Q @ psi, np.zeros((n_unstable, psi.shape[1]))]
+
+    f_mat = linalg.solve(B[idx, idx], A[idx, idx])
+    f_wt = -linalg.solve(B[idx, idx], Q2) @ psi
+    y_wt = G_0_inv[:, idx]
+
+    loose = (
+        G_0_inv
+        @ np.r_[
+            eta_wt_1 @ (np.eye(n_eta) - v_eta @ v_eta.conj().T),
+            np.zeros((n_unstable, n_eta)),
+        ]
+    )
+
+    G_1 = (Z @ G_1 @ Z.conj().T).real
+    C = (Z @ C).real
+    impact = (Z @ impact).real
+    loose = (Z @ loose).real
+    y_wt = Z @ y_wt
+
+    return G_1, C, impact, f_mat, f_wt, y_wt, gev, eu, loose
+
+
+def interpret_gensys_output(eu):
+    message = ""
+    if eu[0] == -2 and eu[1] == -2:
+        message = "Coincident zeros.  Indeterminacy and/or nonexistence. Check that your system is correctly defined."
+    elif eu[0] == -1:
+        message = f"System is indeterminate. There are {eu[2]} loose endogenous variables."
+    elif eu[1] == -1:
+        message = f"Solution exists, but it is not unique -- sunspots."
+
+    return message
```

### Comparing `gEconpy-1.1.0/gEconpy/solvers/perturbation.py` & `gEconpy-1.2.0/gEconpy/solvers/perturbation.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,285 +1,283 @@
-from typing import List, Tuple
-
-import numpy as np
-import sympy as sp
-from numpy.typing import ArrayLike
-from scipy import linalg
-
-from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
-from gEconpy.shared.utilities import eq_to_ss
-from gEconpy.solvers.cycle_reduction import cycle_reduction, solve_shock_matrix
-from gEconpy.solvers.gensys import gensys
-
-
-def print_gensys_results(eu):
-    if eu[0] == 1 and eu[1] == 1:
-        print(
-            "Gensys found a unique solution.\n"
-            "Policy matrices have been stored in attributes model.P, model.Q, model.R, and model.S"
-        )
-
-    else:
-        print(eu)
-
-
-class PerturbationSolver:
-    def __init__(self, model):
-        self.steady_state_dict = model.steady_state_dict
-        self.steady_state_solved = model.steady_state_solved
-        self.param_dict = model.free_param_dict
-        self.system_equations = model.system_equations
-        self.variables = model.variables
-
-        self.shocks = model.shocks
-        self.n_shocks = model.n_shocks
-
-    @staticmethod
-    def solve_policy_function_with_gensys(
-        A: ArrayLike,
-        B: ArrayLike,
-        C: ArrayLike,
-        D: ArrayLike,
-        tol: float = 1e-8,
-        verbose: bool = True,
-    ) -> Tuple:
-        n_eq, n_vars = A.shape
-        _, n_shocks = D.shape
-
-        lead_var_idx = np.where(np.sum(np.abs(C), axis=0) > tol)[0]
-        eqs_and_leads_idx = np.r_[np.arange(n_vars), lead_var_idx + n_vars].tolist()
-
-        n_leads = len(lead_var_idx)
-
-        Gamma_0 = np.vstack([np.hstack([B, C]), np.hstack([-np.eye(n_eq), np.zeros((n_eq, n_eq))])])
-
-        Gamma_1 = np.vstack(
-            [
-                np.hstack([A, np.zeros((n_eq, n_eq))]),
-                np.hstack([np.zeros((n_eq, n_eq)), np.eye(n_eq)]),
-            ]
-        )
-
-        Pi = np.vstack([np.zeros((n_eq, n_eq)), np.eye(n_eq)])
-
-        Psi = np.vstack([D, np.zeros((n_eq, n_shocks))])
-
-        Gamma_0 = Gamma_0[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
-        Gamma_1 = Gamma_1[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
-        Psi = Psi[eqs_and_leads_idx, :]
-        Pi = Pi[eqs_and_leads_idx, :][:, lead_var_idx]
-
-        # Is this necessary?
-        g0 = -np.ascontiguousarray(Gamma_0)  # NOTE THE IMPORTANT MINUS SIGN LURKING
-        g1 = np.ascontiguousarray(Gamma_1)
-        c = np.ascontiguousarray(np.zeros(shape=(n_vars + n_leads, 1)))
-        psi = np.ascontiguousarray(Psi)
-        pi = np.ascontiguousarray(Pi)
-
-        G_1, constant, impact, f_mat, f_wt, y_wt, gev, eu, loose = gensys(g0, g1, c, psi, pi)
-        if verbose:
-            print_gensys_results(eu)
-
-        return G_1, constant, impact, f_mat, f_wt, y_wt, gev, eu, loose
-
-    @staticmethod
-    def solve_policy_function_with_cycle_reduction(
-        A: ArrayLike,
-        B: ArrayLike,
-        C: ArrayLike,
-        D: ArrayLike,
-        max_iter: int = 1000,
-        tol: float = 1e-8,
-        verbose: bool = True,
-    ) -> Tuple[ArrayLike, ArrayLike, str, float]:
-        """
-        Solve quadratic matrix equation of the form $A0x^2 + A1x + A2 = 0$ via cycle reduction algorithm of [1] to
-        obtain the first-order linear approxiate policy matrices T and R.
-
-        Parameters
-        ----------
-        A: Arraylike
-            Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to past variables
-            values that are known when decision-making: those with t-1 subscripts.
-        B: ArrayLike
-            Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to variables that
-            are observed when decision-making: those with t subscripts.
-        C: ArrayLike
-            Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to variables that
-            enter in expectation when decision-making: those with t+1 subscripts.
-        D: ArrayLike
-            Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to exogenous shocks.
-        max_iter: int, default: 1000
-            Maximum number of iterations to perform before giving up.
-        tol: float, default: 1e-7
-            Floating point tolerance used to detect algorithmic convergence
-        verbose: bool, default: True
-            If true, prints the sum of squared residuals that result when the system is computed used the solution.
-
-        Returns
-        -------
-        T: ArrayLike
-            Transition matrix T in state space jargon. Gives the effect of variable values at time t on the
-            values of the variables at time t+1.
-        R: ArrayLike
-            Selection matrix R in state space jargon. Gives the effect of exogenous shocks at the t on the values of
-            variables at time t+1.
-        result: str
-            String describing result of the cycle reduction algorithm
-        log_norm: float
-            Log L1 matrix norm of the first matrix (A2 -> A1 -> A0) that did not converge.
-        """
-
-        # Sympy gives back integers in the case of x/dx = 1, which can screw up the dtypes when passing to numba if
-        # a Jacobian matrix is all constants (i.e. dF/d_shocks) -- cast everything to float64 here to avoid
-        # a numba warning.
-        T, R = None, None
-
-        # A, B, C, D = A.astype('float64'), B.astype('float64'), C.astype('float64'), D.astype('float64')
-
-        T, result, log_norm = cycle_reduction(A, B, C, max_iter, tol, verbose)
-
-        if T is not None:
-            R = solve_shock_matrix(B, C, D, T)
-
-        return T, R, result, log_norm
-
-    def statespace_to_gEcon_representation(self, A, T, R, variables, tol):
-        n_vars = len(variables)
-
-        state_var_idx = np.where(np.abs(T[np.argmax(np.abs(T), axis=0), np.arange(n_vars)]) >= tol)[
-            0
-        ]
-        state_var_mask = np.isin(np.arange(n_vars), state_var_idx)
-
-        n_shocks = self.n_shocks
-        shock_idx = np.arange(n_shocks)
-
-        # variables = np.atleast_1d(variables).squeeze()
-
-        # state_vars = variables[state_var_mask]
-        # L1_state_vars = np.array([x.step_backward() for x in state_vars])
-        # jumpers = np.atleast_1d(variables)[~state_var_mask]
-
-        PP = T.copy()
-        PP[np.where(np.abs(PP) < tol)] = 0
-        QQ = R.copy()
-        QQ = QQ[:n_vars, :]
-        QQ[np.where(np.abs(QQ) < tol)] = 0
-
-        P = PP[state_var_mask, :][:, state_var_mask]
-        Q = QQ[state_var_mask, :][:, shock_idx]
-        R = PP[~state_var_mask, :][:, state_var_idx]
-        S = QQ[~state_var_mask, :][:, shock_idx]
-
-        A_prime = A[:, state_var_mask]
-        R_prime = PP[:, state_var_mask]
-        S_prime = QQ[:, shock_idx]
-
-        return P, Q, R, S, A_prime, R_prime, S_prime
-
-    @staticmethod
-    def residual_norms(B, C, D, Q, P, A_prime, R_prime, S_prime):
-        norm_deterministic = linalg.norm(A_prime + B @ R_prime + C @ R_prime @ P)
-
-        norm_stochastic = linalg.norm(B @ S_prime + C @ R_prime @ Q + D)
-
-        return norm_deterministic, norm_stochastic
-
-    def log_linearize_model(self, not_loglin_variables=None) -> List[sp.Matrix]:
-        """
-        :return: List, a list of Sympy matrices comprised of parameters and steady-state values, see docstring.
-
-        Convert the non-linear model to its log-linear approximation using a first-order Taylor expansion around the
-        deterministic steady state. The specific method of log-linearization is taken from the gEcon User's Guide,
-        page 54, equation 9.9.
-
-            F1 @ T @ y_{t-1} + F2 @ T @ y_t + F3 @ T @ y_{t+1} + F4 @ epsilon_t = 0
-
-        Where T is a diagonal matrix containing steady-state values on the diagonal. Evaluating the matrix
-        multiplications in the expression above obtains:
-
-            A @ y_{t-1} + B @ y_t + C @ y_{t+1} + D @ epsilon = 0
-
-        Matrices A, B, C, and D are returned by this function.
-
-        TODO: Presently, everything is done using sympy, which is extremely slow. This should all be re-written in a
-            way that is Numba and/or CUDA compatible.
-        """
-
-        Fs = []
-        lags, now, leads = self.make_all_variable_time_combinations()
-        shocks = self.shocks
-        for var_group in [lags, now, leads, shocks]:
-            F = []
-
-            # If the user selects a variable to not be log linearized, we need to set the value in T to be one, but
-            # still replace all SS values in A, B, C, D as usual. These dummies facilitate that.
-            # T = sp.diag(*[TimeAwareSymbol(x.base_name + '_T', 'ss') for x in var_group])
-
-            for eq in self.system_equations:
-                F_row = []
-                for var in var_group:
-                    dydx = sp.powsimp(eq_to_ss(eq.diff(var)))
-                    dydx *= 1.0 if var.base_name in not_loglin_variables else var.to_ss()
-                    atoms = dydx.atoms()
-                    if len(atoms) == 1:
-                        x = list(atoms)[0]
-                        if isinstance(x, sp.core.numbers.Number) and x != 0:
-                            dydx = sp.Float(x)
-                    F_row.append(dydx)
-
-                F.append(F_row)
-            F = sp.Matrix(F)
-            # Fs.append(sp.MatMul(F, T, evaluate=False))
-            Fs.append(F)
-
-        return Fs
-
-    def convert_linear_system_to_matrices(self) -> List[sp.Matrix]:
-        """
-
-        :return: List of sympy Matrices representing the linear system
-
-        If the model has already been log-linearized by hand, this method is used to simplify the construction of the
-        solution matrices. Following the gEcon user's guide, page 54, equation 9.10, the solution should be of the form:
-
-            A @ y_{t-1} + B @ y_t + C @ y_{t+1} + D @ epsilon = 0
-
-        This function organizes the model equations and returns matrices A, B, C, and D.
-
-        TODO: Add some checks to ensure that the model is indeed linear so that this can't be erroneously called.
-        """
-
-        Fs = []
-        lags, now, leads = self.make_all_variable_time_combinations()
-        shocks = self.shocks
-        model = self.system_equations
-
-        for var_group, name in zip([lags, now, leads, shocks], ["lags", "now", "leads", "shocks"]):
-            F = (
-                sp.zeros(len(var_group))
-                if name != "shocks"
-                else sp.zeros(rows=len(model), cols=len(var_group))
-            )
-            for i, var in enumerate(var_group):
-                for j, eq in enumerate(model):
-                    args = eq.expand().args
-                    for arg in args:
-                        if var in arg.atoms():
-                            F[j, i] = sp.simplify(arg / var)
-            Fs.append(F)
-
-        return Fs
-
-    def make_all_variable_time_combinations(
-        self,
-    ) -> Tuple[List[TimeAwareSymbol], List[TimeAwareSymbol], List[TimeAwareSymbol]]:
-        """
-        :return: Tuple of three lists, containing all model variables at time steps t-1, t, and t+1, respectively.
-        """
-
-        now = sorted(self.variables, key=lambda x: x.base_name)
-        lags = [x.step_backward() for x in now]
-        leads = [x.step_forward() for x in now]
-
-        return lags, now, leads
+from typing import List, Tuple
+
+import numpy as np
+import sympy as sp
+from sympy.solvers.solveset import NonlinearError
+
+from numpy.typing import ArrayLike
+from scipy import linalg
+
+from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
+from gEconpy.shared.utilities import eq_to_ss
+from gEconpy.solvers.cycle_reduction import cycle_reduction, solve_shock_matrix
+from gEconpy.solvers.gensys import gensys
+
+
+def print_gensys_results(eu):
+    if eu[0] == 1 and eu[1] == 1:
+        print(
+            "Gensys found a unique solution.\n"
+            "Policy matrices have been stored in attributes model.P, model.Q, model.R, and model.S"
+        )
+
+    else:
+        print(eu)
+
+
+class PerturbationSolver:
+    def __init__(self, model):
+        self.steady_state_dict = model.steady_state_dict
+        self.steady_state_solved = model.steady_state_solved
+        self.param_dict = model.free_param_dict
+        self.system_equations = model.system_equations
+        self.variables = model.variables
+
+        self.shocks = model.shocks
+        self.n_shocks = model.n_shocks
+
+    @staticmethod
+    def solve_policy_function_with_gensys(
+            A: ArrayLike,
+            B: ArrayLike,
+            C: ArrayLike,
+            D: ArrayLike,
+            tol: float = 1e-8,
+            verbose: bool = True,
+    ) -> Tuple:
+        n_eq, n_vars = A.shape
+        _, n_shocks = D.shape
+
+        lead_var_idx = np.where(np.sum(np.abs(C), axis=0) > tol)[0]
+        eqs_and_leads_idx = np.r_[np.arange(n_vars), lead_var_idx + n_vars].tolist()
+
+        n_leads = len(lead_var_idx)
+
+        Gamma_0 = np.vstack([np.hstack([B, C]), np.hstack([-np.eye(n_eq), np.zeros((n_eq, n_eq))])])
+
+        Gamma_1 = np.vstack(
+            [
+                np.hstack([A, np.zeros((n_eq, n_eq))]),
+                np.hstack([np.zeros((n_eq, n_eq)), np.eye(n_eq)]),
+            ]
+        )
+
+        Pi = np.vstack([np.zeros((n_eq, n_eq)), np.eye(n_eq)])
+
+        Psi = np.vstack([D, np.zeros((n_eq, n_shocks))])
+
+        Gamma_0 = Gamma_0[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
+        Gamma_1 = Gamma_1[eqs_and_leads_idx, :][:, eqs_and_leads_idx]
+        Psi = Psi[eqs_and_leads_idx, :]
+        Pi = Pi[eqs_and_leads_idx, :][:, lead_var_idx]
+
+        # Is this necessary?
+        g0 = -np.ascontiguousarray(Gamma_0)  # NOTE THE IMPORTANT MINUS SIGN LURKING
+        g1 = np.ascontiguousarray(Gamma_1)
+        c = np.ascontiguousarray(np.zeros(shape=(n_vars + n_leads, 1)))
+        psi = np.ascontiguousarray(Psi)
+        pi = np.ascontiguousarray(Pi)
+
+        G_1, constant, impact, f_mat, f_wt, y_wt, gev, eu, loose = gensys(g0, g1, c, psi, pi)
+        if verbose:
+            print_gensys_results(eu)
+
+        return G_1, constant, impact, f_mat, f_wt, y_wt, gev, eu, loose
+
+    @staticmethod
+    def solve_policy_function_with_cycle_reduction(
+            A: ArrayLike,
+            B: ArrayLike,
+            C: ArrayLike,
+            D: ArrayLike,
+            max_iter: int = 1000,
+            tol: float = 1e-8,
+            verbose: bool = True,
+    ) -> Tuple[ArrayLike, ArrayLike, str, float]:
+        """
+        Solve quadratic matrix equation of the form $A0x^2 + A1x + A2 = 0$ via cycle reduction algorithm of [1] to
+        obtain the first-order linear approxiate policy matrices T and R.
+
+        Parameters
+        ----------
+        A: Arraylike
+            Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to past variables
+            values that are known when decision-making: those with t-1 subscripts.
+        B: ArrayLike
+            Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to variables that
+            are observed when decision-making: those with t subscripts.
+        C: ArrayLike
+            Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to variables that
+            enter in expectation when decision-making: those with t+1 subscripts.
+        D: ArrayLike
+            Jacobian matrix of the DSGE system, evaluated at the steady state, taken with respect to exogenous shocks.
+        max_iter: int, default: 1000
+            Maximum number of iterations to perform before giving up.
+        tol: float, default: 1e-7
+            Floating point tolerance used to detect algorithmic convergence
+        verbose: bool, default: True
+            If true, prints the sum of squared residuals that result when the system is computed used the solution.
+
+        Returns
+        -------
+        T: ArrayLike
+            Transition matrix T in state space jargon. Gives the effect of variable values at time t on the
+            values of the variables at time t+1.
+        R: ArrayLike
+            Selection matrix R in state space jargon. Gives the effect of exogenous shocks at the t on the values of
+            variables at time t+1.
+        result: str
+            String describing result of the cycle reduction algorithm
+        log_norm: float
+            Log L1 matrix norm of the first matrix (A2 -> A1 -> A0) that did not converge.
+        """
+
+        # Sympy gives back integers in the case of x/dx = 1, which can screw up the dtypes when passing to numba if
+        # a Jacobian matrix is all constants (i.e. dF/d_shocks) -- cast everything to float64 here to avoid
+        # a numba warning.
+        T, R = None, None
+
+        # A, B, C, D = A.astype('float64'), B.astype('float64'), C.astype('float64'), D.astype('float64')
+
+        T, result, log_norm = cycle_reduction(A, B, C, max_iter, tol, verbose)
+
+        if T is not None:
+            R = solve_shock_matrix(B, C, D, T)
+
+        return T, R, result, log_norm
+
+    def statespace_to_gEcon_representation(self, A, T, R, variables, tol):
+        n_vars = len(variables)
+
+        state_var_idx = np.where(np.abs(T[np.argmax(np.abs(T), axis=0), np.arange(n_vars)]) >= tol)[
+            0
+        ]
+        state_var_mask = np.isin(np.arange(n_vars), state_var_idx)
+
+        n_shocks = self.n_shocks
+        shock_idx = np.arange(n_shocks)
+
+        # variables = np.atleast_1d(variables).squeeze()
+
+        # state_vars = variables[state_var_mask]
+        # L1_state_vars = np.array([x.step_backward() for x in state_vars])
+        # jumpers = np.atleast_1d(variables)[~state_var_mask]
+
+        PP = T.copy()
+        PP[np.where(np.abs(PP) < tol)] = 0
+        QQ = R.copy()
+        QQ = QQ[:n_vars, :]
+        QQ[np.where(np.abs(QQ) < tol)] = 0
+
+        P = PP[state_var_mask, :][:, state_var_mask]
+        Q = QQ[state_var_mask, :][:, shock_idx]
+        R = PP[~state_var_mask, :][:, state_var_idx]
+        S = QQ[~state_var_mask, :][:, shock_idx]
+
+        A_prime = A[:, state_var_mask]
+        R_prime = PP[:, state_var_mask]
+        S_prime = QQ[:, shock_idx]
+
+        return P, Q, R, S, A_prime, R_prime, S_prime
+
+    @staticmethod
+    def residual_norms(B, C, D, Q, P, A_prime, R_prime, S_prime):
+        norm_deterministic = linalg.norm(A_prime + B @ R_prime + C @ R_prime @ P)
+
+        norm_stochastic = linalg.norm(B @ S_prime + C @ R_prime @ Q + D)
+
+        return norm_deterministic, norm_stochastic
+
+    def log_linearize_model(self, not_loglin_variables=None) -> List[sp.Matrix]:
+        """
+        :return: List, a list of Sympy matrices comprised of parameters and steady-state values, see docstring.
+
+        Convert the non-linear model to its log-linear approximation using a first-order Taylor expansion around the
+        deterministic steady state. The specific method of log-linearization is taken from the gEcon User's Guide,
+        page 54, equation 9.9.
+
+            F1 @ T @ y_{t-1} + F2 @ T @ y_t + F3 @ T @ y_{t+1} + F4 @ epsilon_t = 0
+
+        Where T is a diagonal matrix containing steady-state values on the diagonal. Evaluating the matrix
+        multiplications in the expression above obtains:
+
+            A @ y_{t-1} + B @ y_t + C @ y_{t+1} + D @ epsilon = 0
+
+        Matrices A, B, C, and D are returned by this function.
+
+        TODO: Presently, everything is done using sympy, which is extremely slow. This should all be re-written in a
+            way that is Numba and/or CUDA compatible.
+        """
+
+        Fs = []
+        lags, now, leads = self.make_all_variable_time_combinations()
+        shocks = self.shocks
+        for var_group in [lags, now, leads, shocks]:
+            F = []
+
+            # If the user selects a variable to not be log linearized, we need to set the value in T to be one, but
+            # still replace all SS values in A, B, C, D as usual. These dummies facilitate that.
+            # T = sp.diag(*[TimeAwareSymbol(x.base_name + '_T', 'ss') for x in var_group])
+
+            for eq in self.system_equations:
+                F_row = []
+                for var in var_group:
+                    dydx = sp.powsimp(eq_to_ss(eq.diff(var)))
+                    dydx *= 1.0 if var.base_name in not_loglin_variables else var.to_ss()
+                    atoms = dydx.atoms()
+                    if len(atoms) == 1:
+                        x = list(atoms)[0]
+                        if isinstance(x, sp.core.numbers.Number) and x != 0:
+                            dydx = sp.Float(x)
+                    F_row.append(dydx)
+
+                F.append(F_row)
+            F = sp.Matrix(F)
+            # Fs.append(sp.MatMul(F, T, evaluate=False))
+            Fs.append(F)
+
+        return Fs
+
+    def convert_linear_system_to_matrices(self) -> List[sp.Matrix]:
+        """
+
+        :return: List of sympy Matrices representing the linear system
+
+        If the model has already been log-linearized by hand, this method is used to simplify the construction of the
+        solution matrices. Following the gEcon user's guide, page 54, equation 9.10, the solution should be of the form:
+
+            A @ y_{t-1} + B @ y_t + C @ y_{t+1} + D @ epsilon = 0
+
+        This function organizes the model equations and returns matrices A, B, C, and D.
+        """
+
+        lags, now, leads = self.make_all_variable_time_combinations()
+        shocks = self.shocks
+        model = self.system_equations
+        n = len(lags)
+
+        all_y = lags + now + leads + shocks
+
+        try:
+            A, b = sp.linear_eq_to_matrix(model, all_y)
+        except NonlinearError as sympy_msg:
+            raise ValueError(f'Model does not appear to be linear, check your GCN file. Sympy error: {sympy_msg}')
+
+        offsets = np.array([0, n, n, n, 1])
+        slices = [slice(i, i + offset) for i, offset in zip(offsets.cumsum()[:-1], offsets[1:])]
+
+        Fs = [A[:, idx] for idx in slices]
+
+        return Fs
+
+    def make_all_variable_time_combinations(
+            self,
+    ) -> Tuple[List[TimeAwareSymbol], List[TimeAwareSymbol], List[TimeAwareSymbol]]:
+        """
+        :return: Tuple of three lists, containing all model variables at time steps t-1, t, and t+1, respectively.
+        """
+
+        now = sorted(self.variables, key=lambda x: x.base_name)
+        lags = [x.step_backward() for x in now]
+        leads = [x.step_forward() for x in now]
+
+        return lags, now, leads
```

### Comparing `gEconpy-1.1.0/gEconpy/solvers/steady_state.py` & `gEconpy-1.2.0/gEconpy/solvers/steady_state.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,1433 +1,1436 @@
-from functools import wraps
-from itertools import product
-from typing import Any, Callable, Dict, List, Optional, Tuple, Union
-from warnings import catch_warnings, simplefilter
-
-import numpy as np
-import sympy as sp
-from joblib import Parallel, delayed
-from scipy import optimize
-
-from gEconpy.classes.containers import SymbolDictionary
-from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
-from gEconpy.shared.typing import VariableType
-from gEconpy.shared.utilities import eq_to_ss, substitute_all_equations
-
-# def convert_to_numba_function(expr: sp.Expr, vars: List[str]) -> Callable:
-#     """
-#     Convert a sympy expression into a Numba-compiled function.
-#
-#     Parameters
-#     ----------
-#     expr : sympy.Expr
-#         The sympy expression to be converted.
-#     vars : List[str]
-#         A list of strings containing the names of the variables in the expression.
-#
-#     Returns
-#     -------
-#     numba.types.function
-#         A Numba-compiled function equivalent to the input expression.
-#
-#     Notes
-#     -----
-#     The function returned by this function is pickleable.
-#     """
-#     code = sp.printing.ccode(expr)
-#     # The code string will contain a single line, so we add line breaks to make it a valid block of code
-#     code = "@nb.njit\ndef f({}):\n{}\n    return np.array({})".format(",".join(vars), " " * 4, code)
-#
-#     # Compile the code and return the resulting function
-#     exec(code)
-#     return locals()["f"]
-
-
-def sympy_inputs_to_scipy(f):
-    @wraps(f)
-    def new_f(x0, *args):
-        d, *_ = args
-        return f(*x0, **d)
-
-    return new_f
-
-
-def postprocess_jac(f, shape):
-    @wraps(f)
-    def new_f(*args, **kwargs):
-        return np.array(f(*args, **kwargs)).reshape(shape)
-
-    return new_f
-
-
-class SteadyStateSolver:
-    def __init__(self, model):
-
-        self.variables: List[VariableType] = model.variables
-        self.shocks: List[sp.Add] = model.shocks
-
-        self.n_variables: int = model.n_variables
-
-        self.free_param_dict: SymbolDictionary[str, float] = model.free_param_dict
-        self.params_to_calibrate: List[VariableType] = model.params_to_calibrate
-        self.calibrating_equations: List[sp.Add] = model.calibrating_equations
-        self.shock_dict: Optional[SymbolDictionary[str, float]] = None
-
-        self.system_equations: List[sp.Add] = model.system_equations
-        self.steady_state_relationships: SymbolDictionary[
-            str, Union[float, sp.Add]
-        ] = model.steady_state_relationships
-
-        self.steady_state_system: List[sp.Add] = []
-        self.steady_state_dict: SymbolDictionary[str, float] = SymbolDictionary()
-        self.steady_state_solved: bool = False
-
-        #         self.f_calib_params: Callable = lambda *args, **kwargs: {}
-        #         self.f_ss_resid: Callable = lambda *args, **kwargs: np.inf
-        #         self.f_ss: Callable = lambda *args, **kwargs: np.inf
-
-        self.build_steady_state_system()
-
-    def build_steady_state_system(self):
-
-        ss_vars = map(lambda x: x.to_ss(), self.variables)
-        self.steady_state_dict = SymbolDictionary.fromkeys(ss_vars, None).to_string().sort_keys()
-
-        self.shock_dict = SymbolDictionary.fromkeys(self.shocks, 0.0).to_ss()
-        self.steady_state_system = [
-            eq_to_ss(eq).subs(self.shock_dict).simplify() for eq in self.system_equations
-        ]
-
-    def _validate_optimizer_kwargs(
-        self, optimizer_kwargs: dict, n_eq: int, method: str, use_jac: bool, use_hess: bool
-    ) -> dict:
-        """
-        Validate user-provided keyword arguments to either scipy.optimize.root or scipy.optimize.minimize, and insert
-        good defaults where not provided.
-
-        Note: This function never overwrites user arguments.
-
-        Parameters
-        ----------
-        optimizer_kwargs: dict
-            User-provided arguments for the optimizer
-        n_eq: int
-            Number of remaining steady-state equations after reduction
-        method: str
-            Which family of solution algorithms, minimization or root-finding, to be used.
-        use_jac: bool
-            Whether computation of the jacobian has been requested
-        use_hess: bool
-            Whether computation of the hessian has been requested
-
-        Returns
-        -------
-        optimizer_kwargs: dict
-            Keyword arguments for the scipy function, with "reasonable" defaults inserted where not provided
-        """
-
-        optimizer_kwargs = {} if optimizer_kwargs is None else optimizer_kwargs
-        method_given = hasattr(optimizer_kwargs, "method")
-
-        if method == "root" and not method_given:
-            if use_jac:
-                optimizer_kwargs["method"] = "hybr"
-            else:
-                optimizer_kwargs["method"] = "broyden1"
-
-            if n_eq == 1:
-                optimizer_kwargs["method"] = "lm"
-
-        elif method == "minimize" and not method_given:
-            # Set optimizer_kwargs for minimization
-            if use_hess and use_jac:
-                optimizer_kwargs["method"] = "trust-exact"
-            elif use_jac:
-                optimizer_kwargs["method"] = "BFGS"
-            else:
-                optimizer_kwargs["method"] = "Nelder-Mead"
-
-        if not hasattr(optimizer_kwargs, "tol"):
-            optimizer_kwargs["tol"] = 1e-9
-
-        return optimizer_kwargs
-
-    def solve_steady_state(
-        self,
-        apply_user_simplifications=True,
-        optimizer_kwargs: Optional[Dict[str, Any]] = None,
-        method: Optional[str] = "root",
-        use_jac: Optional[bool] = True,
-        use_hess: Optional[bool] = True,
-    ) -> Callable:
-        """
-        Solving of the steady state proceeds in three steps: solve calibrating equations (if any), gather user provided
-        equations into a function, then solve the remaining equations.
-
-        Calibrating equations are handled first because if the user passed a complete steady state solution, it is
-        unlikely to include solutions for calibrating equations. Calibrating equations are then combined with
-        user supplied equations, and we check if everything necessary to solve the model is now present. If not,
-        a final optimizer step runs to solve for the remaining variables.
-
-        Note that no checks are done in this function to validate the steady state solution. If a user supplies an
-        incorrect steady state, this function will not catch it. It will, however, still fail if an optimizer fails
-        to find a solution.
-
-        Parameters
-        ----------
-        apply_user_simplifications: bool
-            If true, substitute all equations using the steady-state equations provided in the steady_state block
-            of the GCN file.
-        optimizer_kwargs: dict
-            A dictionary of keyword arguments to pass to the scipy optimizer, either root or minimize. See the docstring
-            for scipy.optimize.root or scipy.optimize.minimize for more information.
-        method: str, default: "root"
-            Whether to seek the steady state via root finding algorithm or via minimization of squared errors. "root"
-            requires that the number of unknowns be equal to the number of equations; this assumption can be violated
-            if the user provides only a subset of steady-state relationship (and this subset does not result in
-            elimination of model equations via substitution).
-            One of "root" or "minimize".
-        use_jac: bool
-            A flag indicating whether to use the Jacobian of the steady-state system when solving. Can help the
-            solver on complex problems, but symbolic computation may be slow on large problems. Default is True.
-        use_hess: bool
-            A flag indicating whether to use the Hessian of the loss function of the steady-state system when solving.
-            Ignored if method is "root", as these routines do not use Hessian information.
-
-        Returns
-        -------
-        f_ss: Callable
-            A function that maps a dictionary of parameters to steady state values for all system variables and
-            calibrated parameters.
-        """
-
-        param_dict = self.free_param_dict.to_sympy()
-        params = list(param_dict.to_string().keys())
-        calib_params = self.params_to_calibrate
-        user_provided = self.steady_state_relationships.to_sympy().float_to_values()
-
-        ss_eqs = self.steady_state_system
-        calib_eqs = self.calibrating_equations
-        all_eqs = ss_eqs + calib_eqs
-
-        all_vars_sym = list(self.steady_state_dict.to_sympy().keys())
-        all_vars_and_calib_sym = all_vars_sym + self.params_to_calibrate
-
-        if apply_user_simplifications:
-
-            zeros = np.full_like(all_eqs, False)
-            simplified_eqs = substitute_all_equations(all_eqs, user_provided)
-
-            for i, eq in enumerate(simplified_eqs):
-                subbed_eq = eq.subs(param_dict)
-
-                # Janky, but many expressions won't reduce to zero even if they ought to -> test numerically
-                atoms = [x for x in subbed_eq.atoms() if x in all_vars_and_calib_sym]
-                test_values = {x: np.random.uniform(1e-2, 0.99) for x in atoms}
-                eq_is_zero = sp.Abs(subbed_eq.subs(test_values)) < 1e-8
-                zeros[i] = eq_is_zero
-
-                if isinstance(subbed_eq, sp.Float) and not eq_is_zero:
-                    raise ValueError(
-                        f"Applying user steady state definitions to equation {i}:\n"
-                        f"\t{all_eqs[i]}\n"
-                        f"resulted in non-zero residuals: {subbed_eq}.\n"
-                        f"Please verify the provided steady state relationships are correct."
-                    )
-            eqs_to_solve = [eq for i, eq in enumerate(simplified_eqs) if not zeros[i]]
-        else:
-            eqs_to_solve = all_eqs
-
-        vars_sym = sorted(
-            list({x for eq in eqs_to_solve for x in eq.atoms() if isinstance(x, TimeAwareSymbol)}),
-            key=lambda x: x.name,
-        )
-
-        vars_and_calib_sym = vars_sym + calib_params
-        vars_and_calib_str = [x.name for x in vars_and_calib_sym]
-
-        k_vars = len(vars_sym)
-        k_calib = len(calib_params)
-        n_eq = len(eqs_to_solve)
-
-        if (n_eq != (k_vars + k_calib)) and (n_eq > 0) and (method == "root"):
-            raise ValueError(
-                'method = "root" is only possible when the number of equations (after substitution of '
-                "user-provided steady-state relationships) is equal to the number of (remaining) "
-                f"variables.\nFound {n_eq} equations and {k_vars} variables. This can happen if "
-                f"user-provided steady-state relationships do not result in elimination of model "
-                f"equations after substitution. \nCheck the provided steady state relationships, or "
-                f'use method = "minimize" to attempt to solve via minimization of squared errors.'
-            )
-
-        # Get residuals for all equations, regardless of how much simplification was done
-        f_ss_resid = sp.lambdify([x.name for x in all_vars_and_calib_sym] + params, all_eqs)
-        f_user = sp.lambdify(vars_and_calib_sym + params, list(user_provided.values()))
-
-        optimizer_required = True
-        f_jac_ss = None
-        f_hess_ss = None
-
-        if n_eq == 0:
-            optimizer_required = False
-        if method == "root":
-            if n_eq > 0:
-                # The ccode printer complains about nested lists; make a flat jacobian and reshape it later
-                ss_jac_flat = [eq.diff(x) for eq in eqs_to_solve for x in vars_and_calib_sym]
-
-                f_ss = sympy_inputs_to_scipy(sp.lambdify(vars_and_calib_str + params, eqs_to_solve))
-
-                if use_jac:
-                    f_jac_ss = postprocess_jac(
-                        sympy_inputs_to_scipy(
-                            sp.lambdify(vars_and_calib_str + params, ss_jac_flat)
-                        ),
-                        (n_eq, k_vars + k_calib),
-                    )
-
-        elif method == "minimize":
-            # For minimization, need to form a loss function (use L2 norm -- better options?).
-            loss = sum([eq**2 for eq in eqs_to_solve])
-            f_loss = sympy_inputs_to_scipy(sp.lambdify(vars_and_calib_str + params, loss))
-            if use_jac:
-                f_jac_ss = sympy_inputs_to_scipy(
-                    sp.lambdify(
-                        vars_and_calib_str + params, [loss.diff(x) for x in vars_and_calib_sym]
-                    )
-                )
-            if use_hess:
-                hess_flat = [
-                    loss.diff(x, y) for x in vars_and_calib_sym for y in vars_and_calib_sym
-                ]
-                f_hess_ss = postprocess_jac(
-                    sympy_inputs_to_scipy(sp.lambdify(vars_and_calib_sym + params, hess_flat)),
-                    (k_vars + k_calib, k_vars + k_calib),
-                )
-
-        optimizer_kwargs = self._validate_optimizer_kwargs(
-            optimizer_kwargs, n_eq, method, use_jac, use_hess
-        )
-
-        def ss_func(param_dict):
-            if optimizer_required:
-                x0 = np.full(k_vars + k_calib, 0.8)
-
-                with catch_warnings():
-                    simplefilter("ignore")
-                    if method == "root":
-                        optim = optimize.root(
-                            f_ss, jac=f_jac_ss, x0=x0, args=(param_dict,), **optimizer_kwargs
-                        )
-                    elif method == "minimize":
-                        optim = optimize.minimize(
-                            f_loss,
-                            jac=f_jac_ss,
-                            hess=f_hess_ss,
-                            x0=x0,
-                            args=(param_dict,),
-                            **optimizer_kwargs,
-                        )
-
-                optim_dict = SymbolDictionary(dict(zip(vars_and_calib_sym, optim.x)))
-                success = optim.success
-            else:
-                optim_dict = SymbolDictionary()
-                success = True
-
-            ss_dict = self.steady_state_dict.float_to_values().to_sympy().copy()
-            calib_dict = SymbolDictionary(dict(zip(self.params_to_calibrate, [np.inf] * k_calib)))
-            user_dict = SymbolDictionary(
-                dict(
-                    zip(
-                        user_provided.keys(),
-                        f_user(**optim_dict.to_string(), **param_dict.to_string()),
-                    )
-                )
-            )
-            for k in all_vars_sym:
-                if k in optim_dict.keys():
-                    ss_dict[k] = optim_dict[k]
-                elif k in user_provided.keys():
-                    ss_dict[k] = user_dict[k]
-                else:
-                    raise ValueError(
-                        f"Could not find {k} among either optimizer or user provided solutions"
-                    )
-
-            for k in calib_params:
-                if k in optim_dict.keys():
-                    calib_dict[k] = optim_dict[k]
-                elif k in user_provided.keys():
-                    calib_dict[k] = user_dict[k]
-                else:
-                    raise ValueError(
-                        f"Could not find {k} among either optimizer or user provided solutions"
-                    )
-
-            ss_dict.sort_keys(inplace=True)
-            calib_dict.sort_keys(inplace=True)
-
-            return {
-                "ss_dict": ss_dict.to_string(),
-                "calib_dict": calib_dict.to_string(),
-                "resids": np.array(
-                    f_ss_resid(**ss_dict.to_string(), **calib_dict.to_string(), **param_dict)
-                ),
-                "success": success,
-            }
-
-        return ss_func
-
-
-class SymbolicSteadyStateSolver:
-    def __init__(self):
-        pass
-
-    @staticmethod
-    def score_eq(
-        eq: sp.Expr,
-        var_list: List[sp.Symbol],
-        state_vars: List[sp.Symbol],
-        var_penalty_factor: float = 25,
-        state_var_penalty_factor: float = 5,
-        length_penalty_factor: float = 1,
-    ) -> float:
-
-        """
-        Compute an "unfitness" score for an equation using three simple heuristics:
-            1. The number of jumper variables in the expression
-            2. The number of state variables in the expression
-            3. The total length of the expression
-
-        Expressions with the lowest unfitness will be selected. Setting a lower penalty for state variables will
-        push the system towards finding solutions expressed in state variables if a steady state is parameters only
-        cannot be found.
-
-        Parameters
-        ----------
-        eq: sp.Expr
-            A sympy expression representing a steady-state equation
-        var_list: list of sp.Symbol
-            A list of sympy symbols representing all variables in the model (state and jumper)
-        state_vars: list of sp.Symbol
-            A list of symbol symbols representing all state variables in the model
-        var_penalty_factor: float, default: 25
-            A penalty factor applied to unfitness for each jumper variable in the expression.
-        state_var_penalty_factor: float, default: 5
-            A penalty factor applied to unfitness for each control variable in the expression.
-        length_penalty_factor: float, default: 1
-            A penalty factor applied to each term in the expression
-
-        Returns
-        -------
-        unfitness: float
-            An unfitness score used to select potential substitutions between system equations
-        """
-
-        # If the equation is length zero, it's been reduced away and should never be selected.
-        if eq == 0:
-            return 10000
-
-        var_list = list(set(var_list) - set(state_vars))
-
-        # The equation with the LOWEST score will be chosen to substitute, so punishing state variables less
-        # ensures that equations that have only state variables will be chosen more often.
-        var_penalty = len([x for x in eq.atoms() if x in var_list]) * var_penalty_factor
-        state_var_penalty = (
-            len([x for x in eq.atoms() if x in state_vars]) * state_var_penalty_factor
-        )
-
-        # Prefer shorter equations
-        length_penalty = eq.count_ops() * length_penalty_factor
-
-        return var_penalty + state_var_penalty + length_penalty
-
-    @staticmethod
-    def solve_and_return(eq: sp.Expr, v: sp.Symbol) -> sp.Expr:
-        """
-        Attempt to solve an expression for a given variable. Returns 0 if the expression is not solvable or if the
-        given variable does not appear in the expression. If multiple solutions are found, only the first one is
-        returned.
-
-        Parameters
-        ----------
-        eq: sp.Expr
-            A sympy expression
-        v: sp.Symbol
-            A sympy symbol
-
-        Returns
-        -------
-        solution: sp.Expr
-            Given f(x, ...) =  0, returns x = g(...) if possible, or 0 if not.
-        """
-
-        if v not in eq.atoms():
-            return sp.Float(0)
-        try:
-            solution = sp.solve(eq, v)
-        except Exception:
-            return sp.Float(0)
-
-        if len(solution) > 0:
-            return solution[0]
-
-        return sp.Float(0)
-
-    @staticmethod
-    def clean_substitutions(sub_dict: Dict[sp.Symbol, sp.Expr]) -> Dict[sp.Symbol, sp.Expr]:
-        """
-        "Cleans" a dictionary of substitutions by:
-            1. Delete substitutions in the form of x=x or x=0 (x=0 implies the substitution is redundant with other
-                substitutions in sub_dict)
-            2. If a substitution is of the form x = f(x, ...), attempts to solve the expression x - f(x, ...) = 0 for x,
-                and deletes the substitution if no solution exists.
-            3. Apply all substitutions in sub_dict to expressions in sub_dict to ensure older solutions remain up to
-                date with newly found solutions.
-
-        Parameters
-        ----------
-        sub_dict: dict
-            Dictionary of sp.Symbol keys and sp.Expr values to be passed to the subs method of sympy expressions.
-
-        Returns
-        -------
-        sub_dict: dict
-            Cleaned dictionary of sympy substitutions
-        """
-        result = sub_dict.copy()
-
-        for k, eq in sub_dict.items():
-            # Remove invalid or useless substitutions
-            if eq == 0 or k == eq:
-                del result[k]
-                continue
-
-            # Solve for the sub variable if necessary
-            elif k in eq.atoms():
-                try:
-                    eq = sp.solve(k - eq, k)[0]
-                except Exception:
-                    del result[k]
-                    continue
-            result[k] = eq
-
-        # Substitute subs into the sub dict
-        result = {k: v.subs(result) for k, v in result.items()}
-        return result
-
-    def get_candidates(
-        self,
-        system: List[sp.Expr],
-        variables: List[sp.Symbol],
-        state_variables: List[sp.Symbol],
-        var_penalty_factor: float = 25,
-        state_var_penalty_factor: float = 5,
-        length_penalty_factor: float = 1,
-        cores: int = -1,
-    ) -> Dict[sp.Symbol, Tuple[sp.Expr, float]]:
-        """
-        Attempt to solve every equation in the system for every variable. Scores the results using the score_eq
-        function, and returns (solution, score) pairs with the highest fitness (lowest unfitness).
-
-        Solving equations is parallelized using joblib.
-
-        Parameters
-        ----------
-        system: list of sp.Expr
-            List of steady state equations to be scored
-        variables: list of sp.Symbol
-            List of all variables among all steady state equations
-        state_variables: list of Sp.Symbol
-            List of all state variables among all steady state equations
-        var_penalty_factor: float, default: 25
-            A penalty factor applied to unfitness for each jumper variable in the expression.
-        state_var_penalty_factor: float, default: 5
-            A penalty factor applied to unfitness for each control variable in the expression.
-        length_penalty_factor: float, default: 1
-            A penalty factor applied to each term in the expression
-        cores: int, default -1
-            Number of cores over which to parallelize computation. Passed to joblib.Parallel. -1 for all available
-            cores.
-
-        Returns
-        -------
-        candidates: dict
-            A dictionary of candidate substitutions to simplify the steady state system. One candidate is produced
-            for each variable in the system. Keys are sp.Symbol, and values are (sp.Expr, float) tuples with the
-            candidate substitution and its fitness.
-        """
-        eq_vars = product(system, variables)
-
-        n = len(system)
-        k = len(variables)
-        args = (
-            variables,
-            state_variables,
-            var_penalty_factor,
-            state_var_penalty_factor,
-            length_penalty_factor,
-        )
-
-        with Parallel(cores) as pool:
-            solutions = pool(delayed(self.solve_and_return)(eq, v) for eq, v in eq_vars)
-            scores = np.array(pool(delayed(self.score_eq)(eq, *args) for eq in solutions))
-
-        score_matrix = scores.reshape(n, k)
-        idx_matrix = np.arange(n * k).reshape(n, k)
-        best_idx = idx_matrix[score_matrix.argmin(axis=0), np.arange(k)]
-
-        return dict(zip(variables, [(solutions[idx], scores[idx]) for idx in best_idx]))
-
-    @staticmethod
-    def make_solved_subs(sub_dict, assumptions):
-        res = {}
-        for k, v in sub_dict.items():
-            if not any([isinstance(x, TimeAwareSymbol) for x in v.atoms()]):
-                if v == 1:
-                    continue
-                res[v] = sp.Symbol(k.name + r"^\star", **assumptions[k.base_name])
-
-        return res
-
-    def solve_symbolic_steady_state(
-        self,
-        mod,
-        top_k=3,
-        var_penalty_factor=25,
-        state_var_penalty_factor=5,
-        length_penalty_factor=1,
-        cores=-1,
-        zero_tol=12,
-    ):
-
-        ss_vars = [x.to_ss() for x in mod.variables]
-        state_vars = [x for x in mod.variables if x.base_name == "Y"]
-        ss_system = mod.steady_state_system
-
-        system = ss_system.copy()
-        calib_eqs = [
-            var - eq for var, eq in zip(mod.params_to_calibrate, mod.calibrating_equations)
-        ]
-        system.extend(calib_eqs)
-
-        params = list(mod.free_param_dict.to_sympy().keys())
-        sub_dict = {}
-        unsolved_dict = {}
-
-        while True:
-            candidates = self.get_candidates(
-                system,
-                ss_vars,
-                state_vars,
-                var_penalty_factor=var_penalty_factor,
-                state_var_penalty_factor=state_var_penalty_factor,
-                length_penalty_factor=length_penalty_factor,
-                cores=cores,
-            )
-
-            scores = np.array([score for eq, score in candidates.values()])
-            print(scores)
-            top_k_score_idxs = scores.argsort()[:top_k]
-            for idx in top_k_score_idxs:
-                key = list(candidates.keys())[idx]
-                if candidates[key][0] == 0:
-                    continue
-                sub_dict[key] = candidates[key][0]
-
-            sub_dict = self.clean_substitutions(sub_dict)
-
-            system = [eq.subs(sub_dict) for eq in system]
-            system = [
-                eq
-                for eq in system
-                if not self.test_expr_is_zero(eq.subs(unsolved_dict), params, tol=zero_tol)
-            ]
-            solved_dict = self.make_solved_subs(sub_dict, mod.assumptions)
-            unsolved_dict = {v: k.subs(unsolved_dict) for k, v in solved_dict.items()}
-            system = [eq.subs(solved_dict) for eq in system]
-
-            if len(system) == 0:
-                break
-
-            if min(scores) > 100:
-                break
-
-        to_solve = {x for eq in system for x in eq.atoms() if isinstance(x, TimeAwareSymbol)}
-        system = [eq.simplify() for eq in system]
-        try:
-            final_solutions = sp.solve(system, to_solve, dict=True)
-        except NotImplementedError:
-            final_solutions = [{}]
-
-        return [sub_dict.update(d) for d in final_solutions]
-
-
-# from functools import partial
-# from typing import Any, Callable, Dict, List, Optional, Tuple, Union
-# from warnings import catch_warnings, simplefilter
-#
-# import numpy as np
-# import sympy as sp
-# from numpy.typing import ArrayLike
-# from scipy import optimize
-#
-# from gEconpy.classes.containers import SymbolDictionary
-#
-# from gEconpy.shared.typing import VariableType
-# from gEconpy.shared.utilities import (
-#     float_values_to_sympy_float,
-#     is_variable,
-#     merge_dictionaries,
-#     merge_functions,
-#     safe_string_to_sympy,
-#     sequential,
-#     sort_dictionary,
-#     string_keys_to_sympy,
-#     substitute_all_equations,
-#     symbol_to_string,
-#     sympy_keys_to_strings,
-#     sympy_number_values_to_floats,
-# )
-#
-#
-# class SteadyStateSolver:
-#     def __init__(self, model):
-#
-#         self.variables: List[VariableType] = model.variables
-#         self.shocks: List[sp.Add] = model.shocks
-#
-#         self.n_variables: int = model.n_variables
-#
-#         self.free_param_dict: SymbolDictionary[str, float] = model.free_param_dict
-#         self.params_to_calibrate: List[VariableType] = model.params_to_calibrate
-#         self.calibrating_equations: List[sp.Add] = model.calibrating_equations
-#         self.system_equations: List[sp.Add] = model.system_equations
-#         self.steady_state_relationships: SymbolDictionary[
-#             str, Union[float, sp.Add]
-#         ] = model.steady_state_relationships
-#
-#         self.steady_state_system: List[sp.Add] = []
-#         self.steady_state_dict: SymbolDictionary[str, float] = SymbolDictionary()
-#         self.steady_state_solved: bool = False
-#
-#         self.f_calib_params: Callable = lambda *args, **kwargs: {}
-#         self.f_ss_resid: Callable = lambda *args, **kwargs: np.inf
-#         self.f_ss: Callable = lambda *args, **kwargs: np.inf
-#
-#         self.build_steady_state_system()
-#
-#     def build_steady_state_system(self):
-#         self.steady_state_system = []
-#
-#         all_atoms = [
-#             x for eq in self.system_equations for x in eq.atoms() if is_variable(x)
-#         ]
-#         all_variables = set(all_atoms) - set(self.shocks)
-#         ss_sub_dict = {variable: variable.to_ss() for variable in set(all_variables)}
-#         unique_ss_variables = list(set(list(ss_sub_dict.values())))
-#
-#         steady_state_dict = dict.fromkeys(unique_ss_variables, None)
-#         steady_state_dict = (SymbolDictionary(steady_state_dict)
-#                              .to_string()
-#                              .sort_keys())
-#
-#         self.steady_state_dict = steady_state_dict
-#
-#         for shock in self.shocks:
-#             ss_sub_dict[shock] = 0
-#
-#         for eq in self.system_equations:
-#             self.steady_state_system.append(eq.subs(ss_sub_dict))
-#
-#     def solve_steady_state(
-#         self,
-#         param_bounds: Optional[Dict[str, Tuple[float, float]]] = None,
-#         optimizer_kwargs: Optional[Dict[str, Any]] = None,
-#         use_jac: Optional[bool] = False,
-#     ) -> Callable:
-#         """
-#
-#         Parameters
-#         ----------
-#         param_bounds: dict
-#             A dictionary of string, tuple(float, float) pairs, giving bounds for each variable or parameter to be
-#             solved for. Only used by certain optimizers; check the scipy docs. Pass it here instead of in
-#             optimizer_kwargs to make sure the correct variables have the correct bounds.
-#         optimizer_kwargs: dict
-#             A dictionary of keyword arguments to pass to the scipy optimizer, either root or root_scalar.
-#         use_jac: bool
-#             A flag to symbolically compute the Jacobain function of the model before optimization, can help the solver
-#             on complex problems.
-#
-#         Returns
-#         -------
-#         f_ss: Callable
-#             A function that maps a dictionary of parameters to steady state values for all system variables and
-#             calibrated parameters.
-#
-#         Solving of the steady state proceeds in three steps: solve calibrating equations (if any), gather user provided
-#         equations into a function, then solve the remaining equations.
-#
-#         Calibrating equations are handled first because if the user passed a complete steady state solution, it is
-#         unlikely to include solutions for calibrating equations. Calibrating equations are then combined with
-#         user supplied equations, and we check if everything necessary to solve the model is now present. If not,
-#         a final optimizer step runs to solve for the remaining variables.
-#
-#         Note that no checks are done in this function to validate the steady state solution. If a user supplies an
-#         incorrect steady state, this function will not catch it. It will, however, still fail if an optimizer fails
-#         to find a solution.
-#         """
-#         free_param_dict = self.free_param_dict.copy()
-#         parameters = list(free_param_dict.keys())
-#         variables = list(self.steady_state_dict.keys())
-#
-#         params_to_calibrate = [symbol_to_string(x) for x in self.params_to_calibrate]
-#
-#         n_to_calibrate = len(params_to_calibrate)
-#         has_calibrating_equations = n_to_calibrate > 0
-#
-#         params_and_variables = parameters + params_to_calibrate + variables
-#         steady_state_system = self.steady_state_system
-#
-#         # TODO: Move the creation of this residual function somewhere more logical
-#         self.f_ss_resid = sp.lambdify(params_and_variables, steady_state_system)
-#
-#         # Solve calibrating equations, if any.
-#         if has_calibrating_equations:
-#             f_calib, additional_solutions = self._solve_calibrating_equations(
-#                 param_bounds=param_bounds,
-#                 optimizer_kwargs=optimizer_kwargs,
-#                 use_jac=use_jac,
-#             )
-#         else:
-#             f_calib = lambda *args, **kwargs: {}
-#             additional_solutions = {}
-#
-#         solved_calib_params = list(f_calib(free_param_dict).keys())
-#
-#         # Gather user provided steady state solutions
-#         f_provided = self._gather_provided_solutions(solved_calib_params)
-#
-#         calib_dict = f_calib(free_param_dict)
-#         var_dict = f_provided(free_param_dict, calib_dict)
-#
-#         # If we have everything we're done. We don't need to use final_f, set it to return an empty dictionary.
-#         if (
-#             set(params_and_variables) - set(var_dict.keys()).union(calib_dict.keys())
-#         ) == set(free_param_dict.keys()):
-#             f_ss = self._create_final_function(
-#                 final_f=lambda x: {}, f_calib=f_calib, f_provided=f_provided
-#             )
-#
-#         else:
-#             final_f = self._solve_remaining_equations(
-#                 calib_dict=calib_dict,
-#                 var_dict=var_dict,
-#                 additional_solutions=additional_solutions,
-#                 param_bounds=param_bounds,
-#                 optimizer_kwargs=optimizer_kwargs,
-#                 use_jac=use_jac,
-#             )
-#             f_ss = self._create_final_function(
-#                 final_f=final_f, f_calib=f_calib, f_provided=f_provided
-#             )
-#
-#         return f_ss
-#
-#
-#     def _solve_calibrating_equations(
-#         self,
-#         param_bounds: Optional[Dict[str, Tuple[float, float]]],
-#         optimizer_kwargs: Optional[Dict[str, Any]],
-#         use_jac: bool = False,
-#     ) -> Tuple[Callable, Dict]:
-#         """
-#         Parameters
-#         ----------
-#         param_bounds: dict
-#             See docstring of solve_steady_state for details
-#         optimizer_kwargs: dict
-#             See docstring of solve_steady_state for details
-#         use_jac: bool
-#             See docstring of solve_steady_state for details
-#
-#         Returns
-#         -------
-#         f_calib: callable
-#             A function that maps param_dict to values of calibrated parameteres
-#         additional_solutions: dict
-#             A dictionary of symbolic solutions to non-calibrating parameters that were solved en passant and can be
-#             reused later
-#         """
-#         calibrating_equations = self.calibrating_equations
-#         symbolic_solutions = self.steady_state_relationships.copy()
-#         free_param_dict = self.free_param_dict.copy()
-#         steady_state_system = self.steady_state_system
-#
-#         parameters = list(free_param_dict.keys())
-#         variables = list(self.steady_state_dict.keys())
-#         params_to_calibrate = [symbol_to_string(x) for x in self.params_to_calibrate]
-#         params_and_variables = parameters + params_to_calibrate + variables
-#
-#         unknown_variables = set(variables).union(set(params_to_calibrate)) - set(
-#             symbolic_solutions.keys()
-#         )
-#
-#         n_to_calibrate = len(params_to_calibrate)
-#
-#         additional_solutions = {}
-#
-#         # Make substitutions
-#         calib_with_user_solutions = substitute_all_equations(
-#             calibrating_equations, symbolic_solutions
-#         )
-#
-#         # Try the heuristic solver
-#         calib_solutions, solved_mask = self.heuristic_solver(
-#             {},
-#             calib_with_user_solutions,
-#             calib_with_user_solutions,
-#             [safe_string_to_sympy(x) for x in params_and_variables],
-#         )
-#
-#         # Case 1: We found something! Refine the solution.
-#         if solved_mask.sum() > 0:
-#             # If the heuristic solver worked, we got solutions for variables that will allow us to go back and solve for
-#             # the calibrating parameters.
-#
-#             sub_dict = merge_dictionaries(free_param_dict, calib_solutions)
-#             more_solutions, solved_mask = self.heuristic_solver(
-#                 sub_dict,
-#                 substitute_all_equations(steady_state_system, sub_dict),
-#                 steady_state_system,
-#                 [safe_string_to_sympy(x) for x in params_and_variables],
-#             )
-#
-#             calib_solutions = {
-#                 key: value
-#                 for key, value in more_solutions.items()
-#                 if (key in params_to_calibrate)
-#             }
-#
-#             # We potentially pick up additional solutions from this heuristic pass, we can save them and use them later
-#             # to help the heuristic solver later.
-#             additional_solutions = {
-#                 key: value
-#                 for key, value in more_solutions.items()
-#                 if (key not in params_to_calibrate) and (key not in free_param_dict)
-#             }
-#
-#             calib_solutions = SymbolDictionary(calib_solutions).to_string().sort_keys().values_to_float()
-#             f_calib = lambda *args, **kwargs: calib_solutions
-#
-#         # Case 2: Found nothing, try to use an optimizer
-#         else:
-#             # Here we check how many equations are remaining to solve after accounting for the user's SS info.
-#             # We're looking for the case when all information is given EXCEPT the calibrating parameters.
-#             # If there is more than that, we handle it in the final pass.
-#             calib_remaining_to_solve = list(
-#                 set(unknown_variables) - set(symbolic_solutions.keys())
-#             )
-#             calib_n_eqs = len(calib_remaining_to_solve)
-#             if calib_n_eqs > len(calibrating_equations):
-#
-#                 def f_calib(*args, **kwargs):
-#                     return SymbolDictionary()
-#
-#                 return f_calib, SymbolDictionary()
-#
-#             # TODO: Is there a more elegant way to handle one equation vs many equations here?
-#             if calib_n_eqs == 1:
-#                 calib_with_user_solutions = calib_with_user_solutions[0]
-#
-#                 _f_calib = sp.lambdify(
-#                     calib_remaining_to_solve + parameters, calib_with_user_solutions
-#                 )
-#
-#                 def f_calib(x, kwargs):
-#                     return _f_calib(x, **kwargs)
-#
-#             else:
-#                 _f_calib = sp.lambdify(
-#                     calib_remaining_to_solve + parameters, calib_with_user_solutions
-#                 )
-#
-#                 def f_calib(args, kwargs):
-#                     return _f_calib(*args, **kwargs)
-#
-#             f_jac = None
-#             if use_jac:
-#                 f_jac = self._build_jacobian(
-#                     diff_variables=calib_remaining_to_solve,
-#                     additional_inputs=parameters,
-#                     equations=calib_with_user_solutions,
-#                 )
-#
-#             f_calib = self._bundle_symbolic_solutions_with_optimizer_solutions(
-#                 unknowns=calib_remaining_to_solve,
-#                 f=f_calib,
-#                 f_jac=f_jac,
-#                 param_dict=free_param_dict,
-#                 symbolic_solutions=calib_solutions,
-#                 n_eqs=calib_n_eqs,
-#                 output_names=calib_remaining_to_solve,
-#                 param_bounds=param_bounds,
-#                 optimizer_kwargs=optimizer_kwargs,
-#             )
-#
-#         return f_calib, additional_solutions
-#
-#     def _gather_provided_solutions(self, solved_calib_params) -> Callable:
-#         """
-#         Returns
-#         -------
-#         f_provided: Callable
-#             A function that takes model parameters, both calibrated and otherwise, as keywork arguments, and returns
-#             a dictionary of variable values according to steady state equations supplied by the user
-#         """
-#
-#         free_param_dict = self.free_param_dict.copy()
-#         symbolic_solutions = self.steady_state_relationships.copy()
-#         parameters = list(free_param_dict.keys())
-#
-#         _provided_lambda = sp.lambdify(
-#             parameters + solved_calib_params, [eq for eq in symbolic_solutions.values()]
-#         )
-#
-#         def f_provided(param_dict, calib_dict):
-#             return SymbolDictionary(dict(
-#                 zip(
-#                     symbolic_solutions.keys(),
-#                     _provided_lambda(**param_dict, **calib_dict),
-#                 )
-#             ))
-#
-#         return f_provided
-#
-#     def _solve_remaining_equations(
-#         self,
-#         calib_dict: Dict[str, float],
-#         var_dict: Dict[str, float],
-#         additional_solutions: Dict[str, float],
-#         param_bounds: Optional[Dict[str, Tuple[float, float]]],
-#         optimizer_kwargs: Optional[Dict[str, Any]],
-#         use_jac: bool,
-#     ) -> Callable:
-#         """
-#         Parameters
-#         ----------
-#         calib_dict: Dict
-#             A dictionary of solved calibrating parameters, if any.
-#         var_dict: Dict
-#             A dictionary of user-provided steady-state relationships, if any.
-#         additional_solutions:
-#             A dictionary of variable solutions found en passant by the heuristic solver while solving for the
-#             calibrated parameters, if any.
-#         param_bounds:
-#             See docstring of solve_steady_state for details
-#         optimizer_kwargs:
-#             See docstring of solve_steady_state for details
-#         use_jac:
-#             See docstring of solve_steady_state for details
-#
-#         Returns
-#         -------
-#         f_final: Callable
-#             A function that takes model parameters as keyword arguments and returns steady-state values for each
-#             model variable without an explicit symbolic solution.
-#         """
-#         free_param_dict = self.free_param_dict
-#         steady_state_system = self.steady_state_system
-#         calibrating_equations = self.calibrating_equations
-#
-#         parameters = list(free_param_dict.keys())
-#         variables = list(self.steady_state_dict.keys())
-#         params_to_calibrate = [symbol_to_string(x) for x in self.params_to_calibrate]
-#
-#         sub_dict = merge_dictionaries(calib_dict, var_dict, additional_solutions)
-#         params_and_variables = parameters + params_to_calibrate + variables
-#
-#         ss_solutions, solved_mask = self.heuristic_solver(
-#             sub_dict,
-#             substitute_all_equations(
-#                 steady_state_system + calibrating_equations, sub_dict, free_param_dict
-#             ),
-#             steady_state_system + calibrating_equations,
-#             [safe_string_to_sympy(x) for x in params_and_variables],
-#         )
-#
-#         ss_solutions = {
-#             key: value
-#             for key, value in ss_solutions.items()
-#             if key not in calib_dict.keys()
-#         }
-#         sub_dict.update(ss_solutions)
-#
-#         ss_remaining_to_solve = sorted(
-#             list(
-#                 set(variables + params_to_calibrate)
-#                 - set(ss_solutions.keys())
-#                 - set(calib_dict.keys())
-#             )
-#         )
-#
-#         unsolved_eqs = substitute_all_equations(
-#             [
-#                 eq
-#                 for idx, eq in enumerate(steady_state_system + calibrating_equations)
-#                 if not solved_mask[idx]
-#             ],
-#             sub_dict,
-#         )
-#
-#         n_eqs = len(unsolved_eqs)
-#
-#         _f_unsolved_ss = sp.lambdify(ss_remaining_to_solve + parameters, unsolved_eqs)
-#
-#         def f_unsolved_ss(args, kwargs):
-#             return _f_unsolved_ss(*args, **kwargs)
-#
-#         f_jac = None
-#         if use_jac:
-#             f_jac = self._build_jacobian(
-#                 diff_variables=ss_remaining_to_solve,
-#                 additional_inputs=parameters,
-#                 equations=unsolved_eqs,
-#             )
-#
-#         f_final = self._bundle_symbolic_solutions_with_optimizer_solutions(
-#             unknowns=ss_remaining_to_solve,
-#             f=f_unsolved_ss,
-#             f_jac=f_jac,
-#             param_dict=free_param_dict,
-#             symbolic_solutions=ss_solutions,
-#             n_eqs=n_eqs,
-#             output_names=ss_remaining_to_solve,
-#             param_bounds=param_bounds,
-#             optimizer_kwargs=optimizer_kwargs,
-#         )
-#
-#         return f_final
-#
-#     def _create_final_function(self, final_f, f_calib, f_provided):
-#         """
-#
-#         Parameters
-#         ----------
-#         final_f: Callable
-#             Function generated by solve_remaining_equations
-#         f_calib: Callable
-#             Function generated by _solve_calibrating_equations
-#         f_provided: Callable
-#             Function generated by _gather_provided_solutions
-#
-#         Returns
-#         -------
-#         f_ss: Callable
-#             A single function wrapping the three steady state functions, that returns a complete solution to the
-#             model's steady state as two dictionaries: one with variable values, and one with calibrated parameter
-#             values.
-#         """
-#         calib_params = [x.name for x in self.params_to_calibrate]
-#         ss_vars = [x.to_ss().name for x in self.variables]
-#
-#         def combined_function(param_dict):
-#             ss_out = SymbolDictionary()
-#
-#             calib_dict = f_calib(param_dict).copy()
-#             var_dict = f_provided(param_dict, calib_dict).copy()
-#             final_dict = final_f(param_dict).copy()
-#
-#             for param in calib_params:
-#                 if param in final_dict.keys():
-#                     calib_dict[param] = final_dict[param]
-#                     del final_dict[param]
-#
-#             var_dict_final = {}
-#             for key in var_dict:
-#                 if key in ss_vars:
-#                     var_dict_final[key] = var_dict[key]
-#
-#             ss_out = ss_out | var_dict_final | final_dict
-#
-#             return ss_out.sort_keys(), calib_dict.sort_keys()
-#
-#         return combined_function
-#
-#     def _bundle_symbolic_solutions_with_optimizer_solutions(
-#         self,
-#         unknowns: List[str],
-#         f: Callable,
-#         f_jac: Optional[Callable],
-#         param_dict: Dict[str, float],
-#         symbolic_solutions: Optional[Dict[str, float]],
-#         n_eqs: int,
-#         output_names: List[str],
-#         param_bounds: Optional[Dict[str, Tuple[float, float]]],
-#         optimizer_kwargs: Optional[Dict[str, Any]],
-#     ) -> Callable:
-#
-#         parameters = list(param_dict.keys())
-#
-#         optimize_wrapper = partial(
-#             self._optimize_dispatcher,
-#             unknowns=unknowns,
-#             f=f,
-#             f_jac=f_jac,
-#             n_eqs=n_eqs,
-#             param_bounds=param_bounds,
-#             optimizer_kwargs=optimizer_kwargs,
-#         )
-#         _symbolic_lambda = sp.lambdify(parameters, list(symbolic_solutions.values()))
-#
-#         def solve_optimizer_variables(param_dict):
-#             return SymbolDictionary(dict(zip(output_names, optimize_wrapper(param_dict))))
-#
-#         def solve_symbolic_variables(param_dict):
-#             return SymbolDictionary(dict(zip(symbolic_solutions.keys(), _symbolic_lambda(**param_dict))))
-#
-#         wrapped_f = merge_functions(
-#             [solve_optimizer_variables, solve_symbolic_variables], param_dict
-#         )
-#
-#         return wrapped_f
-#
-#     def _optimize_dispatcher(
-#         self, param_dict, unknowns, f, f_jac, n_eqs, param_bounds, optimizer_kwargs
-#     ):
-#         if n_eqs == 1:
-#             optimize_fun = optimize.root_scalar
-#             if param_bounds is None:
-#                 param_bounds = self._prepare_param_bounds(None, 1)[0]
-#             optimizer_kwargs = self._prepare_optimizer_kwargs(optimizer_kwargs, n_eqs)
-#             optimizer_kwargs.update(
-#                 dict(args=param_dict, method="brentq", bracket=param_bounds)
-#             )
-#
-#         else:
-#             optimize_fun = optimize.root
-#
-#             optimizer_kwargs = self._prepare_optimizer_kwargs(optimizer_kwargs, n_eqs)
-#             optimizer_kwargs.update(dict(args=param_dict, jac=f_jac))
-#
-#         with catch_warnings():
-#             simplefilter("ignore")
-#             result = optimize_fun(f, **optimizer_kwargs)
-#
-#         if hasattr(result, "converged") and result.converged:
-#             return np.atleast_1d(result.root)
-#         elif hasattr(result, "converged") and not result.converged:
-#             raise ValueError(
-#                 f"Optimization failed while solving for steady state solution of the following "
-#                 f'variables: {", ".join([symbol_to_string(x) for x in unknowns])}\n\n {result}'
-#             )
-#
-#         if hasattr(result, "success") and result.success:
-#             return result.x
-#
-#         elif hasattr(result, "success") and not result.success:
-#             raise ValueError(
-#                 f"Optimization failed while solving for steady state solution of the following "
-#                 f'variables: {", ".join([symbol_to_string(x) for x in unknowns])}\n\n {result}'
-#             )
-#
-#     @staticmethod
-#     def _build_jacobian(
-#         diff_variables: List[Union[str, VariableType]],
-#         additional_inputs: List[Union[str, VariableType]],
-#         equations: List[sp.Add],
-#     ) -> Callable:
-#         """
-#         Parameters
-#         ----------
-#         diff_variables: list
-#             A list of variables, as either TimeAwareSymbols or strings that the equations will be differentiated with
-#             respect to.
-#         additional_inputs: list
-#             A list of variables or parameters that will be arguments to the Jacobian function, but that will NOT
-#             be used in differentiation (i.e. the model parameters)
-#         equations: list
-#             A list of equations to be differentiated
-#
-#         Returns
-#         -------
-#         f_jac: Callable
-#             A function that takes diff_variables + additional_inputs as keyword arguments and returns an
-#             len(equations) x len(diff_variables) matrix of derivatives.
-#         """
-#         equations = np.atleast_1d(equations)
-#         sp_variables = [safe_string_to_sympy(x) for x in diff_variables]
-#         _f_jac = sp.lambdify(
-#             diff_variables + additional_inputs,
-#             [[eq.diff(x) for x in sp_variables] for eq in equations],
-#         )
-#
-#         def f_jac(args, kwargs):
-#             return np.array(_f_jac(*args, **kwargs))
-#
-#         return f_jac
-#
-#     @staticmethod
-#     def _prepare_optimizer_kwargs(
-#         optimizer_kwargs: Optional[Dict[str, Any]], n_unknowns: int
-#     ) -> Dict[str, Any]:
-#         if optimizer_kwargs is None:
-#             optimizer_kwargs = {}
-#
-#         arg_names = list(optimizer_kwargs.keys())
-#         if "x0" not in arg_names:
-#             optimizer_kwargs["x0"] = np.full(n_unknowns, 0.8)
-#         if "method" not in arg_names:
-#             optimizer_kwargs["method"] = "hybr"
-#
-#         return optimizer_kwargs
-#
-#     @staticmethod
-#     def _prepare_param_bounds(
-#         param_bounds: Optional[List[Tuple[float, float]]], n_params
-#     ) -> List[Tuple[float, float]]:
-#         if param_bounds is None:
-#             bounds = [(1e-4, 0.999) for _ in range(n_params)]
-#         else:
-#             bounds = [(lower + 1e-4, upper - 1e-4) for lower, upper in param_bounds]
-#
-#         return bounds
-#
-#     def _get_n_unknowns_in_eq(self, eq: sp.Add) -> int:
-#         params_to_calibrate = (
-#             [] if self.params_to_calibrate is None else self.params_to_calibrate
-#         )
-#         unknown_atoms = [
-#             x for x in eq.atoms() if is_variable(x) or x in params_to_calibrate
-#         ]
-#         n_unknowns = len(list(set(unknown_atoms)))
-#
-#         return n_unknowns
-#
-#     def heuristic_solver(
-#         self,
-#         solution_dict: Dict[str, float],
-#         subbed_ss_system: List[Any],
-#         steady_state_system: List[Any],
-#         unknowns: List[str],
-#     ) -> Tuple[Dict[str, float], ArrayLike]:
-#         """
-#         Parameters
-#         ----------
-#         solution_dict: dict
-#             A dictionary of TimeAwareSymbol: float pairs, giving steady-state values that have already been determined
-#
-#         subbed_ss_system: list
-#             A list containing all unsolved steady state equations, pre-substituted with parameter values and known
-#             steady-state values.
-#
-#         steady_state_system: list
-#             A list containing all steady state equations, without substitution
-#
-#         unknowns: list
-#             A list of sympy variables containing unknown values to solve for; variables plus any unsolved calibrated
-#             parameters.
-#
-#         Returns
-#         -------
-#         It is likely that the GCN model will contain simple equations that amount to little more than parameters, for
-#         example declaring that P = 1 in a perfect competition setup. These types of simple expressions can be "solved"
-#         and removed from the system to reduce the dimensionality of the problem given to the numerical solver.
-#
-#         This function performs this simplification in a heuristic way in the following manner. We first look for
-#         "simple" equations, defined as those with only a single unknown variable. Solutions are then substituted back
-#         into the system, equations that have reduced to 0=0 as a result of substitution are removed, then we repeat
-#         the procedure to see if any additional equations have become heuristically solvable as a result of substitution.
-#
-#         The process terminates when no "simple" equations remain.
-#         """
-#
-#         solved_mask = np.array([eq == 0 for eq in subbed_ss_system])
-#         eq_to_var_dict = {}
-#         check_again_mask = np.full_like(solved_mask, True)
-#         solution_dict = sequential(
-#             solution_dict, [float_values_to_sympy_float, string_keys_to_sympy]
-#         )
-#
-#         numeric_solutions = solution_dict.copy()
-#
-#         while True:
-#             solution_dict = {
-#                 key: eq.subs(solution_dict) for key, eq in solution_dict.items()
-#             }
-#             subbed_ss_system = [
-#                 eq.subs(numeric_solutions).simplify() for eq in subbed_ss_system
-#             ]
-#
-#             n_unknowns = np.array(
-#                 [self._get_n_unknowns_in_eq(eq) for eq in subbed_ss_system]
-#             )
-#             eq_len = np.array([len(eq.atoms()) for eq in subbed_ss_system])
-#
-#             solvable_mask = (n_unknowns < 2) & (~solved_mask) & check_again_mask
-#
-#             # Sympy struggles with solving complicated functions inside powers, just avoid them. 5 is a magic number
-#             # for the maximum number of variable in a function to be considered "complicated", needs tuning.
-#             has_power_argument = np.array(
-#                 [
-#                     any([isinstance(arg, sp.core.power.Pow)] for arg in eq.args)
-#                     for eq in subbed_ss_system
-#                 ]
-#             )
-#             solvable_mask &= ~(has_power_argument & (eq_len > 5))
-#
-#             if sum(solvable_mask) == 0:
-#                 break
-#
-#             for idx in np.flatnonzero(solvable_mask):
-#                 # Putting the solved = True flag here is ugly, but it catches equations
-#                 # that are 0 = 0 after substitution
-#                 solved_mask[idx] = True
-#
-#                 eq = subbed_ss_system[idx]
-#
-#                 variables = list({x for x in eq.atoms() if x in unknowns})
-#                 if len(variables) > 0:
-#                     eq_to_var_dict[variables[0]] = idx
-#
-#                     try:
-#                         symbolic_solution = sp.solve(
-#                             steady_state_system[idx], variables[0]
-#                         )
-#                     except NotImplementedError:
-#                         # There are functional forms sympy can't handle;  mark the equation as unsolvable and continue.
-#                         check_again_mask[idx] = False
-#                         solved_mask[idx] = False
-#                         continue
-#
-#                     # The solution should only ever be length 0 or 1, if it's more than 1 something went wrong. Haven't
-#                     # hit this case yet in testing.
-#                     if len(symbolic_solution) == 1:
-#                         solution_dict[variables[0]] = symbolic_solution[0]
-#                         numeric_solutions[variables[0]] = (
-#                             symbolic_solution[0]
-#                             .subs(self.free_param_dict)
-#                             .subs(numeric_solutions)
-#                         )
-#                         check_again_mask[:] = True
-#                         solved_mask[idx] = True
-#
-#                     else:
-#                         # Solver failed; something went wrong. Skip this equation.
-#                         solved_mask[idx] = False
-#                         check_again_mask[idx] = False
-#
-#                 else:
-#                     check_again_mask[idx] = False
-#
-#         numeric_solutions = sympy_number_values_to_floats(numeric_solutions)
-#         for key, eq in numeric_solutions.items():
-#             if not isinstance(eq, float):
-#                 del solution_dict[key]
-#                 solved_mask[eq_to_var_dict[key]] = False
-#
-#         solution_dict = sequential(
-#             solution_dict, [sympy_keys_to_strings, sympy_number_values_to_floats]
-#         )
-#
-#         return solution_dict, solved_mask
+from itertools import product
+from typing import Any, Callable, Dict, List, Optional, Tuple, Union
+from warnings import catch_warnings, simplefilter
+
+import numpy as np
+import sympy as sp
+from joblib import Parallel, delayed
+from scipy import optimize
+
+from gEconpy.classes.containers import SymbolDictionary
+from gEconpy.classes.time_aware_symbol import TimeAwareSymbol
+from gEconpy.numba_tools.utilities import numba_lambdify
+from gEconpy.shared.typing import VariableType
+from gEconpy.shared.utilities import eq_to_ss, substitute_all_equations
+
+
+class SteadyStateSolver:
+    def __init__(self, model):
+
+        self.variables: List[VariableType] = model.variables
+        self.shocks: List[sp.Add] = model.shocks
+
+        self.n_variables: int = model.n_variables
+
+        self.free_param_dict: SymbolDictionary[str, float] = model.free_param_dict
+        self.params_to_calibrate: List[VariableType] = model.params_to_calibrate
+        self.calibrating_equations: List[sp.Add] = model.calibrating_equations
+        self.shock_dict: Optional[SymbolDictionary[str, float]] = None
+
+        self.system_equations: List[sp.Add] = model.system_equations
+        self.steady_state_relationships: SymbolDictionary[
+            str, Union[float, sp.Add]
+        ] = model.steady_state_relationships
+
+        self.steady_state_system: List[sp.Add] = []
+        self.steady_state_dict: SymbolDictionary[str, float] = SymbolDictionary()
+        self.steady_state_solved: bool = False
+
+        self.build_steady_state_system()
+
+    def build_steady_state_system(self):
+
+        ss_vars = map(lambda x: x.to_ss(), self.variables)
+        self.steady_state_dict = SymbolDictionary.fromkeys(ss_vars, None).to_string().sort_keys()
+
+        self.shock_dict = SymbolDictionary.fromkeys(self.shocks, 0.0).to_ss()
+        self.steady_state_system = [
+            eq_to_ss(eq).subs(self.shock_dict).simplify() for eq in self.system_equations
+        ]
+
+    def _validate_optimizer_kwargs(
+            self, optimizer_kwargs: dict, n_eq: int, method: str, use_jac: bool, use_hess: bool
+    ) -> dict:
+        """
+        Validate user-provided keyword arguments to either scipy.optimize.root or scipy.optimize.minimize, and insert
+        good defaults where not provided.
+
+        Note: This function never overwrites user arguments.
+
+        Parameters
+        ----------
+        optimizer_kwargs: dict
+            User-provided arguments for the optimizer
+        n_eq: int
+            Number of remaining steady-state equations after reduction
+        method: str
+            Which family of solution algorithms, minimization or root-finding, to be used.
+        use_jac: bool
+            Whether computation of the jacobian has been requested
+        use_hess: bool
+            Whether computation of the hessian has been requested
+
+        Returns
+        -------
+        optimizer_kwargs: dict
+            Keyword arguments for the scipy function, with "reasonable" defaults inserted where not provided
+        """
+
+        optimizer_kwargs = {} if optimizer_kwargs is None else optimizer_kwargs
+        method_given = hasattr(optimizer_kwargs, "method")
+
+        if method == "root" and not method_given:
+            if use_jac:
+                optimizer_kwargs["method"] = "hybr"
+            else:
+                optimizer_kwargs["method"] = "broyden1"
+
+            if n_eq == 1:
+                optimizer_kwargs["method"] = "lm"
+
+        elif method == "minimize" and not method_given:
+            # Set optimizer_kwargs for minimization
+            if use_hess and use_jac:
+                optimizer_kwargs["method"] = "trust-exact"
+            elif use_jac:
+                optimizer_kwargs["method"] = "BFGS"
+            else:
+                optimizer_kwargs["method"] = "Nelder-Mead"
+
+        if not hasattr(optimizer_kwargs, "tol"):
+            optimizer_kwargs["tol"] = 1e-9
+
+        return optimizer_kwargs
+
+    def solve_steady_state(
+            self,
+            apply_user_simplifications: Optional[bool] = True,
+            model_is_linear: Optional[bool] = True,
+            optimizer_kwargs: Optional[Dict[str, Any]] = None,
+            method: Optional[str] = "root",
+            use_jac: Optional[bool] = True,
+            use_hess: Optional[bool] = True,
+    ) -> Callable:
+        """
+        Solving of the steady state proceeds in three steps: solve calibrating equations (if any), gather user provided
+        equations into a function, then solve the remaining equations.
+
+        Calibrating equations are handled first because if the user passed a complete steady state solution, it is
+        unlikely to include solutions for calibrating equations. Calibrating equations are then combined with
+        user supplied equations, and we check if everything necessary to solve the model is now present. If not,
+        a final optimizer step runs to solve for the remaining variables.
+
+        Note that no checks are done in this function to validate the steady state solution. If a user supplies an
+        incorrect steady state, this function will not catch it. It will, however, still fail if an optimizer fails
+        to find a solution.
+
+        Parameters
+        ----------
+        apply_user_simplifications: bool
+            If true, substitute all equations using the steady-state equations provided in the steady_state block
+            of the GCN file.
+        model_is_linear: bool
+            A flag indicating that the model has already been linearized by the user. In this case, the steady state
+            can be obtained simply by forming an augmented matrix and finding its reduced row-echelon form. If True,
+            all other arguments to this function have no effect. Default is False.
+        optimizer_kwargs: dict
+            A dictionary of keyword arguments to pass to the scipy optimizer, either root or minimize. See the docstring
+            for scipy.optimize.root or scipy.optimize.minimize for more information.
+        method: str, default: "root"
+            Whether to seek the steady state via root finding algorithm or via minimization of squared errors. "root"
+            requires that the number of unknowns be equal to the number of equations; this assumption can be violated
+            if the user provides only a subset of steady-state relationship (and this subset does not result in
+            elimination of model equations via substitution).
+            One of "root" or "minimize".
+        use_jac: bool
+            A flag indicating whether to use the Jacobian of the steady-state system when solving. Can help the
+            solver on complex problems, but symbolic computation may be slow on large problems. Default is True.
+        use_hess: bool
+            A flag indicating whether to use the Hessian of the loss function of the steady-state system when solving.
+            Ignored if method is "root", as these routines do not use Hessian information.
+
+        Returns
+        -------
+        f_ss: Callable
+            A function that maps a dictionary of parameters to steady state values for all system variables and
+            calibrated parameters.
+        """
+
+        param_dict = self.free_param_dict.copy().to_sympy()
+        params = list(param_dict.keys())
+        calib_params = self.params_to_calibrate
+        user_provided = self.steady_state_relationships.copy().to_sympy().float_to_values()
+        ss_eqs = self.steady_state_system.copy()
+        calib_eqs = self.calibrating_equations.copy()
+        all_eqs = ss_eqs + calib_eqs
+
+        all_vars_sym = list(self.steady_state_dict.to_sympy().keys())
+        all_vars_and_calib_sym = all_vars_sym + self.params_to_calibrate
+
+        # This can be skipped if we're working on a linear model (there should be no user simplifications)
+        if apply_user_simplifications and not model_is_linear:
+
+            zeros = np.full_like(all_eqs, False)
+            simplified_eqs = substitute_all_equations(all_eqs, user_provided)
+
+            for i, eq in enumerate(simplified_eqs):
+                subbed_eq = eq.subs(param_dict)
+
+                # Janky, but many expressions won't reduce to zero even if they ought to -> test numerically
+                atoms = [x for x in subbed_eq.atoms() if x in all_vars_and_calib_sym]
+                test_values = {x: np.random.uniform(1e-2, 0.99) for x in atoms}
+                eq_is_zero = sp.Abs(subbed_eq.subs(test_values)) < 1e-8
+                zeros[i] = eq_is_zero
+
+                if isinstance(subbed_eq, sp.Float) and not eq_is_zero:
+                    raise ValueError(
+                        f"Applying user steady state definitions to equation {i}:\n"
+                        f"\t{all_eqs[i]}\n"
+                        f"resulted in non-zero residuals: {subbed_eq}.\n"
+                        f"Please verify the provided steady state relationships are correct."
+                    )
+            eqs_to_solve = [eq for i, eq in enumerate(simplified_eqs) if not zeros[i]]
+        else:
+            eqs_to_solve = all_eqs
+
+        vars_sym = sorted(
+            list({x for eq in eqs_to_solve for x in eq.atoms() if isinstance(x, TimeAwareSymbol)}),
+            key=lambda x: x.name,
+        )
+
+        vars_and_calib_sym = vars_sym + calib_params
+
+        k_vars = len(vars_sym)
+        k_calib = len(calib_params)
+        n_eq = len(eqs_to_solve)
+
+        if (n_eq != (k_vars + k_calib)) and (n_eq > 0) and (method == "root"):
+            raise ValueError(
+                'method = "root" is only possible when the number of equations (after substitution of '
+                "user-provided steady-state relationships) is equal to the number of (remaining) "
+                f"variables.\nFound {n_eq} equations and {k_vars} variables. This can happen if "
+                f"user-provided steady-state relationships do not result in elimination of model "
+                f"equations after substitution. \nCheck the provided steady state relationships, or "
+                f'use method = "minimize" to attempt to solve via minimization of squared errors.'
+            )
+
+        # Get residuals for all equations, regardless of how much simplification was done
+        f_ss_resid = numba_lambdify(
+            exog_vars=all_vars_and_calib_sym, endog_vars=params, expr=[all_eqs]
+        )
+
+        if model_is_linear:
+            # If the model is linear, we can quickly solve for the steady state.
+            # The purpose of using sp.cse here is to undo the deterministic simplifications. If the system is in
+            # steady state, this is made harder.
+            # TODO: Potentially save a "reverse deterministic sub" dict for use here.
+            all_eqs_no_ss = self.system_equations + self.calibrating_equations
+            all_vars_and_calib_sym_no_ss = self.variables + self.params_to_calibrate
+
+            sub_dict, simplified_system = sp.cse(all_eqs_no_ss, ignore=all_vars_and_calib_sym_no_ss)
+
+            shock_subs = {shock.to_ss(): 0 for shock in self.shocks}
+
+            A, b = sp.linear_eq_to_matrix([eq_to_ss(eq).subs(shock_subs) for eq in simplified_system],
+                                          all_vars_and_calib_sym)
+            Ab = sp.Matrix([[A, b]])
+            A_rref, _ = Ab.rref()
+            steady_state_values = A_rref[:, -1].subs(sub_dict * len(sub_dict))
+
+            f_ss = numba_lambdify(
+                exog_vars=params, expr=steady_state_values
+            )
+
+            def ss_func(param_dict):
+                success = True
+                params = np.array(list(param_dict.values()))
+
+                # Need to ravel because the result of Ab.rref() is a column vector
+                ss_values = f_ss(params).ravel()
+                result_dict = SymbolDictionary(dict(zip(all_vars_and_calib_sym, ss_values)))
+
+                ss_dict = self.steady_state_dict.float_to_values().to_sympy().copy()
+                calib_dict = SymbolDictionary(dict(zip(self.params_to_calibrate, [np.inf] * k_calib)))
+
+                for k in ss_dict.keys():
+                    ss_dict[k] = result_dict[k]
+                for k in calib_dict.keys():
+                    calib_dict[k] = result_dict[k]
+
+                return {
+                    "ss_dict": ss_dict.to_string(),
+                    "calib_dict": calib_dict.to_string(),
+                    "resids": np.array(
+                        f_ss_resid(np.array(list(ss_dict.values()) + list(calib_dict.values())), params)
+                    ),
+                    "success": success,
+                }
+
+            return ss_func
+
+        f_user = numba_lambdify(
+            exog_vars=vars_and_calib_sym, endog_vars=params, expr=[list(user_provided.values())]
+        )
+
+        optimizer_required = True
+        f_jac_ss = None
+        f_hess_ss = None
+
+        if n_eq == 0:
+            optimizer_required = False
+
+        elif method == "root":
+            f_ss = numba_lambdify(
+                exog_vars=vars_and_calib_sym, endog_vars=params, expr=[eqs_to_solve]
+            )
+
+            if use_jac:
+                jac = sp.Matrix([[eq.diff(x) for x in vars_and_calib_sym] for eq in eqs_to_solve])
+                f_jac_ss = numba_lambdify(exog_vars=vars_and_calib_sym, endog_vars=params, expr=jac)
+
+        elif method == "minimize":
+            # For minimization, need to form a loss function (use L2 norm -- better options?).
+            loss = sum([eq ** 2 for eq in eqs_to_solve])
+            f_loss = numba_lambdify(exog_vars=vars_and_calib_sym, endog_vars=params, expr=[loss])
+            if use_jac:
+                jac = [loss.diff(x) for x in vars_and_calib_sym]
+
+                f_jac_ss = numba_lambdify(
+                    exog_vars=vars_and_calib_sym, endog_vars=params, expr=[jac]
+                )
+
+            if use_hess:
+                hess = sp.hessian(loss, vars_and_calib_sym)
+                f_hess_ss = numba_lambdify(
+                    exog_vars=vars_and_calib_sym, endog_vars=params, expr=hess
+                )
+
+        optimizer_kwargs = self._validate_optimizer_kwargs(
+            optimizer_kwargs, n_eq, method, use_jac, use_hess
+        )
+
+        def ss_func(param_dict):
+            params = np.array(list(param_dict.values()))
+
+            if optimizer_required:
+                x0 = np.full(k_vars + k_calib, 0.8)
+                with catch_warnings():
+                    simplefilter("ignore")
+                    if method == "root":
+                        optim = optimize.root(
+                            f_ss, jac=f_jac_ss, x0=x0, args=params, **optimizer_kwargs
+                        )
+                    elif method == "minimize":
+                        optim = optimize.minimize(
+                            f_loss,
+                            jac=f_jac_ss,
+                            hess=f_hess_ss,
+                            x0=x0,
+                            args=params,
+                            **optimizer_kwargs,
+                        )
+
+                optim_dict = SymbolDictionary(dict(zip(vars_and_calib_sym, optim.x)))
+                success = optim.success
+            else:
+                optim_dict = SymbolDictionary()
+                success = True
+
+            ss_dict = self.steady_state_dict.float_to_values().to_sympy().copy()
+            calib_dict = SymbolDictionary(dict(zip(self.params_to_calibrate, [np.inf] * k_calib)))
+            user_dict = SymbolDictionary(
+                dict(
+                    zip(
+                        user_provided.keys(),
+                        f_user(np.array(list(optim_dict.values())), params),
+                    )
+                )
+            )
+
+            for k in all_vars_sym:
+                if k in optim_dict.keys():
+                    ss_dict[k] = optim_dict[k]
+                elif k in user_provided.keys():
+                    ss_dict[k] = user_dict[k]
+                else:
+                    raise ValueError(
+                        f"Could not find {k} among either optimizer or user provided solutions"
+                    )
+
+            for k in calib_params:
+                if k in optim_dict.keys():
+                    calib_dict[k] = optim_dict[k]
+                elif k in user_provided.keys():
+                    calib_dict[k] = user_dict[k]
+                else:
+                    raise ValueError(
+                        f"Could not find {k} among either optimizer or user provided solutions"
+                    )
+
+            ss_dict.sort_keys(inplace=True)
+            calib_dict.sort_keys(inplace=True)
+
+            return {
+                "ss_dict": ss_dict.to_string(),
+                "calib_dict": calib_dict.to_string(),
+                "resids": np.array(
+                    f_ss_resid(np.array(list(ss_dict.values()) + list(calib_dict.values())), params)
+                ),
+                "success": success,
+            }
+
+        return ss_func
+
+
+class SymbolicSteadyStateSolver:
+    def __init__(self):
+        pass
+
+    @staticmethod
+    def score_eq(
+            eq: sp.Expr,
+            var_list: List[sp.Symbol],
+            state_vars: List[sp.Symbol],
+            var_penalty_factor: float = 25,
+            state_var_penalty_factor: float = 5,
+            length_penalty_factor: float = 1,
+    ) -> float:
+
+        """
+        Compute an "unfitness" score for an equation using three simple heuristics:
+            1. The number of jumper variables in the expression
+            2. The number of state variables in the expression
+            3. The total length of the expression
+
+        Expressions with the lowest unfitness will be selected. Setting a lower penalty for state variables will
+        push the system towards finding solutions expressed in state variables if a steady state is parameters only
+        cannot be found.
+
+        Parameters
+        ----------
+        eq: sp.Expr
+            A sympy expression representing a steady-state equation
+        var_list: list of sp.Symbol
+            A list of sympy symbols representing all variables in the model (state and jumper)
+        state_vars: list of sp.Symbol
+            A list of symbol symbols representing all state variables in the model
+        var_penalty_factor: float, default: 25
+            A penalty factor applied to unfitness for each jumper variable in the expression.
+        state_var_penalty_factor: float, default: 5
+            A penalty factor applied to unfitness for each control variable in the expression.
+        length_penalty_factor: float, default: 1
+            A penalty factor applied to each term in the expression
+
+        Returns
+        -------
+        unfitness: float
+            An unfitness score used to select potential substitutions between system equations
+        """
+
+        # If the equation is length zero, it's been reduced away and should never be selected.
+        if eq == 0:
+            return 10000
+
+        var_list = list(set(var_list) - set(state_vars))
+
+        # The equation with the LOWEST score will be chosen to substitute, so punishing state variables less
+        # ensures that equations that have only state variables will be chosen more often.
+        var_penalty = len([x for x in eq.atoms() if x in var_list]) * var_penalty_factor
+        state_var_penalty = (
+                len([x for x in eq.atoms() if x in state_vars]) * state_var_penalty_factor
+        )
+
+        # Prefer shorter equations
+        length_penalty = eq.count_ops() * length_penalty_factor
+
+        return var_penalty + state_var_penalty + length_penalty
+
+    @staticmethod
+    def solve_and_return(eq: sp.Expr, v: sp.Symbol) -> sp.Expr:
+        """
+        Attempt to solve an expression for a given variable. Returns 0 if the expression is not solvable or if the
+        given variable does not appear in the expression. If multiple solutions are found, only the first one is
+        returned.
+
+        Parameters
+        ----------
+        eq: sp.Expr
+            A sympy expression
+        v: sp.Symbol
+            A sympy symbol
+
+        Returns
+        -------
+        solution: sp.Expr
+            Given f(x, ...) =  0, returns x = g(...) if possible, or 0 if not.
+        """
+
+        if v not in eq.atoms():
+            return sp.Float(0)
+        try:
+            solution = sp.solve(eq, v)
+        except Exception:
+            return sp.Float(0)
+
+        if len(solution) > 0:
+            return solution[0]
+
+        return sp.Float(0)
+
+    @staticmethod
+    def clean_substitutions(sub_dict: Dict[sp.Symbol, sp.Expr]) -> Dict[sp.Symbol, sp.Expr]:
+        """
+        "Cleans" a dictionary of substitutions by:
+            1. Delete substitutions in the form of x=x or x=0 (x=0 implies the substitution is redundant with other
+                substitutions in sub_dict)
+            2. If a substitution is of the form x = f(x, ...), attempts to solve the expression x - f(x, ...) = 0 for x,
+                and deletes the substitution if no solution exists.
+            3. Apply all substitutions in sub_dict to expressions in sub_dict to ensure older solutions remain up to
+                date with newly found solutions.
+
+        Parameters
+        ----------
+        sub_dict: dict
+            Dictionary of sp.Symbol keys and sp.Expr values to be passed to the subs method of sympy expressions.
+
+        Returns
+        -------
+        sub_dict: dict
+            Cleaned dictionary of sympy substitutions
+        """
+        result = sub_dict.copy()
+
+        for k, eq in sub_dict.items():
+            # Remove invalid or useless substitutions
+            if eq == 0 or k == eq:
+                del result[k]
+                continue
+
+            # Solve for the sub variable if necessary
+            elif k in eq.atoms():
+                try:
+                    eq = sp.solve(k - eq, k)[0]
+                except Exception:
+                    del result[k]
+                    continue
+            result[k] = eq
+
+        # Substitute subs into the sub dict
+        result = {k: v.subs(result) for k, v in result.items()}
+        return result
+
+    def get_candidates(
+            self,
+            system: List[sp.Expr],
+            variables: List[sp.Symbol],
+            state_variables: List[sp.Symbol],
+            var_penalty_factor: float = 25,
+            state_var_penalty_factor: float = 5,
+            length_penalty_factor: float = 1,
+            cores: int = -1,
+    ) -> Dict[sp.Symbol, Tuple[sp.Expr, float]]:
+        """
+        Attempt to solve every equation in the system for every variable. Scores the results using the score_eq
+        function, and returns (solution, score) pairs with the highest fitness (lowest unfitness).
+
+        Solving equations is parallelized using joblib.
+
+        Parameters
+        ----------
+        system: list of sp.Expr
+            List of steady state equations to be scored
+        variables: list of sp.Symbol
+            List of all variables among all steady state equations
+        state_variables: list of Sp.Symbol
+            List of all state variables among all steady state equations
+        var_penalty_factor: float, default: 25
+            A penalty factor applied to unfitness for each jumper variable in the expression.
+        state_var_penalty_factor: float, default: 5
+            A penalty factor applied to unfitness for each control variable in the expression.
+        length_penalty_factor: float, default: 1
+            A penalty factor applied to each term in the expression
+        cores: int, default -1
+            Number of cores over which to parallelize computation. Passed to joblib.Parallel. -1 for all available
+            cores.
+
+        Returns
+        -------
+        candidates: dict
+            A dictionary of candidate substitutions to simplify the steady state system. One candidate is produced
+            for each variable in the system. Keys are sp.Symbol, and values are (sp.Expr, float) tuples with the
+            candidate substitution and its fitness.
+        """
+        eq_vars = product(system, variables)
+
+        n = len(system)
+        k = len(variables)
+        args = (
+            variables,
+            state_variables,
+            var_penalty_factor,
+            state_var_penalty_factor,
+            length_penalty_factor,
+        )
+
+        with Parallel(cores) as pool:
+            solutions = pool(delayed(self.solve_and_return)(eq, v) for eq, v in eq_vars)
+            scores = np.array(pool(delayed(self.score_eq)(eq, *args) for eq in solutions))
+
+        score_matrix = scores.reshape(n, k)
+        idx_matrix = np.arange(n * k).reshape(n, k)
+        best_idx = idx_matrix[score_matrix.argmin(axis=0), np.arange(k)]
+
+        return dict(zip(variables, [(solutions[idx], scores[idx]) for idx in best_idx]))
+
+    @staticmethod
+    def make_solved_subs(sub_dict, assumptions):
+        res = {}
+        for k, v in sub_dict.items():
+            if not any([isinstance(x, TimeAwareSymbol) for x in v.atoms()]):
+                if v == 1:
+                    continue
+                res[v] = sp.Symbol(k.name + r"^\star", **assumptions[k.base_name])
+
+        return res
+
+    def solve_symbolic_steady_state(
+            self,
+            mod,
+            top_k=3,
+            var_penalty_factor=25,
+            state_var_penalty_factor=5,
+            length_penalty_factor=1,
+            cores=-1,
+            zero_tol=12,
+    ):
+
+        ss_vars = [x.to_ss() for x in mod.variables]
+        state_vars = [x for x in mod.variables if x.base_name == "Y"]
+        ss_system = mod.steady_state_system
+
+        system = ss_system.copy()
+        calib_eqs = [
+            var - eq for var, eq in zip(mod.params_to_calibrate, mod.calibrating_equations)
+        ]
+        system.extend(calib_eqs)
+
+        params = list(mod.free_param_dict.to_sympy().keys())
+        sub_dict = {}
+        unsolved_dict = {}
+
+        while True:
+            candidates = self.get_candidates(
+                system,
+                ss_vars,
+                state_vars,
+                var_penalty_factor=var_penalty_factor,
+                state_var_penalty_factor=state_var_penalty_factor,
+                length_penalty_factor=length_penalty_factor,
+                cores=cores,
+            )
+
+            scores = np.array([score for eq, score in candidates.values()])
+            print(scores)
+            top_k_score_idxs = scores.argsort()[:top_k]
+            for idx in top_k_score_idxs:
+                key = list(candidates.keys())[idx]
+                if candidates[key][0] == 0:
+                    continue
+                sub_dict[key] = candidates[key][0]
+
+            sub_dict = self.clean_substitutions(sub_dict)
+
+            system = [eq.subs(sub_dict) for eq in system]
+            system = [
+                eq
+                for eq in system
+                if not self.test_expr_is_zero(eq.subs(unsolved_dict), params, tol=zero_tol)
+            ]
+            solved_dict = self.make_solved_subs(sub_dict, mod.assumptions)
+            unsolved_dict = {v: k.subs(unsolved_dict) for k, v in solved_dict.items()}
+            system = [eq.subs(solved_dict) for eq in system]
+
+            if len(system) == 0:
+                break
+
+            if min(scores) > 100:
+                break
+
+        to_solve = {x for eq in system for x in eq.atoms() if isinstance(x, TimeAwareSymbol)}
+        system = [eq.simplify() for eq in system]
+        try:
+            final_solutions = sp.solve(system, to_solve, dict=True)
+        except NotImplementedError:
+            final_solutions = [{}]
+
+        return [sub_dict.update(d) for d in final_solutions]
+
+# from functools import partial
+# from typing import Any, Callable, Dict, List, Optional, Tuple, Union
+# from warnings import catch_warnings, simplefilter
+#
+# import numpy as np
+# import sympy as sp
+# from numpy.typing import ArrayLike
+# from scipy import optimize
+#
+# from gEconpy.classes.containers import SymbolDictionary
+#
+# from gEconpy.shared.typing import VariableType
+# from gEconpy.shared.utilities import (
+#     float_values_to_sympy_float,
+#     is_variable,
+#     merge_dictionaries,
+#     merge_functions,
+#     safe_string_to_sympy,
+#     sequential,
+#     sort_dictionary,
+#     string_keys_to_sympy,
+#     substitute_all_equations,
+#     symbol_to_string,
+#     sympy_keys_to_strings,
+#     sympy_number_values_to_floats,
+# )
+#
+#
+# class SteadyStateSolver:
+#     def __init__(self, model):
+#
+#         self.variables: List[VariableType] = model.variables
+#         self.shocks: List[sp.Add] = model.shocks
+#
+#         self.n_variables: int = model.n_variables
+#
+#         self.free_param_dict: SymbolDictionary[str, float] = model.free_param_dict
+#         self.params_to_calibrate: List[VariableType] = model.params_to_calibrate
+#         self.calibrating_equations: List[sp.Add] = model.calibrating_equations
+#         self.system_equations: List[sp.Add] = model.system_equations
+#         self.steady_state_relationships: SymbolDictionary[
+#             str, Union[float, sp.Add]
+#         ] = model.steady_state_relationships
+#
+#         self.steady_state_system: List[sp.Add] = []
+#         self.steady_state_dict: SymbolDictionary[str, float] = SymbolDictionary()
+#         self.steady_state_solved: bool = False
+#
+#         self.f_calib_params: Callable = lambda *args, **kwargs: {}
+#         self.f_ss_resid: Callable = lambda *args, **kwargs: np.inf
+#         self.f_ss: Callable = lambda *args, **kwargs: np.inf
+#
+#         self.build_steady_state_system()
+#
+#     def build_steady_state_system(self):
+#         self.steady_state_system = []
+#
+#         all_atoms = [
+#             x for eq in self.system_equations for x in eq.atoms() if is_variable(x)
+#         ]
+#         all_variables = set(all_atoms) - set(self.shocks)
+#         ss_sub_dict = {variable: variable.to_ss() for variable in set(all_variables)}
+#         unique_ss_variables = list(set(list(ss_sub_dict.values())))
+#
+#         steady_state_dict = dict.fromkeys(unique_ss_variables, None)
+#         steady_state_dict = (SymbolDictionary(steady_state_dict)
+#                              .to_string()
+#                              .sort_keys())
+#
+#         self.steady_state_dict = steady_state_dict
+#
+#         for shock in self.shocks:
+#             ss_sub_dict[shock] = 0
+#
+#         for eq in self.system_equations:
+#             self.steady_state_system.append(eq.subs(ss_sub_dict))
+#
+#     def solve_steady_state(
+#         self,
+#         param_bounds: Optional[Dict[str, Tuple[float, float]]] = None,
+#         optimizer_kwargs: Optional[Dict[str, Any]] = None,
+#         use_jac: Optional[bool] = False,
+#     ) -> Callable:
+#         """
+#
+#         Parameters
+#         ----------
+#         param_bounds: dict
+#             A dictionary of string, tuple(float, float) pairs, giving bounds for each variable or parameter to be
+#             solved for. Only used by certain optimizers; check the scipy docs. Pass it here instead of in
+#             optimizer_kwargs to make sure the correct variables have the correct bounds.
+#         optimizer_kwargs: dict
+#             A dictionary of keyword arguments to pass to the scipy optimizer, either root or root_scalar.
+#         use_jac: bool
+#             A flag to symbolically compute the Jacobain function of the model before optimization, can help the solver
+#             on complex problems.
+#
+#         Returns
+#         -------
+#         f_ss: Callable
+#             A function that maps a dictionary of parameters to steady state values for all system variables and
+#             calibrated parameters.
+#
+#         Solving of the steady state proceeds in three steps: solve calibrating equations (if any), gather user provided
+#         equations into a function, then solve the remaining equations.
+#
+#         Calibrating equations are handled first because if the user passed a complete steady state solution, it is
+#         unlikely to include solutions for calibrating equations. Calibrating equations are then combined with
+#         user supplied equations, and we check if everything necessary to solve the model is now present. If not,
+#         a final optimizer step runs to solve for the remaining variables.
+#
+#         Note that no checks are done in this function to validate the steady state solution. If a user supplies an
+#         incorrect steady state, this function will not catch it. It will, however, still fail if an optimizer fails
+#         to find a solution.
+#         """
+#         free_param_dict = self.free_param_dict.copy()
+#         parameters = list(free_param_dict.keys())
+#         variables = list(self.steady_state_dict.keys())
+#
+#         params_to_calibrate = [symbol_to_string(x) for x in self.params_to_calibrate]
+#
+#         n_to_calibrate = len(params_to_calibrate)
+#         has_calibrating_equations = n_to_calibrate > 0
+#
+#         params_and_variables = parameters + params_to_calibrate + variables
+#         steady_state_system = self.steady_state_system
+#
+#         # TODO: Move the creation of this residual function somewhere more logical
+#         self.f_ss_resid = sp.lambdify(params_and_variables, steady_state_system)
+#
+#         # Solve calibrating equations, if any.
+#         if has_calibrating_equations:
+#             f_calib, additional_solutions = self._solve_calibrating_equations(
+#                 param_bounds=param_bounds,
+#                 optimizer_kwargs=optimizer_kwargs,
+#                 use_jac=use_jac,
+#             )
+#         else:
+#             f_calib = lambda *args, **kwargs: {}
+#             additional_solutions = {}
+#
+#         solved_calib_params = list(f_calib(free_param_dict).keys())
+#
+#         # Gather user provided steady state solutions
+#         f_provided = self._gather_provided_solutions(solved_calib_params)
+#
+#         calib_dict = f_calib(free_param_dict)
+#         var_dict = f_provided(free_param_dict, calib_dict)
+#
+#         # If we have everything we're done. We don't need to use final_f, set it to return an empty dictionary.
+#         if (
+#             set(params_and_variables) - set(var_dict.keys()).union(calib_dict.keys())
+#         ) == set(free_param_dict.keys()):
+#             f_ss = self._create_final_function(
+#                 final_f=lambda x: {}, f_calib=f_calib, f_provided=f_provided
+#             )
+#
+#         else:
+#             final_f = self._solve_remaining_equations(
+#                 calib_dict=calib_dict,
+#                 var_dict=var_dict,
+#                 additional_solutions=additional_solutions,
+#                 param_bounds=param_bounds,
+#                 optimizer_kwargs=optimizer_kwargs,
+#                 use_jac=use_jac,
+#             )
+#             f_ss = self._create_final_function(
+#                 final_f=final_f, f_calib=f_calib, f_provided=f_provided
+#             )
+#
+#         return f_ss
+#
+#
+#     def _solve_calibrating_equations(
+#         self,
+#         param_bounds: Optional[Dict[str, Tuple[float, float]]],
+#         optimizer_kwargs: Optional[Dict[str, Any]],
+#         use_jac: bool = False,
+#     ) -> Tuple[Callable, Dict]:
+#         """
+#         Parameters
+#         ----------
+#         param_bounds: dict
+#             See docstring of solve_steady_state for details
+#         optimizer_kwargs: dict
+#             See docstring of solve_steady_state for details
+#         use_jac: bool
+#             See docstring of solve_steady_state for details
+#
+#         Returns
+#         -------
+#         f_calib: callable
+#             A function that maps param_dict to values of calibrated parameteres
+#         additional_solutions: dict
+#             A dictionary of symbolic solutions to non-calibrating parameters that were solved en passant and can be
+#             reused later
+#         """
+#         calibrating_equations = self.calibrating_equations
+#         symbolic_solutions = self.steady_state_relationships.copy()
+#         free_param_dict = self.free_param_dict.copy()
+#         steady_state_system = self.steady_state_system
+#
+#         parameters = list(free_param_dict.keys())
+#         variables = list(self.steady_state_dict.keys())
+#         params_to_calibrate = [symbol_to_string(x) for x in self.params_to_calibrate]
+#         params_and_variables = parameters + params_to_calibrate + variables
+#
+#         unknown_variables = set(variables).union(set(params_to_calibrate)) - set(
+#             symbolic_solutions.keys()
+#         )
+#
+#         n_to_calibrate = len(params_to_calibrate)
+#
+#         additional_solutions = {}
+#
+#         # Make substitutions
+#         calib_with_user_solutions = substitute_all_equations(
+#             calibrating_equations, symbolic_solutions
+#         )
+#
+#         # Try the heuristic solver
+#         calib_solutions, solved_mask = self.heuristic_solver(
+#             {},
+#             calib_with_user_solutions,
+#             calib_with_user_solutions,
+#             [safe_string_to_sympy(x) for x in params_and_variables],
+#         )
+#
+#         # Case 1: We found something! Refine the solution.
+#         if solved_mask.sum() > 0:
+#             # If the heuristic solver worked, we got solutions for variables that will allow us to go back and solve for
+#             # the calibrating parameters.
+#
+#             sub_dict = merge_dictionaries(free_param_dict, calib_solutions)
+#             more_solutions, solved_mask = self.heuristic_solver(
+#                 sub_dict,
+#                 substitute_all_equations(steady_state_system, sub_dict),
+#                 steady_state_system,
+#                 [safe_string_to_sympy(x) for x in params_and_variables],
+#             )
+#
+#             calib_solutions = {
+#                 key: value
+#                 for key, value in more_solutions.items()
+#                 if (key in params_to_calibrate)
+#             }
+#
+#             # We potentially pick up additional solutions from this heuristic pass, we can save them and use them later
+#             # to help the heuristic solver later.
+#             additional_solutions = {
+#                 key: value
+#                 for key, value in more_solutions.items()
+#                 if (key not in params_to_calibrate) and (key not in free_param_dict)
+#             }
+#
+#             calib_solutions = SymbolDictionary(calib_solutions).to_string().sort_keys().values_to_float()
+#             f_calib = lambda *args, **kwargs: calib_solutions
+#
+#         # Case 2: Found nothing, try to use an optimizer
+#         else:
+#             # Here we check how many equations are remaining to solve after accounting for the user's SS info.
+#             # We're looking for the case when all information is given EXCEPT the calibrating parameters.
+#             # If there is more than that, we handle it in the final pass.
+#             calib_remaining_to_solve = list(
+#                 set(unknown_variables) - set(symbolic_solutions.keys())
+#             )
+#             calib_n_eqs = len(calib_remaining_to_solve)
+#             if calib_n_eqs > len(calibrating_equations):
+#
+#                 def f_calib(*args, **kwargs):
+#                     return SymbolDictionary()
+#
+#                 return f_calib, SymbolDictionary()
+#
+#             # TODO: Is there a more elegant way to handle one equation vs many equations here?
+#             if calib_n_eqs == 1:
+#                 calib_with_user_solutions = calib_with_user_solutions[0]
+#
+#                 _f_calib = sp.lambdify(
+#                     calib_remaining_to_solve + parameters, calib_with_user_solutions
+#                 )
+#
+#                 def f_calib(x, kwargs):
+#                     return _f_calib(x, **kwargs)
+#
+#             else:
+#                 _f_calib = sp.lambdify(
+#                     calib_remaining_to_solve + parameters, calib_with_user_solutions
+#                 )
+#
+#                 def f_calib(args, kwargs):
+#                     return _f_calib(*args, **kwargs)
+#
+#             f_jac = None
+#             if use_jac:
+#                 f_jac = self._build_jacobian(
+#                     diff_variables=calib_remaining_to_solve,
+#                     additional_inputs=parameters,
+#                     equations=calib_with_user_solutions,
+#                 )
+#
+#             f_calib = self._bundle_symbolic_solutions_with_optimizer_solutions(
+#                 unknowns=calib_remaining_to_solve,
+#                 f=f_calib,
+#                 f_jac=f_jac,
+#                 param_dict=free_param_dict,
+#                 symbolic_solutions=calib_solutions,
+#                 n_eqs=calib_n_eqs,
+#                 output_names=calib_remaining_to_solve,
+#                 param_bounds=param_bounds,
+#                 optimizer_kwargs=optimizer_kwargs,
+#             )
+#
+#         return f_calib, additional_solutions
+#
+#     def _gather_provided_solutions(self, solved_calib_params) -> Callable:
+#         """
+#         Returns
+#         -------
+#         f_provided: Callable
+#             A function that takes model parameters, both calibrated and otherwise, as keywork arguments, and returns
+#             a dictionary of variable values according to steady state equations supplied by the user
+#         """
+#
+#         free_param_dict = self.free_param_dict.copy()
+#         symbolic_solutions = self.steady_state_relationships.copy()
+#         parameters = list(free_param_dict.keys())
+#
+#         _provided_lambda = sp.lambdify(
+#             parameters + solved_calib_params, [eq for eq in symbolic_solutions.values()]
+#         )
+#
+#         def f_provided(param_dict, calib_dict):
+#             return SymbolDictionary(dict(
+#                 zip(
+#                     symbolic_solutions.keys(),
+#                     _provided_lambda(**param_dict, **calib_dict),
+#                 )
+#             ))
+#
+#         return f_provided
+#
+#     def _solve_remaining_equations(
+#         self,
+#         calib_dict: Dict[str, float],
+#         var_dict: Dict[str, float],
+#         additional_solutions: Dict[str, float],
+#         param_bounds: Optional[Dict[str, Tuple[float, float]]],
+#         optimizer_kwargs: Optional[Dict[str, Any]],
+#         use_jac: bool,
+#     ) -> Callable:
+#         """
+#         Parameters
+#         ----------
+#         calib_dict: Dict
+#             A dictionary of solved calibrating parameters, if any.
+#         var_dict: Dict
+#             A dictionary of user-provided steady-state relationships, if any.
+#         additional_solutions:
+#             A dictionary of variable solutions found en passant by the heuristic solver while solving for the
+#             calibrated parameters, if any.
+#         param_bounds:
+#             See docstring of solve_steady_state for details
+#         optimizer_kwargs:
+#             See docstring of solve_steady_state for details
+#         use_jac:
+#             See docstring of solve_steady_state for details
+#
+#         Returns
+#         -------
+#         f_final: Callable
+#             A function that takes model parameters as keyword arguments and returns steady-state values for each
+#             model variable without an explicit symbolic solution.
+#         """
+#         free_param_dict = self.free_param_dict
+#         steady_state_system = self.steady_state_system
+#         calibrating_equations = self.calibrating_equations
+#
+#         parameters = list(free_param_dict.keys())
+#         variables = list(self.steady_state_dict.keys())
+#         params_to_calibrate = [symbol_to_string(x) for x in self.params_to_calibrate]
+#
+#         sub_dict = merge_dictionaries(calib_dict, var_dict, additional_solutions)
+#         params_and_variables = parameters + params_to_calibrate + variables
+#
+#         ss_solutions, solved_mask = self.heuristic_solver(
+#             sub_dict,
+#             substitute_all_equations(
+#                 steady_state_system + calibrating_equations, sub_dict, free_param_dict
+#             ),
+#             steady_state_system + calibrating_equations,
+#             [safe_string_to_sympy(x) for x in params_and_variables],
+#         )
+#
+#         ss_solutions = {
+#             key: value
+#             for key, value in ss_solutions.items()
+#             if key not in calib_dict.keys()
+#         }
+#         sub_dict.update(ss_solutions)
+#
+#         ss_remaining_to_solve = sorted(
+#             list(
+#                 set(variables + params_to_calibrate)
+#                 - set(ss_solutions.keys())
+#                 - set(calib_dict.keys())
+#             )
+#         )
+#
+#         unsolved_eqs = substitute_all_equations(
+#             [
+#                 eq
+#                 for idx, eq in enumerate(steady_state_system + calibrating_equations)
+#                 if not solved_mask[idx]
+#             ],
+#             sub_dict,
+#         )
+#
+#         n_eqs = len(unsolved_eqs)
+#
+#         _f_unsolved_ss = sp.lambdify(ss_remaining_to_solve + parameters, unsolved_eqs)
+#
+#         def f_unsolved_ss(args, kwargs):
+#             return _f_unsolved_ss(*args, **kwargs)
+#
+#         f_jac = None
+#         if use_jac:
+#             f_jac = self._build_jacobian(
+#                 diff_variables=ss_remaining_to_solve,
+#                 additional_inputs=parameters,
+#                 equations=unsolved_eqs,
+#             )
+#
+#         f_final = self._bundle_symbolic_solutions_with_optimizer_solutions(
+#             unknowns=ss_remaining_to_solve,
+#             f=f_unsolved_ss,
+#             f_jac=f_jac,
+#             param_dict=free_param_dict,
+#             symbolic_solutions=ss_solutions,
+#             n_eqs=n_eqs,
+#             output_names=ss_remaining_to_solve,
+#             param_bounds=param_bounds,
+#             optimizer_kwargs=optimizer_kwargs,
+#         )
+#
+#         return f_final
+#
+#     def _create_final_function(self, final_f, f_calib, f_provided):
+#         """
+#
+#         Parameters
+#         ----------
+#         final_f: Callable
+#             Function generated by solve_remaining_equations
+#         f_calib: Callable
+#             Function generated by _solve_calibrating_equations
+#         f_provided: Callable
+#             Function generated by _gather_provided_solutions
+#
+#         Returns
+#         -------
+#         f_ss: Callable
+#             A single function wrapping the three steady state functions, that returns a complete solution to the
+#             model's steady state as two dictionaries: one with variable values, and one with calibrated parameter
+#             values.
+#         """
+#         calib_params = [x.name for x in self.params_to_calibrate]
+#         ss_vars = [x.to_ss().name for x in self.variables]
+#
+#         def combined_function(param_dict):
+#             ss_out = SymbolDictionary()
+#
+#             calib_dict = f_calib(param_dict).copy()
+#             var_dict = f_provided(param_dict, calib_dict).copy()
+#             final_dict = final_f(param_dict).copy()
+#
+#             for param in calib_params:
+#                 if param in final_dict.keys():
+#                     calib_dict[param] = final_dict[param]
+#                     del final_dict[param]
+#
+#             var_dict_final = {}
+#             for key in var_dict:
+#                 if key in ss_vars:
+#                     var_dict_final[key] = var_dict[key]
+#
+#             ss_out = ss_out | var_dict_final | final_dict
+#
+#             return ss_out.sort_keys(), calib_dict.sort_keys()
+#
+#         return combined_function
+#
+#     def _bundle_symbolic_solutions_with_optimizer_solutions(
+#         self,
+#         unknowns: List[str],
+#         f: Callable,
+#         f_jac: Optional[Callable],
+#         param_dict: Dict[str, float],
+#         symbolic_solutions: Optional[Dict[str, float]],
+#         n_eqs: int,
+#         output_names: List[str],
+#         param_bounds: Optional[Dict[str, Tuple[float, float]]],
+#         optimizer_kwargs: Optional[Dict[str, Any]],
+#     ) -> Callable:
+#
+#         parameters = list(param_dict.keys())
+#
+#         optimize_wrapper = partial(
+#             self._optimize_dispatcher,
+#             unknowns=unknowns,
+#             f=f,
+#             f_jac=f_jac,
+#             n_eqs=n_eqs,
+#             param_bounds=param_bounds,
+#             optimizer_kwargs=optimizer_kwargs,
+#         )
+#         _symbolic_lambda = sp.lambdify(parameters, list(symbolic_solutions.values()))
+#
+#         def solve_optimizer_variables(param_dict):
+#             return SymbolDictionary(dict(zip(output_names, optimize_wrapper(param_dict))))
+#
+#         def solve_symbolic_variables(param_dict):
+#             return SymbolDictionary(dict(zip(symbolic_solutions.keys(), _symbolic_lambda(**param_dict))))
+#
+#         wrapped_f = merge_functions(
+#             [solve_optimizer_variables, solve_symbolic_variables], param_dict
+#         )
+#
+#         return wrapped_f
+#
+#     def _optimize_dispatcher(
+#         self, param_dict, unknowns, f, f_jac, n_eqs, param_bounds, optimizer_kwargs
+#     ):
+#         if n_eqs == 1:
+#             optimize_fun = optimize.root_scalar
+#             if param_bounds is None:
+#                 param_bounds = self._prepare_param_bounds(None, 1)[0]
+#             optimizer_kwargs = self._prepare_optimizer_kwargs(optimizer_kwargs, n_eqs)
+#             optimizer_kwargs.update(
+#                 dict(args=param_dict, method="brentq", bracket=param_bounds)
+#             )
+#
+#         else:
+#             optimize_fun = optimize.root
+#
+#             optimizer_kwargs = self._prepare_optimizer_kwargs(optimizer_kwargs, n_eqs)
+#             optimizer_kwargs.update(dict(args=param_dict, jac=f_jac))
+#
+#         with catch_warnings():
+#             simplefilter("ignore")
+#             result = optimize_fun(f, **optimizer_kwargs)
+#
+#         if hasattr(result, "converged") and result.converged:
+#             return np.atleast_1d(result.root)
+#         elif hasattr(result, "converged") and not result.converged:
+#             raise ValueError(
+#                 f"Optimization failed while solving for steady state solution of the following "
+#                 f'variables: {", ".join([symbol_to_string(x) for x in unknowns])}\n\n {result}'
+#             )
+#
+#         if hasattr(result, "success") and result.success:
+#             return result.x
+#
+#         elif hasattr(result, "success") and not result.success:
+#             raise ValueError(
+#                 f"Optimization failed while solving for steady state solution of the following "
+#                 f'variables: {", ".join([symbol_to_string(x) for x in unknowns])}\n\n {result}'
+#             )
+#
+#     @staticmethod
+#     def _build_jacobian(
+#         diff_variables: List[Union[str, VariableType]],
+#         additional_inputs: List[Union[str, VariableType]],
+#         equations: List[sp.Add],
+#     ) -> Callable:
+#         """
+#         Parameters
+#         ----------
+#         diff_variables: list
+#             A list of variables, as either TimeAwareSymbols or strings that the equations will be differentiated with
+#             respect to.
+#         additional_inputs: list
+#             A list of variables or parameters that will be arguments to the Jacobian function, but that will NOT
+#             be used in differentiation (i.e. the model parameters)
+#         equations: list
+#             A list of equations to be differentiated
+#
+#         Returns
+#         -------
+#         f_jac: Callable
+#             A function that takes diff_variables + additional_inputs as keyword arguments and returns an
+#             len(equations) x len(diff_variables) matrix of derivatives.
+#         """
+#         equations = np.atleast_1d(equations)
+#         sp_variables = [safe_string_to_sympy(x) for x in diff_variables]
+#         _f_jac = sp.lambdify(
+#             diff_variables + additional_inputs,
+#             [[eq.diff(x) for x in sp_variables] for eq in equations],
+#         )
+#
+#         def f_jac(args, kwargs):
+#             return np.array(_f_jac(*args, **kwargs))
+#
+#         return f_jac
+#
+#     @staticmethod
+#     def _prepare_optimizer_kwargs(
+#         optimizer_kwargs: Optional[Dict[str, Any]], n_unknowns: int
+#     ) -> Dict[str, Any]:
+#         if optimizer_kwargs is None:
+#             optimizer_kwargs = {}
+#
+#         arg_names = list(optimizer_kwargs.keys())
+#         if "x0" not in arg_names:
+#             optimizer_kwargs["x0"] = np.full(n_unknowns, 0.8)
+#         if "method" not in arg_names:
+#             optimizer_kwargs["method"] = "hybr"
+#
+#         return optimizer_kwargs
+#
+#     @staticmethod
+#     def _prepare_param_bounds(
+#         param_bounds: Optional[List[Tuple[float, float]]], n_params
+#     ) -> List[Tuple[float, float]]:
+#         if param_bounds is None:
+#             bounds = [(1e-4, 0.999) for _ in range(n_params)]
+#         else:
+#             bounds = [(lower + 1e-4, upper - 1e-4) for lower, upper in param_bounds]
+#
+#         return bounds
+#
+#     def _get_n_unknowns_in_eq(self, eq: sp.Add) -> int:
+#         params_to_calibrate = (
+#             [] if self.params_to_calibrate is None else self.params_to_calibrate
+#         )
+#         unknown_atoms = [
+#             x for x in eq.atoms() if is_variable(x) or x in params_to_calibrate
+#         ]
+#         n_unknowns = len(list(set(unknown_atoms)))
+#
+#         return n_unknowns
+#
+#     def heuristic_solver(
+#         self,
+#         solution_dict: Dict[str, float],
+#         subbed_ss_system: List[Any],
+#         steady_state_system: List[Any],
+#         unknowns: List[str],
+#     ) -> Tuple[Dict[str, float], ArrayLike]:
+#         """
+#         Parameters
+#         ----------
+#         solution_dict: dict
+#             A dictionary of TimeAwareSymbol: float pairs, giving steady-state values that have already been determined
+#
+#         subbed_ss_system: list
+#             A list containing all unsolved steady state equations, pre-substituted with parameter values and known
+#             steady-state values.
+#
+#         steady_state_system: list
+#             A list containing all steady state equations, without substitution
+#
+#         unknowns: list
+#             A list of sympy variables containing unknown values to solve for; variables plus any unsolved calibrated
+#             parameters.
+#
+#         Returns
+#         -------
+#         It is likely that the GCN model will contain simple equations that amount to little more than parameters, for
+#         example declaring that P = 1 in a perfect competition setup. These types of simple expressions can be "solved"
+#         and removed from the system to reduce the dimensionality of the problem given to the numerical solver.
+#
+#         This function performs this simplification in a heuristic way in the following manner. We first look for
+#         "simple" equations, defined as those with only a single unknown variable. Solutions are then substituted back
+#         into the system, equations that have reduced to 0=0 as a result of substitution are removed, then we repeat
+#         the procedure to see if any additional equations have become heuristically solvable as a result of substitution.
+#
+#         The process terminates when no "simple" equations remain.
+#         """
+#
+#         solved_mask = np.array([eq == 0 for eq in subbed_ss_system])
+#         eq_to_var_dict = {}
+#         check_again_mask = np.full_like(solved_mask, True)
+#         solution_dict = sequential(
+#             solution_dict, [float_values_to_sympy_float, string_keys_to_sympy]
+#         )
+#
+#         numeric_solutions = solution_dict.copy()
+#
+#         while True:
+#             solution_dict = {
+#                 key: eq.subs(solution_dict) for key, eq in solution_dict.items()
+#             }
+#             subbed_ss_system = [
+#                 eq.subs(numeric_solutions).simplify() for eq in subbed_ss_system
+#             ]
+#
+#             n_unknowns = np.array(
+#                 [self._get_n_unknowns_in_eq(eq) for eq in subbed_ss_system]
+#             )
+#             eq_len = np.array([len(eq.atoms()) for eq in subbed_ss_system])
+#
+#             solvable_mask = (n_unknowns < 2) & (~solved_mask) & check_again_mask
+#
+#             # Sympy struggles with solving complicated functions inside powers, just avoid them. 5 is a magic number
+#             # for the maximum number of variable in a function to be considered "complicated", needs tuning.
+#             has_power_argument = np.array(
+#                 [
+#                     any([isinstance(arg, sp.core.power.Pow)] for arg in eq.args)
+#                     for eq in subbed_ss_system
+#                 ]
+#             )
+#             solvable_mask &= ~(has_power_argument & (eq_len > 5))
+#
+#             if sum(solvable_mask) == 0:
+#                 break
+#
+#             for idx in np.flatnonzero(solvable_mask):
+#                 # Putting the solved = True flag here is ugly, but it catches equations
+#                 # that are 0 = 0 after substitution
+#                 solved_mask[idx] = True
+#
+#                 eq = subbed_ss_system[idx]
+#
+#                 variables = list({x for x in eq.atoms() if x in unknowns})
+#                 if len(variables) > 0:
+#                     eq_to_var_dict[variables[0]] = idx
+#
+#                     try:
+#                         symbolic_solution = sp.solve(
+#                             steady_state_system[idx], variables[0]
+#                         )
+#                     except NotImplementedError:
+#                         # There are functional forms sympy can't handle;  mark the equation as unsolvable and continue.
+#                         check_again_mask[idx] = False
+#                         solved_mask[idx] = False
+#                         continue
+#
+#                     # The solution should only ever be length 0 or 1, if it's more than 1 something went wrong. Haven't
+#                     # hit this case yet in testing.
+#                     if len(symbolic_solution) == 1:
+#                         solution_dict[variables[0]] = symbolic_solution[0]
+#                         numeric_solutions[variables[0]] = (
+#                             symbolic_solution[0]
+#                             .subs(self.free_param_dict)
+#                             .subs(numeric_solutions)
+#                         )
+#                         check_again_mask[:] = True
+#                         solved_mask[idx] = True
+#
+#                     else:
+#                         # Solver failed; something went wrong. Skip this equation.
+#                         solved_mask[idx] = False
+#                         check_again_mask[idx] = False
+#
+#                 else:
+#                     check_again_mask[idx] = False
+#
+#         numeric_solutions = sympy_number_values_to_floats(numeric_solutions)
+#         for key, eq in numeric_solutions.items():
+#             if not isinstance(eq, float):
+#                 del solution_dict[key]
+#                 solved_mask[eq_to_var_dict[key]] = False
+#
+#         solution_dict = sequential(
+#             solution_dict, [sympy_keys_to_strings, sympy_number_values_to_floats]
+#         )
+#
+#         return solution_dict, solved_mask
```

### Comparing `gEconpy-1.1.0/gEconpy.egg-info/PKG-INFO` & `gEconpy-1.2.0/gEconpy.egg-info/PKG-INFO`

 * *Files 8% similar despite different names*

```diff
@@ -1,286 +1,282 @@
-Metadata-Version: 2.1
-Name: gEconpy
-Version: 1.1.0
-Summary: A package for solving, estimating, and analyzing DSGE models
-Home-page: https://github.com/jessegrabowski/gEcon.py
-Author: Jesse Grabowski
-Author-email: jessegrabowski@gmail.com
-License: UNKNOWN
-Platform: UNKNOWN
-Description-Content-Type: text/markdown
-License-File: LICENSE
-
-# gEconpy
-A collection of tools for working with DSGE models in python, inspired by the fantastic R package gEcon, http://gecon.r-forge.r-project.org/.
-
-Like gEcon, gEconpy solves first order conditions automatically, helping the researcher avoid math errors while facilitating rapid prototyping of models. By working in the optimization problem space rather than the FoC space, modifications to the model are much simpler. Adding an additional term to the utility function, for example, requires modifying only 2-3 lines of code, whereas in FoC space it may require re-solving the entire model by hand.
-
-gEconpy uses the GCN file originally created for the gEcon package. gEcon GCN files are fully compatable with gEconpy, and includes all the great features of GCN files, including:
-* Automatically solve first order conditions
-* Users can include steady-state values in equations without explictly solving for them by hand first!
-* Users can declare "calibrated parameters", requesting a parameter value be found to induce a specific steady-state relationship
-
-gEconpy is still in an unfinished alpha state, but I encourage anyone interested in DSGE modeling to give it a try and and report any bugs you might find.
-
-## Contributing:
-Contributions from anyone are welcome, regardless of previous experience. Please check the Issues tab for open issues, or to create a new issue. 
-
-# Representing a DSGE Model
-Like the R package gEcon, gEconpy uses .GCN files to represent a DSGE model. A GCN file is divided into blocks, each of which represents an optimization problem. Here is one block from the example Real Business Cycle (RBC) model included in the package.
-
-```
-block HOUSEHOLD
-{
-	definitions
-	{
-		u[] = C[] ^ (1 - sigma_C) / (1 - sigma_C) -
-		      L[] ^ (1 + sigma_L) / (1 + sigma_L);
-	};
-
-	controls
-	{
-		C[], L[], I[], K[];
-	};
-
-	objective
-	{
-		U[] = u[] + beta * E[][U[1]];
-	};
-
-	constraints
-	{
-		C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
-		K[] = (1 - delta) * K[-1] + I[] : q[];
-	};
-
-	calibration
-	{
-		# Fixed parameters
-		beta  = 0.99;
-		delta = 0.02;
-
-		# Parameters to estimate
-		sigma_C ~ N(loc=1.5, scale=0.1, lower=1.0) = 1.5;
-		sigma_L ~ N(loc=2.0, scale=0.1, lower=1.0) = 2.0;
-	};
-};
-```
-## Basic Basics
-A .GCN file uses an easy-to-read syntax. Whitespace is not meaningful - lines are terminated with a ";", so long equations can be split into multiple lines for readability. There are no reserved keywords* to avoid -- feel free to write beta instead of betta!
-
-Model variables are written with a name followed by square brackets, as in `U[]`. The square brackets give the time index the variable enters with. Following Dynare conventions, capital stock `K[-1]` enters with a lag, while all other variables enter in the present. Expectations are denoted by wrapping variables with `E[]`, as in `E[U[1]]` for expected utility at t+1. So I lied, there are a couple reserved keywords (hence the asterisk). Finally, you can refer directly to steady-state values as `K[ss]`.
-
-Parameters are written exactly as variables, except they have no square brackets `[]`.
-
-## Anatomy of a Block
-Blocks are divided into five components: `definitions`, `controls`, `objective`, `constraints`, `identities`, `shocks`, and, `calibration`. In this block, we see five of the seven. The blocks have the following functions:
-
-1. `definitions` contains equations that are **not** stored in the model. Instead, they are immediately substituted into all equations **within the same block**. In this example, a definition is used for the instantaneous utility function. It will be immediately substitutited into the Bellman equation written in the `objective` block.
-2. `controls` are the variables under the agent's control. The objective function represented by the block will be solved by forming a Lagrange function and taking derivatives with respect to the controls.
-3. The `objective` block contains only a single equation, and gives the function an agent will try to maximize over an infinite time horizon. In this case, the agent has a CRRA utility function.
-4. `constraints` give the resource constraints that the agent's maximization must respect. All constraints are given their own Lagrange multipiers.
-5. `identities` are equations that are not part of an optimization problem, but that are a part of the model. Unlike equations defined in the `definitions` block, `identities` are saved in the model's system of equations.
-6. `shocks` are where the user defines exogenous shocks, as in `varexo` in Dynare.
-7. The `calibration` block where free parameters, calibrated parameters, and parameter prior distributions are defined.
-
-## Parameter Values and Priors
-
-All parameters must be given values. In the household block above, all parameters are given a value directly. `beta` and `delta` are set fixed, while `sigma_C` and `sigma_L` are given priors and starting values. The `~` operator denotes a Prior, while `=` denotes a fixed value. All parameters must have a fixed value -- this is used as the "default" value when building and solving the model. Priors, on the other hand, are optional. At present, the user can choose from `Normal`, `HalfNormal`, `TruncatedNormal`, `Beta`, `Gamma`, `Inverse_Gamma`, and `Uniform` priors, with more to come as I improve the integration with PyMC. Distributons can be parameterized either using the `loc`, `scale`, `shape` synatx of `scipy.stats`, or directly using the common parameter values from the literature (such as a `mu` and `sigma` for a normal).
-
-
-As an alterantive to setting a parameter value directly, the user can declare a parameter to be calibrated. To do this, give a steady-state relationship that the parameter should be calibrated to ensure is true. The following GCN code block for the firm's optimization problem shows how this is done:
-
-```
-block FIRM
-{
-    controls
-    {
-        K[-1], L[];
-    };
-
-    objective
-    {
-        TC[] = -(r[] * K[-1] + w[] * L[]);
-    };
-
-    constraints
-    {
-        Y[] = A[] * K[-1] ^ alpha * L[] ^ (1 - alpha) : mc[];
-    };
-
-    identities
-    {
-        # Perfect competition
-        mc[] = 1;
-    };
-
-    calibration
-    {
-	L[ss] / K[ss] = 0.36 -> alpha;
-    };
-};
-```
-The `alpha` parameter is set so that in the steady state, the ratio of labor to capital is 0.36. On the back end, gEconpy will use an optimizer to find a value of `alpha` that satsifies the user's condition. Note that calibrated parameters cannot have prior distributions!
-
-## Lagrange Multipliers and First Order Conditions
-As mentioned, all constraints will automatically have a Lagrange multiplier assigned to them. The user name these multipliers himself by putting a colon ":" after an equation, followed by the Lagrange multipler name. From the code above:
-
-```
-C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
-K[] = (1 - delta) * K[-1] + I[] : q[];
-```
-
-The multiplier associated with the budget constraint has been given the name "lambda", as is usual in the literature, while the law of motion of capital has been given the name `q[]`. If the user wanted, she could use these variables in further computations within the block, for example `Q[] = q[] / lambda[]`, Tobin's Q, could be added in the `identities` block.
-
-Interally, first order conditions are solved by first making all substitutions from `definitions`, then forming the following Lagrangian function:
-`L = objective.RHS - lm1 * (control_1.LHS - control_1.RHS) - lm2 * (control_2.LHS - control_2.RHS) ... - lm_k * (control_k.LHS - control_k.RHS)`
-
-Next, the derivative of this Lagrangian is taken with respect to all control variables and all lagrange multipliers. Derivaties are are computed "though time" using `TimeAwareSymbols`, an extension of a normal Sympy symbol. For a control variable x, the total derivative over time is built up as `dL[]/dx[] + beta * dL[+1]/dx + beta * beta * dL[+2]/dx[] ...`. This unrolling terminates when `dL[+n]/dx[] = 0`.
-
-The result of this unrolling and taking derivatives process are the first order conditions (FoC). All model FoCs, along with objectives, constraints, and identities, are saved into the system of equations that represents the model.
-
-## Steady State
-
-After finding FoCs, the system will be ready to find a steady state and solve for a first-order linear approximation. To help the process, the user can write a `STEADY_STATE` block. This is a special reserved keyword block that can be placed anywhere in the GCN file. It should contain only `defintions` and `identities` as components. Here is an example of a steady state block for the RBC model:
-
-```
-block STEADY_STATE
-{
-    definitions
-    {
-      # If this is empty you can delete this, but it is also nice for writing common parameter or variable combinations.
-    };
-
-    identities
-    {
-        A[ss] = 1;
-        P[ss] = 1;
-        r[ss] = P[ss] * (1 / beta - (1 - delta));
-        w[ss] = (1 - alpha) * P[ss] ^ (1 / (1 - alpha)) * (alpha / r[ss]) ^ (alpha / (1 - alpha));
-        Y[ss] = (r[ss] / (r[ss] - delta * alpha)) ^ (sigma_C / (sigma_C + sigma_L)) *
-            (w[ss] / P[ss] * (w[ss] / P[ss] / (1 - alpha)) ^ sigma_L) ^ (1 / (sigma_C + sigma_L));
-
-        I[ss] = (delta * alpha / r[ss]) * Y[ss];
-        C[ss] = Y[ss] ^ (-sigma_L / sigma_C) * ((1 - alpha) ^ (-sigma_L) * (w[ss] / P[ss]) ^ (1 + sigma_L)) ^ (1 / sigma_C);
-        K[ss] = alpha * Y[ss] * P[ss] / r[ss];
-        L[ss] = (1 - alpha) * Y[ss] * P[ss] / w[ss];
-
-
-        U[ss] = (1 / (1 - beta)) * (C[ss] ^ (1 - sigma_C) / (1 - sigma_C) - L[ss] ^ (1 + sigma_L) / (1 + sigma_L));
-        lambda[ss] = C[ss] ^ (-sigma_C) / P[ss];
-        q[ss] = lambda[ss];
-        TC[ss] = -(r[ss] * K[ss] + w[ss] * L[ss]);
-    };
-};
-```
-
-It is not necessary to write an empty `definitions` component; this was done just to show where it goes. All information from the model block, including parameters and variables, are available to use in the `STEADY_STATE` block regardless of where they appear relative to each other (you can put the STEADY_STATE block at the top if you wish -- the file is not parsed top-to-bottom).
-
-Note that these equations are not checked in any way -- if you put something in the `STEADY_STATE` block, it is taken as the Word of God, and model solving proceeds from there. If you are having trouble finding a steady state, be sure to double check these equations.
-
-Finally, you **do not** have to provide the complete steady state system! You can include only equations, and the rest will be passed to an optimizer to be solved.
-
-## Solving the model
-
-Once a GCN file is written, using gEcon to do analysis is easy, as this code block shows:
-```python
-file_path = 'GCN Files/RBC_basic.gcn'
-model = gEconModel(file_path, verbose=True)
-```
-
-When the model is loaded, you will get a message about the number of equations and variables, as well as some other basic model descriptions. You can then solve for the stead state:
-```python
-model.steady_state()
->>> Steady state found! Sum of squared residuals is 2.9196536232567403e-19
-```
-
-And get the linearized state space representation
-
-```python
-model.solve_model()
->>>Solution found, sum of squared residuals:  7.075155451456433e-30
->>>Norm of deterministic part: 0.000000000
->>>Norm of stochastic part:    0.000000000
-```
-
-To see how to do simulations, IRFs, and compute moments, see the example notebook.
-
-# Other Features
-
-## Dynare Code Generation
-
-Since Dynare is still the gold standard in DSGE modeling, and this is a wacky open source package written by a literally who?, gEconpy has the ability to automatically convert a solved model into a Dynare mod file. This is done as follows:
-
-```python
-from gEconpy.shared.dynare_convert import make_mod_file
-print(make_mod_file(model))
-```
-
-Output:
-```
-var A, C, I, K, L, TC, U, Y, mc, q, r, var_lambda, w;
-varexo epsilon_A;
-
-parameters param_alpha, param_beta, param_delta, rho_A;
-parameters sigma_C, sigma_L;
-
-param_alpha = 0.35;
-param_beta = 0.99;
-param_delta = 0.02;
-rho_A = 0.95;
-sigma_C = 1.5;
-sigma_L = 2.0;
-
-model;
--C - I + K(-1) * r + L * w = 0;
-I - K + K(-1) *(1 - param_delta) = 0;
-C ^(1 - sigma_C) /(1 - sigma_C) - L ^(sigma_L + 1) /(sigma_L + 1) - U + U(1) * param_beta = 0;
--var_lambda + C ^(- sigma_C) = 0;
--L ^ sigma_L + var_lambda * w = 0;
-q - var_lambda = 0;
-param_beta *(q(1) *(1 - param_delta) + r(1) * var_lambda(1)) - q = 0;
-1 - mc = 0;
-A * K(-1) ^ param_alpha * L ^(1 - param_alpha) - Y = 0;
--K(-1) * r - L * w - TC = 0;
-A * K(-1) ^(param_alpha - 1) * L ^(1 - param_alpha) * param_alpha * mc - r = 0;
-A * K(-1) ^ param_alpha * mc *(1 - param_alpha) / L ^ param_alpha - w = 0;
-epsilon_A + rho_A * log(A(-1)) - log(A) = 0;
-end;
-
-initval;
-A = 1.0000;
-C = 2.3584;
-I = 0.7146;
-K = 35.7323;
-L = 0.8201;
-TC = -3.0731;
-U = -148.6156;
-Y = 3.0731;
-var_lambda = 0.2761;
-mc = 1.0000;
-q = 0.2761;
-r = 0.0301;
-w = 2.4358;
-end;
-
-steady;
-check(qz_zero_threshold=1e-20);
-
-shocks;
-var epsilon_A;
-stderr 0.01;
-end;
-
-stoch_simul(order=1, irf=100, qz_zero_threshold=1e-20);
-```
-
-### Warings about Dynare Code
-* No efforts are made to provide symbolic solutions to the steady-state! If you include steady state values in your equations and do not provide a symbolic solution in the STEADY_STATE block, the .mod file will not work "out of the box".
-* If your model includes calibrated equations, the generated Dynare code **will not** work out of the box. You need to analyically compute the steady state values and add a deterministic relationship (that beings with `#`) to the model block.
-
-
-## Estimation
-
-Coming soon!
-
-
+Metadata-Version: 2.1
+Name: gEconpy
+Version: 1.2.0
+Summary: A package for solving, estimating, and analyzing DSGE models
+Home-page: https://github.com/jessegrabowski/gEcon.py
+Author: Jesse Grabowski
+Author-email: jessegrabowski@gmail.com
+Description-Content-Type: text/markdown
+License-File: LICENSE
+
+# gEconpy
+A collection of tools for working with DSGE models in python, inspired by the fantastic R package gEcon, http://gecon.r-forge.r-project.org/.
+
+Like gEcon, gEconpy solves first order conditions automatically, helping the researcher avoid math errors while facilitating rapid prototyping of models. By working in the optimization problem space rather than the FoC space, modifications to the model are much simpler. Adding an additional term to the utility function, for example, requires modifying only 2-3 lines of code, whereas in FoC space it may require re-solving the entire model by hand.
+
+gEconpy uses the GCN file originally created for the gEcon package. gEcon GCN files are fully compatable with gEconpy, and includes all the great features of GCN files, including:
+* Automatically solve first order conditions
+* Users can include steady-state values in equations without explictly solving for them by hand first!
+* Users can declare "calibrated parameters", requesting a parameter value be found to induce a specific steady-state relationship
+
+gEconpy is still in an unfinished alpha state, but I encourage anyone interested in DSGE modeling to give it a try and and report any bugs you might find.
+
+## Contributing:
+Contributions from anyone are welcome, regardless of previous experience. Please check the Issues tab for open issues, or to create a new issue. 
+
+# Representing a DSGE Model
+Like the R package gEcon, gEconpy uses .GCN files to represent a DSGE model. A GCN file is divided into blocks, each of which represents an optimization problem. Here is one block from the example Real Business Cycle (RBC) model included in the package.
+
+```
+block HOUSEHOLD
+{
+	definitions
+	{
+		u[] = C[] ^ (1 - sigma_C) / (1 - sigma_C) -
+		      L[] ^ (1 + sigma_L) / (1 + sigma_L);
+	};
+
+	controls
+	{
+		C[], L[], I[], K[];
+	};
+
+	objective
+	{
+		U[] = u[] + beta * E[][U[1]];
+	};
+
+	constraints
+	{
+		C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
+		K[] = (1 - delta) * K[-1] + I[] : q[];
+	};
+
+	calibration
+	{
+		# Fixed parameters
+		beta  = 0.99;
+		delta = 0.02;
+
+		# Parameters to estimate
+		sigma_C ~ N(loc=1.5, scale=0.1, lower=1.0) = 1.5;
+		sigma_L ~ N(loc=2.0, scale=0.1, lower=1.0) = 2.0;
+	};
+};
+```
+## Basic Basics
+A .GCN file uses an easy-to-read syntax. Whitespace is not meaningful - lines are terminated with a ";", so long equations can be split into multiple lines for readability. There are no reserved keywords* to avoid -- feel free to write beta instead of betta!
+
+Model variables are written with a name followed by square brackets, as in `U[]`. The square brackets give the time index the variable enters with. Following Dynare conventions, capital stock `K[-1]` enters with a lag, while all other variables enter in the present. Expectations are denoted by wrapping variables with `E[]`, as in `E[U[1]]` for expected utility at t+1. So I lied, there are a couple reserved keywords (hence the asterisk). Finally, you can refer directly to steady-state values as `K[ss]`.
+
+Parameters are written exactly as variables, except they have no square brackets `[]`.
+
+## Anatomy of a Block
+Blocks are divided into five components: `definitions`, `controls`, `objective`, `constraints`, `identities`, `shocks`, and, `calibration`. In this block, we see five of the seven. The blocks have the following functions:
+
+1. `definitions` contains equations that are **not** stored in the model. Instead, they are immediately substituted into all equations **within the same block**. In this example, a definition is used for the instantaneous utility function. It will be immediately substitutited into the Bellman equation written in the `objective` block.
+2. `controls` are the variables under the agent's control. The objective function represented by the block will be solved by forming a Lagrange function and taking derivatives with respect to the controls.
+3. The `objective` block contains only a single equation, and gives the function an agent will try to maximize over an infinite time horizon. In this case, the agent has a CRRA utility function.
+4. `constraints` give the resource constraints that the agent's maximization must respect. All constraints are given their own Lagrange multipiers.
+5. `identities` are equations that are not part of an optimization problem, but that are a part of the model. Unlike equations defined in the `definitions` block, `identities` are saved in the model's system of equations.
+6. `shocks` are where the user defines exogenous shocks, as in `varexo` in Dynare.
+7. The `calibration` block where free parameters, calibrated parameters, and parameter prior distributions are defined.
+
+## Parameter Values and Priors
+
+All parameters must be given values. In the household block above, all parameters are given a value directly. `beta` and `delta` are set fixed, while `sigma_C` and `sigma_L` are given priors and starting values. The `~` operator denotes a Prior, while `=` denotes a fixed value. All parameters must have a fixed value -- this is used as the "default" value when building and solving the model. Priors, on the other hand, are optional. At present, the user can choose from `Normal`, `HalfNormal`, `TruncatedNormal`, `Beta`, `Gamma`, `Inverse_Gamma`, and `Uniform` priors, with more to come as I improve the integration with PyMC. Distributons can be parameterized either using the `loc`, `scale`, `shape` synatx of `scipy.stats`, or directly using the common parameter values from the literature (such as a `mu` and `sigma` for a normal).
+
+
+As an alterantive to setting a parameter value directly, the user can declare a parameter to be calibrated. To do this, give a steady-state relationship that the parameter should be calibrated to ensure is true. The following GCN code block for the firm's optimization problem shows how this is done:
+
+```
+block FIRM
+{
+    controls
+    {
+        K[-1], L[];
+    };
+
+    objective
+    {
+        TC[] = -(r[] * K[-1] + w[] * L[]);
+    };
+
+    constraints
+    {
+        Y[] = A[] * K[-1] ^ alpha * L[] ^ (1 - alpha) : mc[];
+    };
+
+    identities
+    {
+        # Perfect competition
+        mc[] = 1;
+    };
+
+    calibration
+    {
+	L[ss] / K[ss] = 0.36 -> alpha;
+    };
+};
+```
+The `alpha` parameter is set so that in the steady state, the ratio of labor to capital is 0.36. On the back end, gEconpy will use an optimizer to find a value of `alpha` that satsifies the user's condition. Note that calibrated parameters cannot have prior distributions!
+
+## Lagrange Multipliers and First Order Conditions
+As mentioned, all constraints will automatically have a Lagrange multiplier assigned to them. The user name these multipliers himself by putting a colon ":" after an equation, followed by the Lagrange multipler name. From the code above:
+
+```
+C[] + I[] = r[] * K[-1] + w[] * L[] : lambda[];
+K[] = (1 - delta) * K[-1] + I[] : q[];
+```
+
+The multiplier associated with the budget constraint has been given the name "lambda", as is usual in the literature, while the law of motion of capital has been given the name `q[]`. If the user wanted, she could use these variables in further computations within the block, for example `Q[] = q[] / lambda[]`, Tobin's Q, could be added in the `identities` block.
+
+Interally, first order conditions are solved by first making all substitutions from `definitions`, then forming the following Lagrangian function:
+`L = objective.RHS - lm1 * (control_1.LHS - control_1.RHS) - lm2 * (control_2.LHS - control_2.RHS) ... - lm_k * (control_k.LHS - control_k.RHS)`
+
+Next, the derivative of this Lagrangian is taken with respect to all control variables and all lagrange multipliers. Derivaties are are computed "though time" using `TimeAwareSymbols`, an extension of a normal Sympy symbol. For a control variable x, the total derivative over time is built up as `dL[]/dx[] + beta * dL[+1]/dx + beta * beta * dL[+2]/dx[] ...`. This unrolling terminates when `dL[+n]/dx[] = 0`.
+
+The result of this unrolling and taking derivatives process are the first order conditions (FoC). All model FoCs, along with objectives, constraints, and identities, are saved into the system of equations that represents the model.
+
+## Steady State
+
+After finding FoCs, the system will be ready to find a steady state and solve for a first-order linear approximation. To help the process, the user can write a `STEADY_STATE` block. This is a special reserved keyword block that can be placed anywhere in the GCN file. It should contain only `defintions` and `identities` as components. Here is an example of a steady state block for the RBC model:
+
+```
+block STEADY_STATE
+{
+    definitions
+    {
+      # If this is empty you can delete this, but it is also nice for writing common parameter or variable combinations.
+    };
+
+    identities
+    {
+        A[ss] = 1;
+        P[ss] = 1;
+        r[ss] = P[ss] * (1 / beta - (1 - delta));
+        w[ss] = (1 - alpha) * P[ss] ^ (1 / (1 - alpha)) * (alpha / r[ss]) ^ (alpha / (1 - alpha));
+        Y[ss] = (r[ss] / (r[ss] - delta * alpha)) ^ (sigma_C / (sigma_C + sigma_L)) *
+            (w[ss] / P[ss] * (w[ss] / P[ss] / (1 - alpha)) ^ sigma_L) ^ (1 / (sigma_C + sigma_L));
+
+        I[ss] = (delta * alpha / r[ss]) * Y[ss];
+        C[ss] = Y[ss] ^ (-sigma_L / sigma_C) * ((1 - alpha) ^ (-sigma_L) * (w[ss] / P[ss]) ^ (1 + sigma_L)) ^ (1 / sigma_C);
+        K[ss] = alpha * Y[ss] * P[ss] / r[ss];
+        L[ss] = (1 - alpha) * Y[ss] * P[ss] / w[ss];
+
+
+        U[ss] = (1 / (1 - beta)) * (C[ss] ^ (1 - sigma_C) / (1 - sigma_C) - L[ss] ^ (1 + sigma_L) / (1 + sigma_L));
+        lambda[ss] = C[ss] ^ (-sigma_C) / P[ss];
+        q[ss] = lambda[ss];
+        TC[ss] = -(r[ss] * K[ss] + w[ss] * L[ss]);
+    };
+};
+```
+
+It is not necessary to write an empty `definitions` component; this was done just to show where it goes. All information from the model block, including parameters and variables, are available to use in the `STEADY_STATE` block regardless of where they appear relative to each other (you can put the STEADY_STATE block at the top if you wish -- the file is not parsed top-to-bottom).
+
+Note that these equations are not checked in any way -- if you put something in the `STEADY_STATE` block, it is taken as the Word of God, and model solving proceeds from there. If you are having trouble finding a steady state, be sure to double check these equations.
+
+Finally, you **do not** have to provide the complete steady state system! You can include only equations, and the rest will be passed to an optimizer to be solved.
+
+## Solving the model
+
+Once a GCN file is written, using gEcon to do analysis is easy, as this code block shows:
+```python
+file_path = 'GCN Files/RBC_basic.gcn'
+model = gEconModel(file_path, verbose=True)
+```
+
+When the model is loaded, you will get a message about the number of equations and variables, as well as some other basic model descriptions. You can then solve for the stead state:
+```python
+model.steady_state()
+>>> Steady state found! Sum of squared residuals is 2.9196536232567403e-19
+```
+
+And get the linearized state space representation
+
+```python
+model.solve_model()
+>>>Solution found, sum of squared residuals:  7.075155451456433e-30
+>>>Norm of deterministic part: 0.000000000
+>>>Norm of stochastic part:    0.000000000
+```
+
+To see how to do simulations, IRFs, and compute moments, see the example notebook.
+
+# Other Features
+
+## Dynare Code Generation
+
+Since Dynare is still the gold standard in DSGE modeling, and this is a wacky open source package written by a literally who?, gEconpy has the ability to automatically convert a solved model into a Dynare mod file. This is done as follows:
+
+```python
+from gEconpy.shared.dynare_convert import make_mod_file
+print(make_mod_file(model))
+```
+
+Output:
+```
+var A, C, I, K, L, TC, U, Y, mc, q, r, var_lambda, w;
+varexo epsilon_A;
+
+parameters param_alpha, param_beta, param_delta, rho_A;
+parameters sigma_C, sigma_L;
+
+param_alpha = 0.35;
+param_beta = 0.99;
+param_delta = 0.02;
+rho_A = 0.95;
+sigma_C = 1.5;
+sigma_L = 2.0;
+
+model;
+-C - I + K(-1) * r + L * w = 0;
+I - K + K(-1) *(1 - param_delta) = 0;
+C ^(1 - sigma_C) /(1 - sigma_C) - L ^(sigma_L + 1) /(sigma_L + 1) - U + U(1) * param_beta = 0;
+-var_lambda + C ^(- sigma_C) = 0;
+-L ^ sigma_L + var_lambda * w = 0;
+q - var_lambda = 0;
+param_beta *(q(1) *(1 - param_delta) + r(1) * var_lambda(1)) - q = 0;
+1 - mc = 0;
+A * K(-1) ^ param_alpha * L ^(1 - param_alpha) - Y = 0;
+-K(-1) * r - L * w - TC = 0;
+A * K(-1) ^(param_alpha - 1) * L ^(1 - param_alpha) * param_alpha * mc - r = 0;
+A * K(-1) ^ param_alpha * mc *(1 - param_alpha) / L ^ param_alpha - w = 0;
+epsilon_A + rho_A * log(A(-1)) - log(A) = 0;
+end;
+
+initval;
+A = 1.0000;
+C = 2.3584;
+I = 0.7146;
+K = 35.7323;
+L = 0.8201;
+TC = -3.0731;
+U = -148.6156;
+Y = 3.0731;
+var_lambda = 0.2761;
+mc = 1.0000;
+q = 0.2761;
+r = 0.0301;
+w = 2.4358;
+end;
+
+steady;
+check(qz_zero_threshold=1e-20);
+
+shocks;
+var epsilon_A;
+stderr 0.01;
+end;
+
+stoch_simul(order=1, irf=100, qz_zero_threshold=1e-20);
+```
+
+### Warings about Dynare Code
+* No efforts are made to provide symbolic solutions to the steady-state! If you include steady state values in your equations and do not provide a symbolic solution in the STEADY_STATE block, the .mod file will not work "out of the box".
+* If your model includes calibrated equations, the generated Dynare code **will not** work out of the box. You need to analyically compute the steady state values and add a deterministic relationship (that beings with `#`) to the model block.
+
+
+## Estimation
+
+Coming soon!
```

### Comparing `gEconpy-1.1.0/gEconpy.egg-info/SOURCES.txt` & `gEconpy-1.2.0/gEconpy.egg-info/SOURCES.txt`

 * *Files 27% similar despite different names*

```diff
@@ -19,19 +19,19 @@
 gEconpy/estimation/__init__.py
 gEconpy/estimation/estimate.py
 gEconpy/estimation/estimation_utilities.py
 gEconpy/estimation/kalman_filter.py
 gEconpy/estimation/kalman_smoother.py
 gEconpy/exceptions/__init__.py
 gEconpy/exceptions/exceptions.py
-gEconpy/numba_linalg/LAPACK.py
-gEconpy/numba_linalg/__init__.py
-gEconpy/numba_linalg/intrinsics.py
-gEconpy/numba_linalg/overloads.py
-gEconpy/numba_linalg/utilities.py
+gEconpy/numba_tools/LAPACK.py
+gEconpy/numba_tools/__init__.py
+gEconpy/numba_tools/intrinsics.py
+gEconpy/numba_tools/overloads.py
+gEconpy/numba_tools/utilities.py
 gEconpy/parser/__init__.py
 gEconpy/parser/constants.py
 gEconpy/parser/file_loaders.py
 gEconpy/parser/gEcon_parser.py
 gEconpy/parser/parse_distributions.py
 gEconpy/parser/parse_equations.py
 gEconpy/parser/parse_plaintext.py
@@ -46,8 +46,18 @@
 gEconpy/shared/statsmodel_convert.py
 gEconpy/shared/typing.py
 gEconpy/shared/utilities.py
 gEconpy/solvers/__init__.py
 gEconpy/solvers/cycle_reduction.py
 gEconpy/solvers/gensys.py
 gEconpy/solvers/perturbation.py
-gEconpy/solvers/steady_state.py
+gEconpy/solvers/steady_state.py
+tests/test_block.py
+tests/test_containers.py
+tests/test_distribution_parser.py
+tests/test_estimation.py
+tests/test_gensys.py
+tests/test_kalman_filter.py
+tests/test_model.py
+tests/test_parser.py
+tests/test_steady_state.py
+tests/test_time_aware_symbols.py
```

