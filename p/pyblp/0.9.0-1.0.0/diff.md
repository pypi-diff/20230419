# Comparing `tmp/pyblp-0.9.0.tar.gz` & `tmp/pyblp-1.0.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist\pyblp-0.9.0.tar", last modified: Tue Mar  3 00:36:55 2020, max compression
+gzip compressed data, was "dist\pyblp-1.0.0.tar", last modified: Wed Apr 19 16:31:37 2023, max compression
```

## Comparing `pyblp-0.9.0.tar` & `pyblp-1.0.0.tar`

### file list

```diff
@@ -1,134 +1,141 @@
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:54.000000 pyblp-0.9.0/docs/
--rw-rw-rw-   0        0        0    10685 2020-02-10 21:14:03.000000 pyblp-0.9.0/docs/api.rst
--rw-rw-rw-   0        0        0    21473 2020-03-03 00:31:59.000000 pyblp-0.9.0/docs/background.rst
--rw-rw-rw-   0        0        0     6949 2020-01-05 19:32:28.000000 pyblp-0.9.0/docs/conf.py
--rw-rw-rw-   0        0        0      347 2019-12-27 22:41:53.000000 pyblp-0.9.0/docs/contributing.rst
--rw-rw-rw-   0        0        0      670 2019-12-27 22:41:53.000000 pyblp-0.9.0/docs/index.rst
--rw-rw-rw-   0        0        0      296 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/introduction.rst
--rw-rw-rw-   0        0        0       45 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/legal.rst
--rw-rw-rw-   0        0        0    12538 2020-01-07 21:19:33.000000 pyblp-0.9.0/docs/notation.rst
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:54.000000 pyblp-0.9.0/docs/notebooks/
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/docs/notebooks/api/
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/
--rw-rw-rw-   0        0        0    10559 2020-03-02 23:50:09.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_blp_instruments-checkpoint.ipynb
--rw-rw-rw-   0        0        0    11895 2020-03-02 23:50:13.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_differentiation_instruments-checkpoint.ipynb
--rw-rw-rw-   0        0        0     1878 2020-03-02 23:50:16.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_id_data-checkpoint.ipynb
--rw-rw-rw-   0        0        0     3219 2020-03-02 23:50:19.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_integration-checkpoint.ipynb
--rw-rw-rw-   0        0        0    12103 2020-03-02 23:50:21.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_matrix-checkpoint.ipynb
--rw-rw-rw-   0        0        0     5426 2020-03-02 23:50:24.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_ownership-checkpoint.ipynb
--rw-rw-rw-   0        0        0    14290 2020-03-02 23:50:27.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/data-checkpoint.ipynb
--rw-rw-rw-   0        0        0    20222 2020-03-03 00:16:51.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/data_to_dict-checkpoint.ipynb
--rw-rw-rw-   0        0        0     3509 2020-03-02 23:51:02.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/formulation-checkpoint.ipynb
--rw-rw-rw-   0        0        0     2465 2020-03-02 23:51:04.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/integration-checkpoint.ipynb
--rw-rw-rw-   0        0        0     3454 2020-03-02 23:51:06.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/iteration-checkpoint.ipynb
--rw-rw-rw-   0        0        0     3093 2020-03-02 23:51:08.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/optimization-checkpoint.ipynb
--rw-rw-rw-   0        0        0     3858 2020-03-02 23:51:10.000000 pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/parallel-checkpoint.ipynb
--rw-rw-rw-   0        0        0    10559 2020-03-02 23:50:09.000000 pyblp-0.9.0/docs/notebooks/api/build_blp_instruments.ipynb
--rw-rw-rw-   0        0        0    11895 2020-03-02 23:50:13.000000 pyblp-0.9.0/docs/notebooks/api/build_differentiation_instruments.ipynb
--rw-rw-rw-   0        0        0     1878 2020-03-02 23:50:16.000000 pyblp-0.9.0/docs/notebooks/api/build_id_data.ipynb
--rw-rw-rw-   0        0        0     3219 2020-03-02 23:50:19.000000 pyblp-0.9.0/docs/notebooks/api/build_integration.ipynb
--rw-rw-rw-   0        0        0    12103 2020-03-02 23:50:21.000000 pyblp-0.9.0/docs/notebooks/api/build_matrix.ipynb
--rw-rw-rw-   0        0        0     5426 2020-03-02 23:50:24.000000 pyblp-0.9.0/docs/notebooks/api/build_ownership.ipynb
--rw-rw-rw-   0        0        0    14290 2020-03-02 23:50:27.000000 pyblp-0.9.0/docs/notebooks/api/data.ipynb
--rw-rw-rw-   0        0        0    20222 2020-03-03 00:16:51.000000 pyblp-0.9.0/docs/notebooks/api/data_to_dict.ipynb
--rw-rw-rw-   0        0        0     3509 2020-03-02 23:51:02.000000 pyblp-0.9.0/docs/notebooks/api/formulation.ipynb
--rw-rw-rw-   0        0        0     2465 2020-03-02 23:51:04.000000 pyblp-0.9.0/docs/notebooks/api/integration.ipynb
--rw-rw-rw-   0        0        0     3454 2020-03-02 23:51:06.000000 pyblp-0.9.0/docs/notebooks/api/iteration.ipynb
--rw-rw-rw-   0        0        0     3093 2020-03-02 23:51:08.000000 pyblp-0.9.0/docs/notebooks/api/optimization.ipynb
--rw-rw-rw-   0        0        0     3858 2020-03-02 23:51:10.000000 pyblp-0.9.0/docs/notebooks/api/parallel.ipynb
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/docs/notebooks/tutorial/
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/docs/notebooks/tutorial/.ipynb_checkpoints/
--rw-rw-rw-   0        0        0    29543 2020-03-03 00:07:43.000000 pyblp-0.9.0/docs/notebooks/tutorial/.ipynb_checkpoints/blp-checkpoint.ipynb
--rw-rw-rw-   0        0        0    34303 2020-03-02 23:52:22.000000 pyblp-0.9.0/docs/notebooks/tutorial/.ipynb_checkpoints/logit_nested-checkpoint.ipynb
--rw-rw-rw-   0        0        0    56468 2020-03-03 00:16:57.000000 pyblp-0.9.0/docs/notebooks/tutorial/.ipynb_checkpoints/nevo-checkpoint.ipynb
--rw-rw-rw-   0        0        0   109069 2020-03-03 00:08:00.000000 pyblp-0.9.0/docs/notebooks/tutorial/.ipynb_checkpoints/post_estimation-checkpoint.ipynb
--rw-rw-rw-   0        0        0    13802 2020-03-03 00:08:05.000000 pyblp-0.9.0/docs/notebooks/tutorial/.ipynb_checkpoints/simulation-checkpoint.ipynb
--rw-rw-rw-   0        0        0    29543 2020-03-03 00:07:43.000000 pyblp-0.9.0/docs/notebooks/tutorial/blp.ipynb
--rw-rw-rw-   0        0        0    34303 2020-03-02 23:52:22.000000 pyblp-0.9.0/docs/notebooks/tutorial/logit_nested.ipynb
--rw-rw-rw-   0        0        0    56468 2020-03-03 00:16:57.000000 pyblp-0.9.0/docs/notebooks/tutorial/nevo.ipynb
--rw-rw-rw-   0        0        0   109069 2020-03-03 00:08:00.000000 pyblp-0.9.0/docs/notebooks/tutorial/post_estimation.ipynb
--rw-rw-rw-   0        0        0    13802 2020-03-03 00:08:05.000000 pyblp-0.9.0/docs/notebooks/tutorial/simulation.ipynb
--rw-rw-rw-   0        0        0     9959 2020-03-03 00:32:07.000000 pyblp-0.9.0/docs/references.rst
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/docs/static/
--rw-rw-rw-   0        0        0     1343 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/static/override.css
--rw-rw-rw-   0        0        0      245 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/static/override.js
--rw-rw-rw-   0        0        0      299 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/static/preamble.tex
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/docs/templates/
--rw-rw-rw-   0        0        0      101 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/templates/breadcrumbs.html
--rw-rw-rw-   0        0        0      103 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/templates/class_without_methods.rst
--rw-rw-rw-   0        0        0      105 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/templates/class_without_methods_or_signature.rst
--rw-rw-rw-   0        0        0      378 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/templates/class_without_signature.rst
--rw-rw-rw-   0        0        0      373 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/templates/class_with_signature.rst
--rw-rw-rw-   0        0        0       38 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/templates/nbsphinx_epilog.rst
--rw-rw-rw-   0        0        0      484 2019-07-31 00:23:18.000000 pyblp-0.9.0/docs/templates/nbsphinx_prolog.rst
--rw-rw-rw-   0        0        0     3447 2019-12-27 22:41:53.000000 pyblp-0.9.0/docs/testing.rst
--rw-rw-rw-   0        0        0      512 2019-12-27 22:41:53.000000 pyblp-0.9.0/docs/tutorial.rst
--rw-rw-rw-   0        0        0     3236 2020-03-03 00:32:27.000000 pyblp-0.9.0/docs/versions.rst
--rw-rw-rw-   0        0        0     1061 2019-07-31 00:23:18.000000 pyblp-0.9.0/LICENSE.txt
--rw-rw-rw-   0        0        0      238 2019-07-31 00:23:18.000000 pyblp-0.9.0/MANIFEST.in
--rw-rw-rw-   0        0        0     7252 2020-03-03 00:36:55.000000 pyblp-0.9.0/PKG-INFO
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/pyblp/
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/pyblp/configurations/
--rw-rw-rw-   0        0        0    27477 2019-12-29 15:25:34.000000 pyblp-0.9.0/pyblp/configurations/formulation.py
--rw-rw-rw-   0        0        0    46690 2020-03-03 00:31:45.000000 pyblp-0.9.0/pyblp/configurations/integration.py
--rw-rw-rw-   0        0        0    21930 2019-12-31 16:32:30.000000 pyblp-0.9.0/pyblp/configurations/iteration.py
--rw-rw-rw-   0        0        0    23108 2020-01-09 19:03:20.000000 pyblp-0.9.0/pyblp/configurations/optimization.py
--rw-rw-rw-   0        0        0       30 2019-07-31 00:23:18.000000 pyblp-0.9.0/pyblp/configurations/__init__.py
--rw-rw-rw-   0        0        0    24686 2020-03-03 00:12:52.000000 pyblp-0.9.0/pyblp/construction.py
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/pyblp/data/
--rw-rw-rw-   0        0        0   501476 2020-01-13 17:07:13.000000 pyblp-0.9.0/pyblp/data/blp_agents.csv
--rw-rw-rw-   0        0        0   765587 2020-01-11 19:52:04.000000 pyblp-0.9.0/pyblp/data/blp_products.csv
--rw-rw-rw-   0        0        0   327112 2019-07-31 00:23:18.000000 pyblp-0.9.0/pyblp/data/nevo_agents.csv
--rw-rw-rw-   0        0        0   642162 2019-07-31 00:23:18.000000 pyblp-0.9.0/pyblp/data/nevo_products.csv
--rw-rw-rw-   0        0        0     1966 2020-01-11 20:06:55.000000 pyblp-0.9.0/pyblp/data/__init__.py
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/pyblp/economies/
--rw-rw-rw-   0        0        0    15369 2020-01-25 00:12:41.000000 pyblp-0.9.0/pyblp/economies/economy.py
--rw-rw-rw-   0        0        0    88337 2020-03-03 00:31:36.000000 pyblp-0.9.0/pyblp/economies/problem.py
--rw-rw-rw-   0        0        0    45898 2020-01-25 14:31:45.000000 pyblp-0.9.0/pyblp/economies/simulation.py
--rw-rw-rw-   0        0        0       43 2019-07-31 00:23:18.000000 pyblp-0.9.0/pyblp/economies/__init__.py
--rw-rw-rw-   0        0        0    12185 2019-12-29 16:14:58.000000 pyblp-0.9.0/pyblp/exceptions.py
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/pyblp/markets/
--rw-rw-rw-   0        0        0    47753 2020-01-25 15:12:01.000000 pyblp-0.9.0/pyblp/markets/market.py
--rw-rw-rw-   0        0        0    11499 2019-12-29 15:44:46.000000 pyblp-0.9.0/pyblp/markets/problem_market.py
--rw-rw-rw-   0        0        0    14233 2020-02-10 21:32:09.000000 pyblp-0.9.0/pyblp/markets/results_market.py
--rw-rw-rw-   0        0        0     4287 2020-01-05 01:39:05.000000 pyblp-0.9.0/pyblp/markets/simulation_market.py
--rw-rw-rw-   0        0        0      652 2019-07-31 00:23:18.000000 pyblp-0.9.0/pyblp/markets/simulation_results_market.py
--rw-rw-rw-   0        0        0       52 2019-07-31 00:23:18.000000 pyblp-0.9.0/pyblp/markets/__init__.py
--rw-rw-rw-   0        0        0    10257 2019-12-29 15:54:21.000000 pyblp-0.9.0/pyblp/moments.py
--rw-rw-rw-   0        0        0     5348 2020-01-13 17:00:13.000000 pyblp-0.9.0/pyblp/options.py
--rw-rw-rw-   0        0        0    25822 2020-01-25 15:11:37.000000 pyblp-0.9.0/pyblp/parameters.py
--rw-rw-rw-   0        0        0    16345 2020-02-10 00:17:38.000000 pyblp-0.9.0/pyblp/primitives.py
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/pyblp/results/
--rw-rw-rw-   0        0        0    15325 2020-02-10 21:33:14.000000 pyblp-0.9.0/pyblp/results/bootstrapped_results.py
--rw-rw-rw-   0        0        0     6346 2020-02-10 00:21:10.000000 pyblp-0.9.0/pyblp/results/importance_sampling_results.py
--rw-rw-rw-   0        0        0    19146 2019-12-27 22:41:53.000000 pyblp-0.9.0/pyblp/results/optimal_instrument_results.py
--rw-rw-rw-   0        0        0    80162 2020-03-03 00:31:40.000000 pyblp-0.9.0/pyblp/results/problem_results.py
--rw-rw-rw-   0        0        0    38611 2020-02-10 21:36:35.000000 pyblp-0.9.0/pyblp/results/results.py
--rw-rw-rw-   0        0        0    15447 2020-01-25 00:15:42.000000 pyblp-0.9.0/pyblp/results/simulation_results.py
--rw-rw-rw-   0        0        0       35 2019-07-31 00:23:18.000000 pyblp-0.9.0/pyblp/results/__init__.py
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/pyblp/utilities/
--rw-rw-rw-   0        0        0     5363 2019-12-29 15:51:06.000000 pyblp-0.9.0/pyblp/utilities/algebra.py
--rw-rw-rw-   0        0        0    19903 2020-01-16 23:49:00.000000 pyblp-0.9.0/pyblp/utilities/basics.py
--rw-rw-rw-   0        0        0     5246 2020-01-25 14:40:30.000000 pyblp-0.9.0/pyblp/utilities/statistics.py
--rw-rw-rw-   0        0        0       30 2019-07-31 00:23:18.000000 pyblp-0.9.0/pyblp/utilities/__init__.py
--rw-rw-rw-   0        0        0       57 2020-03-02 23:48:58.000000 pyblp-0.9.0/pyblp/version.py
--rw-rw-rw-   0        0        0     1657 2019-12-27 22:41:53.000000 pyblp-0.9.0/pyblp/__init__.py
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/pyblp.egg-info/
--rw-rw-rw-   0        0        0        1 2020-03-03 00:36:54.000000 pyblp-0.9.0/pyblp.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0     7252 2020-03-03 00:36:54.000000 pyblp-0.9.0/pyblp.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0      187 2020-03-03 00:36:54.000000 pyblp-0.9.0/pyblp.egg-info/requires.txt
--rw-rw-rw-   0        0        0     4028 2020-03-03 00:36:54.000000 pyblp-0.9.0/pyblp.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0       12 2020-03-03 00:36:54.000000 pyblp-0.9.0/pyblp.egg-info/top_level.txt
--rw-rw-rw-   0        0        0     6145 2020-03-03 00:31:09.000000 pyblp-0.9.0/README.rst
--rw-rw-rw-   0        0        0       72 2019-12-27 22:41:53.000000 pyblp-0.9.0/requirements.txt
--rw-rw-rw-   0        0        0      277 2020-03-03 00:36:55.000000 pyblp-0.9.0/setup.cfg
--rw-rw-rw-   0        0        0     1769 2019-07-31 00:23:18.000000 pyblp-0.9.0/setup.py
-drwxrwxrwx   0        0        0        0 2020-03-03 00:36:55.000000 pyblp-0.9.0/tests/
--rw-rw-rw-   0        0        0    19857 2020-01-25 22:08:44.000000 pyblp-0.9.0/tests/conftest.py
--rw-rw-rw-   0        0        0    58802 2020-03-02 21:54:04.000000 pyblp-0.9.0/tests/test_blp.py
--rw-rw-rw-   0        0        0     9300 2019-07-31 00:23:18.000000 pyblp-0.9.0/tests/test_formulation.py
--rw-rw-rw-   0        0        0     5544 2020-01-16 00:21:49.000000 pyblp-0.9.0/tests/test_integration.py
--rw-rw-rw-   0        0        0     5038 2019-07-31 00:23:18.000000 pyblp-0.9.0/tests/test_iteration.py
--rw-rw-rw-   0        0        0     4066 2019-07-31 00:23:18.000000 pyblp-0.9.0/tests/test_optimization.py
--rw-rw-rw-   0        0        0       22 2019-07-31 00:23:18.000000 pyblp-0.9.0/tests/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/docs/
+-rw-rw-rw-   0        0        0    11617 2022-08-01 20:06:35.000000 pyblp-1.0.0/docs/api.rst
+-rw-rw-rw-   0        0        0    27034 2023-04-18 19:22:14.000000 pyblp-1.0.0/docs/background.rst
+-rw-rw-rw-   0        0        0     7077 2023-04-19 16:18:14.000000 pyblp-1.0.0/docs/conf.py
+-rw-rw-rw-   0        0        0      347 2023-01-25 21:32:43.000000 pyblp-1.0.0/docs/contributing.rst
+-rw-rw-rw-   0        0        0      670 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/index.rst
+-rw-rw-rw-   0        0        0       87 2023-04-18 19:16:57.000000 pyblp-1.0.0/docs/introduction.rst
+-rw-rw-rw-   0        0        0       45 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/legal.rst
+-rw-rw-rw-   0        0        0    12162 2022-06-08 19:22:02.000000 pyblp-1.0.0/docs/notation.rst
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/docs/notebooks/
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/docs/notebooks/api/
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/
+-rw-rw-rw-   0        0        0    31385 2023-04-18 19:29:13.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/build_blp_instruments-checkpoint.ipynb
+-rw-rw-rw-   0        0        0    29293 2023-04-18 19:30:07.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/build_differentiation_instruments-checkpoint.ipynb
+-rw-rw-rw-   0        0        0     1891 2023-04-18 19:30:10.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/build_id_data-checkpoint.ipynb
+-rw-rw-rw-   0        0        0     3232 2023-04-18 19:30:11.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/build_integration-checkpoint.ipynb
+-rw-rw-rw-   0        0        0    12117 2023-04-18 19:30:13.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/build_matrix-checkpoint.ipynb
+-rw-rw-rw-   0        0        0     5439 2023-04-18 19:30:15.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/build_ownership-checkpoint.ipynb
+-rw-rw-rw-   0        0        0    14521 2023-04-18 19:30:17.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/data-checkpoint.ipynb
+-rw-rw-rw-   0        0        0    20517 2023-04-18 19:30:19.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/data_to_dict-checkpoint.ipynb
+-rw-rw-rw-   0        0        0     3514 2023-04-18 19:30:20.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/formulation-checkpoint.ipynb
+-rw-rw-rw-   0        0        0     2478 2023-04-18 19:30:22.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/integration-checkpoint.ipynb
+-rw-rw-rw-   0        0        0     3467 2023-04-18 19:30:24.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/iteration-checkpoint.ipynb
+-rw-rw-rw-   0        0        0     4060 2023-01-12 23:16:23.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/micro_dataset-checkpoint.ipynb
+-rw-rw-rw-   0        0        0     3405 2023-01-12 23:16:20.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/micro_part-checkpoint.ipynb
+-rw-rw-rw-   0        0        0     3106 2023-04-18 19:30:26.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/optimization-checkpoint.ipynb
+-rw-rw-rw-   0        0        0     3926 2023-04-18 19:30:27.000000 pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/parallel-checkpoint.ipynb
+-rw-rw-rw-   0        0        0    31385 2023-04-18 19:29:13.000000 pyblp-1.0.0/docs/notebooks/api/build_blp_instruments.ipynb
+-rw-rw-rw-   0        0        0    29293 2023-04-18 19:30:07.000000 pyblp-1.0.0/docs/notebooks/api/build_differentiation_instruments.ipynb
+-rw-rw-rw-   0        0        0     1891 2023-04-18 19:30:10.000000 pyblp-1.0.0/docs/notebooks/api/build_id_data.ipynb
+-rw-rw-rw-   0        0        0     3232 2023-04-18 19:30:11.000000 pyblp-1.0.0/docs/notebooks/api/build_integration.ipynb
+-rw-rw-rw-   0        0        0    12117 2023-04-18 19:30:13.000000 pyblp-1.0.0/docs/notebooks/api/build_matrix.ipynb
+-rw-rw-rw-   0        0        0     5439 2023-04-18 19:30:15.000000 pyblp-1.0.0/docs/notebooks/api/build_ownership.ipynb
+-rw-rw-rw-   0        0        0    14521 2023-04-18 19:30:17.000000 pyblp-1.0.0/docs/notebooks/api/data.ipynb
+-rw-rw-rw-   0        0        0    20517 2023-04-18 19:30:19.000000 pyblp-1.0.0/docs/notebooks/api/data_to_dict.ipynb
+-rw-rw-rw-   0        0        0     3514 2023-04-18 19:30:20.000000 pyblp-1.0.0/docs/notebooks/api/formulation.ipynb
+-rw-rw-rw-   0        0        0     2478 2023-04-18 19:30:22.000000 pyblp-1.0.0/docs/notebooks/api/integration.ipynb
+-rw-rw-rw-   0        0        0     3467 2023-04-18 19:30:24.000000 pyblp-1.0.0/docs/notebooks/api/iteration.ipynb
+-rw-rw-rw-   0        0        0     3106 2023-04-18 19:30:26.000000 pyblp-1.0.0/docs/notebooks/api/optimization.ipynb
+-rw-rw-rw-   0        0        0     3926 2023-04-18 19:30:27.000000 pyblp-1.0.0/docs/notebooks/api/parallel.ipynb
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/docs/notebooks/tutorial/
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/
+-rw-rw-rw-   0        0        0    29637 2023-04-18 19:34:47.000000 pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/blp-checkpoint.ipynb
+-rw-rw-rw-   0        0        0    34724 2023-04-18 19:31:05.000000 pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/logit_nested-checkpoint.ipynb
+-rw-rw-rw-   0        0        0    59665 2023-04-18 19:48:10.000000 pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/nevo-checkpoint.ipynb
+-rw-rw-rw-   0        0        0    59766 2023-04-18 20:05:52.000000 pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/petrin-checkpoint.ipynb
+-rw-rw-rw-   0        0        0   199510 2023-04-18 19:35:02.000000 pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/post_estimation-checkpoint.ipynb
+-rw-rw-rw-   0        0        0    14403 2023-04-18 19:32:11.000000 pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/simulation-checkpoint.ipynb
+-rw-rw-rw-   0        0        0    29637 2023-04-18 19:34:47.000000 pyblp-1.0.0/docs/notebooks/tutorial/blp.ipynb
+-rw-rw-rw-   0        0        0    34724 2023-04-18 19:31:05.000000 pyblp-1.0.0/docs/notebooks/tutorial/logit_nested.ipynb
+-rw-rw-rw-   0        0        0    59665 2023-04-18 19:48:10.000000 pyblp-1.0.0/docs/notebooks/tutorial/nevo.ipynb
+-rw-rw-rw-   0        0        0    59766 2023-04-18 20:05:52.000000 pyblp-1.0.0/docs/notebooks/tutorial/petrin.ipynb
+-rw-rw-rw-   0        0        0   199510 2023-04-18 19:35:02.000000 pyblp-1.0.0/docs/notebooks/tutorial/post_estimation.ipynb
+-rw-rw-rw-   0        0        0    14403 2023-04-18 19:32:11.000000 pyblp-1.0.0/docs/notebooks/tutorial/simulation.ipynb
+-rw-rw-rw-   0        0        0    11051 2023-04-18 19:21:19.000000 pyblp-1.0.0/docs/references.rst
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/docs/static/
+-rw-rw-rw-   0        0        0     1343 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/static/override.css
+-rw-rw-rw-   0        0        0      245 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/static/override.js
+-rw-rw-rw-   0        0        0      299 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/static/preamble.tex
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/docs/templates/
+-rw-rw-rw-   0        0        0      101 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/templates/breadcrumbs.html
+-rw-rw-rw-   0        0        0      103 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/templates/class_without_methods.rst
+-rw-rw-rw-   0        0        0      105 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/templates/class_without_methods_or_signature.rst
+-rw-rw-rw-   0        0        0      378 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/templates/class_without_signature.rst
+-rw-rw-rw-   0        0        0      373 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/templates/class_with_signature.rst
+-rw-rw-rw-   0        0        0       38 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/templates/nbsphinx_epilog.rst
+-rw-rw-rw-   0        0        0      484 2021-06-11 17:07:44.000000 pyblp-1.0.0/docs/templates/nbsphinx_prolog.rst
+-rw-rw-rw-   0        0        0     3613 2023-04-19 16:19:47.000000 pyblp-1.0.0/docs/testing.rst
+-rw-rw-rw-   0        0        0      549 2023-01-13 01:37:52.000000 pyblp-1.0.0/docs/tutorial.rst
+-rw-rw-rw-   0        0        0     4489 2023-04-18 19:28:39.000000 pyblp-1.0.0/docs/versions.rst
+-rw-rw-rw-   0        0        0     1084 2021-06-11 17:07:44.000000 pyblp-1.0.0/LICENSE.txt
+-rw-rw-rw-   0        0        0      238 2021-06-11 17:07:44.000000 pyblp-1.0.0/MANIFEST.in
+-rw-rw-rw-   0        0        0     7769 2023-04-19 16:31:37.000000 pyblp-1.0.0/PKG-INFO
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp/
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp/configurations/
+-rw-rw-rw-   0        0        0    28826 2023-03-21 22:25:06.000000 pyblp-1.0.0/pyblp/configurations/formulation.py
+-rw-rw-rw-   0        0        0    46683 2021-11-13 19:48:09.000000 pyblp-1.0.0/pyblp/configurations/integration.py
+-rw-rw-rw-   0        0        0    22392 2022-05-21 13:14:05.000000 pyblp-1.0.0/pyblp/configurations/iteration.py
+-rw-rw-rw-   0        0        0    23921 2022-08-15 13:04:07.000000 pyblp-1.0.0/pyblp/configurations/optimization.py
+-rw-rw-rw-   0        0        0       30 2021-06-11 17:07:44.000000 pyblp-1.0.0/pyblp/configurations/__init__.py
+-rw-rw-rw-   0        0        0    25523 2023-04-10 22:11:10.000000 pyblp-1.0.0/pyblp/construction.py
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp/data/
+-rw-rw-rw-   0        0        0   501476 2021-06-11 17:07:44.000000 pyblp-1.0.0/pyblp/data/blp_agents.csv
+-rw-rw-rw-   0        0        0   765587 2021-06-11 17:07:44.000000 pyblp-1.0.0/pyblp/data/blp_products.csv
+-rw-rw-rw-   0        0        0   327112 2021-06-11 17:07:44.000000 pyblp-1.0.0/pyblp/data/nevo_agents.csv
+-rw-rw-rw-   0        0        0   642162 2021-06-11 17:07:44.000000 pyblp-1.0.0/pyblp/data/nevo_products.csv
+-rw-rw-rw-   0        0        0  2203389 2022-07-26 21:45:21.000000 pyblp-1.0.0/pyblp/data/petrin_agents.csv
+-rw-rw-rw-   0        0        0     2376 2022-05-16 13:19:23.000000 pyblp-1.0.0/pyblp/data/petrin_covariances.csv
+-rw-rw-rw-   0        0        0  1078568 2022-07-26 21:40:51.000000 pyblp-1.0.0/pyblp/data/petrin_products.csv
+-rw-rw-rw-   0        0        0      193 2022-07-26 22:03:23.000000 pyblp-1.0.0/pyblp/data/petrin_values.csv
+-rw-rw-rw-   0        0        0     4000 2022-07-27 18:25:54.000000 pyblp-1.0.0/pyblp/data/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp/economies/
+-rw-rw-rw-   0        0        0    18874 2023-02-24 21:29:19.000000 pyblp-1.0.0/pyblp/economies/economy.py
+-rw-rw-rw-   0        0        0   112388 2023-02-24 21:22:13.000000 pyblp-1.0.0/pyblp/economies/problem.py
+-rw-rw-rw-   0        0        0    53947 2023-02-24 21:22:29.000000 pyblp-1.0.0/pyblp/economies/simulation.py
+-rw-rw-rw-   0        0        0       43 2021-06-11 17:07:44.000000 pyblp-1.0.0/pyblp/economies/__init__.py
+-rw-rw-rw-   0        0        0    12954 2022-12-16 21:01:23.000000 pyblp-1.0.0/pyblp/exceptions.py
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp/markets/
+-rw-rw-rw-   0        0        0    27785 2023-04-10 22:16:27.000000 pyblp-1.0.0/pyblp/markets/economy_results_market.py
+-rw-rw-rw-   0        0        0    98930 2023-04-10 22:16:00.000000 pyblp-1.0.0/pyblp/markets/market.py
+-rw-rw-rw-   0        0        0    12282 2022-12-16 21:01:23.000000 pyblp-1.0.0/pyblp/markets/problem_market.py
+-rw-rw-rw-   0        0        0     6334 2022-12-13 19:38:21.000000 pyblp-1.0.0/pyblp/markets/simulation_market.py
+-rw-rw-rw-   0        0        0       52 2021-06-11 17:07:44.000000 pyblp-1.0.0/pyblp/markets/__init__.py
+-rw-rw-rw-   0        0        0    23057 2023-04-18 19:23:42.000000 pyblp-1.0.0/pyblp/micro.py
+-rw-rw-rw-   0        0        0     9356 2023-03-10 23:34:55.000000 pyblp-1.0.0/pyblp/options.py
+-rw-rw-rw-   0        0        0    29455 2022-12-13 19:38:21.000000 pyblp-1.0.0/pyblp/parameters.py
+-rw-rw-rw-   0        0        0    28301 2023-03-21 22:21:26.000000 pyblp-1.0.0/pyblp/primitives.py
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp/results/
+-rw-rw-rw-   0        0        0    16753 2022-06-18 19:40:25.000000 pyblp-1.0.0/pyblp/results/bootstrapped_results.py
+-rw-rw-rw-   0        0        0    90409 2023-02-24 21:32:29.000000 pyblp-1.0.0/pyblp/results/economy_results.py
+-rw-rw-rw-   0        0        0     6637 2023-04-10 22:26:20.000000 pyblp-1.0.0/pyblp/results/importance_sampling_results.py
+-rw-rw-rw-   0        0        0    22114 2022-12-13 19:38:21.000000 pyblp-1.0.0/pyblp/results/optimal_instrument_results.py
+-rw-rw-rw-   0        0        0    79894 2023-04-10 22:21:46.000000 pyblp-1.0.0/pyblp/results/problem_results.py
+-rw-rw-rw-   0        0        0    19108 2022-06-19 20:46:43.000000 pyblp-1.0.0/pyblp/results/simulation_results.py
+-rw-rw-rw-   0        0        0       35 2021-06-11 17:07:44.000000 pyblp-1.0.0/pyblp/results/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp/utilities/
+-rw-rw-rw-   0        0        0     8071 2022-05-20 13:22:47.000000 pyblp-1.0.0/pyblp/utilities/algebra.py
+-rw-rw-rw-   0        0        0    24837 2023-04-11 17:10:01.000000 pyblp-1.0.0/pyblp/utilities/basics.py
+-rw-rw-rw-   0        0        0     6021 2021-06-11 17:07:44.000000 pyblp-1.0.0/pyblp/utilities/statistics.py
+-rw-rw-rw-   0        0        0       30 2021-06-11 17:07:44.000000 pyblp-1.0.0/pyblp/utilities/__init__.py
+-rw-rw-rw-   0        0        0       57 2023-04-18 19:23:55.000000 pyblp-1.0.0/pyblp/version.py
+-rw-rw-rw-   0        0        0     1738 2022-08-01 20:05:54.000000 pyblp-1.0.0/pyblp/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp.egg-info/
+-rw-rw-rw-   0        0        0        1 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0     7769 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0      248 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp.egg-info/requires.txt
+-rw-rw-rw-   0        0        0     4361 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0       12 2023-04-19 16:31:37.000000 pyblp-1.0.0/pyblp.egg-info/top_level.txt
+-rw-rw-rw-   0        0        0     7490 2023-04-19 16:15:23.000000 pyblp-1.0.0/README.rst
+-rw-rw-rw-   0        0        0       72 2021-06-11 17:07:44.000000 pyblp-1.0.0/requirements.txt
+-rw-rw-rw-   0        0        0      283 2023-04-19 16:31:37.000000 pyblp-1.0.0/setup.cfg
+-rw-rw-rw-   0        0        0     1976 2022-06-07 14:45:18.000000 pyblp-1.0.0/setup.py
+drwxrwxrwx   0        0        0        0 2023-04-19 16:31:37.000000 pyblp-1.0.0/tests/
+-rw-rw-rw-   0        0        0    32845 2023-04-18 20:38:33.000000 pyblp-1.0.0/tests/conftest.py
+-rw-rw-rw-   0        0        0    90359 2023-04-18 20:35:25.000000 pyblp-1.0.0/tests/test_blp.py
+-rw-rw-rw-   0        0        0     9323 2021-11-10 16:56:28.000000 pyblp-1.0.0/tests/test_formulation.py
+-rw-rw-rw-   0        0        0     5544 2021-06-11 17:07:44.000000 pyblp-1.0.0/tests/test_integration.py
+-rw-rw-rw-   0        0        0     5420 2022-05-21 13:16:47.000000 pyblp-1.0.0/tests/test_iteration.py
+-rw-rw-rw-   0        0        0     4154 2022-08-15 13:04:30.000000 pyblp-1.0.0/tests/test_optimization.py
+-rw-rw-rw-   0        0        0       22 2021-06-11 17:07:44.000000 pyblp-1.0.0/tests/__init__.py
```

### Comparing `pyblp-0.9.0/docs/api.rst` & `pyblp-1.0.0/docs/api.rst`

 * *Files 4% similar despite different names*

```diff
@@ -18,29 +18,31 @@
 
    Formulation
    Integration
    Iteration
    Optimization
 
 
-Data Construction Functions
+Data Manipulation Functions
 ---------------------------
 
-There are also a number of convenience functions that can be used to construct common components of product and agent data.
+There are also a number of convenience functions that can be used to construct common components of product and agent data, or manipulate other PyBLP objects.
 
 .. autosummary::
    :toctree: _api
 
    build_matrix
    build_blp_instruments
    build_differentiation_instruments
    build_id_data
    build_ownership
    build_integration
    data_to_dict
+   save_pickle
+   read_pickle
 
 
 Problem Class
 -------------
 
 Given data and appropriate configurations, a BLP-type problem can be structured by initializing the following class.
 
@@ -57,75 +59,82 @@
 
    Problem.solve
 
 
 Micro Moment Classes
 --------------------
 
-Micro moment configurations can be passed to :meth:`Problem.solve`. Only one type of micro moment is currently supported.
+Micro dataset configurations are passed to micro part configurations, which are passed to micro moment configurations, which in turn can be passed to :meth:`Problem.solve`.
 
 .. autosummary::
    :toctree: _api
    :template: class_with_signature.rst
 
-   FirstChoiceCovarianceMoment
+   MicroDataset
+   MicroPart
+   MicroMoment
 
 
 Problem Results Class
 ---------------------
 
 Solved problems return the following results class.
 
 .. autosummary::
    :nosignatures:
    :toctree: _api
    :template: class_without_signature.rst
 
    ProblemResults
 
-The results can be converted into a dictionary.
+The results can be pickled or converted into a dictionary.
 
 .. autosummary::
    :toctree: _api
 
+   ProblemResults.to_pickle
    ProblemResults.to_dict
 
 The following methods test the validity of overidentifying and model restrictions.
 
 .. autosummary::
    :toctree: _api
 
    ProblemResults.run_hansen_test
    ProblemResults.run_distance_test
    ProblemResults.run_lm_test
    ProblemResults.run_wald_test
 
-In addition to class attributes, other post-estimation outputs can be estimated market-by-market with the following methods, which each return an array.
+In addition to class attributes, other post-estimation outputs can be estimated market-by-market with the following methods, each of which return an array.
 
 .. autosummary::
    :toctree: _api
 
    ProblemResults.compute_aggregate_elasticities
    ProblemResults.compute_elasticities
+   ProblemResults.compute_demand_jacobians
+   ProblemResults.compute_demand_hessians
+   ProblemResults.compute_profit_hessians
    ProblemResults.compute_diversion_ratios
    ProblemResults.compute_long_run_diversion_ratios
    ProblemResults.compute_probabilities
    ProblemResults.extract_diagonals
    ProblemResults.extract_diagonal_means
    ProblemResults.compute_delta
    ProblemResults.compute_costs
+   ProblemResults.compute_passthrough
    ProblemResults.compute_approximate_prices
    ProblemResults.compute_prices
    ProblemResults.compute_shares
    ProblemResults.compute_hhi
    ProblemResults.compute_markups
    ProblemResults.compute_profits
    ProblemResults.compute_consumer_surpluses
 
-A parametric bootstrap can be used, for example, to compute standard errors for the above post-estimation outputs. The following method returns a results class with all of the above methods, which returns a distribution of post-estimation outputs corresponding to different bootstrapped samples.
+A parametric bootstrap can be used, for example, to compute standard errors forpost-estimation outputs. The following method returns a results class with the same methods in the list directly above, which returns a distribution of post-estimation outputs corresponding to different bootstrapped samples.
 
 .. autosummary::
    :toctree: _api
 
    ProblemResults.bootstrap
 
 Optimal instruments, which also return a results class instead of an array, can be estimated with the following method.
@@ -138,32 +147,43 @@
 Importance sampling can be used to create new integration nodes and weights. Its method also returns a results class.
 
 .. autosummary::
    :toctree: _api
 
    ProblemResults.importance_sampling
 
+The following methods can compute micro moment values, compute scores from micro data, or simulate such data.
 
-Boostrapped Problem Results Class
----------------------------------
+.. autosummary::
+   :toctree: _api
+
+   ProblemResults.compute_micro_values
+   ProblemResults.compute_micro_scores
+   ProblemResults.compute_agent_scores
+   ProblemResults.simulate_micro_data
+
+
+Bootstrapped Problem Results Class
+----------------------------------
 
 Parametric bootstrap computation returns the following class.
 
 .. autosummary::
    :nosignatures:
    :toctree: _api
    :template: class_without_methods_or_signature.rst
 
    BootstrappedResults
 
-This class has all of the same methods as :class:`ProblemResults`, except for :meth:`ProblemResults.bootstrap`, :meth:`ProblemResults.compute_optimal_instruments`, and :meth:`ProblemResults.importance_sampling`. It can also be converted into a dictionary.
+This class has many of the same methods as :meth:`ProblemResults`. It can also be pickled or converted into a dictionary.
 
 .. autosummary::
    :toctree: _api
 
+   BootstrappedResults.to_pickle
    BootstrappedResults.to_dict
 
 
 Optimal Instrument Results Class
 --------------------------------
 
 Optimal instrument computation returns the following results class.
@@ -171,19 +191,20 @@
 .. autosummary::
    :nosignatures:
    :toctree: _api
    :template: class_without_signature.rst
 
    OptimalInstrumentResults
 
-The results can be converted into a dictionary.
+The results can be pickled or converted into a dictionary.
 
 .. autosummary::
    :toctree: _api
 
+   OptimalInstrumentResults.to_pickle
    OptimalInstrumentResults.to_dict
 
 They can also be converted into a :class:`Problem` with the following method.
 
 .. autosummary::
    :toctree: _api
 
@@ -207,19 +228,20 @@
 .. autosummary::
    :nosignatures:
    :toctree: _api
    :template: class_without_signature.rst
 
    ImportanceSamplingResults
 
-The results can be converted into a dictionary.
+The results can be pickled or converted into a dictionary.
 
 .. autosummary::
    :toctree: _api
 
+   ImportanceSamplingResults.to_pickle
    ImportanceSamplingResults.to_dict
 
 They can also be converted into a :class:`Problem` with the following method.
 
 .. autosummary::
    :toctree: _api
 
@@ -265,39 +287,33 @@
 ------------------------
 
 Solved simulations return the following results class.
 
 .. autosummary::
    :nosignatures:
    :toctree: _api
-   :template: class_without_signature.rst
+   :template: class_without_methods_or_signature.rst
 
    SimulationResults
 
-The results can be converted into a dictionary.
+This class has many of the same methods as :class:`ProblemResults`. It can also be pickled or converted into a dictionary.
 
 .. autosummary::
    :toctree: _api
 
+   SimulationResults.to_pickle
    SimulationResults.to_dict
 
-They can also be converted into a :class:`Problem` with the following method.
+It can also be converted into a :class:`Problem` with the following method.
 
 .. autosummary::
    :toctree: _api
 
    SimulationResults.to_problem
 
-Simulation results can also be used to compute micro moment values.
-
-.. autosummary::
-   :toctree: _api
-
-   SimulationResults.compute_micro
-
 
 Structured Data Classes
 -----------------------
 
 Product and agent data that are passed or constructed by :class:`Problem` and :class:`Simulation` are structured internally into classes with field names that more closely resemble BLP notation. Although these structured data classes are not directly constructable, they can be accessed with :class:`Problem` and :class:`Simulation` class attributes. It can be helpful to compare these structured data classes with the data or configurations used to create them.
 
 .. autosummary::
@@ -343,42 +359,49 @@
    :template: class_without_signature.rst
 
    exceptions.MultipleErrors
    exceptions.NonpositiveCostsError
    exceptions.NonpositiveSyntheticCostsError
    exceptions.InvalidParameterCovariancesError
    exceptions.InvalidMomentCovariancesError
+   exceptions.GenericNumericalError
    exceptions.DeltaNumericalError
    exceptions.CostsNumericalError
    exceptions.MicroMomentsNumericalError
    exceptions.XiByThetaJacobianNumericalError
    exceptions.OmegaByThetaJacobianNumericalError
    exceptions.MicroMomentsByThetaJacobianNumericalError
    exceptions.MicroMomentCovariancesNumericalError
    exceptions.SyntheticPricesNumericalError
    exceptions.SyntheticSharesNumericalError
    exceptions.SyntheticDeltaNumericalError
    exceptions.SyntheticCostsNumericalError
+   exceptions.SyntheticMicroDataNumericalError
    exceptions.SyntheticMicroMomentsNumericalError
+   exceptions.MicroScoresNumericalError
    exceptions.EquilibriumRealizationNumericalError
+   exceptions.JacobianRealizationNumericalError
    exceptions.PostEstimationNumericalError
    exceptions.AbsorptionError
+   exceptions.ClippedSharesError
    exceptions.ThetaConvergenceError
    exceptions.DeltaConvergenceError
    exceptions.SyntheticPricesConvergenceError
    exceptions.SyntheticDeltaConvergenceError
    exceptions.EquilibriumPricesConvergenceError
    exceptions.ObjectiveReversionError
    exceptions.GradientReversionError
    exceptions.DeltaReversionError
    exceptions.CostsReversionError
    exceptions.MicroMomentsReversionError
    exceptions.XiByThetaJacobianReversionError
    exceptions.OmegaByThetaJacobianReversionError
    exceptions.MicroMomentsByThetaJacobianReversionError
    exceptions.HessianEigenvaluesError
+   exceptions.ProfitHessianEigenvaluesError
    exceptions.FittedValuesInversionError
    exceptions.SharesByXiJacobianInversionError
    exceptions.IntraFirmJacobianInversionError
+   exceptions.PassthroughInversionError
    exceptions.LinearParameterCovariancesInversionError
    exceptions.GMMParameterCovariancesInversionError
    exceptions.GMMMomentCovariancesInversionError
```

### Comparing `pyblp-0.9.0/docs/background.rst` & `pyblp-1.0.0/docs/background.rst`

 * *Files 25% similar despite different names*

```diff
@@ -3,96 +3,96 @@
 
 The following sections provide a very brief overview of the BLP model and how it is estimated. This goal is to concisely introduce the notation and terminology used throughout the rest of the documentation. For a more in-depth overview, refer to :ref:`references:Conlon and Gortmaker (2020)`.
 
 
 The Model
 ---------
 
-There are :math:`t = 1, 2, \dotsc, T` markets, each with :math:`j = 1, 2, \dotsc, J_t` products produced by :math:`f = 1, 2, \dotsc, F_t` firms, for a total of :math:`N` products across all markets. There are :math:`i = 1, 2, \dotsc, I_t` agents who choose among the :math:`J_t` products and an outside good :math:`j = 0`.
+There are :math:`t = 1, 2, \dotsc, T` markets, each with :math:`j = 1, 2, \dotsc, J_t` products produced by :math:`f = 1, 2, \dotsc, F_t` firms, for a total of :math:`N` products across all markets. There are :math:`i = 1, 2, \dotsc, I_t` individuals/agents who choose among the :math:`J_t` products and an outside good :math:`j = 0`. These numbers also represent sets. For example, :math:`J_t = \{1, 2, \dots, J_t\}`.
 
 
 Demand
 ~~~~~~
 
 Observed demand-side product characteristics are contained in the :math:`N \times K_1` matrix of linear characteristics, :math:`X_1`, and the :math:`N \times K_2` matrix of nonlinear characteristics, :math:`X_2`, which is typically a subset of :math:`X_1`. Unobserved demand-side product characteristics, :math:`\xi`, are a :math:`N \times 1` vector.
 
 In market :math:`t`, observed agent characteristics are a :math:`I_t \times D` matrix called demographics, :math:`d`. Unobserved agent characteristics are a :math:`I_t \times K_2` matrix, :math:`\nu`.
 
 The indirect utility of agent :math:`i` from purchasing product :math:`j` in market :math:`t` is
 
-.. math:: U_{jti} = \underbrace{\delta_{jt} + \mu_{jti}}_{V_{jti}} + \epsilon_{jti},
+.. math:: U_{ijt} = \underbrace{\delta_{jt} + \mu_{ijt}}_{V_{ijt}} + \epsilon_{ijt},
    :label: utilities
 
 in which the mean utility is, in vector-matrix form,
 
 .. math:: \delta = \underbrace{X_1^\text{en}\alpha + X_1^\text{ex}\beta^\text{ex}}_{X_1\beta} + \xi.
 
 The :math:`K_1 \times 1` vector of demand-side linear parameters, :math:`\beta`, is partitioned into two components: :math:`\alpha` is a :math:`K_1^\text{en} \times 1` vector of parameters on the :math:`N \times K_1^\text{en}` submatrix of endogenous characteristics, :math:`X_1^\text{en}`, and :math:`\beta^\text{ex}` is a :math:`K_1^\text{ex} \times 1` vector of parameters on the :math:`N \times K_1^\text{ex}` submatrix of exogenous characteristics, :math:`X_1^\text{ex}`. Usually, :math:`X_1^\text{en} = p`, prices, so :math:`\alpha` is simply a scalar.
 
-The agent-specific portion of utility in a single market is
+The agent-specific portion of utility in a single market is, in vector-matrix form,
 
 .. math:: \mu = X_2(\Sigma \nu' + \Pi d').
    :label: mu
 
-The model incorporates both observable (demographic) and unobservable taste heterogeneity though random coefficients. For the unobserved heterogeneity, we let :math:`\nu` denote independent draws from the standard normal distribution. These are scaled by a :math:`K_2 \times K_2` lower-triangular matrix :math:`\Sigma`, which denotes the Cholesky root of the covariance matrix for unobserved taste heterogeneity. The :math:`K_2 \times D` matrix :math:`\Pi` measures how agent tastes vary with demographics.
+The model incorporates both observable (demographics) and unobservable taste heterogeneity though random coefficients. For the unobserved heterogeneity, we let :math:`\nu` denote independent draws from the standard normal distribution. These are scaled by a :math:`K_2 \times K_2` lower-triangular matrix :math:`\Sigma`, which denotes the Cholesky root of the covariance matrix for unobserved taste heterogeneity. The :math:`K_2 \times D` matrix :math:`\Pi` measures how agent tastes vary with demographics.
 
-In the above expression, random coefficients are assumed to be normally distributed. To incorporate one or more lognormal random coefficients, the associated columns in parenthesized expression can be exponentiated before being pre-multiplied by :math:`X_2`. For example, this allows for the coefficient on price to be lognormal so that demand slopes down for all agents. For lognormal random coefficients, a constant column is typically included in :math:`d` so that its coefficients in :math:`\Pi` parametrize the means of the logs of the random coefficients.
+In the above expression, random coefficients are assumed to be normally distributed, but this expression supports all elliptical distributions. To incorporate one or more lognormal random coefficients, the associated columns in the parenthesized expression can be exponentiated before being pre-multiplied by :math:`X_2`. For example, this allows for the coefficient on price to be lognormal so that demand slopes down for all agents. For lognormal random coefficients, a constant column is typically included in :math:`d` so that its coefficients in :math:`\Pi` parametrize the means of the logs of the random coefficients. More generally, all log-elliptical distributions are supported. A logit link function is also supported.
 
-Random idiosyncratic preferences, :math:`\epsilon_{jti}`, are assumed to be Type I Extreme Value, so that conditional on the heterogeneous coefficients, marketshares follow the well known logit form. Aggregate marketshares are obtained by integrating over the distribution of individual heterogeneity. They are approximated with Monte Carlo integration or quadrature rules defined by the :math:`I_t \times K_2` matrix of integration nodes, :math:`\nu`, and a :math:`I_t \times 1` vector of integration weights, :math:`w`:
+Random idiosyncratic preferences, :math:`\epsilon_{ijt}`, are assumed to be Type I Extreme Value, so that conditional on the heterogeneous coefficients, market shares follow the well-known logit form. Aggregate market shares are obtained by integrating over the distribution of individual heterogeneity. They are approximated with Monte Carlo integration or quadrature rules defined by the :math:`I_t \times K_2` matrix of integration nodes, :math:`\nu`, and an :math:`I_t \times 1` vector of integration weights, :math:`w`:
 
-.. math:: s_{jt} \approx \sum_{i=1}^{I_t} w_{it} s_{jti},
+.. math:: s_{jt} \approx \sum_{i \in I_t} w_{it} s_{ijt},
    :label: shares
 
 where the probability that agent :math:`i` chooses product :math:`j` in market :math:`t` is
 
-.. math:: s_{jti} = \frac{\exp V_{jti}}{1 + \sum_{k=1}^{J_t} \exp V_{kti}}.
+.. math:: s_{ijt} = \frac{\exp V_{ijt}}{1 + \sum_{k \in J_t} \exp V_{ikt}}.
    :label: probabilities
 
-There is a one in the denominator because the utility of the outside good is normalized to :math:`U_{0ti} = 0`.
+There is a one in the denominator because the utility of the outside good is normalized to :math:`U_{i0t} = 0`. The scale of utility is normalized by the variance of :math:`\epsilon_{ijt}`.
 
    
 Supply
 ~~~~~~
 
 Observed supply-side product characteristics are contained in the :math:`N \times K_3` matrix of supply-side characteristics, :math:`X_3`. Prices cannot be supply-side characteristics, but non-price product characteristics often overlap with the demand-side characteristics in :math:`X_1` and :math:`X_2`. Unobserved supply-side product characteristics, :math:`\omega`, are a :math:`N \times 1` vector.
 
-Firm :math:`f` chooses prices in market :math:`t` to maximize the profits of its products :math:`\mathscr{J}_{ft} \subset \{1, 2, \ldots, J_t\}`:
+Firm :math:`f` chooses prices in market :math:`t` to maximize the profits of its products :math:`J_{ft} \subset J_t`:
 
-.. math:: \pi_{ft} = \sum_{j \in \mathscr{J}_{ft}} (p_{jt} - c_{jt})s_{jt}.
+.. math:: \pi_{ft} = \sum_{j \in J_{ft}} (p_{jt} - c_{jt})s_{jt}.
 
 In a single market, the corresponding multi-product differentiated Bertrand first order conditions are, in vector-matrix form,
 
 .. math:: p - c = \underbrace{\Delta^{-1}s}_{\eta},
    :label: eta
 
 where the multi-product Bertrand markup :math:`\eta` depends on :math:`\Delta`, a :math:`J_t \times J_t` matrix of intra-firm (negative) demand derivatives:
 
 .. math:: \Delta = -\mathscr{H} \odot \frac{\partial s}{\partial p}.
 
-Here, :math:`\mathscr{H}` denotes the market-level ownership or product holdings matrix, where :math:`\mathscr{H}_{jk}` is typically :math:`1` if the same firm produces products :math:`j` and :math:`k`, and :math:`0` otherwise.
+Here, :math:`\mathscr{H}` denotes the market-level ownership or product holdings matrix in the market, where :math:`\mathscr{H}_{jk}` is typically :math:`1` if the same firm produces products :math:`j` and :math:`k`, and :math:`0` otherwise.
 
 To include a supply side, we must specify a functional form for marginal costs:
 
 .. math:: \tilde{c} = f(c) = X_3\gamma + \omega.
    :label: costs
 
 The most common choices are :math:`f(c) = c` and :math:`f(c) = \log(c)`.
 
 
 Estimation
 ----------
 
-A demand side is always estimated but including a supply side is optional. With only a demand side, there are three sets of parameters to be estimated: :math:`\beta` (which may include :math:`\alpha`), :math:`\Sigma` and :math:`\Pi`. With a supply side, there is also :math:`\gamma`. The linear parameters, :math:`\beta` and :math:`\gamma`, are typically concentrated out of the problem. The exception is :math:`\alpha`, which cannot be concentrated out when there is a supply side because it is needed to compute demand derivatives and hence marginal costs. Linear parameters that are not concentrated out along with unknown nonlinear parameters in :math:`\Sigma` and :math:`\Pi` are collectively denoted :math:`\theta`, a :math:`P \times 1` vector.
+A demand side is always estimated but including a supply side is optional. With only a demand side, there are three sets of parameters to be estimated: :math:`\beta` (which may include :math:`\alpha`), :math:`\Sigma` and :math:`\Pi`. With a supply side, there is also :math:`\gamma`. The linear parameters, :math:`\beta` and :math:`\gamma`, are typically concentrated out of the problem. The exception is :math:`\alpha`, which cannot be concentrated out when there is a supply side because it is needed to compute demand derivatives and hence marginal costs. Linear parameters that are not concentrated out along with unknown nonlinear parameters in :math:`\Sigma` and :math:`\Pi` are collectively denoted :math:`\theta`.
 
 The GMM problem is
 
 .. math:: \min_\theta q(\theta) = \bar{g}(\theta)'W\bar{g}(\theta),
    :label: objective
 
-in which :math:`q(\theta)` is the GMM objective. By default, PyBLP scales this value by :math:`N` so that objectives across different problem sizes are comparable, but this behavior can be disabled. In some of the BLP literature and in earlier versions of this package, the objective was scaled by :math:`N^2`.
+in which :math:`q(\theta)` is the GMM objective. By default, PyBLP scales this value by :math:`N` so that objectives across different problem sizes are comparable. This behavior can be disabled. In some of the BLP literature and in earlier versions of this package, the objective was scaled by :math:`N^2`.
 
 Here, :math:`W` is a :math:`M \times M` weighting matrix and :math:`\bar{g}` is a :math:`M \times 1` vector of averaged demand- and supply-side moments:
 
 .. math:: \bar{g} = \begin{bmatrix} \bar{g}_D \\ \bar{g}_S \end{bmatrix} = \frac{1}{N} \begin{bmatrix} \sum_{j,t} Z_{D,jt}'\xi_{jt} \\ \sum_{j,t} Z_{S,jt}'\omega_{jt} \end{bmatrix}
    :label: averaged_moments
 
 where :math:`Z_D` and :math:`Z_S` are :math:`N \times M_D` and :math:`N \times M_S` matrices of demand- and supply-side instruments.
@@ -104,41 +104,41 @@
 
 In each GMM stage, a nonlinear optimizer finds the :math:`\hat{\theta}` that minimizes the GMM objective value :math:`q(\theta)`.
 
 
 The Objective
 ~~~~~~~~~~~~~
 
-Given a :math:`\hat{\theta}`, the first step to computing the objective :math:`q(\hat{\theta})` is to compute :math:`\delta(\hat{\theta})` in each market with the following standard contraction:
+Given a :math:`\theta`, the first step to computing the objective :math:`q(\theta)` is to compute :math:`\delta(\theta)` in each market with the following standard contraction:
 
-.. math:: \delta_{jt} \leftarrow \delta_{jt} + \log s_{jt} - \log s_{jt}(\delta, \hat{\theta})
+.. math:: \delta_{jt} \leftarrow \delta_{jt} + \log s_{jt} - \log s_{jt}(\delta, \theta)
    :label: contraction
 
-where :math:`s` are the market's observed shares and :math:`s(\delta, \hat{\theta})` are calculated marketshares. Iteration terminates when the norm of the change in :math:`\delta(\hat{\theta})` is less than a small number.
+where :math:`s` are the market's observed shares and :math:`s(\delta, \theta)` are calculated market shares. Iteration terminates when the norm of the change in :math:`\delta(\theta)` is less than a small number.
 
 With a supply side, marginal costs are then computed according to :eq:`eta`:
 
-.. math:: c_{jt}(\hat{\theta}) = p_{jt} - \eta_{jt}(\hat{\theta}).
+.. math:: c_{jt}(\theta) = p_{jt} - \eta_{jt}(\theta).
 
 Concentrated out linear parameters are recovered with linear IV-GMM:
 
-.. math:: \begin{bmatrix} \hat{\beta}^\text{ex} \\ \hat{\gamma} \end{bmatrix} = (X'ZWZ'X)^{-1}X'ZWZ'Y(\hat{\theta})
+.. math:: \begin{bmatrix} \hat{\beta}^\text{ex} \\ \hat{\gamma} \end{bmatrix} = (X'ZWZ'X)^{-1}X'ZWZ'Y(\theta)
    :label: iv
 
 where
 
-.. math:: X = \begin{bmatrix} X_1^\text{ex} & 0 \\ 0 & X_3 \end{bmatrix}, \quad Z = \begin{bmatrix} Z_D & 0 \\ 0 & Z_S \end{bmatrix}, \quad Y(\hat{\theta}) = \begin{bmatrix} \delta(\hat{\theta}) - X_1^\text{en}\hat{\alpha} & 0 \\ 0 & \tilde{c}(\hat{\theta}) \end{bmatrix}.
+.. math:: X = \begin{bmatrix} X_1^\text{ex} & 0 \\ 0 & X_3 \end{bmatrix}, \quad Z = \begin{bmatrix} Z_D & 0 \\ 0 & Z_S \end{bmatrix}, \quad Y(\theta) = \begin{bmatrix} \delta(\theta) - X_1^\text{en}\hat{\alpha} \\ \tilde{c}(\theta) \end{bmatrix}.
 
-With only a demand side, :math:`\alpha` can be concentrated out, so :math:`X = X_1`, :math:`Z = Z_D`, and :math:`Y = \delta(\hat{\theta})` recover the full :math:`\hat{\beta}` in :eq:`iv`.
+With only a demand side, :math:`\alpha` can be concentrated out, so :math:`X = X_1`, :math:`Z = Z_D`, and :math:`Y = \delta(\theta)` recover the full :math:`\hat{\beta}` in :eq:`iv`.
 
-Finally, the unobserved product characteristics (structural errors),
+Finally, the unobserved product characteristics (i.e., the structural errors),
 
-.. math:: \begin{bmatrix} \xi(\hat{\theta}) \\ \omega(\hat{\theta}) \end{bmatrix} = \begin{bmatrix} \delta(\hat{\theta}) - X_1\hat{\beta} \\ \tilde{c}(\hat{\theta}) - X_3\hat{\gamma} \end{bmatrix},
+.. math:: \begin{bmatrix} \xi(\theta) \\ \omega(\theta) \end{bmatrix} = \begin{bmatrix} \delta(\theta) - X_1\hat{\beta} \\ \tilde{c}(\theta) - X_3\hat{\gamma} \end{bmatrix},
 
-are interacted with the instruments to form :math:`\bar{g}(\hat{\theta})` in :eq:`averaged_moments`, which give the GMM objective :math:`q(\hat{\theta})` in :eq:`objective`.
+are interacted with the instruments to form :math:`\bar{g}(\theta)` in :eq:`averaged_moments`, which gives the GMM objective :math:`q(\theta)` in :eq:`objective`.
 
 
 The Gradient
 ~~~~~~~~~~~~
 
 The gradient of the GMM objective in :eq:`objective` is 
 
@@ -152,20 +152,22 @@
 
 Writing :math:`\delta` as an implicit function of :math:`s` in :eq:`shares` gives the demand-side Jacobian:
 
 .. math:: \frac{\partial\xi}{\partial\theta} = \frac{\partial\delta}{\partial\theta} = -\left(\frac{\partial s}{\partial\delta}\right)^{-1}\frac{\partial s}{\partial\theta}.
 
 The supply-side Jacobian is derived from the definition of :math:`\tilde{c}` in :eq:`costs`:
 
-.. math:: \frac{\partial\omega}{\partial\theta} = \frac{\partial\tilde{c}}{\partial\theta_p} = -\frac{\partial\tilde{c}}{\partial c}\frac{\partial\eta}{\partial\theta}.
+.. math:: \frac{\partial\omega}{\partial\theta} = \frac{\partial\tilde{c}}{\partial\theta} = -\frac{\partial\tilde{c}}{\partial c}\frac{\partial\eta}{\partial\theta}.
 
 The second term in this expression is derived from the definition of :math:`\eta` in :eq:`eta`:
 
 .. math:: \frac{\partial\eta}{\partial\theta} = -\Delta^{-1}\left(\frac{\partial\Delta}{\partial\theta}\eta + \frac{\partial\Delta}{\partial\xi}\eta\frac{\partial\xi}{\partial\theta}\right).
 
+One thing to note is that :math:`\frac{\partial\xi}{\partial\theta} = \frac{\partial\delta}{\partial\theta}` and :math:`\frac{\partial\omega}{\partial\theta} = \frac{\partial\tilde{c}}{\partial\theta}` need not hold during optimization if we concentrate out linear parameters because these are then functions of :math:`\theta`. Fortunately, one can use orthogonality conditions to show that it is fine to treat these parameters as fixed when computing the gradient.
+
 
 Weighting Matrices
 ~~~~~~~~~~~~~~~~~~
 
 Conventionally, the 2SLS weighting matrix is used in the first stage:
 
 .. math:: W = \begin{bmatrix} (Z_D'Z_D / N)^{-1} & 0 \\ 0 & (Z_S'Z_S / N)^{-1} \end{bmatrix}.
@@ -174,49 +176,52 @@
 With two-step GMM, :math:`W` is updated before the second stage according to 
 
 .. math:: W = S^{-1}.
    :label: W
 
 For heteroscedasticity robust weighting matrices,
 
-.. math:: S = \frac{1}{N}\sum_{j,t}^N g_{jt}g_{jt}'.
+.. math:: S = \frac{1}{N}\sum_{j,t} g_{jt}g_{jt}'.
    :label: robust_S
 
 For clustered weighting matrices with :math:`c = 1, 2, \dotsc, C` clusters,
 
 .. math:: S = \frac{1}{N}\sum_{c=1}^C g_cg_c',
    :label: clustered_S
 
-where, letting the set :math:`\mathscr{J}_{ct} \subset \{1, 2, \ldots, J_t\}` denote products in cluster :math:`c` and market :math:`t`,
+where, letting the set :math:`J_{ct} \subset J_t` denote products in cluster :math:`c` and market :math:`t`,
 
-.. math:: g_c = \sum_{t=1}^T \sum_{j\in\mathscr{J}_{ct}} g_{jt}.
+.. math:: g_c = \sum_{t \in T} \sum_{j \in J_{ct}} g_{jt}.
 
 For unadjusted weighting matrices,
 
 .. math:: S = \frac{1}{N} \begin{bmatrix} \sigma_\xi^2 Z_D'Z_D & \sigma_{\xi\omega} Z_D'Z_S \\ \sigma_{\xi\omega} Z_S'Z_D & \sigma_\omega^2 Z_S'Z_S \end{bmatrix}
    :label: unadjusted_S
 
-where
+where :math:`\sigma_\xi^2`, :math:`\sigma_\omega^2`, and :math:`\sigma_{\xi\omega}` are estimates of the variances and covariance between the structural errors.
+
+Simulation error can be accounted for by resampling agents :math:`r = 1, \dots, R` times, evaluating each :math:`\bar{g}_r`, and adding the following to :math:`S`:
 
-.. math:: \text{Var}(\xi, \omega) = \begin{bmatrix} \sigma_\xi^2 & \sigma_{\xi\omega} \\ \sigma_{\xi\omega} & \sigma_\omega^2 \end{bmatrix}.
+.. math:: \frac{1}{R - 1} \sum_{r=1}^R (\bar{g}_r - \bar{\bar{g}})(\bar{g}_r - \bar{\bar{g}})', \quad \bar{\bar{g}} = \frac{1}{R} \sum_{r=1}^R \bar{g}_r.
+   :label: simulation_S
 
 
 Standard Errors
 ~~~~~~~~~~~~~~~
 
-The covariance matrix of the estimated parameters is
+An estimate of the asymptotic covariance matrix of :math:`\sqrt{N}(\hat{\theta} - \theta_0)` is
 
-.. math:: \text{Var}(\hat{\theta}) = (\bar{G}'W\bar{G})^{-1}\bar{G}'WSW\bar{G}(\bar{G}'W\bar{G})^{-1}.
+.. math:: (\bar{G}'W\bar{G})^{-1}\bar{G}'WSW\bar{G}(\bar{G}'W\bar{G})^{-1}.
    :label: covariances
 
 Standard errors are the square root of the diagonal of this matrix divided by :math:`N`.
 
 If the weighting matrix was chosen such that :math:`W = S^{-1}`, this simplifies to
 
-.. math:: \text{Var}(\hat{\theta}) = (\bar{G}'W\bar{G})^{-1}.
+.. math:: (\bar{G}'W\bar{G})^{-1}.
    :label: unadjusted_covariances
 
 Standard errors extracted from this simpler expression are called unadjusted.
 
 
 Fixed Effects
 -------------
@@ -226,90 +231,128 @@
 .. math:: \begin{bmatrix} \xi_{jt} \\ \omega_{jt} \end{bmatrix} = \begin{bmatrix} \xi_{k_1} + \xi_{k_2} + \cdots + \xi_{k_{E_D}} + \Delta\xi_{jt} \\ \omega_{\ell_1} + \omega_{\ell_2} + \cdots + \omega_{\ell_{E_S}} + \Delta\omega_{jt} \end{bmatrix}
    :label: fe
 
 where :math:`k_1, k_2, \dotsc, k_{E_D}` and :math:`\ell_1, \ell_2, \dotsc, \ell_{E_S}` index unobserved characteristics that are fixed across :math:`E_D` and :math:`E_S` dimensions. For example, with :math:`E_D = 1` dimension of product fixed effects, :math:`\xi_{jt} = \xi_j + \Delta\xi_{jt}`.
 
 Small numbers of fixed effects can be estimated with dummy variables in :math:`X_1`, :math:`X_3`, :math:`Z_D`, and :math:`Z_S`. However, this approach does not scale with high dimensional fixed effects because it requires constructing and inverting an infeasibly large matrix in :eq:`iv`. 
 
-Instead, fixed effects are typically absorbed into :math:`X`, :math:`Z`, and :math:`Y(\hat{\theta})` in :eq:`iv`. With one fixed effect, these matrices are simply de-meaned within each level of the fixed effect. Both :math:`X` and :math:`Z` can be de-meaned just once, but :math:`Y(\hat{\theta})` must be de-meaned for each new :math:`\hat{\theta}`.
+Instead, fixed effects are typically absorbed into :math:`X`, :math:`Z`, and :math:`Y(\theta)` in :eq:`iv`. With one fixed effect, these matrices are simply de-meaned within each level of the fixed effect. Both :math:`X` and :math:`Z` can be de-meaned just once, but :math:`Y(\theta)` must be de-meaned for each new :math:`\theta`.
 
 This procedure is equivalent to replacing each column of the matrices with residuals from a regression of the column on the fixed effect. The Frish-Waugh-Lovell (FWL) theorem of :ref:`references:Frisch and Waugh (1933)` and :ref:`references:Lovell (1963)` guarantees that using these residualized matrices gives the same results as including fixed effects as dummy variables. When :math:`E_D > 1` or :math:`E_S > 1`, the matrices are residualized with more involved algorithms.
 
 Once fixed effects have been absorbed, estimation is as described above with the structural errors :math:`\Delta\xi` and :math:`\Delta\omega`.
 
 
 Micro Moments
 -------------
 
-In the spirit of :ref:`references:Imbens and Lancaster (1994)`, :ref:`references:Petrin (2002)`, and :ref:`references:Berry, Levinsohn, and Pakes (2004)`, more detailed micro data on individual agent decisions can be used to supplement the standard demand- and supply-side moments :math:`\bar{g}_D` and :math:`\bar{g}_S` in :eq:`averaged_moments` with an additional :math:`m = 1, 2, \ldots, M_M` averaged micro moments, :math:`\bar{g}_M`, for a total of :math:`M = M_D + M_S + M_M` averaged moments:
+More detailed micro data on individual choices can be used to supplement the standard demand- and supply-side moments :math:`\bar{g}_D` and :math:`\bar{g}_S` in :eq:`averaged_moments` with an additional :math:`m = 1, 2, \ldots, M_M` micro moments, :math:`\bar{g}_M`, for a total of :math:`M = M_D + M_S + M_M` moments:
 
 .. math:: \bar{g} = \begin{bmatrix} \bar{g}_D \\ \bar{g}_S \\ \bar{g}_M \end{bmatrix}.
 
-Each micro moment :math:`m` is approximated in a set :math:`\mathscr{T}_m \subset \{1, 2, \ldots, T\}` of markets in which its micro data are relevant and then averaged across these markets:
+:ref:`references:Conlon and Gortmaker (2023)` provides a standardized framework for incorporating micro moments into BLP-style estimation. What follows is a simplified summary of this framework. Each micro moment :math:`m` is the difference between an observed value :math:`f_m(\bar{v})` and its simulated analogue :math:`f_m(v)`:
+
+.. math:: \bar{g}_{M,m} = f_m(\bar{v}) - f_m(v),
+    :label: micro_moment
+
+in which :math:`f_m(\cdot)` is a function that maps a vector of :math:`p = 1, \ldots, P_M` micro moment parts :math:`\bar{v} = (\bar{v}_1, \dots, \bar{v}_{P_M})'` or :math:`v = (v_1, \dots, v_{P_M})'` into a micro statistic. Each sample micro moment part :math:`p` is an average over observations :math:`n \in N_{d_m}` in the associated micro dataset :math:`d_p`:
+
+.. math:: \bar{v}_p = \frac{1}{N_{d_p}} \sum_{n \in N_{d_p}} v_{pi_nj_nt_n}.
+    :label: observed_micro_part
+
+Its simulated analogue is
+
+.. math:: v_p = \frac{\sum_{t \in T} \sum_{i \in I_t} \sum_{j \in J_t \cup \{0\}} w_{it} s_{ijt} w_{d_pijt} v_{pijt}}{\sum_{t \in T} \sum_{i \in I_t} \sum_{j \in J_t \cup \{0\}} w_{it} s_{ijt} w_{d_pijt}},
+    :label: simulated_micro_part
+
+In which :math:`w_{it} s_{ijt} w_{d_pijt}` is the probability an observation in the micro dataset is for an agent :math:`i` who chooses :math:`j` in market :math:`t`.
+
+The simplest type of micro moment is just an average over the entire sample, with :math:`f_m(v) = v_1`. For example, with :math:`v_{1ijt}` equal to the income for an agent :math:`i` who chooses :math:`j` in market :math:`t`, micro moment :math:`m` would match the average income in dataset :math:`d_p`. Observed values such as conditional expectations, covariances, correlations, or regression coefficients can be matched by choosing the appropriate function :math:`f_m`. For example, with :math:`v_{2ijt}` equal to the interaction between income and an indicator for the choice of the outside option, and with :math:`v_{3ijt}` equal to an indicator for the choiced of the outside option, :math:`f_m(v) = v_2 / v_3` would match an observed conditional mean income within those who choose the outside option.
+
+A micro dataset :math:`d`, often a survey, is defined by survey weights :math:`w_{dijt}`. For example, :math:`w_{dijt} = 1\{j \neq 0, t \in T_d\}` defines a micro dataset that is a selected sample of inside purchasers in a few markets :math:`T_d \subset T`, giving each market an equal sampling weight. Different micro datasets are independent.
+
+A micro dataset will often admit multiple micro moment parts. Each micro moment part :math:`p` is defined by its dataset :math:`d_p` and micro values :math:`v_{pijt}`. For example, a micro moment part :math:`p` with :math:`v_{pijt} = y_{it}x_{jt}` delivers the mean :math:`\bar{v}_p` or expectation :math:`v_p` of an interaction between some demographic :math:`y_{it}` and some product characteristic :math:`x_{jt}`.
+
+A micro moment is a function of one or more micro moment parts. The simplest type is a function of only one micro moment part, and matches the simple average defined by the micro moment part. For example, :math:`f_m(v) = v_p` with :math:`v_{pijt} = y_{it} x_{jt}` matches the mean of an interaction between :math:`y_{it}` and :math:`x_{jt}`. Non-simple averages such as conditional means, covariances, correlations, or regression coefficients can be matched by choosing an appropriate function :math:`f_m`. For example, :math:`f_m(v) = v_1 / v_2` with :math:`v_{1ijt} = y_{it}x_{jt}1\{j \neq 0\}` and :math:`v_{2ijt} = 1\{j \neq 0\}` matches the conditional mean of an interaction between :math:`y_{it}` and :math:`x_{jt}` among those who do not choose the outside option :math:`j = 0`.
+
+Technically, if not all micro moments :math:`m` are simple averages :math:`f_m(v) = v_m`, then the resulting estimator will no longer be a GMM estimator, but rather a more generic minimum distance estimator, since these "micro moments" are not technically sample moments. Regardless, the package uses GMM terminology for simplicity's sake, and the statistical expressions are all the same. Micro moments are computed for each :math:`\theta` and contribute to the GMM (or minimum distance) objective :math:`q(\theta)` in :eq:`objective`. Their derivatives with respect to :math:`\theta` are added as rows to :math:`\bar{G}` in :eq:`averaged_moments_jacobian`, and blocks are added to both :math:`W` and :math:`S` in :eq:`2sls_W` and :eq:`W`. The covariance between standard moments and micro moments is zero, so these matrices are block-diagonal. The delta method delivers the covariance matrix for the micro moments:
+
+.. math:: S_M = \frac{\partial f(v)}{\partial v'} S_P \frac{\partial f(v)'}{\partial v}.
+   :label: scaled_micro_moment_covariances
+
+The scaled covariance between micro moment parts :math:`p` and :math:`q` in :math:`S_P` is zero if they are based on different micro datasets :math:`d_p` \neq d_q`; otherwise, if based on the same dataset :math:`d_p = d_q = d`,
+
+.. math:: S_{P,pq} = \frac{N}{N_d} \text{Cov}(v_{pi_nj_nt_n}, v_{qi_nj_nt_n}),
+   :label: scaled_micro_part_covariance
+
+in which
+
+.. math:: \text{Cov}(v_{pi_nj_nt_n}, v_{qi_nj_nt_n}) = \frac{\sum_{t \in T} \sum_{i \in I_t} \sum_{j \in J_t \cup \{0\}} w_{it} s_{ijt} w_{dijt} (v_{pijt} - v_p)(v_{qijt} - v_q)}{\sum_{t \in T} \sum_{i \in I_t} \sum_{j \in J_t \cup \{0\}} w_{it} s_{ijt} w_{dijt}}.
+    :label: micro_part_covariance
+
+Micro moment parts based on second choice are averages over values :math:`v_{pijkt}` where :math:`k` indexes second choices, and are based on datasets defined by survey weights :math:`w_{dijkt}`. A sample micro moment part is
 
-.. math:: \bar{g}_{M,m} \approx \frac{1}{|\mathscr{T}_m|} \sum_{t\in\mathscr{T}_m} \sum_{i=1}^{I_t} w_{it} g_{M,mti}.
-   :label: averaged_micro_moments
+.. math:: \bar{v}_p = \frac{1}{N_{d_p}} \sum_{n \in N_{d_p}} v_{pi_nj_nk_nt_n}.
 
-The vector :math:`\bar{g}_M` contains sample analogues of micro moment conditions :math:`E[g_{M,mti}] = 0` where :math:`g_{M,mti}` is typically a function of choice probabilities, data in market :math:`t`, and a statistic computed from survey data that the moment aims to match.
+Its simulated analogue is
 
-Mico moments are computed for each :math:`\hat{\theta}` and contribute to the GMM objective :math:`q(\hat{\theta})` in :eq:`objective`. Their derivatives with respect to :math:`\theta` are added as rows to :math:`\bar{G}` in :eq:`averaged_moments_jacobian`, and blocks are added to both :math:`W` and :math:`S` in :eq:`2sls_W` and :eq:`W`. The covariance between standard moments and micro moments is assumed to be zero, so these matrices will be block-diagonal. The covariance between micro moments :math:`m` and :math:`n` in :math:`S` is set to zero if :math:`\mathscr{T}_{mn} = \mathscr{T}_m \cap \mathscr{T}_n = \emptyset` and otherwise is approximated by
+.. math:: v_p = \frac{\sum_{t \in T} \sum_{i \in I_t} \sum_{j, k \in J_t \cup \{0\}} w_{it} s_{ijt} s_{ik(-j)t} w_{d_pijkt} v_{pijkt}}{\sum_{t \in T} \sum_{i \in I_t} \sum_{j, k \in J_t \cup \{0\}} w_{it} s_{ijt} s_{ik(-j)t} w_{d_pijkt}},
 
-.. math:: \text{Cov}(\bar{g}_{M,m}, \bar{g}_{M,n}) \approx \frac{1}{|\mathscr{T}_{mn}|} \sum_{t\in\mathscr{T}_{mn}} \sum_{i=1}^{I_t} w_{it}(g_{M,mti} - \bar{g}_{M,mt})(g_{M,nti} - \bar{g}_{M,nt})
-   :label: averaged_micro_moment_covariances
+in which :math:`s_{ik(-j)t}` is the probability of choosing :math:`k` when :math:`j` is removed from the choice set. One can also define micro moment parts based on second choices where a group of products :math:`h(j)` containing the first choice :math:`j` is removed from the choice set. In this case, the above second choice probabilities become :math:`s_{ik(-h(j))t}`.
 
-where :math:`\bar{g}_{M,mt} = \sum_i w_{it} g_{M,mti}`.
+Covariances are defined analogously.
 
 
 Random Coefficients Nested Logit
 --------------------------------
 
-Incorporating parameters that measure within nesting group correlation gives the random coefficients nested logit (RCNL) model of :ref:`references:Brenkers and Verboven (2006)` and :ref:`references:Grigolon and Verboven (2014)`. There are :math:`h = 1, 2, \dotsc, H` nesting groups and each product :math:`j` is assigned to a group :math:`h(j)`. The set :math:`\mathscr{J}_{ht} \subset \{1, 2, \ldots, J_t\}` denotes the products in group :math:`h` and market :math:`t`.
+Incorporating parameters that measure within nesting group correlation gives the random coefficients nested logit (RCNL) model of :ref:`references:Brenkers and Verboven (2006)` and :ref:`references:Grigolon and Verboven (2014)`. There are :math:`h = 1, 2, \dotsc, H` nesting groups and each product :math:`j` is assigned to a group :math:`h(j)`. The set :math:`J_{ht} \subset J_t` denotes the products in group :math:`h` and market :math:`t`.
 
 In the RCNL model, idiosyncratic preferences are partitioned into
 
-.. math:: \epsilon_{jti} = \bar{\epsilon}_{h(j)ti} + (1 - \rho_{h(j)})\bar{\epsilon}_{jti}
+.. math:: \epsilon_{ijt} = \bar{\epsilon}_{ih(j)t} + (1 - \rho_{h(j)})\bar{\epsilon}_{ijt}
 
-where :math:`\bar{\epsilon}_{jti}` is Type I Extreme Value and :math:`\bar{\epsilon}_{h(j)ti}` is distributed such that :math:`\epsilon_{jti}` is still Type I Extreme Value. 
+where :math:`\bar{\epsilon}_{ijt}` is Type I Extreme Value and :math:`\bar{\epsilon}_{ih(j)t}` is distributed such that :math:`\epsilon_{ijt}` is still Type I Extreme Value. 
 
 The nesting parameters, :math:`\rho`, can either be a :math:`H \times 1` vector or a scalar so that for all groups :math:`\rho_h = \rho`. Letting :math:`\rho \to 0` gives the standard BLP model and :math:`\rho \to 1` gives division by zero errors. With :math:`\rho_h \in (0, 1)`, the expression for choice probabilities in :eq:`probabilities` becomes more complicated:
 
-.. math:: s_{jti} = \frac{\exp[V_{jti} / (1 - \rho_{h(j)})]}{\exp[V_{h(j)ti} / (1 - \rho_{h(j)})]}\cdot\frac{\exp V_{h(j)ti}}{1 + \sum_{h=1}^H \exp V_{hti}}
+.. math:: s_{ijt} = \frac{\exp[V_{ijt} / (1 - \rho_{h(j)})]}{\exp[V_{ih(j)t} / (1 - \rho_{h(j)})]}\cdot\frac{\exp V_{ih(j)t}}{1 + \sum_{h \in H} \exp V_{iht}}
    :label: nested_probabilities
 
 where 
 
-.. math:: V_{hti} = (1 - \rho_h)\log\sum_{k\in\mathscr{J}_{ht}} \exp[V_{kti} / (1 - \rho_h)].
+.. math:: V_{iht} = (1 - \rho_h)\log\sum_{k \in J_{ht}} \exp[V_{ikt} / (1 - \rho_h)].
    :label: inclusive_value
 
-The contraction for :math:`\delta(\hat{\theta})` in :eq:`contraction` is also slightly different:
+The contraction for :math:`\delta(\theta)` in :eq:`contraction` is also slightly different:
 
-.. math:: \delta_{jt} \leftarrow \delta_{jt} + (1 - \rho_{h(j)})[\log s_{jt} - \log s_{jt}(\delta, \hat{\theta})].
+.. math:: \delta_{jt} \leftarrow \delta_{jt} + (1 - \rho_{h(j)})[\log s_{jt} - \log s_{jt}(\delta, \theta)].
    :label: nested_contraction
 
 Otherwise, estimation is as described above with :math:`\rho` included in :math:`\theta`.
 
 
 Logit and Nested Logit
 ----------------------
 
 Letting :math:`\Sigma = 0` gives the simpler logit (or nested logit) model where there is a closed-form solution for :math:`\delta`. In the logit model,
 
 .. math:: \delta_{jt} = \log s_{jt} - \log s_{0t},
    :label: logit_delta
 
-and a lack of nonlinear parameters means that nonlinear optimization is not needed.
+and a lack of nonlinear parameters means that nonlinear optimization is often unneeded.
 
 In the nested logit model, :math:`\rho` must be optimized over, but there is still a closed-form solution for :math:`\delta`:
 
 .. math:: \delta_{jt} = \log s_{jt} - \log s_{0t} - \rho_{h(j)}[\log s_{jt} - \log s_{h(j)t}].
    :label: nested_logit_delta
 
 where
 
-.. math:: s_{ht} = \sum_{j\in\mathscr{J}_{ht}} s_{jt}.
+.. math:: s_{ht} = \sum_{j \in J_{ht}} s_{jt}.
 
 In both models, a supply side can still be estimated jointly with demand. Estimation is as described above with a representative agent in each market: :math:`I_t = 1` and :math:`w_1 = 1`.
 
 
 Equilibrium Prices
 ------------------
 
@@ -318,19 +361,19 @@
 Instead, :ref:`references:Morrow and Skerlos (2011)` reformulate the solution to :eq:`eta`:
 
 .. math:: p - c = \underbrace{\Lambda^{-1}(\mathscr{H} \odot \Gamma)'(p - c) - \Lambda^{-1}s}_{\zeta}
    :label: zeta
 
 where :math:`\Lambda` is a diagonal :math:`J_t \times J_t` matrix approximated by
 
-.. math:: \Lambda_{jj} \approx \sum_{i=1}^{I_t} w_{it} s_{jti}\frac{\partial U_{jti}}{\partial p_{jt}}
+.. math:: \Lambda_{jj} \approx \sum_{i \in I_t} w_{it} s_{ijt}\frac{\partial U_{ijt}}{\partial p_{jt}}
 
 and :math:`\Gamma` is a dense :math:`J_t \times J_t` matrix approximated by
 
-.. math:: \Gamma_{jk} \approx \sum_{i=1}^{I_t} w_{it} s_{jti}s_{kti}\frac{\partial U_{jti}}{\partial p_{jt}}.
+.. math:: \Gamma_{jk} \approx \sum_{i \in I_t} w_{it} s_{ijt}s_{ikt}\frac{\partial U_{ikt}}{\partial p_{kt}}.
 
 Equilibrium prices are computed by iterating over the :math:`\zeta`-markup equation in :eq:`zeta`,
 
 .. math:: p \leftarrow c + \zeta(p),
    :label: zeta_contraction
 
 which, unlike :eq:`eta`, is a contraction. Iteration terminates when the norm of firms' first order conditions, :math:`||\Lambda(p)(p - c - \zeta(p))||`, is less than a small number.
```

### Comparing `pyblp-0.9.0/docs/conf.py` & `pyblp-1.0.0/docs/conf.py`

 * *Files 4% similar despite different names*

```diff
@@ -25,15 +25,15 @@
 html_static_path = ['static']
 templates_path = ['templates']
 exclude_patterns = ['_build', '_downloads', 'notebooks', 'templates', '**.ipynb_checkpoints']
 
 # configure project information
 language = 'en'
 project = 'PyBLP'
-author = 'Jeff Gortmaker'
+author = 'Jeff Gortmaker and Christopher Conlon'
 copyright = f'{datetime.datetime.now().year}, {author}'
 release = version = pyblp.__version__
 
 # configure build information
 nitpicky = True
 tls_verify = False
 master_doc = 'index'
@@ -76,14 +76,17 @@
 math_number_all = True
 numfig_secnum_depth = 0
 autosummary_generate = True
 numpydoc_show_class_members = False
 autosectionlabel_prefix_document = True
 nbsphinx_prolog = read('templates/nbsphinx_prolog.rst')
 nbsphinx_epilog = read('templates/nbsphinx_epilog.rst')
+linkcheck_ignore = [
+    'https://www.anaconda.com/download',  # 403 forbidden, but fine in browser
+]
 
 # configure HTML information
 html_theme = 'sphinx_rtd_theme'
 
 # configure LaTeX information
 latex_elements = {
     'preamble': read('static/preamble.tex')
```

### Comparing `pyblp-0.9.0/docs/index.rst` & `pyblp-1.0.0/docs/index.rst`

 * *Files identical despite different names*

### Comparing `pyblp-0.9.0/docs/notation.rst` & `pyblp-1.0.0/docs/notation.rst`

 * *Files 4% similar despite different names*

```diff
@@ -1,73 +1,65 @@
 Notation
 ========
 
-The notation in PyBLP is a customized amalgamation of the notation employed by :ref:`references:Berry, Levinsohn, and Pakes (1995)`, :ref:`references:Nevo (2000)`, :ref:`references:Morrow and Skerlos (2011)`, :ref:`references:Grigolon and Verboven (2014)`, and others.
+The notation in PyBLP is a customized amalgamation of the notation employed by :ref:`references:Berry, Levinsohn, and Pakes (1995)`, :ref:`references:Nevo (2000a)`, :ref:`references:Morrow and Skerlos (2011)`, :ref:`references:Grigolon and Verboven (2014)`, and others.
 
 
 Indices
 -------
 
-=========  =====================
+=========  ==================
 Index      Description
-=========  =====================
+=========  ==================
 :math:`j`  Products
 :math:`t`  Markets
-:math:`i`  Agents or individuals
+:math:`i`  Agents/individuals
 :math:`f`  Firms
 :math:`h`  Nests
 :math:`c`  Clusters
 :math:`m`  Micro moments
-=========  =====================
+=========  ==================
 
 
-Dimensions
-----------
+Dimensions/Sets
+---------------
 
-========================  ==================================================================================================
-Dimension                 Description
-========================  ==================================================================================================
-:math:`T`                 Markets
-:math:`N`                 Products across all markets
-:math:`F`                 Firms across all markets
-:math:`I`                 Agents across all markets
-:math:`J_t`               Products in market :math:`t`
-:math:`F_t`               Firms in market :math:`t`
-:math:`I_t`               Agents in market :math:`t`
-:math:`K_1`               Demand-side linear product characteristics
-:math:`K_1^\text{ex}`     Exogenous demand-side linear product characteristics
-:math:`K_1^\text{en}`     Endogenous demand-side linear product characteristics
-:math:`K_2`               Demand-side nonlinear product characteristics
-:math:`K_3`               Supply-side product characteristics
-:math:`K_3^\text{ex}`     Exogenous supply-side product characteristics
-:math:`K_3^\text{en}`     Endogenous supply-side product characteristics
-:math:`D`                 Demographic variables
-:math:`M_D`               Demand-side instruments
-:math:`M_S`               Supply-side instruments
-:math:`M_M`               Micro moments
-:math:`M`                 Moments: the sum of :math:`M_D`, :math:`M_S`, and :math:`M_M`
-:math:`E_D`               Absorbed dimensions of demand-side fixed effects
-:math:`E_S`               Absorbed dimensions of supply-side fixed effects
-:math:`H`                 Nesting groups
-:math:`C`                 Clusters
-:math:`P`                 Parameters
-========================  ==================================================================================================
-
-
-Sets
-----
-
-========================  ========================================================
-Set                       Description
-========================  ========================================================
-:math:`\mathscr{J}_{ft}`  Products produced by firm :math:`f` in market :math:`t`
-:math:`\mathscr{J}_{ht}`  Products in nesting group :math:`h` and market :math:`t`
-:math:`\mathscr{J}_{ct}`  Products in cluster :math:`c` and market :math:`t`
-:math:`\mathscr{T}_m`     Markets over which micro moment :math:`m` is averaged
-========================  ========================================================
+=====================  ==========================================================================
+Dimension/Set          Description
+=====================  ==========================================================================
+:math:`T`              Markets
+:math:`N`              Products across all markets
+:math:`F`              Firms across all markets
+:math:`I`              Agents across all markets
+:math:`J_t`            Products in market :math:`t`
+:math:`F_t`            Firms in market :math:`t`
+:math:`J_{ft}`         Products produced by firm :math:`f` in market :math:`t`
+:math:`I_t`            Agents in market :math:`t`
+:math:`K_1`            Demand-side linear product characteristics
+:math:`K_1^\text{ex}`  Exogenous demand-side linear product characteristics
+:math:`K_1^\text{en}`  Endogenous demand-side linear product characteristics
+:math:`K_2`            Demand-side nonlinear product characteristics
+:math:`K_3`            Supply-side product characteristics
+:math:`K_3^\text{ex}`  Exogenous supply-side product characteristics
+:math:`K_3^\text{en}`  Endogenous supply-side product characteristics
+:math:`D`              Demographic variables
+:math:`M_D`            Demand-side instruments
+:math:`M_S`            Supply-side instruments
+:math:`M_M`            Micro moments
+:math:`T_m`            Markets over which micro moment :math:`m` is averaged
+:math:`T_{mn}`         Markets over which micro moments :math:`m` and :math:`n` are both averaged
+:math:`N_m`            Observations underlying observed micro moment value :math:`m`.
+:math:`M`              All moments
+:math:`E_D`            Absorbed dimensions of demand-side fixed effects
+:math:`E_S`            Absorbed dimensions of supply-side fixed effects
+:math:`H`              Nesting groups
+:math:`J_{ht}`         Products in nesting group :math:`h` and market :math:`t`
+:math:`C`              Clusters
+:math:`J_{ct}`         Products in cluster :math:`c` and market :math:`t`
+=====================  ==========================================================================
 
 
 Matrices, Vectors, and Scalars
 ------------------------------
 
 =====================================================  ==============================  ====================================================================================
 Symbol                                                 Dimensions                      Description
@@ -78,17 +70,17 @@
 :math:`X_2`                                            :math:`N \times K_2`            Demand-side Nonlinear product characteristics
 :math:`X_3`                                            :math:`N \times K_3`            Supply-side product characteristics
 :math:`X_3^\text{ex}`                                  :math:`N \times K_3^\text{ex}`  Exogenous supply-side product characteristics
 :math:`X_3^\text{en}`                                  :math:`N \times K_3^\text{en}`  Endogenous supply-side product characteristics
 :math:`\xi`                                            :math:`N \times 1`              Unobserved demand-side product characteristics
 :math:`\omega`                                         :math:`N \times 1`              Unobserved supply-side product characteristics
 :math:`p`                                              :math:`N \times 1`              Prices
-:math:`s` (:math:`s_{jt}`)                             :math:`N \times 1`              Marketshares
+:math:`s` (:math:`s_{jt}`)                             :math:`N \times 1`              Market shares
 :math:`s` (:math:`s_{ht}`)                             :math:`H \times 1`              Group shares in a market :math:`t`
-:math:`s` (:math:`s_{jti}`)                            :math:`N \times I_t`            Choice probabilities in a market :math:`t`
+:math:`s` (:math:`s_{ijt}`)                            :math:`N \times I_t`            Choice probabilities in a market :math:`t`
 :math:`c`                                              :math:`N \times 1`              Marginal costs
 :math:`\tilde{c}`                                      :math:`N \times 1`              Linear or log-linear marginal costs, :math:`c` or :math:`\log c` 
 :math:`\eta`                                           :math:`N \times 1`              Markup term from the BLP-markup equation
 :math:`\zeta`                                          :math:`N \times 1`              Markup term from the :math:`\zeta`-markup equation
 :math:`\mathscr{H}`                                    :math:`J_t \times J_t`          Ownership or product holdings matrix in market :math:`t`
 :math:`\kappa`                                         :math:`F_t \times F_t`          Cooperation matrix in market :math:`t`
 :math:`\Delta`                                         :math:`J_t \times J_t`          Intra-firm matrix of (negative) demand derivatives in market :math:`t`
@@ -96,19 +88,19 @@
 :math:`\Gamma`                                         :math:`J_t \times J_t`          Another matrix used to decompose :math:`\eta` and :math:`\zeta` in market :math:`t`
 :math:`d`                                              :math:`I_t \times D`            Observed agent characteristics called demographics in market :math:`t`
 :math:`\nu`                                            :math:`I_t \times K_2`          Unobserved agent characteristics called integration nodes in market :math:`t`
 :math:`w`                                              :math:`I_t \times 1`            Integration weights in market :math:`t`
 :math:`\delta`                                         :math:`N \times 1`              Mean utility
 :math:`\mu`                                            :math:`J_t \times I_t`          Agent-specific portion of utility in market :math:`t`
 :math:`\epsilon`                                       :math:`N \times 1`              Type I Extreme Value idiosyncratic preferences
-:math:`\bar{\epsilon}` (:math:`\bar{\epsilon}_{jti}`)  :math:`N \times 1`              Type I Extreme Value term used to decompose :math:`\epsilon`
-:math:`\bar{\epsilon}` (:math:`\bar{\epsilon}_{hti}`)  :math:`N \times 1`              Group-specific term used to decompose :math:`\epsilon`
+:math:`\bar{\epsilon}` (:math:`\bar{\epsilon}_{ijt}`)  :math:`N \times 1`              Type I Extreme Value term used to decompose :math:`\epsilon`
+:math:`\bar{\epsilon}` (:math:`\bar{\epsilon}_{iht}`)  :math:`N \times 1`              Group-specific term used to decompose :math:`\epsilon`
 :math:`U`                                              :math:`J_t \times I_t`          Indirect utilities
-:math:`V` (:math:`V_{jti}`)                            :math:`J_t \times I_t`          Indirect utilities minus :math:`\epsilon`
-:math:`V` (:math:`V_{hti}`)                            :math:`J_t \times I_t`          Inclusive value of a nesting group
+:math:`V` (:math:`V_{ijt}`)                            :math:`J_t \times I_t`          Indirect utilities minus :math:`\epsilon`
+:math:`V` (:math:`V_{iht}`)                            :math:`J_t \times I_t`          Inclusive value of a nesting group
 :math:`\pi` (:math:`\pi_{jt}`)                         :math:`N \times 1`              Population-normalized gross expected profits
 :math:`\pi` (:math:`\pi_{ft}`)                         :math:`F_t \times 1`            Population-normalized gross expected profits of a firm in market :math:`t`
 :math:`\beta`                                          :math:`K_1 \times 1`            Demand-side linear parameters
 :math:`\beta^\text{ex}`                                :math:`K_1^\text{ex} \times 1`  Parameters in :math:`\beta` on exogenous product characteristics
 :math:`\alpha`                                         :math:`K_1^\text{en} \times 1`  Parameters in :math:`\beta` on endogenous product characteristics
 :math:`\Sigma`                                         :math:`K_2 \times K_2`          Cholesky root of the covariance matrix for unobserved taste heterogeneity
 :math:`\Pi`                                            :math:`K_2 \times D`            Parameters that measures how agent tastes vary with demographics
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_blp_instruments-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/build_matrix-checkpoint.ipynb`

 * *Files 16% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9426863809360108%*

 * *Differences: {"'cells'": "{0: {'source': ['# Building a Matrix Example']}, 1: {'outputs': {0: {'data': "*

 * *            '{\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': ["In this example, we\'ll load '*

 * *            'the fake cereal data from :ref:`references:Nevo (2000a)` and create a simple matrix '*

 * *            'involving a constant, prices, and shares."]}, 3: {\'outputs\': {0: {\'data\': '*

 * *            '{\'text/plain\': [\'1 + prices + shares\']}}}, \'source\': {insert: [(0, "formulation '*

 * *            '= pyblp.Formulat […]*

```diff
@@ -1,25 +1,25 @@
 {
     "cells": [
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "# Building \"Sums of Characteristics\" BLP Instruments Example"
+                "# Building a Matrix Example"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -30,35 +30,35 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this example, we'll load the automobile product data from :ref:`references:Berry, Levinsohn, and Pakes (1995)` and build some very simple excluded demand-side instruments for the problem. These instruments are different from the pre-built ones included in the automobile product data file, which used principal component analysis to alleviate a collinearity problem"
+                "In this example, we'll load the fake cereal data from :ref:`references:Nevo (2000a)` and create a simple matrix involving a constant, prices, and shares."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "1 + hpwt + air + mpd + space"
+                            "1 + prices + shares"
                         ]
                     },
                     "execution_count": 2,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "formulation = pyblp.Formulation('1 + hpwt + air + mpd + space')\n",
+                "formulation = pyblp.Formulation('1 + prices + shares')\n",
                 "formulation"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 3,
             "metadata": {},
@@ -81,248 +81,309 @@
                             "    }\n",
                             "</style>\n",
                             "<table border=\"1\" class=\"dataframe\">\n",
                             "  <thead>\n",
                             "    <tr style=\"text-align: right;\">\n",
                             "      <th></th>\n",
                             "      <th>market_ids</th>\n",
-                            "      <th>clustering_ids</th>\n",
-                            "      <th>car_ids</th>\n",
+                            "      <th>city_ids</th>\n",
+                            "      <th>quarter</th>\n",
+                            "      <th>product_ids</th>\n",
                             "      <th>firm_ids</th>\n",
-                            "      <th>region</th>\n",
+                            "      <th>brand_ids</th>\n",
                             "      <th>shares</th>\n",
                             "      <th>prices</th>\n",
-                            "      <th>hpwt</th>\n",
-                            "      <th>air</th>\n",
-                            "      <th>mpd</th>\n",
+                            "      <th>sugar</th>\n",
+                            "      <th>mushy</th>\n",
                             "      <th>...</th>\n",
-                            "      <th>supply_instruments2</th>\n",
-                            "      <th>supply_instruments3</th>\n",
-                            "      <th>supply_instruments4</th>\n",
-                            "      <th>supply_instruments5</th>\n",
-                            "      <th>supply_instruments6</th>\n",
-                            "      <th>supply_instruments7</th>\n",
-                            "      <th>supply_instruments8</th>\n",
-                            "      <th>supply_instruments9</th>\n",
-                            "      <th>supply_instruments10</th>\n",
-                            "      <th>supply_instruments11</th>\n",
+                            "      <th>demand_instruments10</th>\n",
+                            "      <th>demand_instruments11</th>\n",
+                            "      <th>demand_instruments12</th>\n",
+                            "      <th>demand_instruments13</th>\n",
+                            "      <th>demand_instruments14</th>\n",
+                            "      <th>demand_instruments15</th>\n",
+                            "      <th>demand_instruments16</th>\n",
+                            "      <th>demand_instruments17</th>\n",
+                            "      <th>demand_instruments18</th>\n",
+                            "      <th>demand_instruments19</th>\n",
                             "    </tr>\n",
                             "  </thead>\n",
                             "  <tbody>\n",
                             "    <tr>\n",
                             "      <th>0</th>\n",
-                            "      <td>1971</td>\n",
-                            "      <td>AMGREM71</td>\n",
-                            "      <td>129</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>US</td>\n",
-                            "      <td>0.001051</td>\n",
-                            "      <td>4.935802</td>\n",
-                            "      <td>0.528997</td>\n",
-                            "      <td>0</td>\n",
-                            "      <td>1.888146</td>\n",
+                            "      <td>C01Q1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>F1B04</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>4</td>\n",
+                            "      <td>0.012417</td>\n",
+                            "      <td>0.072088</td>\n",
+                            "      <td>2</td>\n",
+                            "      <td>1</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.705933</td>\n",
-                            "      <td>1.595656</td>\n",
-                            "      <td>87.0</td>\n",
-                            "      <td>-61.959985</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>46.060389</td>\n",
-                            "      <td>29.786989</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.888146</td>\n",
+                            "      <td>2.116358</td>\n",
+                            "      <td>-0.154708</td>\n",
+                            "      <td>-0.005796</td>\n",
+                            "      <td>0.014538</td>\n",
+                            "      <td>0.126244</td>\n",
+                            "      <td>0.067345</td>\n",
+                            "      <td>0.068423</td>\n",
+                            "      <td>0.034800</td>\n",
+                            "      <td>0.126346</td>\n",
+                            "      <td>0.035484</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>1</th>\n",
-                            "      <td>1971</td>\n",
-                            "      <td>AMHORN71</td>\n",
-                            "      <td>130</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>US</td>\n",
-                            "      <td>0.000670</td>\n",
-                            "      <td>5.516049</td>\n",
-                            "      <td>0.494324</td>\n",
-                            "      <td>0</td>\n",
-                            "      <td>1.935989</td>\n",
+                            "      <td>C01Q1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>F1B06</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>6</td>\n",
+                            "      <td>0.007809</td>\n",
+                            "      <td>0.114178</td>\n",
+                            "      <td>18</td>\n",
+                            "      <td>1</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.680910</td>\n",
-                            "      <td>1.490295</td>\n",
-                            "      <td>87.0</td>\n",
-                            "      <td>-61.959985</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>46.060389</td>\n",
-                            "      <td>29.786989</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.935989</td>\n",
+                            "      <td>-7.374091</td>\n",
+                            "      <td>-0.576412</td>\n",
+                            "      <td>0.012991</td>\n",
+                            "      <td>0.076143</td>\n",
+                            "      <td>0.029736</td>\n",
+                            "      <td>0.087867</td>\n",
+                            "      <td>0.110501</td>\n",
+                            "      <td>0.087784</td>\n",
+                            "      <td>0.049872</td>\n",
+                            "      <td>0.072579</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2</th>\n",
-                            "      <td>1971</td>\n",
-                            "      <td>AMJAVL71</td>\n",
-                            "      <td>132</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>US</td>\n",
-                            "      <td>0.000341</td>\n",
-                            "      <td>7.108642</td>\n",
-                            "      <td>0.467613</td>\n",
-                            "      <td>0</td>\n",
-                            "      <td>1.716799</td>\n",
+                            "      <td>C01Q1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>F1B07</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>7</td>\n",
+                            "      <td>0.012995</td>\n",
+                            "      <td>0.132391</td>\n",
+                            "      <td>4</td>\n",
+                            "      <td>1</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.801067</td>\n",
-                            "      <td>1.357703</td>\n",
-                            "      <td>87.0</td>\n",
-                            "      <td>-61.959985</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>46.060389</td>\n",
-                            "      <td>29.786989</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.716799</td>\n",
+                            "      <td>2.187872</td>\n",
+                            "      <td>-0.207346</td>\n",
+                            "      <td>0.003509</td>\n",
+                            "      <td>0.091781</td>\n",
+                            "      <td>0.163773</td>\n",
+                            "      <td>0.111881</td>\n",
+                            "      <td>0.108226</td>\n",
+                            "      <td>0.086439</td>\n",
+                            "      <td>0.122347</td>\n",
+                            "      <td>0.101842</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>3</th>\n",
-                            "      <td>1971</td>\n",
-                            "      <td>AMMATA71</td>\n",
-                            "      <td>134</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>US</td>\n",
-                            "      <td>0.000522</td>\n",
-                            "      <td>6.839506</td>\n",
-                            "      <td>0.426540</td>\n",
+                            "      <td>C01Q1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>F1B09</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>9</td>\n",
+                            "      <td>0.005770</td>\n",
+                            "      <td>0.130344</td>\n",
+                            "      <td>3</td>\n",
                             "      <td>0</td>\n",
-                            "      <td>1.687871</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.818061</td>\n",
-                            "      <td>1.261347</td>\n",
-                            "      <td>87.0</td>\n",
-                            "      <td>-61.959985</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>46.060389</td>\n",
-                            "      <td>29.786989</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.687871</td>\n",
+                            "      <td>2.704576</td>\n",
+                            "      <td>0.040748</td>\n",
+                            "      <td>-0.003724</td>\n",
+                            "      <td>0.094732</td>\n",
+                            "      <td>0.135274</td>\n",
+                            "      <td>0.088090</td>\n",
+                            "      <td>0.101767</td>\n",
+                            "      <td>0.101777</td>\n",
+                            "      <td>0.110741</td>\n",
+                            "      <td>0.104332</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>4</th>\n",
-                            "      <td>1971</td>\n",
-                            "      <td>AMAMBS71</td>\n",
-                            "      <td>136</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>US</td>\n",
-                            "      <td>0.000442</td>\n",
-                            "      <td>8.928395</td>\n",
-                            "      <td>0.452489</td>\n",
+                            "      <td>C01Q1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>F1B11</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>11</td>\n",
+                            "      <td>0.017934</td>\n",
+                            "      <td>0.154823</td>\n",
+                            "      <td>12</td>\n",
                             "      <td>0</td>\n",
-                            "      <td>1.504286</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.933210</td>\n",
-                            "      <td>1.237365</td>\n",
-                            "      <td>87.0</td>\n",
-                            "      <td>-61.959985</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>46.060389</td>\n",
-                            "      <td>29.786989</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.504286</td>\n",
+                            "      <td>1.261242</td>\n",
+                            "      <td>0.034836</td>\n",
+                            "      <td>-0.000568</td>\n",
+                            "      <td>0.102451</td>\n",
+                            "      <td>0.130640</td>\n",
+                            "      <td>0.084818</td>\n",
+                            "      <td>0.101075</td>\n",
+                            "      <td>0.125169</td>\n",
+                            "      <td>0.133464</td>\n",
+                            "      <td>0.121111</td>\n",
                             "    </tr>\n",
                             "  </tbody>\n",
                             "</table>\n",
-                            "<p>5 rows \u00d7 33 columns</p>\n",
+                            "<p>5 rows \u00d7 30 columns</p>\n",
                             "</div>"
                         ],
                         "text/plain": [
-                            "   market_ids clustering_ids  car_ids  firm_ids region    shares    prices  \\\n",
-                            "0        1971       AMGREM71      129        15     US  0.001051  4.935802   \n",
-                            "1        1971       AMHORN71      130        15     US  0.000670  5.516049   \n",
-                            "2        1971       AMJAVL71      132        15     US  0.000341  7.108642   \n",
-                            "3        1971       AMMATA71      134        15     US  0.000522  6.839506   \n",
-                            "4        1971       AMAMBS71      136        15     US  0.000442  8.928395   \n",
+                            "  market_ids  city_ids  quarter product_ids  firm_ids  brand_ids    shares  \\\n",
+                            "0      C01Q1         1        1       F1B04         1          4  0.012417   \n",
+                            "1      C01Q1         1        1       F1B06         1          6  0.007809   \n",
+                            "2      C01Q1         1        1       F1B07         1          7  0.012995   \n",
+                            "3      C01Q1         1        1       F1B09         1          9  0.005770   \n",
+                            "4      C01Q1         1        1       F1B11         1         11  0.017934   \n",
                             "\n",
-                            "       hpwt  air       mpd  ...  supply_instruments2  supply_instruments3  \\\n",
-                            "0  0.528997    0  1.888146  ...                  0.0             1.705933   \n",
-                            "1  0.494324    0  1.935989  ...                  0.0             1.680910   \n",
-                            "2  0.467613    0  1.716799  ...                  0.0             1.801067   \n",
-                            "3  0.426540    0  1.687871  ...                  0.0             1.818061   \n",
-                            "4  0.452489    0  1.504286  ...                  0.0             1.933210   \n",
+                            "     prices  sugar  mushy  ...  demand_instruments10  demand_instruments11  \\\n",
+                            "0  0.072088      2      1  ...              2.116358             -0.154708   \n",
+                            "1  0.114178     18      1  ...             -7.374091             -0.576412   \n",
+                            "2  0.132391      4      1  ...              2.187872             -0.207346   \n",
+                            "3  0.130344      3      0  ...              2.704576              0.040748   \n",
+                            "4  0.154823     12      0  ...              1.261242              0.034836   \n",
                             "\n",
-                            "   supply_instruments4  supply_instruments5  supply_instruments6  \\\n",
-                            "0             1.595656                 87.0           -61.959985   \n",
-                            "1             1.490295                 87.0           -61.959985   \n",
-                            "2             1.357703                 87.0           -61.959985   \n",
-                            "3             1.261347                 87.0           -61.959985   \n",
-                            "4             1.237365                 87.0           -61.959985   \n",
+                            "   demand_instruments12  demand_instruments13  demand_instruments14  \\\n",
+                            "0             -0.005796              0.014538              0.126244   \n",
+                            "1              0.012991              0.076143              0.029736   \n",
+                            "2              0.003509              0.091781              0.163773   \n",
+                            "3             -0.003724              0.094732              0.135274   \n",
+                            "4             -0.000568              0.102451              0.130640   \n",
                             "\n",
-                            "   supply_instruments7  supply_instruments8  supply_instruments9  \\\n",
-                            "0                  0.0            46.060389            29.786989   \n",
-                            "1                  0.0            46.060389            29.786989   \n",
-                            "2                  0.0            46.060389            29.786989   \n",
-                            "3                  0.0            46.060389            29.786989   \n",
-                            "4                  0.0            46.060389            29.786989   \n",
+                            "   demand_instruments15  demand_instruments16  demand_instruments17  \\\n",
+                            "0              0.067345              0.068423              0.034800   \n",
+                            "1              0.087867              0.110501              0.087784   \n",
+                            "2              0.111881              0.108226              0.086439   \n",
+                            "3              0.088090              0.101767              0.101777   \n",
+                            "4              0.084818              0.101075              0.125169   \n",
                             "\n",
-                            "   supply_instruments10  supply_instruments11  \n",
-                            "0                   0.0              1.888146  \n",
-                            "1                   0.0              1.935989  \n",
-                            "2                   0.0              1.716799  \n",
-                            "3                   0.0              1.687871  \n",
-                            "4                   0.0              1.504286  \n",
+                            "   demand_instruments18  demand_instruments19  \n",
+                            "0              0.126346              0.035484  \n",
+                            "1              0.049872              0.072579  \n",
+                            "2              0.122347              0.101842  \n",
+                            "3              0.110741              0.104332  \n",
+                            "4              0.133464              0.121111  \n",
                             "\n",
-                            "[5 rows x 33 columns]"
+                            "[5 rows x 30 columns]"
                         ]
                     },
                     "execution_count": 3,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "product_data = pd.read_csv(pyblp.data.BLP_PRODUCTS_LOCATION)\n",
+                "product_data = pd.read_csv(pyblp.data.NEVO_PRODUCTS_LOCATION)\n",
                 "product_data.head()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 4,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "(2217, 10)"
+                            "array([[1.        , 0.07208794, 0.01241721],\n",
+                            "       [1.        , 0.11417849, 0.00780939],\n",
+                            "       [1.        , 0.13239066, 0.01299451],\n",
+                            "       ...,\n",
+                            "       [1.        , 0.13701741, 0.00222918],\n",
+                            "       [1.        , 0.10017433, 0.01146267],\n",
+                            "       [1.        , 0.12755747, 0.02620832]])"
                         ]
                     },
                     "execution_count": 4,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "instruments = pyblp.build_blp_instruments(formulation, product_data)\n",
-                "instruments.shape"
+                "matrix = pyblp.build_matrix(formulation, product_data)\n",
+                "matrix"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "For various reasons, we may want to absorb fixed effects into the matrix. This can be done with the `absorb` argument of :class:`Formulation`. We'll now re-create the matrix, absorbing product-specific fixed effects. Note that the constant column is now ignored."
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 5,
+            "metadata": {},
+            "outputs": [
+                {
+                    "data": {
+                        "text/plain": [
+                            "prices + shares + Absorb[product_ids]"
+                        ]
+                    },
+                    "execution_count": 5,
+                    "metadata": {},
+                    "output_type": "execute_result"
+                }
+            ],
+            "source": [
+                "absorb_formulation = pyblp.Formulation('prices + shares', absorb='product_ids')\n",
+                "absorb_formulation"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 6,
+            "metadata": {},
+            "outputs": [
+                {
+                    "data": {
+                        "text/plain": [
+                            "array([[-0.01124832, -0.00052161],\n",
+                            "       [-0.00713476, -0.03144549],\n",
+                            "       [ 0.02367765, -0.01664996],\n",
+                            "       ...,\n",
+                            "       [ 0.03371995, -0.00779841],\n",
+                            "       [-0.00417404, -0.0117508 ],\n",
+                            "       [-0.01195648,  0.00666695]])"
+                        ]
+                    },
+                    "execution_count": 6,
+                    "metadata": {},
+                    "output_type": "execute_result"
+                }
+            ],
+            "source": [
+                "demeaned_matrix = pyblp.build_matrix(absorb_formulation, product_data)\n",
+                "demeaned_matrix"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_differentiation_instruments-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/build_matrix.ipynb`

 * *Files 26% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9617622368900287%*

 * *Differences: {"'cells'": "{0: {'source': ['# Building a Matrix Example']}, 1: {'outputs': {0: {'data': "*

 * *            '{\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': ["In this example, we\'ll load '*

 * *            'the fake cereal data from :ref:`references:Nevo (2000a)` and create a simple matrix '*

 * *            'involving a constant, prices, and shares."]}, 3: {\'outputs\': {0: {\'data\': '*

 * *            '{\'text/plain\': [\'1 + prices + shares\']}}}, \'source\': {insert: [(0, "formulation '*

 * *            '= pyblp.Formulat […]*

```diff
@@ -1,25 +1,25 @@
 {
     "cells": [
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "# Building Differentiation Instruments Example"
+                "# Building a Matrix Example"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -30,35 +30,35 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this example, we'll load the automobile product data from :ref:`references:Berry, Levinsohn, and Pakes (1995)` and build some very simple excluded demand-side instruments for the problem in the spirit of :ref:`references:Gandhi and Houde (2017)`."
+                "In this example, we'll load the fake cereal data from :ref:`references:Nevo (2000a)` and create a simple matrix involving a constant, prices, and shares."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "hpwt + air + mpd + space"
+                            "1 + prices + shares"
                         ]
                     },
                     "execution_count": 2,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "formulation = pyblp.Formulation('0 + hpwt + air + mpd + space')\n",
+                "formulation = pyblp.Formulation('1 + prices + shares')\n",
                 "formulation"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 3,
             "metadata": {},
@@ -81,299 +81,309 @@
                             "    }\n",
                             "</style>\n",
                             "<table border=\"1\" class=\"dataframe\">\n",
                             "  <thead>\n",
                             "    <tr style=\"text-align: right;\">\n",
                             "      <th></th>\n",
                             "      <th>market_ids</th>\n",
-                            "      <th>clustering_ids</th>\n",
-                            "      <th>car_ids</th>\n",
+                            "      <th>city_ids</th>\n",
+                            "      <th>quarter</th>\n",
+                            "      <th>product_ids</th>\n",
                             "      <th>firm_ids</th>\n",
-                            "      <th>region</th>\n",
+                            "      <th>brand_ids</th>\n",
                             "      <th>shares</th>\n",
                             "      <th>prices</th>\n",
-                            "      <th>hpwt</th>\n",
-                            "      <th>air</th>\n",
-                            "      <th>mpd</th>\n",
+                            "      <th>sugar</th>\n",
+                            "      <th>mushy</th>\n",
                             "      <th>...</th>\n",
-                            "      <th>supply_instruments2</th>\n",
-                            "      <th>supply_instruments3</th>\n",
-                            "      <th>supply_instruments4</th>\n",
-                            "      <th>supply_instruments5</th>\n",
-                            "      <th>supply_instruments6</th>\n",
-                            "      <th>supply_instruments7</th>\n",
-                            "      <th>supply_instruments8</th>\n",
-                            "      <th>supply_instruments9</th>\n",
-                            "      <th>supply_instruments10</th>\n",
-                            "      <th>supply_instruments11</th>\n",
+                            "      <th>demand_instruments10</th>\n",
+                            "      <th>demand_instruments11</th>\n",
+                            "      <th>demand_instruments12</th>\n",
+                            "      <th>demand_instruments13</th>\n",
+                            "      <th>demand_instruments14</th>\n",
+                            "      <th>demand_instruments15</th>\n",
+                            "      <th>demand_instruments16</th>\n",
+                            "      <th>demand_instruments17</th>\n",
+                            "      <th>demand_instruments18</th>\n",
+                            "      <th>demand_instruments19</th>\n",
                             "    </tr>\n",
                             "  </thead>\n",
                             "  <tbody>\n",
                             "    <tr>\n",
                             "      <th>0</th>\n",
-                            "      <td>1971</td>\n",
-                            "      <td>AMGREM71</td>\n",
-                            "      <td>129</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>US</td>\n",
-                            "      <td>0.001051</td>\n",
-                            "      <td>4.935802</td>\n",
-                            "      <td>0.528997</td>\n",
-                            "      <td>0</td>\n",
-                            "      <td>1.888146</td>\n",
+                            "      <td>C01Q1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>F1B04</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>4</td>\n",
+                            "      <td>0.012417</td>\n",
+                            "      <td>0.072088</td>\n",
+                            "      <td>2</td>\n",
+                            "      <td>1</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.705933</td>\n",
-                            "      <td>1.595656</td>\n",
-                            "      <td>87.0</td>\n",
-                            "      <td>-61.959985</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>46.060389</td>\n",
-                            "      <td>29.786989</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.888146</td>\n",
+                            "      <td>2.116358</td>\n",
+                            "      <td>-0.154708</td>\n",
+                            "      <td>-0.005796</td>\n",
+                            "      <td>0.014538</td>\n",
+                            "      <td>0.126244</td>\n",
+                            "      <td>0.067345</td>\n",
+                            "      <td>0.068423</td>\n",
+                            "      <td>0.034800</td>\n",
+                            "      <td>0.126346</td>\n",
+                            "      <td>0.035484</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>1</th>\n",
-                            "      <td>1971</td>\n",
-                            "      <td>AMHORN71</td>\n",
-                            "      <td>130</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>US</td>\n",
-                            "      <td>0.000670</td>\n",
-                            "      <td>5.516049</td>\n",
-                            "      <td>0.494324</td>\n",
-                            "      <td>0</td>\n",
-                            "      <td>1.935989</td>\n",
+                            "      <td>C01Q1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>F1B06</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>6</td>\n",
+                            "      <td>0.007809</td>\n",
+                            "      <td>0.114178</td>\n",
+                            "      <td>18</td>\n",
+                            "      <td>1</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.680910</td>\n",
-                            "      <td>1.490295</td>\n",
-                            "      <td>87.0</td>\n",
-                            "      <td>-61.959985</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>46.060389</td>\n",
-                            "      <td>29.786989</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.935989</td>\n",
+                            "      <td>-7.374091</td>\n",
+                            "      <td>-0.576412</td>\n",
+                            "      <td>0.012991</td>\n",
+                            "      <td>0.076143</td>\n",
+                            "      <td>0.029736</td>\n",
+                            "      <td>0.087867</td>\n",
+                            "      <td>0.110501</td>\n",
+                            "      <td>0.087784</td>\n",
+                            "      <td>0.049872</td>\n",
+                            "      <td>0.072579</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2</th>\n",
-                            "      <td>1971</td>\n",
-                            "      <td>AMJAVL71</td>\n",
-                            "      <td>132</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>US</td>\n",
-                            "      <td>0.000341</td>\n",
-                            "      <td>7.108642</td>\n",
-                            "      <td>0.467613</td>\n",
-                            "      <td>0</td>\n",
-                            "      <td>1.716799</td>\n",
+                            "      <td>C01Q1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>F1B07</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>7</td>\n",
+                            "      <td>0.012995</td>\n",
+                            "      <td>0.132391</td>\n",
+                            "      <td>4</td>\n",
+                            "      <td>1</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.801067</td>\n",
-                            "      <td>1.357703</td>\n",
-                            "      <td>87.0</td>\n",
-                            "      <td>-61.959985</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>46.060389</td>\n",
-                            "      <td>29.786989</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.716799</td>\n",
+                            "      <td>2.187872</td>\n",
+                            "      <td>-0.207346</td>\n",
+                            "      <td>0.003509</td>\n",
+                            "      <td>0.091781</td>\n",
+                            "      <td>0.163773</td>\n",
+                            "      <td>0.111881</td>\n",
+                            "      <td>0.108226</td>\n",
+                            "      <td>0.086439</td>\n",
+                            "      <td>0.122347</td>\n",
+                            "      <td>0.101842</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>3</th>\n",
-                            "      <td>1971</td>\n",
-                            "      <td>AMMATA71</td>\n",
-                            "      <td>134</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>US</td>\n",
-                            "      <td>0.000522</td>\n",
-                            "      <td>6.839506</td>\n",
-                            "      <td>0.426540</td>\n",
+                            "      <td>C01Q1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>F1B09</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>9</td>\n",
+                            "      <td>0.005770</td>\n",
+                            "      <td>0.130344</td>\n",
+                            "      <td>3</td>\n",
                             "      <td>0</td>\n",
-                            "      <td>1.687871</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.818061</td>\n",
-                            "      <td>1.261347</td>\n",
-                            "      <td>87.0</td>\n",
-                            "      <td>-61.959985</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>46.060389</td>\n",
-                            "      <td>29.786989</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.687871</td>\n",
+                            "      <td>2.704576</td>\n",
+                            "      <td>0.040748</td>\n",
+                            "      <td>-0.003724</td>\n",
+                            "      <td>0.094732</td>\n",
+                            "      <td>0.135274</td>\n",
+                            "      <td>0.088090</td>\n",
+                            "      <td>0.101767</td>\n",
+                            "      <td>0.101777</td>\n",
+                            "      <td>0.110741</td>\n",
+                            "      <td>0.104332</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>4</th>\n",
-                            "      <td>1971</td>\n",
-                            "      <td>AMAMBS71</td>\n",
-                            "      <td>136</td>\n",
-                            "      <td>15</td>\n",
-                            "      <td>US</td>\n",
-                            "      <td>0.000442</td>\n",
-                            "      <td>8.928395</td>\n",
-                            "      <td>0.452489</td>\n",
+                            "      <td>C01Q1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>F1B11</td>\n",
+                            "      <td>1</td>\n",
+                            "      <td>11</td>\n",
+                            "      <td>0.017934</td>\n",
+                            "      <td>0.154823</td>\n",
+                            "      <td>12</td>\n",
                             "      <td>0</td>\n",
-                            "      <td>1.504286</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.933210</td>\n",
-                            "      <td>1.237365</td>\n",
-                            "      <td>87.0</td>\n",
-                            "      <td>-61.959985</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>46.060389</td>\n",
-                            "      <td>29.786989</td>\n",
-                            "      <td>0.0</td>\n",
-                            "      <td>1.504286</td>\n",
+                            "      <td>1.261242</td>\n",
+                            "      <td>0.034836</td>\n",
+                            "      <td>-0.000568</td>\n",
+                            "      <td>0.102451</td>\n",
+                            "      <td>0.130640</td>\n",
+                            "      <td>0.084818</td>\n",
+                            "      <td>0.101075</td>\n",
+                            "      <td>0.125169</td>\n",
+                            "      <td>0.133464</td>\n",
+                            "      <td>0.121111</td>\n",
                             "    </tr>\n",
                             "  </tbody>\n",
                             "</table>\n",
-                            "<p>5 rows \u00d7 33 columns</p>\n",
+                            "<p>5 rows \u00d7 30 columns</p>\n",
                             "</div>"
                         ],
                         "text/plain": [
-                            "   market_ids clustering_ids  car_ids  firm_ids region    shares    prices  \\\n",
-                            "0        1971       AMGREM71      129        15     US  0.001051  4.935802   \n",
-                            "1        1971       AMHORN71      130        15     US  0.000670  5.516049   \n",
-                            "2        1971       AMJAVL71      132        15     US  0.000341  7.108642   \n",
-                            "3        1971       AMMATA71      134        15     US  0.000522  6.839506   \n",
-                            "4        1971       AMAMBS71      136        15     US  0.000442  8.928395   \n",
+                            "  market_ids  city_ids  quarter product_ids  firm_ids  brand_ids    shares  \\\n",
+                            "0      C01Q1         1        1       F1B04         1          4  0.012417   \n",
+                            "1      C01Q1         1        1       F1B06         1          6  0.007809   \n",
+                            "2      C01Q1         1        1       F1B07         1          7  0.012995   \n",
+                            "3      C01Q1         1        1       F1B09         1          9  0.005770   \n",
+                            "4      C01Q1         1        1       F1B11         1         11  0.017934   \n",
                             "\n",
-                            "       hpwt  air       mpd  ...  supply_instruments2  supply_instruments3  \\\n",
-                            "0  0.528997    0  1.888146  ...                  0.0             1.705933   \n",
-                            "1  0.494324    0  1.935989  ...                  0.0             1.680910   \n",
-                            "2  0.467613    0  1.716799  ...                  0.0             1.801067   \n",
-                            "3  0.426540    0  1.687871  ...                  0.0             1.818061   \n",
-                            "4  0.452489    0  1.504286  ...                  0.0             1.933210   \n",
+                            "     prices  sugar  mushy  ...  demand_instruments10  demand_instruments11  \\\n",
+                            "0  0.072088      2      1  ...              2.116358             -0.154708   \n",
+                            "1  0.114178     18      1  ...             -7.374091             -0.576412   \n",
+                            "2  0.132391      4      1  ...              2.187872             -0.207346   \n",
+                            "3  0.130344      3      0  ...              2.704576              0.040748   \n",
+                            "4  0.154823     12      0  ...              1.261242              0.034836   \n",
                             "\n",
-                            "   supply_instruments4  supply_instruments5  supply_instruments6  \\\n",
-                            "0             1.595656                 87.0           -61.959985   \n",
-                            "1             1.490295                 87.0           -61.959985   \n",
-                            "2             1.357703                 87.0           -61.959985   \n",
-                            "3             1.261347                 87.0           -61.959985   \n",
-                            "4             1.237365                 87.0           -61.959985   \n",
+                            "   demand_instruments12  demand_instruments13  demand_instruments14  \\\n",
+                            "0             -0.005796              0.014538              0.126244   \n",
+                            "1              0.012991              0.076143              0.029736   \n",
+                            "2              0.003509              0.091781              0.163773   \n",
+                            "3             -0.003724              0.094732              0.135274   \n",
+                            "4             -0.000568              0.102451              0.130640   \n",
                             "\n",
-                            "   supply_instruments7  supply_instruments8  supply_instruments9  \\\n",
-                            "0                  0.0            46.060389            29.786989   \n",
-                            "1                  0.0            46.060389            29.786989   \n",
-                            "2                  0.0            46.060389            29.786989   \n",
-                            "3                  0.0            46.060389            29.786989   \n",
-                            "4                  0.0            46.060389            29.786989   \n",
+                            "   demand_instruments15  demand_instruments16  demand_instruments17  \\\n",
+                            "0              0.067345              0.068423              0.034800   \n",
+                            "1              0.087867              0.110501              0.087784   \n",
+                            "2              0.111881              0.108226              0.086439   \n",
+                            "3              0.088090              0.101767              0.101777   \n",
+                            "4              0.084818              0.101075              0.125169   \n",
                             "\n",
-                            "   supply_instruments10  supply_instruments11  \n",
-                            "0                   0.0              1.888146  \n",
-                            "1                   0.0              1.935989  \n",
-                            "2                   0.0              1.716799  \n",
-                            "3                   0.0              1.687871  \n",
-                            "4                   0.0              1.504286  \n",
+                            "   demand_instruments18  demand_instruments19  \n",
+                            "0              0.126346              0.035484  \n",
+                            "1              0.049872              0.072579  \n",
+                            "2              0.122347              0.101842  \n",
+                            "3              0.110741              0.104332  \n",
+                            "4              0.133464              0.121111  \n",
                             "\n",
-                            "[5 rows x 33 columns]"
+                            "[5 rows x 30 columns]"
                         ]
                     },
                     "execution_count": 3,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "product_data = pd.read_csv(pyblp.data.BLP_PRODUCTS_LOCATION)\n",
+                "product_data = pd.read_csv(pyblp.data.NEVO_PRODUCTS_LOCATION)\n",
                 "product_data.head()"
             ]
         },
         {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "Note that we're excluding the constant column because it yields collinear constant columns of differentiation instruments.\n",
-                "\n",
-                "We'll first build \"local\" differentiation instruments, which are constructed by default, and which consist of counts of \"close\" rival and non-rival products in each market."
-            ]
-        },
-        {
             "cell_type": "code",
             "execution_count": 4,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "(2217, 8)"
+                            "array([[1.        , 0.07208794, 0.01241721],\n",
+                            "       [1.        , 0.11417849, 0.00780939],\n",
+                            "       [1.        , 0.13239066, 0.01299451],\n",
+                            "       ...,\n",
+                            "       [1.        , 0.13701741, 0.00222918],\n",
+                            "       [1.        , 0.10017433, 0.01146267],\n",
+                            "       [1.        , 0.12755747, 0.02620832]])"
                         ]
                     },
                     "execution_count": 4,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "local_instruments = pyblp.build_differentiation_instruments(\n",
-                "    formulation, \n",
-                "    product_data\n",
-                ")\n",
-                "local_instruments.shape"
+                "matrix = pyblp.build_matrix(formulation, product_data)\n",
+                "matrix"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Next, we'll build a more continuous \"quadratic\" version of the instruments, which consist of sums over squared differences between rival and non-rival products in each market."
+                "For various reasons, we may want to absorb fixed effects into the matrix. This can be done with the `absorb` argument of :class:`Formulation`. We'll now re-create the matrix, absorbing product-specific fixed effects. Note that the constant column is now ignored."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 5,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "(2217, 8)"
+                            "prices + shares + Absorb[product_ids]"
                         ]
                     },
                     "execution_count": 5,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "quadratic_instruments = pyblp.build_differentiation_instruments(\n",
-                "    formulation, \n",
-                "    product_data,\n",
-                "    version='quadratic'\n",
-                ")\n",
-                "quadratic_instruments.shape"
+                "absorb_formulation = pyblp.Formulation('prices + shares', absorb='product_ids')\n",
+                "absorb_formulation"
             ]
         },
         {
-            "cell_type": "markdown",
+            "cell_type": "code",
+            "execution_count": 6,
             "metadata": {},
+            "outputs": [
+                {
+                    "data": {
+                        "text/plain": [
+                            "array([[-0.01124832, -0.00052161],\n",
+                            "       [-0.00713476, -0.03144549],\n",
+                            "       [ 0.02367765, -0.01664996],\n",
+                            "       ...,\n",
+                            "       [ 0.03371995, -0.00779841],\n",
+                            "       [-0.00417404, -0.0117508 ],\n",
+                            "       [-0.01195648,  0.00666695]])"
+                        ]
+                    },
+                    "execution_count": 6,
+                    "metadata": {},
+                    "output_type": "execute_result"
+                }
+            ],
             "source": [
-                "We could also use `interact=True` to include interaction terms in either version of instruments, which would help capture covariances between different product characteristics."
+                "demeaned_matrix = pyblp.build_matrix(absorb_formulation, product_data)\n",
+                "demeaned_matrix"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_id_data-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/build_id_data-checkpoint.ipynb`

 * *Files 6% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9923642113095238%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -68,27 +68,27 @@
                 "id_data = pyblp.build_id_data(T=2, J=5, F=4)\n",
                 "id_data"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_integration-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/build_integration-checkpoint.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9924618675595238%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -111,27 +111,27 @@
             "source": [
                 "If we wanted to construct nodes and weights for each market, we could call :func:`build_integration` once for each market, add a column of market IDs, and stack the arrays."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_matrix-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/data-checkpoint.ipynb`

 * *Files 25% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9541220238095238%*

 * *Differences: {"'cells'": "{0: {'source': ['# Loading Data Example']}, 1: {'outputs': {0: {'data': "*

 * *            '{\'text/plain\': ["\'1.0.0\'"]}}}, \'source\': {delete: [1]}}, 2: {\'source\': ["Any '*

 * *            'number of functions can be used to load the example data into memory. In this '*

 * *            'example, we\'ll first use [NumPy](https://numpy.org/)."]}, 3: {\'outputs\': [], '*

 * *            '\'source\': [\'import numpy as np\\n\', "blp_product_data = '*

 * *            'np.recfromcsv(pyblp.data.BLP_PRODUCTS_LOCATION, e […]*

```diff
@@ -1,71 +1,85 @@
 {
     "cells": [
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "# Building a Matrix Example"
+                "# Loading Data Example"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "import pyblp\n",
-                "import pandas as pd\n",
                 "\n",
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this example, we'll load the fake cereal data from :ref:`references:Nevo (2000)` and create a simple matrix involving a constant, prices, and shares."
+                "Any number of functions can be used to load the example data into memory. In this example, we'll first use [NumPy](https://numpy.org/)."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "1 + prices + shares"
-                        ]
-                    },
-                    "execution_count": 2,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
+            "outputs": [],
+            "source": [
+                "import numpy as np\n",
+                "blp_product_data = np.recfromcsv(pyblp.data.BLP_PRODUCTS_LOCATION, encoding='utf-8')\n",
+                "blp_agent_data = np.recfromcsv(pyblp.data.BLP_AGENTS_LOCATION, encoding='utf-8')"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
             "source": [
-                "formulation = pyblp.Formulation('1 + prices + shares')\n",
-                "formulation"
+                "Record arrays can be cumbersome to manipulate. A more flexible alternative is the [pandas](https://pandas.pydata.org/) DataFrame. Unlike NumPy, pyblp does not directly depend on pandas, but it can be useful when manipulating data."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 3,
             "metadata": {},
+            "outputs": [],
+            "source": [
+                "import pandas as pd\n",
+                "blp_product_data = pd.read_csv(pyblp.data.BLP_PRODUCTS_LOCATION)\n",
+                "blp_agent_data = pd.read_csv(pyblp.data.BLP_AGENTS_LOCATION)"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "Another benefit of DataFrame objects is that they display nicely in Jupyter notebooks."
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 4,
+            "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/html": [
                             "<div>\n",
                             "<style scoped>\n",
                             "    .dataframe tbody tr th:only-of-type {\n",
@@ -81,309 +95,354 @@
                             "    }\n",
                             "</style>\n",
                             "<table border=\"1\" class=\"dataframe\">\n",
                             "  <thead>\n",
                             "    <tr style=\"text-align: right;\">\n",
                             "      <th></th>\n",
                             "      <th>market_ids</th>\n",
-                            "      <th>city_ids</th>\n",
-                            "      <th>quarter</th>\n",
-                            "      <th>product_ids</th>\n",
+                            "      <th>clustering_ids</th>\n",
+                            "      <th>car_ids</th>\n",
                             "      <th>firm_ids</th>\n",
-                            "      <th>brand_ids</th>\n",
+                            "      <th>region</th>\n",
                             "      <th>shares</th>\n",
                             "      <th>prices</th>\n",
-                            "      <th>sugar</th>\n",
-                            "      <th>mushy</th>\n",
+                            "      <th>hpwt</th>\n",
+                            "      <th>air</th>\n",
+                            "      <th>mpd</th>\n",
                             "      <th>...</th>\n",
-                            "      <th>demand_instruments10</th>\n",
-                            "      <th>demand_instruments11</th>\n",
-                            "      <th>demand_instruments12</th>\n",
-                            "      <th>demand_instruments13</th>\n",
-                            "      <th>demand_instruments14</th>\n",
-                            "      <th>demand_instruments15</th>\n",
-                            "      <th>demand_instruments16</th>\n",
-                            "      <th>demand_instruments17</th>\n",
-                            "      <th>demand_instruments18</th>\n",
-                            "      <th>demand_instruments19</th>\n",
+                            "      <th>supply_instruments2</th>\n",
+                            "      <th>supply_instruments3</th>\n",
+                            "      <th>supply_instruments4</th>\n",
+                            "      <th>supply_instruments5</th>\n",
+                            "      <th>supply_instruments6</th>\n",
+                            "      <th>supply_instruments7</th>\n",
+                            "      <th>supply_instruments8</th>\n",
+                            "      <th>supply_instruments9</th>\n",
+                            "      <th>supply_instruments10</th>\n",
+                            "      <th>supply_instruments11</th>\n",
                             "    </tr>\n",
                             "  </thead>\n",
                             "  <tbody>\n",
                             "    <tr>\n",
                             "      <th>0</th>\n",
-                            "      <td>C01Q1</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>F1B04</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>4</td>\n",
-                            "      <td>0.012417</td>\n",
-                            "      <td>0.072088</td>\n",
-                            "      <td>2</td>\n",
-                            "      <td>1</td>\n",
+                            "      <td>1971</td>\n",
+                            "      <td>AMGREM71</td>\n",
+                            "      <td>129</td>\n",
+                            "      <td>15</td>\n",
+                            "      <td>US</td>\n",
+                            "      <td>0.001051</td>\n",
+                            "      <td>4.935802</td>\n",
+                            "      <td>0.528997</td>\n",
+                            "      <td>0</td>\n",
+                            "      <td>1.888146</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>2.116358</td>\n",
-                            "      <td>-0.154708</td>\n",
-                            "      <td>-0.005796</td>\n",
-                            "      <td>0.014538</td>\n",
-                            "      <td>0.126244</td>\n",
-                            "      <td>0.067345</td>\n",
-                            "      <td>0.068423</td>\n",
-                            "      <td>0.034800</td>\n",
-                            "      <td>0.126346</td>\n",
-                            "      <td>0.035484</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>1.705933</td>\n",
+                            "      <td>1.595656</td>\n",
+                            "      <td>87.0</td>\n",
+                            "      <td>-61.959985</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>46.060389</td>\n",
+                            "      <td>29.786989</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>1.888146</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>1</th>\n",
-                            "      <td>C01Q1</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>F1B06</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>6</td>\n",
-                            "      <td>0.007809</td>\n",
-                            "      <td>0.114178</td>\n",
-                            "      <td>18</td>\n",
-                            "      <td>1</td>\n",
+                            "      <td>1971</td>\n",
+                            "      <td>AMHORN71</td>\n",
+                            "      <td>130</td>\n",
+                            "      <td>15</td>\n",
+                            "      <td>US</td>\n",
+                            "      <td>0.000670</td>\n",
+                            "      <td>5.516049</td>\n",
+                            "      <td>0.494324</td>\n",
+                            "      <td>0</td>\n",
+                            "      <td>1.935989</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>-7.374091</td>\n",
-                            "      <td>-0.576412</td>\n",
-                            "      <td>0.012991</td>\n",
-                            "      <td>0.076143</td>\n",
-                            "      <td>0.029736</td>\n",
-                            "      <td>0.087867</td>\n",
-                            "      <td>0.110501</td>\n",
-                            "      <td>0.087784</td>\n",
-                            "      <td>0.049872</td>\n",
-                            "      <td>0.072579</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>1.680910</td>\n",
+                            "      <td>1.490295</td>\n",
+                            "      <td>87.0</td>\n",
+                            "      <td>-61.959985</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>46.060389</td>\n",
+                            "      <td>29.786989</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>1.935989</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2</th>\n",
-                            "      <td>C01Q1</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>F1B07</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>7</td>\n",
-                            "      <td>0.012995</td>\n",
-                            "      <td>0.132391</td>\n",
-                            "      <td>4</td>\n",
-                            "      <td>1</td>\n",
+                            "      <td>1971</td>\n",
+                            "      <td>AMJAVL71</td>\n",
+                            "      <td>132</td>\n",
+                            "      <td>15</td>\n",
+                            "      <td>US</td>\n",
+                            "      <td>0.000341</td>\n",
+                            "      <td>7.108642</td>\n",
+                            "      <td>0.467613</td>\n",
+                            "      <td>0</td>\n",
+                            "      <td>1.716799</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>2.187872</td>\n",
-                            "      <td>-0.207346</td>\n",
-                            "      <td>0.003509</td>\n",
-                            "      <td>0.091781</td>\n",
-                            "      <td>0.163773</td>\n",
-                            "      <td>0.111881</td>\n",
-                            "      <td>0.108226</td>\n",
-                            "      <td>0.086439</td>\n",
-                            "      <td>0.122347</td>\n",
-                            "      <td>0.101842</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>1.801067</td>\n",
+                            "      <td>1.357703</td>\n",
+                            "      <td>87.0</td>\n",
+                            "      <td>-61.959985</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>46.060389</td>\n",
+                            "      <td>29.786989</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>1.716799</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>3</th>\n",
-                            "      <td>C01Q1</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>F1B09</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>9</td>\n",
-                            "      <td>0.005770</td>\n",
-                            "      <td>0.130344</td>\n",
-                            "      <td>3</td>\n",
+                            "      <td>1971</td>\n",
+                            "      <td>AMMATA71</td>\n",
+                            "      <td>134</td>\n",
+                            "      <td>15</td>\n",
+                            "      <td>US</td>\n",
+                            "      <td>0.000522</td>\n",
+                            "      <td>6.839506</td>\n",
+                            "      <td>0.426540</td>\n",
                             "      <td>0</td>\n",
+                            "      <td>1.687871</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>2.704576</td>\n",
-                            "      <td>0.040748</td>\n",
-                            "      <td>-0.003724</td>\n",
-                            "      <td>0.094732</td>\n",
-                            "      <td>0.135274</td>\n",
-                            "      <td>0.088090</td>\n",
-                            "      <td>0.101767</td>\n",
-                            "      <td>0.101777</td>\n",
-                            "      <td>0.110741</td>\n",
-                            "      <td>0.104332</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>1.818061</td>\n",
+                            "      <td>1.261347</td>\n",
+                            "      <td>87.0</td>\n",
+                            "      <td>-61.959985</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>46.060389</td>\n",
+                            "      <td>29.786989</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>1.687871</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>4</th>\n",
-                            "      <td>C01Q1</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>F1B11</td>\n",
-                            "      <td>1</td>\n",
-                            "      <td>11</td>\n",
-                            "      <td>0.017934</td>\n",
-                            "      <td>0.154823</td>\n",
-                            "      <td>12</td>\n",
+                            "      <td>1971</td>\n",
+                            "      <td>AMAMBS71</td>\n",
+                            "      <td>136</td>\n",
+                            "      <td>15</td>\n",
+                            "      <td>US</td>\n",
+                            "      <td>0.000442</td>\n",
+                            "      <td>8.928395</td>\n",
+                            "      <td>0.452489</td>\n",
                             "      <td>0</td>\n",
+                            "      <td>1.504286</td>\n",
                             "      <td>...</td>\n",
-                            "      <td>1.261242</td>\n",
-                            "      <td>0.034836</td>\n",
-                            "      <td>-0.000568</td>\n",
-                            "      <td>0.102451</td>\n",
-                            "      <td>0.130640</td>\n",
-                            "      <td>0.084818</td>\n",
-                            "      <td>0.101075</td>\n",
-                            "      <td>0.125169</td>\n",
-                            "      <td>0.133464</td>\n",
-                            "      <td>0.121111</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>1.933210</td>\n",
+                            "      <td>1.237365</td>\n",
+                            "      <td>87.0</td>\n",
+                            "      <td>-61.959985</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>46.060389</td>\n",
+                            "      <td>29.786989</td>\n",
+                            "      <td>0.0</td>\n",
+                            "      <td>1.504286</td>\n",
                             "    </tr>\n",
                             "  </tbody>\n",
                             "</table>\n",
-                            "<p>5 rows \u00d7 30 columns</p>\n",
+                            "<p>5 rows \u00d7 33 columns</p>\n",
                             "</div>"
                         ],
                         "text/plain": [
-                            "  market_ids  city_ids  quarter product_ids  firm_ids  brand_ids    shares  \\\n",
-                            "0      C01Q1         1        1       F1B04         1          4  0.012417   \n",
-                            "1      C01Q1         1        1       F1B06         1          6  0.007809   \n",
-                            "2      C01Q1         1        1       F1B07         1          7  0.012995   \n",
-                            "3      C01Q1         1        1       F1B09         1          9  0.005770   \n",
-                            "4      C01Q1         1        1       F1B11         1         11  0.017934   \n",
+                            "   market_ids clustering_ids  car_ids  firm_ids region    shares    prices  \\\n",
+                            "0        1971       AMGREM71      129        15     US  0.001051  4.935802   \n",
+                            "1        1971       AMHORN71      130        15     US  0.000670  5.516049   \n",
+                            "2        1971       AMJAVL71      132        15     US  0.000341  7.108642   \n",
+                            "3        1971       AMMATA71      134        15     US  0.000522  6.839506   \n",
+                            "4        1971       AMAMBS71      136        15     US  0.000442  8.928395   \n",
                             "\n",
-                            "     prices  sugar  mushy  ...  demand_instruments10  demand_instruments11  \\\n",
-                            "0  0.072088      2      1  ...              2.116358             -0.154708   \n",
-                            "1  0.114178     18      1  ...             -7.374091             -0.576412   \n",
-                            "2  0.132391      4      1  ...              2.187872             -0.207346   \n",
-                            "3  0.130344      3      0  ...              2.704576              0.040748   \n",
-                            "4  0.154823     12      0  ...              1.261242              0.034836   \n",
+                            "       hpwt  air       mpd  ...  supply_instruments2  supply_instruments3  \\\n",
+                            "0  0.528997    0  1.888146  ...                  0.0             1.705933   \n",
+                            "1  0.494324    0  1.935989  ...                  0.0             1.680910   \n",
+                            "2  0.467613    0  1.716799  ...                  0.0             1.801067   \n",
+                            "3  0.426540    0  1.687871  ...                  0.0             1.818061   \n",
+                            "4  0.452489    0  1.504286  ...                  0.0             1.933210   \n",
                             "\n",
-                            "   demand_instruments12  demand_instruments13  demand_instruments14  \\\n",
-                            "0             -0.005796              0.014538              0.126244   \n",
-                            "1              0.012991              0.076143              0.029736   \n",
-                            "2              0.003509              0.091781              0.163773   \n",
-                            "3             -0.003724              0.094732              0.135274   \n",
-                            "4             -0.000568              0.102451              0.130640   \n",
+                            "   supply_instruments4  supply_instruments5  supply_instruments6  \\\n",
+                            "0             1.595656                 87.0           -61.959985   \n",
+                            "1             1.490295                 87.0           -61.959985   \n",
+                            "2             1.357703                 87.0           -61.959985   \n",
+                            "3             1.261347                 87.0           -61.959985   \n",
+                            "4             1.237365                 87.0           -61.959985   \n",
                             "\n",
-                            "   demand_instruments15  demand_instruments16  demand_instruments17  \\\n",
-                            "0              0.067345              0.068423              0.034800   \n",
-                            "1              0.087867              0.110501              0.087784   \n",
-                            "2              0.111881              0.108226              0.086439   \n",
-                            "3              0.088090              0.101767              0.101777   \n",
-                            "4              0.084818              0.101075              0.125169   \n",
+                            "   supply_instruments7  supply_instruments8  supply_instruments9  \\\n",
+                            "0                  0.0            46.060389            29.786989   \n",
+                            "1                  0.0            46.060389            29.786989   \n",
+                            "2                  0.0            46.060389            29.786989   \n",
+                            "3                  0.0            46.060389            29.786989   \n",
+                            "4                  0.0            46.060389            29.786989   \n",
                             "\n",
-                            "   demand_instruments18  demand_instruments19  \n",
-                            "0              0.126346              0.035484  \n",
-                            "1              0.049872              0.072579  \n",
-                            "2              0.122347              0.101842  \n",
-                            "3              0.110741              0.104332  \n",
-                            "4              0.133464              0.121111  \n",
+                            "   supply_instruments10  supply_instruments11  \n",
+                            "0                   0.0              1.888146  \n",
+                            "1                   0.0              1.935989  \n",
+                            "2                   0.0              1.716799  \n",
+                            "3                   0.0              1.687871  \n",
+                            "4                   0.0              1.504286  \n",
                             "\n",
-                            "[5 rows x 30 columns]"
-                        ]
-                    },
-                    "execution_count": 3,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
-            "source": [
-                "product_data = pd.read_csv(pyblp.data.NEVO_PRODUCTS_LOCATION)\n",
-                "product_data.head()"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 4,
-            "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "array([[1.        , 0.07208794, 0.01241721],\n",
-                            "       [1.        , 0.11417849, 0.00780939],\n",
-                            "       [1.        , 0.13239066, 0.01299451],\n",
-                            "       ...,\n",
-                            "       [1.        , 0.13701741, 0.00222918],\n",
-                            "       [1.        , 0.10017433, 0.01146267],\n",
-                            "       [1.        , 0.12755747, 0.02620832]])"
+                            "[5 rows x 33 columns]"
                         ]
                     },
                     "execution_count": 4,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "matrix = pyblp.build_matrix(formulation, product_data)\n",
-                "matrix"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "For various reasons, we may want to absorb fixed effects into the matrix. This can be done with the `absorb` argument of :class:`Formulation`. We'll now re-create the matrix, absorbing product-specific fixed effects. Note that the constant column is now ignored."
+                "blp_product_data.head()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 5,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
+                        "text/html": [
+                            "<div>\n",
+                            "<style scoped>\n",
+                            "    .dataframe tbody tr th:only-of-type {\n",
+                            "        vertical-align: middle;\n",
+                            "    }\n",
+                            "\n",
+                            "    .dataframe tbody tr th {\n",
+                            "        vertical-align: top;\n",
+                            "    }\n",
+                            "\n",
+                            "    .dataframe thead th {\n",
+                            "        text-align: right;\n",
+                            "    }\n",
+                            "</style>\n",
+                            "<table border=\"1\" class=\"dataframe\">\n",
+                            "  <thead>\n",
+                            "    <tr style=\"text-align: right;\">\n",
+                            "      <th></th>\n",
+                            "      <th>market_ids</th>\n",
+                            "      <th>weights</th>\n",
+                            "      <th>nodes0</th>\n",
+                            "      <th>nodes1</th>\n",
+                            "      <th>nodes2</th>\n",
+                            "      <th>nodes3</th>\n",
+                            "      <th>nodes4</th>\n",
+                            "      <th>income</th>\n",
+                            "    </tr>\n",
+                            "  </thead>\n",
+                            "  <tbody>\n",
+                            "    <tr>\n",
+                            "      <th>0</th>\n",
+                            "      <td>1971</td>\n",
+                            "      <td>0.000543</td>\n",
+                            "      <td>1.192188</td>\n",
+                            "      <td>0.478777</td>\n",
+                            "      <td>0.980830</td>\n",
+                            "      <td>-0.824410</td>\n",
+                            "      <td>2.473301</td>\n",
+                            "      <td>109.560369</td>\n",
+                            "    </tr>\n",
+                            "    <tr>\n",
+                            "      <th>1</th>\n",
+                            "      <td>1971</td>\n",
+                            "      <td>0.000723</td>\n",
+                            "      <td>1.497074</td>\n",
+                            "      <td>-2.026204</td>\n",
+                            "      <td>-1.741316</td>\n",
+                            "      <td>1.412568</td>\n",
+                            "      <td>-0.747468</td>\n",
+                            "      <td>45.457314</td>\n",
+                            "    </tr>\n",
+                            "    <tr>\n",
+                            "      <th>2</th>\n",
+                            "      <td>1971</td>\n",
+                            "      <td>0.000544</td>\n",
+                            "      <td>1.438081</td>\n",
+                            "      <td>0.813280</td>\n",
+                            "      <td>-1.749974</td>\n",
+                            "      <td>-1.203509</td>\n",
+                            "      <td>0.049558</td>\n",
+                            "      <td>127.146548</td>\n",
+                            "    </tr>\n",
+                            "    <tr>\n",
+                            "      <th>3</th>\n",
+                            "      <td>1971</td>\n",
+                            "      <td>0.000701</td>\n",
+                            "      <td>1.768655</td>\n",
+                            "      <td>-0.177453</td>\n",
+                            "      <td>0.286602</td>\n",
+                            "      <td>0.391517</td>\n",
+                            "      <td>0.683669</td>\n",
+                            "      <td>22.604045</td>\n",
+                            "    </tr>\n",
+                            "    <tr>\n",
+                            "      <th>4</th>\n",
+                            "      <td>1971</td>\n",
+                            "      <td>0.000549</td>\n",
+                            "      <td>0.849970</td>\n",
+                            "      <td>-0.135337</td>\n",
+                            "      <td>0.735920</td>\n",
+                            "      <td>1.036247</td>\n",
+                            "      <td>-1.143436</td>\n",
+                            "      <td>170.226032</td>\n",
+                            "    </tr>\n",
+                            "  </tbody>\n",
+                            "</table>\n",
+                            "</div>"
+                        ],
                         "text/plain": [
-                            "prices + shares + Absorb[product_ids]"
+                            "   market_ids   weights    nodes0    nodes1    nodes2    nodes3    nodes4  \\\n",
+                            "0        1971  0.000543  1.192188  0.478777  0.980830 -0.824410  2.473301   \n",
+                            "1        1971  0.000723  1.497074 -2.026204 -1.741316  1.412568 -0.747468   \n",
+                            "2        1971  0.000544  1.438081  0.813280 -1.749974 -1.203509  0.049558   \n",
+                            "3        1971  0.000701  1.768655 -0.177453  0.286602  0.391517  0.683669   \n",
+                            "4        1971  0.000549  0.849970 -0.135337  0.735920  1.036247 -1.143436   \n",
+                            "\n",
+                            "       income  \n",
+                            "0  109.560369  \n",
+                            "1   45.457314  \n",
+                            "2  127.146548  \n",
+                            "3   22.604045  \n",
+                            "4  170.226032  "
                         ]
                     },
                     "execution_count": 5,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "absorb_formulation = pyblp.Formulation('prices + shares', absorb='product_ids')\n",
-                "absorb_formulation"
+                "blp_agent_data.head()"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 6,
+            "cell_type": "markdown",
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "array([[-0.01124832, -0.00052161],\n",
-                            "       [-0.00713476, -0.03144549],\n",
-                            "       [ 0.02367765, -0.01664996],\n",
-                            "       ...,\n",
-                            "       [ 0.03371995, -0.00779841],\n",
-                            "       [-0.00417404, -0.0117508 ],\n",
-                            "       [-0.01195648,  0.00666695]])"
-                        ]
-                    },
-                    "execution_count": 6,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
             "source": [
-                "demeaned_matrix = pyblp.build_matrix(absorb_formulation, product_data)\n",
-                "demeaned_matrix"
+                "[This tutorial](build_blp_instruments.ipynb) demonstrates how the instruments included in this dataset can be constructed from scratch."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/build_ownership-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/build_ownership-checkpoint.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9924885010822511%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -188,27 +188,27 @@
                 "single_ownership = pyblp.build_ownership(id_data, 'single')\n",
                 "single_ownership"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/data-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/data.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9799813988095238%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, insert: [(9, '*

 * *            "OrderedDict([('cell_type', 'markdown'), ('metadata', OrderedDict()), ('source', "*

 * *            "['[This tutorial](build_blp_instruments.ipynb) demonstrates how the instruments "*

 * *            "included in this dataset can be constructed from scratch.'])]))]}",*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -411,31 +411,38 @@
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "blp_agent_data.head()"
             ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "[This tutorial](build_blp_instruments.ipynb) demonstrates how the instruments included in this dataset can be constructed from scratch."
+            ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/data_to_dict-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/data_to_dict-checkpoint.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9885740803297699%*

 * *Differences: {"'cells'": "{1: {'metadata': {replace: OrderedDict([('scrolled', True)])}, 'outputs': {0: "*

 * *            '{\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': {insert: [(0, "In this '*

 * *            "example, we'll convert a dataset constructed by PyBLP into a dictionary that can more "*

 * *            'easily ingested by other Python packages. Note that you can also '*

 * *            '[pickle](https://docs.python.org/3/library/pickle.html#module-pickle) most PyBLP '*

 * *            'objects, which may be more c […]*

```diff
@@ -6,20 +6,22 @@
             "source": [
                 "# Converting Data into a Dictionary Example"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 1,
-            "metadata": {},
+            "metadata": {
+                "scrolled": true
+            },
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -34,17 +36,17 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this example, we'll convert a dataset constructed by PyBLP into a dictionary that can more easily ingested by other Python packages.\n",
+                "In this example, we'll convert a dataset constructed by PyBLP into a dictionary that can more easily ingested by other Python packages. Note that you can also [pickle](https://docs.python.org/3/library/pickle.html#module-pickle) most PyBLP objects, which may be more convenient.\n",
                 "\n",
-                "First we'll initialize a :class:`Problem` with the fake cereal data from :ref:`references:Nevo (2000)`."
+                "First we'll initialize a :class:`Problem` with the fake cereal data from :ref:`references:Nevo (2000a)`."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
             "metadata": {},
             "outputs": [
@@ -89,22 +91,22 @@
             "cell_type": "code",
             "execution_count": 3,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "rec.array([(['C01Q1'], [1], ['F1B04'], [], [], [], [], [0.], [-2.5e-01,  4.1e-02, -1.6e+00, -2.7e-01, -1.0e-02,  6.9e-03, -9.2e-01,  5.1e-03,  1.3e-01,  2.8e-01,  2.0e-01,  2.5e-01, -4.1e-03, -3.6e-02,  7.1e-02,  1.2e-02,  1.7e-02, -1.5e-02,  8.1e-02, -1.6e-02], [], [-0.], [], [], [0.1]),\n",
-                            "           (['C01Q1'], [1], ['F1B06'], [], [], [], [], [0.], [-2.1e-01,  5.7e-02, -1.0e+01,  1.5e-01,  4.0e-02,  6.1e-03,  1.1e+00,  8.6e-02,  1.1e-01, -2.7e-02, -1.2e+00, -1.3e-01,  2.6e-03, -6.8e-03, -4.5e-02,  6.7e-05,  3.1e-02,  5.8e-03, -3.2e-02, -1.1e-02], [], [-0.], [], [], [0.1]),\n",
-                            "           (['C01Q1'], [1], ['F1B07'], [], [], [], [], [0.], [-2.1e-01,  4.6e-02, -2.3e+00, -3.0e-02,  2.4e-03, -1.3e-02,  3.3e-01, -1.7e-01, -2.3e-01,  3.1e-01,  1.0e+00,  2.0e-01,  9.9e-04,  1.8e-02,  8.2e-02,  3.5e-02,  2.8e-02,  1.3e-02,  4.7e-02,  2.7e-02], [], [ 0.], [], [], [0.1]),\n",
+                            "rec.array([(['C01Q1'], [1], ['F1B04'], [], [], ['F1B04'], [], [], [0.], [-2.5e-01,  4.1e-02, -1.6e+00, -2.7e-01, -1.0e-02,  6.9e-03, -9.2e-01,  5.1e-03,  1.3e-01,  2.8e-01,  2.0e-01,  2.5e-01, -4.1e-03, -3.6e-02,  7.1e-02,  1.2e-02,  1.7e-02, -1.5e-02,  8.1e-02, -1.6e-02], [], [-0.], [], [], [0.1]),\n",
+                            "           (['C01Q1'], [1], ['F1B06'], [], [], ['F1B06'], [], [], [0.], [-2.1e-01,  5.7e-02, -1.0e+01,  1.5e-01,  4.0e-02,  6.1e-03,  1.1e+00,  8.6e-02,  1.1e-01, -2.7e-02, -1.2e+00, -1.3e-01,  2.6e-03, -6.8e-03, -4.5e-02,  6.7e-05,  3.1e-02,  5.8e-03, -3.2e-02, -1.1e-02], [], [-0.], [], [], [0.1]),\n",
+                            "           (['C01Q1'], [1], ['F1B07'], [], [], ['F1B07'], [], [], [0.], [-2.1e-01,  4.6e-02, -2.3e+00, -3.0e-02,  2.4e-03, -1.3e-02,  3.3e-01, -1.7e-01, -2.3e-01,  3.1e-01,  1.0e+00,  2.0e-01,  9.9e-04,  1.8e-02,  8.2e-02,  3.5e-02,  2.8e-02,  1.3e-02,  4.7e-02,  2.7e-02], [], [ 0.], [], [], [0.1]),\n",
                             "           ...,\n",
-                            "           (['C65Q2'], [4], ['F4B10'], [], [], [], [], [0.], [-1.2e-01, -3.2e-04, -1.1e+00,  1.8e-01,  3.6e-02, -1.9e-02,  2.4e-01,  5.4e-02, -3.2e-01,  8.7e-02,  2.7e+00,  1.6e-01,  8.8e-04,  3.8e-02,  1.9e-02, -5.2e-02, -1.8e-02,  3.7e-02, -5.8e-02,  3.6e-02], [], [ 0.], [], [], [0.1]),\n",
-                            "           (['C65Q2'], [4], ['F4B12'], [], [], [], [], [0.], [-2.0e-01,  3.3e-04, -5.1e-01, -4.5e-03,  3.2e-02,  6.1e-03,  5.7e-01,  2.3e-02,  1.1e-01,  1.9e-01,  2.1e+00,  1.3e-01, -8.1e-03, -1.2e-02, -3.6e-02, -4.3e-03, -1.7e-02, -6.6e-03,  7.2e-03, -1.5e-02], [], [-0.], [], [], [0.1]),\n",
-                            "           (['C65Q2'], [6], ['F6B18'], [], [], [], [], [0.], [-1.4e-01,  3.5e-03, -2.9e-01,  2.9e-01,  3.9e-02,  2.0e-02, -1.9e+00, -4.0e-02,  3.8e-01,  1.1e-01,  3.4e+00,  1.1e-01, -6.1e-03, -1.2e-03, -4.7e-02, -2.4e-02, -2.1e-02, -2.9e-02, -2.6e-02, -2.5e-02], [], [-0.], [], [], [0.1])],\n",
-                            "          dtype=[('market_ids', 'O', (1,)), ('firm_ids', 'O', (1,)), ('demand_ids', 'O', (1,)), ('supply_ids', 'O', (0,)), ('nesting_ids', 'O', (0,)), ('clustering_ids', 'O', (0,)), ('ownership', '<f8', (0,)), ('shares', '<f8', (1,)), ('ZD', '<f8', (20,)), ('ZS', '<f8', (0,)), (((prices,), 'X1'), '<f8', (1,)), (((), 'X2'), '<f8', (0,)), (((), 'X3'), '<f8', (0,)), ('prices', '<f8', (1,))])"
+                            "           (['C65Q2'], [4], ['F4B10'], [], [], ['F4B10'], [], [], [0.], [-1.2e-01, -3.2e-04, -1.1e+00,  1.8e-01,  3.6e-02, -1.9e-02,  2.4e-01,  5.4e-02, -3.2e-01,  8.7e-02,  2.7e+00,  1.6e-01,  8.8e-04,  3.8e-02,  1.9e-02, -5.2e-02, -1.8e-02,  3.7e-02, -5.8e-02,  3.6e-02], [], [ 0.], [], [], [0.1]),\n",
+                            "           (['C65Q2'], [4], ['F4B12'], [], [], ['F4B12'], [], [], [0.], [-2.0e-01,  3.3e-04, -5.1e-01, -4.5e-03,  3.2e-02,  6.1e-03,  5.7e-01,  2.3e-02,  1.1e-01,  1.9e-01,  2.1e+00,  1.3e-01, -8.1e-03, -1.2e-02, -3.6e-02, -4.3e-03, -1.7e-02, -6.6e-03,  7.2e-03, -1.5e-02], [], [-0.], [], [], [0.1]),\n",
+                            "           (['C65Q2'], [6], ['F6B18'], [], [], ['F6B18'], [], [], [0.], [-1.4e-01,  3.5e-03, -2.9e-01,  2.9e-01,  3.9e-02,  2.0e-02, -1.9e+00, -4.0e-02,  3.8e-01,  1.1e-01,  3.4e+00,  1.1e-01, -6.1e-03, -1.2e-03, -4.7e-02, -2.4e-02, -2.1e-02, -2.9e-02, -2.6e-02, -2.5e-02], [], [-0.], [], [], [0.1])],\n",
+                            "          dtype=[('market_ids', 'O', (1,)), ('firm_ids', 'O', (1,)), ('demand_ids', 'O', (1,)), ('supply_ids', 'O', (0,)), ('nesting_ids', 'O', (0,)), ('product_ids', 'O', (1,)), ('clustering_ids', 'O', (0,)), ('ownership', '<f8', (0,)), ('shares', '<f8', (1,)), ('ZD', '<f8', (20,)), ('ZS', '<f8', (0,)), (((prices,), 'X1'), '<f8', (1,)), (((), 'X2'), '<f8', (0,)), (((), 'X3'), '<f8', (0,)), ('prices', '<f8', (1,))])"
                         ]
                     },
                     "execution_count": 3,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -126,15 +128,15 @@
             "execution_count": 4,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
-                        "{'market_ids': 2256, 'firm_ids': 2256, 'demand_ids': 2256, 'shares': 2256, 'ZD0': 2256, 'ZD1': 2256, 'ZD2': 2256, 'ZD3': 2256, 'ZD4': 2256, 'ZD5': 2256, 'ZD6': 2256, 'ZD7': 2256, 'ZD8': 2256, 'ZD9': 2256, 'ZD10': 2256, 'ZD11': 2256, 'ZD12': 2256, 'ZD13': 2256, 'ZD14': 2256, 'ZD15': 2256, 'ZD16': 2256, 'ZD17': 2256, 'ZD18': 2256, 'ZD19': 2256, 'X1': 2256, 'prices': 2256}\n"
+                        "{'market_ids': 2256, 'firm_ids': 2256, 'demand_ids': 2256, 'product_ids': 2256, 'shares': 2256, 'ZD0': 2256, 'ZD1': 2256, 'ZD2': 2256, 'ZD3': 2256, 'ZD4': 2256, 'ZD5': 2256, 'ZD6': 2256, 'ZD7': 2256, 'ZD8': 2256, 'ZD9': 2256, 'ZD10': 2256, 'ZD11': 2256, 'ZD12': 2256, 'ZD13': 2256, 'ZD14': 2256, 'ZD15': 2256, 'ZD16': 2256, 'ZD17': 2256, 'ZD18': 2256, 'ZD19': 2256, 'X1': 2256, 'prices': 2256}\n"
                     ]
                 },
                 {
                     "data": {
                         "text/html": [
                             "<div>\n",
                             "<style scoped>\n",
@@ -153,21 +155,21 @@
                             "<table border=\"1\" class=\"dataframe\">\n",
                             "  <thead>\n",
                             "    <tr style=\"text-align: right;\">\n",
                             "      <th></th>\n",
                             "      <th>market_ids</th>\n",
                             "      <th>firm_ids</th>\n",
                             "      <th>demand_ids</th>\n",
+                            "      <th>product_ids</th>\n",
                             "      <th>shares</th>\n",
                             "      <th>ZD0</th>\n",
                             "      <th>ZD1</th>\n",
                             "      <th>ZD2</th>\n",
                             "      <th>ZD3</th>\n",
                             "      <th>ZD4</th>\n",
-                            "      <th>ZD5</th>\n",
                             "      <th>...</th>\n",
                             "      <th>ZD12</th>\n",
                             "      <th>ZD13</th>\n",
                             "      <th>ZD14</th>\n",
                             "      <th>ZD15</th>\n",
                             "      <th>ZD16</th>\n",
                             "      <th>ZD17</th>\n",
@@ -179,21 +181,21 @@
                             "  </thead>\n",
                             "  <tbody>\n",
                             "    <tr>\n",
                             "      <th>0</th>\n",
                             "      <td>C01Q1</td>\n",
                             "      <td>1</td>\n",
                             "      <td>F1B04</td>\n",
+                            "      <td>F1B04</td>\n",
                             "      <td>0.012417</td>\n",
                             "      <td>-0.249518</td>\n",
                             "      <td>0.040943</td>\n",
                             "      <td>-1.577566</td>\n",
                             "      <td>-0.269073</td>\n",
                             "      <td>-0.010004</td>\n",
-                            "      <td>0.006934</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.004142</td>\n",
                             "      <td>-0.035593</td>\n",
                             "      <td>0.070587</td>\n",
                             "      <td>0.011768</td>\n",
                             "      <td>0.017287</td>\n",
                             "      <td>-0.015031</td>\n",
@@ -203,21 +205,21 @@
                             "      <td>0.072088</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>1</th>\n",
                             "      <td>C01Q1</td>\n",
                             "      <td>1</td>\n",
                             "      <td>F1B06</td>\n",
+                            "      <td>F1B06</td>\n",
                             "      <td>0.007809</td>\n",
                             "      <td>-0.205951</td>\n",
                             "      <td>0.057100</td>\n",
                             "      <td>-10.383954</td>\n",
                             "      <td>0.150476</td>\n",
                             "      <td>0.039816</td>\n",
-                            "      <td>0.006058</td>\n",
                             "      <td>...</td>\n",
                             "      <td>0.002585</td>\n",
                             "      <td>-0.006776</td>\n",
                             "      <td>-0.045453</td>\n",
                             "      <td>0.000067</td>\n",
                             "      <td>0.031229</td>\n",
                             "      <td>0.005841</td>\n",
@@ -227,21 +229,21 @@
                             "      <td>0.114178</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2</th>\n",
                             "      <td>C01Q1</td>\n",
                             "      <td>1</td>\n",
                             "      <td>F1B07</td>\n",
+                            "      <td>F1B07</td>\n",
                             "      <td>0.012995</td>\n",
                             "      <td>-0.212031</td>\n",
                             "      <td>0.046246</td>\n",
                             "      <td>-2.278160</td>\n",
                             "      <td>-0.029976</td>\n",
                             "      <td>0.002390</td>\n",
-                            "      <td>-0.013229</td>\n",
                             "      <td>...</td>\n",
                             "      <td>0.000992</td>\n",
                             "      <td>0.018425</td>\n",
                             "      <td>0.081555</td>\n",
                             "      <td>0.034975</td>\n",
                             "      <td>0.027932</td>\n",
                             "      <td>0.013156</td>\n",
@@ -251,21 +253,21 @@
                             "      <td>0.132391</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>3</th>\n",
                             "      <td>C01Q1</td>\n",
                             "      <td>1</td>\n",
                             "      <td>F1B09</td>\n",
+                            "      <td>F1B09</td>\n",
                             "      <td>0.005770</td>\n",
                             "      <td>-0.170725</td>\n",
                             "      <td>0.049143</td>\n",
                             "      <td>-1.159784</td>\n",
                             "      <td>-0.244789</td>\n",
                             "      <td>0.002848</td>\n",
-                            "      <td>-0.019942</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.004274</td>\n",
                             "      <td>0.026440</td>\n",
                             "      <td>0.064169</td>\n",
                             "      <td>0.021496</td>\n",
                             "      <td>0.032372</td>\n",
                             "      <td>0.033063</td>\n",
@@ -275,21 +277,21 @@
                             "      <td>0.130344</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>4</th>\n",
                             "      <td>C01Q1</td>\n",
                             "      <td>1</td>\n",
                             "      <td>F1B11</td>\n",
+                            "      <td>F1B11</td>\n",
                             "      <td>0.017934</td>\n",
                             "      <td>-0.164983</td>\n",
                             "      <td>0.047168</td>\n",
                             "      <td>-4.737563</td>\n",
                             "      <td>-0.070873</td>\n",
                             "      <td>0.012273</td>\n",
-                            "      <td>0.017403</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.004694</td>\n",
                             "      <td>-0.029179</td>\n",
                             "      <td>-0.000454</td>\n",
                             "      <td>-0.045272</td>\n",
                             "      <td>-0.025446</td>\n",
                             "      <td>-0.006794</td>\n",
@@ -323,21 +325,21 @@
                             "      <td>...</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2251</th>\n",
                             "      <td>C65Q2</td>\n",
                             "      <td>3</td>\n",
                             "      <td>F3B14</td>\n",
+                            "      <td>F3B14</td>\n",
                             "      <td>0.024702</td>\n",
                             "      <td>-0.126940</td>\n",
                             "      <td>0.002240</td>\n",
                             "      <td>-1.067171</td>\n",
                             "      <td>0.150626</td>\n",
                             "      <td>0.037091</td>\n",
-                            "      <td>0.021697</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.004787</td>\n",
                             "      <td>-0.012775</td>\n",
                             "      <td>-0.059399</td>\n",
                             "      <td>0.043775</td>\n",
                             "      <td>0.059339</td>\n",
                             "      <td>-0.021934</td>\n",
@@ -347,21 +349,21 @@
                             "      <td>0.126086</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2252</th>\n",
                             "      <td>C65Q2</td>\n",
                             "      <td>4</td>\n",
                             "      <td>F4B02</td>\n",
+                            "      <td>F4B02</td>\n",
                             "      <td>0.007914</td>\n",
                             "      <td>-0.109756</td>\n",
                             "      <td>0.011192</td>\n",
                             "      <td>0.458133</td>\n",
                             "      <td>0.066193</td>\n",
                             "      <td>0.006838</td>\n",
-                            "      <td>-0.024803</td>\n",
                             "      <td>...</td>\n",
                             "      <td>0.009385</td>\n",
                             "      <td>0.037487</td>\n",
                             "      <td>0.086225</td>\n",
                             "      <td>0.060856</td>\n",
                             "      <td>0.028264</td>\n",
                             "      <td>0.051264</td>\n",
@@ -371,21 +373,21 @@
                             "      <td>0.199167</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2253</th>\n",
                             "      <td>C65Q2</td>\n",
                             "      <td>4</td>\n",
                             "      <td>F4B10</td>\n",
+                            "      <td>F4B10</td>\n",
                             "      <td>0.002229</td>\n",
                             "      <td>-0.119689</td>\n",
                             "      <td>-0.000324</td>\n",
                             "      <td>-1.109521</td>\n",
                             "      <td>0.175027</td>\n",
                             "      <td>0.036227</td>\n",
-                            "      <td>-0.018574</td>\n",
                             "      <td>...</td>\n",
                             "      <td>0.000884</td>\n",
                             "      <td>0.037634</td>\n",
                             "      <td>0.019278</td>\n",
                             "      <td>-0.052403</td>\n",
                             "      <td>-0.018107</td>\n",
                             "      <td>0.036733</td>\n",
@@ -395,21 +397,21 @@
                             "      <td>0.137017</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2254</th>\n",
                             "      <td>C65Q2</td>\n",
                             "      <td>4</td>\n",
                             "      <td>F4B12</td>\n",
+                            "      <td>F4B12</td>\n",
                             "      <td>0.011463</td>\n",
                             "      <td>-0.201890</td>\n",
                             "      <td>0.000334</td>\n",
                             "      <td>-0.507311</td>\n",
                             "      <td>-0.004538</td>\n",
                             "      <td>0.031569</td>\n",
-                            "      <td>0.006083</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.008093</td>\n",
                             "      <td>-0.011750</td>\n",
                             "      <td>-0.036333</td>\n",
                             "      <td>-0.004333</td>\n",
                             "      <td>-0.017427</td>\n",
                             "      <td>-0.006647</td>\n",
@@ -419,79 +421,79 @@
                             "      <td>0.100174</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2255</th>\n",
                             "      <td>C65Q2</td>\n",
                             "      <td>6</td>\n",
                             "      <td>F6B18</td>\n",
+                            "      <td>F6B18</td>\n",
                             "      <td>0.026208</td>\n",
                             "      <td>-0.139453</td>\n",
                             "      <td>0.003468</td>\n",
                             "      <td>-0.285143</td>\n",
                             "      <td>0.291132</td>\n",
                             "      <td>0.039259</td>\n",
-                            "      <td>0.020151</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.006138</td>\n",
                             "      <td>-0.001181</td>\n",
                             "      <td>-0.046888</td>\n",
                             "      <td>-0.023637</td>\n",
                             "      <td>-0.021410</td>\n",
                             "      <td>-0.029402</td>\n",
                             "      <td>-0.025971</td>\n",
                             "      <td>-0.025435</td>\n",
                             "      <td>-0.011956</td>\n",
                             "      <td>0.127557</td>\n",
                             "    </tr>\n",
                             "  </tbody>\n",
                             "</table>\n",
-                            "<p>2256 rows \u00d7 26 columns</p>\n",
+                            "<p>2256 rows \u00d7 27 columns</p>\n",
                             "</div>"
                         ],
                         "text/plain": [
-                            "     market_ids firm_ids demand_ids    shares       ZD0       ZD1        ZD2  \\\n",
-                            "0         C01Q1        1      F1B04  0.012417 -0.249518  0.040943  -1.577566   \n",
-                            "1         C01Q1        1      F1B06  0.007809 -0.205951  0.057100 -10.383954   \n",
-                            "2         C01Q1        1      F1B07  0.012995 -0.212031  0.046246  -2.278160   \n",
-                            "3         C01Q1        1      F1B09  0.005770 -0.170725  0.049143  -1.159784   \n",
-                            "4         C01Q1        1      F1B11  0.017934 -0.164983  0.047168  -4.737563   \n",
-                            "...         ...      ...        ...       ...       ...       ...        ...   \n",
-                            "2251      C65Q2        3      F3B14  0.024702 -0.126940  0.002240  -1.067171   \n",
-                            "2252      C65Q2        4      F4B02  0.007914 -0.109756  0.011192   0.458133   \n",
-                            "2253      C65Q2        4      F4B10  0.002229 -0.119689 -0.000324  -1.109521   \n",
-                            "2254      C65Q2        4      F4B12  0.011463 -0.201890  0.000334  -0.507311   \n",
-                            "2255      C65Q2        6      F6B18  0.026208 -0.139453  0.003468  -0.285143   \n",
+                            "     market_ids firm_ids demand_ids product_ids    shares       ZD0       ZD1  \\\n",
+                            "0         C01Q1        1      F1B04       F1B04  0.012417 -0.249518  0.040943   \n",
+                            "1         C01Q1        1      F1B06       F1B06  0.007809 -0.205951  0.057100   \n",
+                            "2         C01Q1        1      F1B07       F1B07  0.012995 -0.212031  0.046246   \n",
+                            "3         C01Q1        1      F1B09       F1B09  0.005770 -0.170725  0.049143   \n",
+                            "4         C01Q1        1      F1B11       F1B11  0.017934 -0.164983  0.047168   \n",
+                            "...         ...      ...        ...         ...       ...       ...       ...   \n",
+                            "2251      C65Q2        3      F3B14       F3B14  0.024702 -0.126940  0.002240   \n",
+                            "2252      C65Q2        4      F4B02       F4B02  0.007914 -0.109756  0.011192   \n",
+                            "2253      C65Q2        4      F4B10       F4B10  0.002229 -0.119689 -0.000324   \n",
+                            "2254      C65Q2        4      F4B12       F4B12  0.011463 -0.201890  0.000334   \n",
+                            "2255      C65Q2        6      F6B18       F6B18  0.026208 -0.139453  0.003468   \n",
                             "\n",
-                            "           ZD3       ZD4       ZD5  ...      ZD12      ZD13      ZD14  \\\n",
-                            "0    -0.269073 -0.010004  0.006934  ... -0.004142 -0.035593  0.070587   \n",
-                            "1     0.150476  0.039816  0.006058  ...  0.002585 -0.006776 -0.045453   \n",
-                            "2    -0.029976  0.002390 -0.013229  ...  0.000992  0.018425  0.081555   \n",
-                            "3    -0.244789  0.002848 -0.019942  ... -0.004274  0.026440  0.064169   \n",
-                            "4    -0.070873  0.012273  0.017403  ... -0.004694 -0.029179 -0.000454   \n",
-                            "...        ...       ...       ...  ...       ...       ...       ...   \n",
-                            "2251  0.150626  0.037091  0.021697  ... -0.004787 -0.012775 -0.059399   \n",
-                            "2252  0.066193  0.006838 -0.024803  ...  0.009385  0.037487  0.086225   \n",
-                            "2253  0.175027  0.036227 -0.018574  ...  0.000884  0.037634  0.019278   \n",
-                            "2254 -0.004538  0.031569  0.006083  ... -0.008093 -0.011750 -0.036333   \n",
-                            "2255  0.291132  0.039259  0.020151  ... -0.006138 -0.001181 -0.046888   \n",
+                            "            ZD2       ZD3       ZD4  ...      ZD12      ZD13      ZD14  \\\n",
+                            "0     -1.577566 -0.269073 -0.010004  ... -0.004142 -0.035593  0.070587   \n",
+                            "1    -10.383954  0.150476  0.039816  ...  0.002585 -0.006776 -0.045453   \n",
+                            "2     -2.278160 -0.029976  0.002390  ...  0.000992  0.018425  0.081555   \n",
+                            "3     -1.159784 -0.244789  0.002848  ... -0.004274  0.026440  0.064169   \n",
+                            "4     -4.737563 -0.070873  0.012273  ... -0.004694 -0.029179 -0.000454   \n",
+                            "...         ...       ...       ...  ...       ...       ...       ...   \n",
+                            "2251  -1.067171  0.150626  0.037091  ... -0.004787 -0.012775 -0.059399   \n",
+                            "2252   0.458133  0.066193  0.006838  ...  0.009385  0.037487  0.086225   \n",
+                            "2253  -1.109521  0.175027  0.036227  ...  0.000884  0.037634  0.019278   \n",
+                            "2254  -0.507311 -0.004538  0.031569  ... -0.008093 -0.011750 -0.036333   \n",
+                            "2255  -0.285143  0.291132  0.039259  ... -0.006138 -0.001181 -0.046888   \n",
                             "\n",
                             "          ZD15      ZD16      ZD17      ZD18      ZD19        X1    prices  \n",
                             "0     0.011768  0.017287 -0.015031  0.081201 -0.015833 -0.011248  0.072088  \n",
                             "1     0.000067  0.031229  0.005841 -0.032121 -0.010614 -0.007135  0.114178  \n",
                             "2     0.034975  0.027932  0.013156  0.047484  0.026800  0.023678  0.132391  \n",
                             "3     0.021496  0.032372  0.033063  0.045501  0.036154  0.029725  0.130344  \n",
                             "4    -0.045272 -0.025446 -0.006794 -0.007560 -0.011364 -0.015585  0.154823  \n",
                             "...        ...       ...       ...       ...       ...       ...       ...  \n",
                             "2251  0.043775  0.059339 -0.021934  0.034592 -0.021052 -0.017337  0.126086  \n",
                             "2252  0.060856  0.028264  0.051264  0.032965  0.033324  0.044542  0.199167  \n",
                             "2253 -0.052403 -0.018107  0.036733 -0.057647  0.035662  0.033720  0.137017  \n",
                             "2254 -0.004333 -0.017427 -0.006647  0.007228 -0.015403 -0.004174  0.100174  \n",
                             "2255 -0.023637 -0.021410 -0.029402 -0.025971 -0.025435 -0.011956  0.127557  \n",
                             "\n",
-                            "[2256 rows x 26 columns]"
+                            "[2256 rows x 27 columns]"
                         ]
                     },
                     "execution_count": 4,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -502,27 +504,27 @@
                 "df = pd.DataFrame(pyblp.data_to_dict(problem.products))\n",
                 "df"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/formulation-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/formulation-checkpoint.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9901579034391534%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 4: {\'source\': '*

 * *            '["Next, we\'ll design a second matrix with an intercept, with first- and '*

 * *            'second-degree size terms, with categorical product IDs and years, and with the '*

 * *            'interaction of the last two. The first formulation will include the fixed effects as '*

 * *            'indicator variables, and the second will absorb them."]}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 […]*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -57,15 +57,15 @@
                 "formulation"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Next, we'll design a second matrix with an intercept, with first- and second-degree size terms, with categorical product IDs and years, and with the interaction of the last two. The first formulation will include include the fixed effects as indicator variables, and the second will absorb them."
+                "Next, we'll design a second matrix with an intercept, with first- and second-degree size terms, with categorical product IDs and years, and with the interaction of the last two. The first formulation will include the fixed effects as indicator variables, and the second will absorb them."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 3,
             "metadata": {},
             "outputs": [
@@ -133,27 +133,27 @@
                 "formulation = pyblp.Formulation('year:(log(income) + C(education))')\n",
                 "formulation"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/integration-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/integration-checkpoint.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9924293154761905%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -84,27 +84,27 @@
                 "integration = pyblp.Integration('grid', size=7)\n",
                 "integration"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/iteration-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/iteration-checkpoint.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9924618675595238%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -115,27 +115,27 @@
                 "iteration = pyblp.Iteration(custom_method)\n",
                 "iteration"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/optimization-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/optimization-checkpoint.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9924618675595238%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -111,27 +111,27 @@
                 "optimization = pyblp.Optimization(custom_method, compute_gradient=False)\n",
                 "optimization"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/.ipynb_checkpoints/parallel-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/api/.ipynb_checkpoints/parallel-checkpoint.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9888164083880678%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': '*

 * *            '["In this example, we\'ll use parallel processing to compute elasticities '*

 * *            'market-by-market for a simple Logit problem configured with some of the fake cereal '*

 * *            'data from :ref:`references:Nevo (2000a)`."]}, 3: {\'outputs\': {0: {\'data\': '*

 * *            "{'text/plain': {insert: [(1, '==========================================\\n'), (2, "*

 * *            "'GMM   Objective  Cl […]*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -32,32 +32,32 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this example, we'll use parallel processing to compute elasticities market-by-market for a simple Logit problem configured with some of the fake cereal data from :ref:`references:Nevo (2000)`."
+                "In this example, we'll use parallel processing to compute elasticities market-by-market for a simple Logit problem configured with some of the fake cereal data from :ref:`references:Nevo (2000a)`."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "=================================\n",
-                            "GMM   Objective  Weighting Matrix\n",
-                            "Step    Value    Condition Number\n",
-                            "----  ---------  ----------------\n",
-                            " 2    +1.9E+02       +5.7E+07    \n",
-                            "=================================\n",
+                            "==========================================\n",
+                            "GMM   Objective  Clipped  Weighting Matrix\n",
+                            "Step    Value    Shares   Condition Number\n",
+                            "----  ---------  -------  ----------------\n",
+                            " 2    +1.9E+02      0         +5.7E+07    \n",
+                            "==========================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
                             "========================\n",
                             "Computation   Objective \n",
                             "   Time      Evaluations\n",
                             "-----------  -----------\n",
                             " 00:00:00         2     \n",
@@ -95,15 +95,15 @@
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "Starting a pool of 2 processes ...\n",
                         "Started the process pool after 00:00:00.\n",
                         "Computing elasticities with respect to prices ...\n",
-                        "Finished after 00:00:03.\n",
+                        "Finished after 00:00:02.\n",
                         "\n",
                         "Terminating the pool of 2 processes ...\n",
                         "Terminated the process pool after 00:00:00.\n"
                     ]
                 }
             ],
             "source": [
@@ -120,27 +120,27 @@
                 "\n",
                 "If the problem were much larger, running :meth:`Problem.solve` and :meth:`ProblemResults.compute_elasticities` under the ``with`` statement could substantially speed up estimation and elasticity computation."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/build_id_data.ipynb` & `pyblp-1.0.0/docs/notebooks/api/build_id_data.ipynb`

 * *Files 6% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9923642113095238%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -68,27 +68,27 @@
                 "id_data = pyblp.build_id_data(T=2, J=5, F=4)\n",
                 "id_data"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/build_integration.ipynb` & `pyblp-1.0.0/docs/notebooks/api/build_integration.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9924618675595238%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -111,27 +111,27 @@
             "source": [
                 "If we wanted to construct nodes and weights for each market, we could call :func:`build_integration` once for each market, add a column of market IDs, and stack the arrays."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/build_ownership.ipynb` & `pyblp-1.0.0/docs/notebooks/api/build_ownership.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9924885010822511%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -188,27 +188,27 @@
                 "single_ownership = pyblp.build_ownership(id_data, 'single')\n",
                 "single_ownership"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/data_to_dict.ipynb` & `pyblp-1.0.0/docs/notebooks/api/data_to_dict.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9885740803297699%*

 * *Differences: {"'cells'": "{1: {'metadata': {replace: OrderedDict([('scrolled', True)])}, 'outputs': {0: "*

 * *            '{\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': {insert: [(0, "In this '*

 * *            "example, we'll convert a dataset constructed by PyBLP into a dictionary that can more "*

 * *            'easily ingested by other Python packages. Note that you can also '*

 * *            '[pickle](https://docs.python.org/3/library/pickle.html#module-pickle) most PyBLP '*

 * *            'objects, which may be more c […]*

```diff
@@ -6,20 +6,22 @@
             "source": [
                 "# Converting Data into a Dictionary Example"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 1,
-            "metadata": {},
+            "metadata": {
+                "scrolled": true
+            },
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -34,17 +36,17 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this example, we'll convert a dataset constructed by PyBLP into a dictionary that can more easily ingested by other Python packages.\n",
+                "In this example, we'll convert a dataset constructed by PyBLP into a dictionary that can more easily ingested by other Python packages. Note that you can also [pickle](https://docs.python.org/3/library/pickle.html#module-pickle) most PyBLP objects, which may be more convenient.\n",
                 "\n",
-                "First we'll initialize a :class:`Problem` with the fake cereal data from :ref:`references:Nevo (2000)`."
+                "First we'll initialize a :class:`Problem` with the fake cereal data from :ref:`references:Nevo (2000a)`."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
             "metadata": {},
             "outputs": [
@@ -89,22 +91,22 @@
             "cell_type": "code",
             "execution_count": 3,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "rec.array([(['C01Q1'], [1], ['F1B04'], [], [], [], [], [0.], [-2.5e-01,  4.1e-02, -1.6e+00, -2.7e-01, -1.0e-02,  6.9e-03, -9.2e-01,  5.1e-03,  1.3e-01,  2.8e-01,  2.0e-01,  2.5e-01, -4.1e-03, -3.6e-02,  7.1e-02,  1.2e-02,  1.7e-02, -1.5e-02,  8.1e-02, -1.6e-02], [], [-0.], [], [], [0.1]),\n",
-                            "           (['C01Q1'], [1], ['F1B06'], [], [], [], [], [0.], [-2.1e-01,  5.7e-02, -1.0e+01,  1.5e-01,  4.0e-02,  6.1e-03,  1.1e+00,  8.6e-02,  1.1e-01, -2.7e-02, -1.2e+00, -1.3e-01,  2.6e-03, -6.8e-03, -4.5e-02,  6.7e-05,  3.1e-02,  5.8e-03, -3.2e-02, -1.1e-02], [], [-0.], [], [], [0.1]),\n",
-                            "           (['C01Q1'], [1], ['F1B07'], [], [], [], [], [0.], [-2.1e-01,  4.6e-02, -2.3e+00, -3.0e-02,  2.4e-03, -1.3e-02,  3.3e-01, -1.7e-01, -2.3e-01,  3.1e-01,  1.0e+00,  2.0e-01,  9.9e-04,  1.8e-02,  8.2e-02,  3.5e-02,  2.8e-02,  1.3e-02,  4.7e-02,  2.7e-02], [], [ 0.], [], [], [0.1]),\n",
+                            "rec.array([(['C01Q1'], [1], ['F1B04'], [], [], ['F1B04'], [], [], [0.], [-2.5e-01,  4.1e-02, -1.6e+00, -2.7e-01, -1.0e-02,  6.9e-03, -9.2e-01,  5.1e-03,  1.3e-01,  2.8e-01,  2.0e-01,  2.5e-01, -4.1e-03, -3.6e-02,  7.1e-02,  1.2e-02,  1.7e-02, -1.5e-02,  8.1e-02, -1.6e-02], [], [-0.], [], [], [0.1]),\n",
+                            "           (['C01Q1'], [1], ['F1B06'], [], [], ['F1B06'], [], [], [0.], [-2.1e-01,  5.7e-02, -1.0e+01,  1.5e-01,  4.0e-02,  6.1e-03,  1.1e+00,  8.6e-02,  1.1e-01, -2.7e-02, -1.2e+00, -1.3e-01,  2.6e-03, -6.8e-03, -4.5e-02,  6.7e-05,  3.1e-02,  5.8e-03, -3.2e-02, -1.1e-02], [], [-0.], [], [], [0.1]),\n",
+                            "           (['C01Q1'], [1], ['F1B07'], [], [], ['F1B07'], [], [], [0.], [-2.1e-01,  4.6e-02, -2.3e+00, -3.0e-02,  2.4e-03, -1.3e-02,  3.3e-01, -1.7e-01, -2.3e-01,  3.1e-01,  1.0e+00,  2.0e-01,  9.9e-04,  1.8e-02,  8.2e-02,  3.5e-02,  2.8e-02,  1.3e-02,  4.7e-02,  2.7e-02], [], [ 0.], [], [], [0.1]),\n",
                             "           ...,\n",
-                            "           (['C65Q2'], [4], ['F4B10'], [], [], [], [], [0.], [-1.2e-01, -3.2e-04, -1.1e+00,  1.8e-01,  3.6e-02, -1.9e-02,  2.4e-01,  5.4e-02, -3.2e-01,  8.7e-02,  2.7e+00,  1.6e-01,  8.8e-04,  3.8e-02,  1.9e-02, -5.2e-02, -1.8e-02,  3.7e-02, -5.8e-02,  3.6e-02], [], [ 0.], [], [], [0.1]),\n",
-                            "           (['C65Q2'], [4], ['F4B12'], [], [], [], [], [0.], [-2.0e-01,  3.3e-04, -5.1e-01, -4.5e-03,  3.2e-02,  6.1e-03,  5.7e-01,  2.3e-02,  1.1e-01,  1.9e-01,  2.1e+00,  1.3e-01, -8.1e-03, -1.2e-02, -3.6e-02, -4.3e-03, -1.7e-02, -6.6e-03,  7.2e-03, -1.5e-02], [], [-0.], [], [], [0.1]),\n",
-                            "           (['C65Q2'], [6], ['F6B18'], [], [], [], [], [0.], [-1.4e-01,  3.5e-03, -2.9e-01,  2.9e-01,  3.9e-02,  2.0e-02, -1.9e+00, -4.0e-02,  3.8e-01,  1.1e-01,  3.4e+00,  1.1e-01, -6.1e-03, -1.2e-03, -4.7e-02, -2.4e-02, -2.1e-02, -2.9e-02, -2.6e-02, -2.5e-02], [], [-0.], [], [], [0.1])],\n",
-                            "          dtype=[('market_ids', 'O', (1,)), ('firm_ids', 'O', (1,)), ('demand_ids', 'O', (1,)), ('supply_ids', 'O', (0,)), ('nesting_ids', 'O', (0,)), ('clustering_ids', 'O', (0,)), ('ownership', '<f8', (0,)), ('shares', '<f8', (1,)), ('ZD', '<f8', (20,)), ('ZS', '<f8', (0,)), (((prices,), 'X1'), '<f8', (1,)), (((), 'X2'), '<f8', (0,)), (((), 'X3'), '<f8', (0,)), ('prices', '<f8', (1,))])"
+                            "           (['C65Q2'], [4], ['F4B10'], [], [], ['F4B10'], [], [], [0.], [-1.2e-01, -3.2e-04, -1.1e+00,  1.8e-01,  3.6e-02, -1.9e-02,  2.4e-01,  5.4e-02, -3.2e-01,  8.7e-02,  2.7e+00,  1.6e-01,  8.8e-04,  3.8e-02,  1.9e-02, -5.2e-02, -1.8e-02,  3.7e-02, -5.8e-02,  3.6e-02], [], [ 0.], [], [], [0.1]),\n",
+                            "           (['C65Q2'], [4], ['F4B12'], [], [], ['F4B12'], [], [], [0.], [-2.0e-01,  3.3e-04, -5.1e-01, -4.5e-03,  3.2e-02,  6.1e-03,  5.7e-01,  2.3e-02,  1.1e-01,  1.9e-01,  2.1e+00,  1.3e-01, -8.1e-03, -1.2e-02, -3.6e-02, -4.3e-03, -1.7e-02, -6.6e-03,  7.2e-03, -1.5e-02], [], [-0.], [], [], [0.1]),\n",
+                            "           (['C65Q2'], [6], ['F6B18'], [], [], ['F6B18'], [], [], [0.], [-1.4e-01,  3.5e-03, -2.9e-01,  2.9e-01,  3.9e-02,  2.0e-02, -1.9e+00, -4.0e-02,  3.8e-01,  1.1e-01,  3.4e+00,  1.1e-01, -6.1e-03, -1.2e-03, -4.7e-02, -2.4e-02, -2.1e-02, -2.9e-02, -2.6e-02, -2.5e-02], [], [-0.], [], [], [0.1])],\n",
+                            "          dtype=[('market_ids', 'O', (1,)), ('firm_ids', 'O', (1,)), ('demand_ids', 'O', (1,)), ('supply_ids', 'O', (0,)), ('nesting_ids', 'O', (0,)), ('product_ids', 'O', (1,)), ('clustering_ids', 'O', (0,)), ('ownership', '<f8', (0,)), ('shares', '<f8', (1,)), ('ZD', '<f8', (20,)), ('ZS', '<f8', (0,)), (((prices,), 'X1'), '<f8', (1,)), (((), 'X2'), '<f8', (0,)), (((), 'X3'), '<f8', (0,)), ('prices', '<f8', (1,))])"
                         ]
                     },
                     "execution_count": 3,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -126,15 +128,15 @@
             "execution_count": 4,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
-                        "{'market_ids': 2256, 'firm_ids': 2256, 'demand_ids': 2256, 'shares': 2256, 'ZD0': 2256, 'ZD1': 2256, 'ZD2': 2256, 'ZD3': 2256, 'ZD4': 2256, 'ZD5': 2256, 'ZD6': 2256, 'ZD7': 2256, 'ZD8': 2256, 'ZD9': 2256, 'ZD10': 2256, 'ZD11': 2256, 'ZD12': 2256, 'ZD13': 2256, 'ZD14': 2256, 'ZD15': 2256, 'ZD16': 2256, 'ZD17': 2256, 'ZD18': 2256, 'ZD19': 2256, 'X1': 2256, 'prices': 2256}\n"
+                        "{'market_ids': 2256, 'firm_ids': 2256, 'demand_ids': 2256, 'product_ids': 2256, 'shares': 2256, 'ZD0': 2256, 'ZD1': 2256, 'ZD2': 2256, 'ZD3': 2256, 'ZD4': 2256, 'ZD5': 2256, 'ZD6': 2256, 'ZD7': 2256, 'ZD8': 2256, 'ZD9': 2256, 'ZD10': 2256, 'ZD11': 2256, 'ZD12': 2256, 'ZD13': 2256, 'ZD14': 2256, 'ZD15': 2256, 'ZD16': 2256, 'ZD17': 2256, 'ZD18': 2256, 'ZD19': 2256, 'X1': 2256, 'prices': 2256}\n"
                     ]
                 },
                 {
                     "data": {
                         "text/html": [
                             "<div>\n",
                             "<style scoped>\n",
@@ -153,21 +155,21 @@
                             "<table border=\"1\" class=\"dataframe\">\n",
                             "  <thead>\n",
                             "    <tr style=\"text-align: right;\">\n",
                             "      <th></th>\n",
                             "      <th>market_ids</th>\n",
                             "      <th>firm_ids</th>\n",
                             "      <th>demand_ids</th>\n",
+                            "      <th>product_ids</th>\n",
                             "      <th>shares</th>\n",
                             "      <th>ZD0</th>\n",
                             "      <th>ZD1</th>\n",
                             "      <th>ZD2</th>\n",
                             "      <th>ZD3</th>\n",
                             "      <th>ZD4</th>\n",
-                            "      <th>ZD5</th>\n",
                             "      <th>...</th>\n",
                             "      <th>ZD12</th>\n",
                             "      <th>ZD13</th>\n",
                             "      <th>ZD14</th>\n",
                             "      <th>ZD15</th>\n",
                             "      <th>ZD16</th>\n",
                             "      <th>ZD17</th>\n",
@@ -179,21 +181,21 @@
                             "  </thead>\n",
                             "  <tbody>\n",
                             "    <tr>\n",
                             "      <th>0</th>\n",
                             "      <td>C01Q1</td>\n",
                             "      <td>1</td>\n",
                             "      <td>F1B04</td>\n",
+                            "      <td>F1B04</td>\n",
                             "      <td>0.012417</td>\n",
                             "      <td>-0.249518</td>\n",
                             "      <td>0.040943</td>\n",
                             "      <td>-1.577566</td>\n",
                             "      <td>-0.269073</td>\n",
                             "      <td>-0.010004</td>\n",
-                            "      <td>0.006934</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.004142</td>\n",
                             "      <td>-0.035593</td>\n",
                             "      <td>0.070587</td>\n",
                             "      <td>0.011768</td>\n",
                             "      <td>0.017287</td>\n",
                             "      <td>-0.015031</td>\n",
@@ -203,21 +205,21 @@
                             "      <td>0.072088</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>1</th>\n",
                             "      <td>C01Q1</td>\n",
                             "      <td>1</td>\n",
                             "      <td>F1B06</td>\n",
+                            "      <td>F1B06</td>\n",
                             "      <td>0.007809</td>\n",
                             "      <td>-0.205951</td>\n",
                             "      <td>0.057100</td>\n",
                             "      <td>-10.383954</td>\n",
                             "      <td>0.150476</td>\n",
                             "      <td>0.039816</td>\n",
-                            "      <td>0.006058</td>\n",
                             "      <td>...</td>\n",
                             "      <td>0.002585</td>\n",
                             "      <td>-0.006776</td>\n",
                             "      <td>-0.045453</td>\n",
                             "      <td>0.000067</td>\n",
                             "      <td>0.031229</td>\n",
                             "      <td>0.005841</td>\n",
@@ -227,21 +229,21 @@
                             "      <td>0.114178</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2</th>\n",
                             "      <td>C01Q1</td>\n",
                             "      <td>1</td>\n",
                             "      <td>F1B07</td>\n",
+                            "      <td>F1B07</td>\n",
                             "      <td>0.012995</td>\n",
                             "      <td>-0.212031</td>\n",
                             "      <td>0.046246</td>\n",
                             "      <td>-2.278160</td>\n",
                             "      <td>-0.029976</td>\n",
                             "      <td>0.002390</td>\n",
-                            "      <td>-0.013229</td>\n",
                             "      <td>...</td>\n",
                             "      <td>0.000992</td>\n",
                             "      <td>0.018425</td>\n",
                             "      <td>0.081555</td>\n",
                             "      <td>0.034975</td>\n",
                             "      <td>0.027932</td>\n",
                             "      <td>0.013156</td>\n",
@@ -251,21 +253,21 @@
                             "      <td>0.132391</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>3</th>\n",
                             "      <td>C01Q1</td>\n",
                             "      <td>1</td>\n",
                             "      <td>F1B09</td>\n",
+                            "      <td>F1B09</td>\n",
                             "      <td>0.005770</td>\n",
                             "      <td>-0.170725</td>\n",
                             "      <td>0.049143</td>\n",
                             "      <td>-1.159784</td>\n",
                             "      <td>-0.244789</td>\n",
                             "      <td>0.002848</td>\n",
-                            "      <td>-0.019942</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.004274</td>\n",
                             "      <td>0.026440</td>\n",
                             "      <td>0.064169</td>\n",
                             "      <td>0.021496</td>\n",
                             "      <td>0.032372</td>\n",
                             "      <td>0.033063</td>\n",
@@ -275,21 +277,21 @@
                             "      <td>0.130344</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>4</th>\n",
                             "      <td>C01Q1</td>\n",
                             "      <td>1</td>\n",
                             "      <td>F1B11</td>\n",
+                            "      <td>F1B11</td>\n",
                             "      <td>0.017934</td>\n",
                             "      <td>-0.164983</td>\n",
                             "      <td>0.047168</td>\n",
                             "      <td>-4.737563</td>\n",
                             "      <td>-0.070873</td>\n",
                             "      <td>0.012273</td>\n",
-                            "      <td>0.017403</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.004694</td>\n",
                             "      <td>-0.029179</td>\n",
                             "      <td>-0.000454</td>\n",
                             "      <td>-0.045272</td>\n",
                             "      <td>-0.025446</td>\n",
                             "      <td>-0.006794</td>\n",
@@ -323,21 +325,21 @@
                             "      <td>...</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2251</th>\n",
                             "      <td>C65Q2</td>\n",
                             "      <td>3</td>\n",
                             "      <td>F3B14</td>\n",
+                            "      <td>F3B14</td>\n",
                             "      <td>0.024702</td>\n",
                             "      <td>-0.126940</td>\n",
                             "      <td>0.002240</td>\n",
                             "      <td>-1.067171</td>\n",
                             "      <td>0.150626</td>\n",
                             "      <td>0.037091</td>\n",
-                            "      <td>0.021697</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.004787</td>\n",
                             "      <td>-0.012775</td>\n",
                             "      <td>-0.059399</td>\n",
                             "      <td>0.043775</td>\n",
                             "      <td>0.059339</td>\n",
                             "      <td>-0.021934</td>\n",
@@ -347,21 +349,21 @@
                             "      <td>0.126086</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2252</th>\n",
                             "      <td>C65Q2</td>\n",
                             "      <td>4</td>\n",
                             "      <td>F4B02</td>\n",
+                            "      <td>F4B02</td>\n",
                             "      <td>0.007914</td>\n",
                             "      <td>-0.109756</td>\n",
                             "      <td>0.011192</td>\n",
                             "      <td>0.458133</td>\n",
                             "      <td>0.066193</td>\n",
                             "      <td>0.006838</td>\n",
-                            "      <td>-0.024803</td>\n",
                             "      <td>...</td>\n",
                             "      <td>0.009385</td>\n",
                             "      <td>0.037487</td>\n",
                             "      <td>0.086225</td>\n",
                             "      <td>0.060856</td>\n",
                             "      <td>0.028264</td>\n",
                             "      <td>0.051264</td>\n",
@@ -371,21 +373,21 @@
                             "      <td>0.199167</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2253</th>\n",
                             "      <td>C65Q2</td>\n",
                             "      <td>4</td>\n",
                             "      <td>F4B10</td>\n",
+                            "      <td>F4B10</td>\n",
                             "      <td>0.002229</td>\n",
                             "      <td>-0.119689</td>\n",
                             "      <td>-0.000324</td>\n",
                             "      <td>-1.109521</td>\n",
                             "      <td>0.175027</td>\n",
                             "      <td>0.036227</td>\n",
-                            "      <td>-0.018574</td>\n",
                             "      <td>...</td>\n",
                             "      <td>0.000884</td>\n",
                             "      <td>0.037634</td>\n",
                             "      <td>0.019278</td>\n",
                             "      <td>-0.052403</td>\n",
                             "      <td>-0.018107</td>\n",
                             "      <td>0.036733</td>\n",
@@ -395,21 +397,21 @@
                             "      <td>0.137017</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2254</th>\n",
                             "      <td>C65Q2</td>\n",
                             "      <td>4</td>\n",
                             "      <td>F4B12</td>\n",
+                            "      <td>F4B12</td>\n",
                             "      <td>0.011463</td>\n",
                             "      <td>-0.201890</td>\n",
                             "      <td>0.000334</td>\n",
                             "      <td>-0.507311</td>\n",
                             "      <td>-0.004538</td>\n",
                             "      <td>0.031569</td>\n",
-                            "      <td>0.006083</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.008093</td>\n",
                             "      <td>-0.011750</td>\n",
                             "      <td>-0.036333</td>\n",
                             "      <td>-0.004333</td>\n",
                             "      <td>-0.017427</td>\n",
                             "      <td>-0.006647</td>\n",
@@ -419,79 +421,79 @@
                             "      <td>0.100174</td>\n",
                             "    </tr>\n",
                             "    <tr>\n",
                             "      <th>2255</th>\n",
                             "      <td>C65Q2</td>\n",
                             "      <td>6</td>\n",
                             "      <td>F6B18</td>\n",
+                            "      <td>F6B18</td>\n",
                             "      <td>0.026208</td>\n",
                             "      <td>-0.139453</td>\n",
                             "      <td>0.003468</td>\n",
                             "      <td>-0.285143</td>\n",
                             "      <td>0.291132</td>\n",
                             "      <td>0.039259</td>\n",
-                            "      <td>0.020151</td>\n",
                             "      <td>...</td>\n",
                             "      <td>-0.006138</td>\n",
                             "      <td>-0.001181</td>\n",
                             "      <td>-0.046888</td>\n",
                             "      <td>-0.023637</td>\n",
                             "      <td>-0.021410</td>\n",
                             "      <td>-0.029402</td>\n",
                             "      <td>-0.025971</td>\n",
                             "      <td>-0.025435</td>\n",
                             "      <td>-0.011956</td>\n",
                             "      <td>0.127557</td>\n",
                             "    </tr>\n",
                             "  </tbody>\n",
                             "</table>\n",
-                            "<p>2256 rows \u00d7 26 columns</p>\n",
+                            "<p>2256 rows \u00d7 27 columns</p>\n",
                             "</div>"
                         ],
                         "text/plain": [
-                            "     market_ids firm_ids demand_ids    shares       ZD0       ZD1        ZD2  \\\n",
-                            "0         C01Q1        1      F1B04  0.012417 -0.249518  0.040943  -1.577566   \n",
-                            "1         C01Q1        1      F1B06  0.007809 -0.205951  0.057100 -10.383954   \n",
-                            "2         C01Q1        1      F1B07  0.012995 -0.212031  0.046246  -2.278160   \n",
-                            "3         C01Q1        1      F1B09  0.005770 -0.170725  0.049143  -1.159784   \n",
-                            "4         C01Q1        1      F1B11  0.017934 -0.164983  0.047168  -4.737563   \n",
-                            "...         ...      ...        ...       ...       ...       ...        ...   \n",
-                            "2251      C65Q2        3      F3B14  0.024702 -0.126940  0.002240  -1.067171   \n",
-                            "2252      C65Q2        4      F4B02  0.007914 -0.109756  0.011192   0.458133   \n",
-                            "2253      C65Q2        4      F4B10  0.002229 -0.119689 -0.000324  -1.109521   \n",
-                            "2254      C65Q2        4      F4B12  0.011463 -0.201890  0.000334  -0.507311   \n",
-                            "2255      C65Q2        6      F6B18  0.026208 -0.139453  0.003468  -0.285143   \n",
+                            "     market_ids firm_ids demand_ids product_ids    shares       ZD0       ZD1  \\\n",
+                            "0         C01Q1        1      F1B04       F1B04  0.012417 -0.249518  0.040943   \n",
+                            "1         C01Q1        1      F1B06       F1B06  0.007809 -0.205951  0.057100   \n",
+                            "2         C01Q1        1      F1B07       F1B07  0.012995 -0.212031  0.046246   \n",
+                            "3         C01Q1        1      F1B09       F1B09  0.005770 -0.170725  0.049143   \n",
+                            "4         C01Q1        1      F1B11       F1B11  0.017934 -0.164983  0.047168   \n",
+                            "...         ...      ...        ...         ...       ...       ...       ...   \n",
+                            "2251      C65Q2        3      F3B14       F3B14  0.024702 -0.126940  0.002240   \n",
+                            "2252      C65Q2        4      F4B02       F4B02  0.007914 -0.109756  0.011192   \n",
+                            "2253      C65Q2        4      F4B10       F4B10  0.002229 -0.119689 -0.000324   \n",
+                            "2254      C65Q2        4      F4B12       F4B12  0.011463 -0.201890  0.000334   \n",
+                            "2255      C65Q2        6      F6B18       F6B18  0.026208 -0.139453  0.003468   \n",
                             "\n",
-                            "           ZD3       ZD4       ZD5  ...      ZD12      ZD13      ZD14  \\\n",
-                            "0    -0.269073 -0.010004  0.006934  ... -0.004142 -0.035593  0.070587   \n",
-                            "1     0.150476  0.039816  0.006058  ...  0.002585 -0.006776 -0.045453   \n",
-                            "2    -0.029976  0.002390 -0.013229  ...  0.000992  0.018425  0.081555   \n",
-                            "3    -0.244789  0.002848 -0.019942  ... -0.004274  0.026440  0.064169   \n",
-                            "4    -0.070873  0.012273  0.017403  ... -0.004694 -0.029179 -0.000454   \n",
-                            "...        ...       ...       ...  ...       ...       ...       ...   \n",
-                            "2251  0.150626  0.037091  0.021697  ... -0.004787 -0.012775 -0.059399   \n",
-                            "2252  0.066193  0.006838 -0.024803  ...  0.009385  0.037487  0.086225   \n",
-                            "2253  0.175027  0.036227 -0.018574  ...  0.000884  0.037634  0.019278   \n",
-                            "2254 -0.004538  0.031569  0.006083  ... -0.008093 -0.011750 -0.036333   \n",
-                            "2255  0.291132  0.039259  0.020151  ... -0.006138 -0.001181 -0.046888   \n",
+                            "            ZD2       ZD3       ZD4  ...      ZD12      ZD13      ZD14  \\\n",
+                            "0     -1.577566 -0.269073 -0.010004  ... -0.004142 -0.035593  0.070587   \n",
+                            "1    -10.383954  0.150476  0.039816  ...  0.002585 -0.006776 -0.045453   \n",
+                            "2     -2.278160 -0.029976  0.002390  ...  0.000992  0.018425  0.081555   \n",
+                            "3     -1.159784 -0.244789  0.002848  ... -0.004274  0.026440  0.064169   \n",
+                            "4     -4.737563 -0.070873  0.012273  ... -0.004694 -0.029179 -0.000454   \n",
+                            "...         ...       ...       ...  ...       ...       ...       ...   \n",
+                            "2251  -1.067171  0.150626  0.037091  ... -0.004787 -0.012775 -0.059399   \n",
+                            "2252   0.458133  0.066193  0.006838  ...  0.009385  0.037487  0.086225   \n",
+                            "2253  -1.109521  0.175027  0.036227  ...  0.000884  0.037634  0.019278   \n",
+                            "2254  -0.507311 -0.004538  0.031569  ... -0.008093 -0.011750 -0.036333   \n",
+                            "2255  -0.285143  0.291132  0.039259  ... -0.006138 -0.001181 -0.046888   \n",
                             "\n",
                             "          ZD15      ZD16      ZD17      ZD18      ZD19        X1    prices  \n",
                             "0     0.011768  0.017287 -0.015031  0.081201 -0.015833 -0.011248  0.072088  \n",
                             "1     0.000067  0.031229  0.005841 -0.032121 -0.010614 -0.007135  0.114178  \n",
                             "2     0.034975  0.027932  0.013156  0.047484  0.026800  0.023678  0.132391  \n",
                             "3     0.021496  0.032372  0.033063  0.045501  0.036154  0.029725  0.130344  \n",
                             "4    -0.045272 -0.025446 -0.006794 -0.007560 -0.011364 -0.015585  0.154823  \n",
                             "...        ...       ...       ...       ...       ...       ...       ...  \n",
                             "2251  0.043775  0.059339 -0.021934  0.034592 -0.021052 -0.017337  0.126086  \n",
                             "2252  0.060856  0.028264  0.051264  0.032965  0.033324  0.044542  0.199167  \n",
                             "2253 -0.052403 -0.018107  0.036733 -0.057647  0.035662  0.033720  0.137017  \n",
                             "2254 -0.004333 -0.017427 -0.006647  0.007228 -0.015403 -0.004174  0.100174  \n",
                             "2255 -0.023637 -0.021410 -0.029402 -0.025971 -0.025435 -0.011956  0.127557  \n",
                             "\n",
-                            "[2256 rows x 26 columns]"
+                            "[2256 rows x 27 columns]"
                         ]
                     },
                     "execution_count": 4,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -502,27 +504,27 @@
                 "df = pd.DataFrame(pyblp.data_to_dict(problem.products))\n",
                 "df"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/formulation.ipynb` & `pyblp-1.0.0/docs/notebooks/api/formulation.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9901579034391534%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 4: {\'source\': '*

 * *            '["Next, we\'ll design a second matrix with an intercept, with first- and '*

 * *            'second-degree size terms, with categorical product IDs and years, and with the '*

 * *            'interaction of the last two. The first formulation will include the fixed effects as '*

 * *            'indicator variables, and the second will absorb them."]}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 […]*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -57,15 +57,15 @@
                 "formulation"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Next, we'll design a second matrix with an intercept, with first- and second-degree size terms, with categorical product IDs and years, and with the interaction of the last two. The first formulation will include include the fixed effects as indicator variables, and the second will absorb them."
+                "Next, we'll design a second matrix with an intercept, with first- and second-degree size terms, with categorical product IDs and years, and with the interaction of the last two. The first formulation will include the fixed effects as indicator variables, and the second will absorb them."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 3,
             "metadata": {},
             "outputs": [
@@ -133,27 +133,27 @@
                 "formulation = pyblp.Formulation('year:(log(income) + C(education))')\n",
                 "formulation"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/integration.ipynb` & `pyblp-1.0.0/docs/notebooks/api/integration.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9924293154761905%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -84,27 +84,27 @@
                 "integration = pyblp.Integration('grid', size=7)\n",
                 "integration"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/iteration.ipynb` & `pyblp-1.0.0/docs/notebooks/api/iteration.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9924618675595238%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -115,27 +115,27 @@
                 "iteration = pyblp.Iteration(custom_method)\n",
                 "iteration"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/optimization.ipynb` & `pyblp-1.0.0/docs/notebooks/api/optimization.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9924618675595238%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}}',*

 * * "'metadata'": "{'kernelspec': {'display_name': 'Python 3 (ipykernel)'}, 'language_info': "*

 * *               "{'version': '3.9.15'}}"}*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -111,27 +111,27 @@
                 "optimization = pyblp.Optimization(custom_method, compute_gradient=False)\n",
                 "optimization"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/api/parallel.ipynb` & `pyblp-1.0.0/docs/notebooks/api/parallel.ipynb`

 * *Files 3% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9888164083880678%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': '*

 * *            '["In this example, we\'ll use parallel processing to compute elasticities '*

 * *            'market-by-market for a simple Logit problem configured with some of the fake cereal '*

 * *            'data from :ref:`references:Nevo (2000a)`."]}, 3: {\'outputs\': {0: {\'data\': '*

 * *            "{'text/plain': {insert: [(1, '==========================================\\n'), (2, "*

 * *            "'GMM   Objective  Cl […]*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -32,32 +32,32 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this example, we'll use parallel processing to compute elasticities market-by-market for a simple Logit problem configured with some of the fake cereal data from :ref:`references:Nevo (2000)`."
+                "In this example, we'll use parallel processing to compute elasticities market-by-market for a simple Logit problem configured with some of the fake cereal data from :ref:`references:Nevo (2000a)`."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "=================================\n",
-                            "GMM   Objective  Weighting Matrix\n",
-                            "Step    Value    Condition Number\n",
-                            "----  ---------  ----------------\n",
-                            " 2    +1.9E+02       +5.7E+07    \n",
-                            "=================================\n",
+                            "==========================================\n",
+                            "GMM   Objective  Clipped  Weighting Matrix\n",
+                            "Step    Value    Shares   Condition Number\n",
+                            "----  ---------  -------  ----------------\n",
+                            " 2    +1.9E+02      0         +5.7E+07    \n",
+                            "==========================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
                             "========================\n",
                             "Computation   Objective \n",
                             "   Time      Evaluations\n",
                             "-----------  -----------\n",
                             " 00:00:00         2     \n",
@@ -95,15 +95,15 @@
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "Starting a pool of 2 processes ...\n",
                         "Started the process pool after 00:00:00.\n",
                         "Computing elasticities with respect to prices ...\n",
-                        "Finished after 00:00:03.\n",
+                        "Finished after 00:00:02.\n",
                         "\n",
                         "Terminating the pool of 2 processes ...\n",
                         "Terminated the process pool after 00:00:00.\n"
                     ]
                 }
             ],
             "source": [
@@ -120,27 +120,27 @@
                 "\n",
                 "If the problem were much larger, running :meth:`Problem.solve` and :meth:`ProblemResults.compute_elasticities` under the ``with`` statement could substantially speed up estimation and elasticity computation."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/tutorial/.ipynb_checkpoints/blp-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/blp-checkpoint.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9897808878354342%*

 * *Differences: {"'cells'": "{0: {'source': ['# Supply Side Tutorial with Automobile Data']}, 1: {'outputs': {0: "*

 * *            '{\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': {insert: [(0, "In this '*

 * *            "tutorial, we'll use data from :ref:`references:Berry, Levinsohn, and Pakes (1995)` to "*

 * *            "solve the paper's automobile problem. This tutorial is similar to the [fake cereal "*

 * *            'tutorial](nevo.ipynb), but exhibits some other features of pyblp:\\n"), (7, \'## '*

 * *            "Load […]*

```diff
@@ -1,25 +1,25 @@
 {
     "cells": [
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "# Random Coefficients Logit Tutorial with the Automobile Data"
+                "# Supply Side Tutorial with Automobile Data"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -33,27 +33,22 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this tutorial, we'll use data from :ref:`references:Berry, Levinsohn, and Pakes (1995)` to solve the paper's automobile problem.\n",
-                "\n",
-                "\n",
-                "## Application of Random Coefficients Logit with the Automobile Data\n",
-                "\n",
-                "This tutorial is similar to the [fake cereal tutorial](nevo.ipynb), but exhibits some other features of pyblp:\n",
+                "In this tutorial, we'll use data from :ref:`references:Berry, Levinsohn, and Pakes (1995)` to solve the paper's automobile problem. This tutorial is similar to the [fake cereal tutorial](nevo.ipynb), but exhibits some other features of pyblp:\n",
                 "\n",
                 "- Incorporating a supply side into demand estimation.\n",
                 "- Allowing for simple price-income demographic effects.\n",
                 "- Calculating clustered standard errors.\n",
                 "\n",
                 "\n",
-                "### Loading the Data\n",
+                "## Loading Data\n",
                 "\n",
                 "We'll use [pandas](https://pandas.pydata.org/) to load two sets of data:\n",
                 "\n",
                 "1. `product_data`, which contains prices, shares, and other product characteristics.\n",
                 "2. `agent_data`, which contains draws from the distribution of heterogeneity."
             ]
         },
@@ -414,15 +409,15 @@
                 "agent_data.head()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Setting up the Problem\n",
+                "## Setting up the Problem\n",
                 "\n",
                 "Unlike the fake cereal problem, we won't absorb any fixed effects in the automobile problem, so the linear part of demand $X_1$ has more components. We also need to specify a formula for the random coefficients $X_2$, including a random coefficient on the constant, which captures correlation among all inside goods.\n",
                 "\n",
                 "The primary new addition to the model relative to the fake cereal problem is that we add a supply side formula for product characteristics that contribute to marginal costs, $X_3$. The [patsy](https://patsy.readthedocs.io/en/stable/)-style formulas support functions of regressors such as the `log` function used below.\n",
                 "\n",
                 "We stack the three product formulations in order: $X_1$, $X_2$, and $X_3$."
             ]
@@ -484,41 +479,50 @@
                 "agent_formulation"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "As in the cereal example, the :class:`Problem` can be constructed by combining the `product_formulations`, `product_data`, `agent_formulation`, and `agent_data`. We'll also choose the functional form of marginal costs $c_{jt}$. A linear marginal cost specification is the default setting, so we'll need to use the `costs_type` argument of :meth:`Problem` to employ the log-linear specification used by :ref:`references:Berry, Levinsohn, and Pakes (1995)`."
+                "As in the cereal example, the :class:`Problem` can be constructed by combining the `product_formulations`, `product_data`, `agent_formulation`, and `agent_data`. We'll also choose the functional form of marginal costs $c_{jt}$. A linear marginal cost specification is the default setting, so we'll need to use the `costs_type` argument of :meth:`Problem` to employ the log-linear specification used by :ref:`references:Berry, Levinsohn, and Pakes (1995)`.\n",
+                "\n",
+                "When initializing the problem, we get a warning about integration weights not summing to one. This is because the above product data were created by the original paper with importance sampling. To disable this warning, we could increase `pyblp.options.weights_tol`."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 6,
             "metadata": {},
             "outputs": [
                 {
+                    "name": "stderr",
+                    "output_type": "stream",
+                    "text": [
+                        "Integration weights in the following markets sum to a value that differs from 1 by more than options.weights_tol: all markets. Sometimes this is fine, for example when weights were built with importance sampling. Otherwise, it is a sign that there is a data problem.\n"
+                    ]
+                },
+                {
                     "data": {
                         "text/plain": [
                             "Dimensions:\n",
                             "=======================================================\n",
                             " T    N     F    I     K1    K2    K3    D    MD    MS \n",
                             "---  ----  ---  ----  ----  ----  ----  ---  ----  ----\n",
                             "20   2217  26   4000   5     6     6     1    13    18 \n",
                             "=======================================================\n",
                             "\n",
                             "Formulations:\n",
-                            "=====================================================================================\n",
-                            "       Column Indices:            0          1       2       3          4         5  \n",
-                            "-----------------------------  --------  ---------  ----  --------  ----------  -----\n",
-                            " X1: Linear Characteristics       1        hpwt     air     mpd       space          \n",
-                            "X2: Nonlinear Characteristics     1       prices    hpwt    air        mpd      space\n",
-                            "X3: Log Cost Characteristics      1      log(hpwt)  air   log(mpg)  log(space)  trend\n",
-                            "       d: Demographics         1/income                                              \n",
-                            "====================================================================================="
+                            "=======================================================================================\n",
+                            "       Column Indices:             0           1       2       3          4         5  \n",
+                            "-----------------------------  ----------  ---------  ----  --------  ----------  -----\n",
+                            " X1: Linear Characteristics        1         hpwt     air     mpd       space          \n",
+                            "X2: Nonlinear Characteristics      1        prices    hpwt    air        mpd      space\n",
+                            "X3: Log Cost Characteristics       1       log(hpwt)  air   log(mpg)  log(space)  trend\n",
+                            "       d: Demographics         1*1/income                                              \n",
+                            "======================================================================================="
                         ]
                     },
                     "execution_count": 6,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -547,15 +551,15 @@
                 "The formulations table describes all four formulas for demand-side linear characteristics, demand-side nonlinear characteristics, supply-side characteristics, and demographics."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Solving the Problem"
+                "## Solving the Problem"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "The only remaining decisions are:\n",
@@ -564,35 +568,26 @@
                 "- Potentially choosing bounds for $\\Sigma$ and $\\Pi$.\n",
                 "\n",
                 "The decisions we will use are:\n",
                 "\n",
                 "- Use published estimates as our starting values in $\\Sigma_0$.\n",
                 "- Interact the inverse of income, $1 / y_i$, only with prices, and use the published estimate on $\\log(y_i - p_j)$ as our starting value for $\\alpha$ in $\\Pi_0$.\n",
                 "- Bound $\\Sigma_0$ to be positive since it is a diagonal matrix where the diagonal consists of standard deviations.\n",
-                "- Constrain the $p_j / y_i$ coefficient to be negative. Specifically, we'll use a bound that's slightly smaller than zero because when the parameter is exactly zero, there are matrix inversion problems with estimating marginal costs.\n",
                 "\n",
                 "When using a routine that supports bounds, it's usually a good idea to set your own more bounds so that the routine doesn't try out large parameter values that create numerical issues."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 7,
             "metadata": {},
             "outputs": [],
             "source": [
                 "initial_sigma = np.diag([3.612, 0, 4.628, 1.818, 1.050, 2.056])\n",
-                "initial_pi = np.c_[[0, -43.501, 0, 0, 0, 0]]\n",
-                "sigma_bounds = (\n",
-                "   np.zeros_like(initial_sigma),\n",
-                "   np.diag([100, 0, 100, 100, 100, 100])\n",
-                ")\n",
-                "pi_bounds = (\n",
-                "   np.c_[[0, -100, 0, 0, 0, 0]],\n",
-                "   np.c_[[0, -0.1, 0, 0, 0, 0]]\n",
-                ")"
+                "initial_pi = np.c_[[0, -43.501, 0, 0, 0, 0]]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Note that there are only 5 nonzeros on the diagonal of $\\Sigma$, which means that we only need 5 columns of integration nodes to integrate over these 5 dimensions of unobserved heterogeneity. Indeed, `agent_data` contains exactly 5 columns of nodes. If we were to ignore the $\\log(y_i - p_j)$ term (by not configuring $\\Pi$) and include a term on prices in $\\Sigma$ instead, we would have needed 6 columns of integration nodes in our `agent_data`.\n",
@@ -607,32 +602,32 @@
             "execution_count": 8,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================================\n",
-                            "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue    Costs   Condition Number  Condition Number \n",
-                            "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
-                            " 2    +5.0E+02     +1.9E-06        +4.9E-01         +5.1E+02         0         +4.2E+09          +3.8E+08     \n",
-                            "==============================================================================================================\n",
+                            "=======================================================================================================================\n",
+                            "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares    Costs   Condition Number  Condition Number \n",
+                            "----  ---------  -------------  ---------------  ---------------  -------  -------  ----------------  -----------------\n",
+                            " 2    +5.0E+02     +4.1E-08        +4.9E-01         +5.1E+02         0        0         +4.2E+09          +3.8E+08     \n",
+                            "=======================================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:14:08         59           157         42290       129955   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:03:29       No           55           112         34229       104904   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs Adjusted for 999 Clusters in Parentheses):\n",
                             "===================================================================================================\n",
-                            "Sigma:      1        prices      hpwt        air         mpd        space     |   Pi:     1/income \n",
+                            "Sigma:      1        prices      hpwt        air         mpd        space     |   Pi:    1*1/income\n",
                             "------  ----------  --------  ----------  ----------  ----------  ----------  |  ------  ----------\n",
                             "  1      +2.0E+00                                                             |    1      +0.0E+00 \n",
                             "        (+6.1E+00)                                                            |                    \n",
                             "                                                                              |                    \n",
                             "prices   +0.0E+00   +0.0E+00                                                  |  prices   -4.5E+01 \n",
                             "                                                                              |          (+9.2E+00)\n",
                             "                                                                              |                    \n",
@@ -671,16 +666,14 @@
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "results = problem.solve(\n",
                 "    initial_sigma,\n",
                 "    initial_pi,\n",
-                "    sigma_bounds=sigma_bounds,\n",
-                "    pi_bounds=pi_bounds,\n",
                 "    costs_bounds=(0.001, None),\n",
                 "    W_type='clustered',\n",
                 "    se_type='clustered',\n",
                 "    initial_update=True,\n",
                 ")\n",
                 "results"
             ]
@@ -691,29 +684,29 @@
             "source": [
                 "There are some discrepancies between our results and the original paper, but results are similar."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         },
         "pycharm": {
             "stem_cell": {
                 "cell_type": "raw",
                 "metadata": {
                     "collapsed": false
                 },
```

### Comparing `pyblp-0.9.0/docs/notebooks/tutorial/.ipynb_checkpoints/logit_nested-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/logit_nested-checkpoint.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.994614054565705%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': '*

 * *            '{insert: [(0, "In this tutorial, we\'ll use data from :ref:`references:Nevo (2000a)` '*

 * *            "to solve the paper's fake cereal problem. Locations of CSV files that contain the "*

 * *            'data are in the :mod:`data` module.\\n"), (2, \'We will compare two simple models, '*

 * *            'the plain (IIA) logit model and the nested logit (GEV) model using the fake cereal '*

 * *            "datase […]*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -33,25 +33,25 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this tutorial, we'll use data from :ref:`references:Nevo (2000)` to solve the paper's fake cereal problem. Locations of CSV files that contain the data are in the :mod:`data` module.\n",
+                "In this tutorial, we'll use data from :ref:`references:Nevo (2000a)` to solve the paper's fake cereal problem. Locations of CSV files that contain the data are in the :mod:`data` module.\n",
                 "\n",
-                "We will compare two simple models, the plain (IIA) logit model and the nested logit (GEV) model using the fake cereal dataset of :ref:`references:Nevo (2000)`.\n",
+                "We will compare two simple models, the plain (IIA) logit model and the nested logit (GEV) model using the fake cereal dataset of :ref:`references:Nevo (2000a)`.\n",
                 "\n",
                 "## Theory of Plain Logit\n",
                 "\n",
                 "Let's start with the plain logit model under independence of irrelevant alternatives (IIA). In this  model (indirect) utility is given by\n",
                 "\n",
-                "$$U_{jti} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt} + \\epsilon_{jti},$$\n",
+                "$$U_{ijt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt} + \\epsilon_{ijt},$$\n",
                 "\n",
-                "where $\\epsilon_{jti}$ is distributed IID with the Type I Extreme Value (Gumbel) distribution. It is common to normalize the mean utility of the outside good to zero so that $U_{0ti} = \\epsilon_{0ti}$. This gives us aggregate marketshares\n",
+                "where $\\epsilon_{ijt}$ is distributed IID with the Type I Extreme Value (Gumbel) distribution. It is common to normalize the mean utility of the outside good to zero so that $U_{i0t} = \\epsilon_{i0t}$. This gives us aggregate market shares\n",
                 "\n",
                 "$$s_{jt} = \\frac{\\exp(\\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt})}{1 + \\sum_k \\exp(\\alpha p_{kt} + x_{kt} \\beta^\\text{ex} + \\xi_{kt})}.$$\n",
                 "\n",
                 "If we take logs we get\n",
                 "\n",
                 "$$\\log s_{jt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt} - \\log \\sum_k \\exp(\\alpha p_{kt} + x_{kt} \\beta^\\text{ex} + \\xi_{kt})$$\n",
                 "\n",
@@ -70,20 +70,20 @@
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Application of Plain Logit\n",
                 "\n",
                 "A Logit :class:`Problem` can be created by simply excluding the formulation for the nonlinear parameters, $X_2$, along with any agent information. In other words, it requires only specifying the _linear component_ of demand.\n",
                 "\n",
-                "We'll set up and solve a simple version of the fake data cereal problem from :ref:`references:Nevo (2000)`. Since we won't include any demand-side nonlinear characteristics or parameters, we don't have to worry about configuring an optimization routine.\n",
+                "We'll set up and solve a simple version of the fake data cereal problem from :ref:`references:Nevo (2000a)`. Since we won't include any demand-side nonlinear characteristics or parameters, we don't have to worry about configuring an optimization routine.\n",
                 "\n",
                 "There are some important reserved variable names:\n",
                 "\n",
                 "- `market_ids` are the unique market identifiers which we subscript with $t$.\n",
-                "- `shares` specifies the marketshares which need to be between zero and one, and within a market ID, $\\sum_{j} s_{jt} \\leq 1$.\n",
+                "- `shares` specifies the market shares which need to be between zero and one, and within a market ID, $\\sum_{j} s_{jt} \\leq 1$.\n",
                 "- `prices` are prices $p_{jt}$. These have some special properties and are _always_ treated as endogenous.\n",
                 "- `demand_instruments0`, `demand_instruments1`, and so on are numbered demand instruments. These represent only the _excluded_ instruments. The exogenous regressors in $X_1$ will be automatically added to the set of instruments.\n",
                 "\n",
                 "We begin with two steps:\n",
                 "\n",
                 "1. Load the product data which at a minimum consists of `market_ids`, `shares`, `prices`, and at least a single column of demand instruments, `demand_instruments0`.\n",
                 "2. Define a :class:`Formulation` for the $X_1$ (linear) demand model.\n",
@@ -439,20 +439,20 @@
             "execution_count": 5,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "=================================\n",
-                            "GMM   Objective  Weighting Matrix\n",
-                            "Step    Value    Condition Number\n",
-                            "----  ---------  ----------------\n",
-                            " 2    +1.9E+02       +5.7E+07    \n",
-                            "=================================\n",
+                            "==========================================\n",
+                            "GMM   Objective  Clipped  Weighting Matrix\n",
+                            "Step    Value    Shares   Condition Number\n",
+                            "----  ---------  -------  ----------------\n",
+                            " 2    +1.9E+02      0         +5.7E+07    \n",
+                            "==========================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
                             "========================\n",
                             "Computation   Objective \n",
                             "   Time      Evaluations\n",
                             "-----------  -----------\n",
                             " 00:00:00         2     \n",
@@ -481,19 +481,19 @@
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Theory of Nested Logit\n",
                 "\n",
                 "We can extend the logit model to allow for correlation within a group $h$ so that\n",
                 "\n",
-                "$$U_{jti} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt} + \\bar{\\epsilon}_{h(j)ti} + (1 - \\rho) \\bar{\\epsilon}_{jti}.$$\n",
+                "$$U_{ijt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt} + \\bar{\\epsilon}_{h(j)ti} + (1 - \\rho) \\bar{\\epsilon}_{ijt}.$$\n",
                 "\n",
                 "Now, we require that $\\epsilon_{jti} = \\bar{\\epsilon}_{h(j)ti} + (1 - \\rho) \\bar{\\epsilon}_{jti}$ is distributed IID with the Type I Extreme Value (Gumbel) distribution. As $\\rho \\rightarrow 1$, all consumers stay within their group. As $\\rho \\rightarrow 0$, this collapses to the IIA logit. Note that if we wanted, we could allow $\\rho$ to differ between groups with the notation $\\rho_{h(j)}$.\n",
                 "\n",
-                "This gives us aggregate marketshares as the product of two logits, the within group logit and the across group logit:\n",
+                "This gives us aggregate market shares as the product of two logits, the within group logit and the across group logit:\n",
                 "\n",
                 "$$s_{jt} = \\frac{\\exp[V_{jt} / (1 - \\rho)]}{\\exp[V_{h(j)t} / (1 - \\rho)]}\\cdot\\frac{\\exp V_{h(j)t}}{1 + \\sum_h \\exp V_{ht}},$$\n",
                 "\n",
                 "where $V_{jt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt}$.\n",
                 "\n",
                 "After some work we again obtain the linear estimating equation:\n",
                 "\n",
@@ -555,28 +555,28 @@
             "execution_count": 7,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "=============================================================================\n",
-                            "GMM   Objective    Projected    Reduced   Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Gradient Norm  Hessian   Condition Number  Condition Number \n",
-                            "----  ---------  -------------  --------  ----------------  -----------------\n",
-                            " 2    +2.0E+02     +7.9E-10     +1.1E+04      +2.0E+09          +3.0E+04     \n",
-                            "=============================================================================\n",
+                            "======================================================================================\n",
+                            "GMM   Objective    Projected    Reduced   Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Gradient Norm  Hessian   Shares   Condition Number  Condition Number \n",
+                            "----  ---------  -------------  --------  -------  ----------------  -----------------\n",
+                            " 2    +2.0E+02     +3.0E-09     +1.1E+04     0         +2.0E+09          +3.0E+04     \n",
+                            "======================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "======================================\n",
-                            "Computation  Optimization   Objective \n",
-                            "   Time       Iterations   Evaluations\n",
-                            "-----------  ------------  -----------\n",
-                            " 00:00:04         3             8     \n",
-                            "======================================\n",
+                            "=================================================\n",
+                            "Computation  Optimizer  Optimization   Objective \n",
+                            "   Time      Converged   Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------\n",
+                            " 00:00:02       Yes          3             8     \n",
+                            "=================================================\n",
                             "\n",
                             "Rho Estimates (Robust SEs in Parentheses):\n",
                             "==========\n",
                             "All Groups\n",
                             "----------\n",
                             " +9.8E-01 \n",
                             "(+1.4E-02)\n",
@@ -654,28 +654,28 @@
             "execution_count": 9,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "=============================================================================\n",
-                            "GMM   Objective    Projected    Reduced   Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Gradient Norm  Hessian   Condition Number  Condition Number \n",
-                            "----  ---------  -------------  --------  ----------------  -----------------\n",
-                            " 2    +6.9E+02     +8.2E-09     +5.6E+03      +5.1E+08          +2.0E+04     \n",
-                            "=============================================================================\n",
+                            "======================================================================================\n",
+                            "GMM   Objective    Projected    Reduced   Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Gradient Norm  Hessian   Shares   Condition Number  Condition Number \n",
+                            "----  ---------  -------------  --------  -------  ----------------  -----------------\n",
+                            " 2    +6.9E+02     +5.5E-09     +5.6E+03     0         +5.1E+08          +2.0E+04     \n",
+                            "======================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "======================================\n",
-                            "Computation  Optimization   Objective \n",
-                            "   Time       Iterations   Evaluations\n",
-                            "-----------  ------------  -----------\n",
-                            " 00:00:05         4            13     \n",
-                            "======================================\n",
+                            "=================================================\n",
+                            "Computation  Optimizer  Optimization   Objective \n",
+                            "   Time      Converged   Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------\n",
+                            " 00:00:02       Yes          3             8     \n",
+                            "=================================================\n",
                             "\n",
                             "Rho Estimates (Robust SEs in Parentheses):\n",
                             "==========\n",
                             "All Groups\n",
                             "----------\n",
                             " +8.9E-01 \n",
                             "(+1.9E-02)\n",
@@ -790,20 +790,20 @@
             "execution_count": 13,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "====================================================\n",
-                            "GMM   Objective  Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Condition Number  Condition Number \n",
-                            "----  ---------  ----------------  -----------------\n",
-                            " 2    +2.0E+02       +2.1E+09          +1.1E+04     \n",
-                            "====================================================\n",
+                            "=============================================================\n",
+                            "GMM   Objective  Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Shares   Condition Number  Condition Number \n",
+                            "----  ---------  -------  ----------------  -----------------\n",
+                            " 2    +2.0E+02      0         +2.1E+09          +1.1E+04     \n",
+                            "=============================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
                             "========================\n",
                             "Computation   Objective \n",
                             "   Time      Evaluations\n",
                             "-----------  -----------\n",
                             " 00:00:00         2     \n",
@@ -840,20 +840,20 @@
             "execution_count": 14,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "====================================================\n",
-                            "GMM   Objective  Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Condition Number  Condition Number \n",
-                            "----  ---------  ----------------  -----------------\n",
-                            " 2    +7.0E+02       +5.5E+08          +7.7E+03     \n",
-                            "====================================================\n",
+                            "=============================================================\n",
+                            "GMM   Objective  Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Shares   Condition Number  Condition Number \n",
+                            "----  ---------  -------  ----------------  -----------------\n",
+                            " 2    +7.0E+02      0         +5.5E+08          +7.7E+03     \n",
+                            "=============================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
                             "========================\n",
                             "Computation   Objective \n",
                             "   Time      Evaluations\n",
                             "-----------  -----------\n",
                             " 00:00:00         2     \n",
@@ -889,15 +889,15 @@
             "cell_type": "code",
             "execution_count": 15,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "array([-86.37368446])"
+                            "array([-86.37368445])"
                         ]
                     },
                     "execution_count": 15,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -924,29 +924,29 @@
             "source": [
                 "nl2_results2.beta[0] / (1 - nl2_results2.beta[1])"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         },
         "pycharm": {
             "stem_cell": {
                 "cell_type": "raw",
                 "metadata": {
                     "collapsed": false
                 },
```

### Comparing `pyblp-0.9.0/docs/notebooks/tutorial/.ipynb_checkpoints/nevo-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/nevo-checkpoint.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9912587015925616%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': '*

 * *            '{insert: [(0, "In this tutorial, we\'ll use data from :ref:`references:Nevo (2000a)` '*

 * *            "to solve the paper's fake cereal problem. Locations of CSV files that contain the "*

 * *            'data are in the :mod:`data` module.\\n"), (7, \'$$u_{ijt} = \\\\alpha_i p_{jt} + '*

 * *            "x_{jt} \\\\beta_i^\\\\text{ex} + \\\\xi_{jt} + \\\\epsilon_{ijt}$$\\n'), (11, "*

 * *            "'Conditional  […]*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -33,54 +33,54 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this tutorial, we'll use data from :ref:`references:Nevo (2000)` to solve the paper's fake cereal problem. Locations of CSV files that contain the data are in the :mod:`data` module.\n",
+                "In this tutorial, we'll use data from :ref:`references:Nevo (2000a)` to solve the paper's fake cereal problem. Locations of CSV files that contain the data are in the :mod:`data` module.\n",
                 "\n",
                 "## Theory of Random Coefficients Logit\n",
                 "\n",
                 "The random coefficients model extends the plain logit model by allowing for correlated tastes for different product characteristics.\n",
                 "In this  model (indirect) utility is given by\n",
                 "\n",
-                "$$u_{jti} = \\alpha_i p_{jt} + x_{jt} \\beta_i^\\text{ex} + \\xi_{jt} + \\epsilon_{jti}$$\n",
+                "$$u_{ijt} = \\alpha_i p_{jt} + x_{jt} \\beta_i^\\text{ex} + \\xi_{jt} + \\epsilon_{ijt}$$\n",
                 "\n",
                 "The main addition is that $\\beta_i = (\\alpha_i, \\beta_i^\\text{ex})$ have individual specific subscripts $i$.\n",
                 "\n",
-                "Conditional on $\\beta_i$, the individual marketshares follow the same logit form as before. But now we must integrate over heterogeneous individuals to get the aggregate marketshares:\n",
+                "Conditional on $\\beta_i$, the individual market share follow the same logit form as before. But now we must integrate over heterogeneous individuals to get the aggregate market share:\n",
                 "\n",
                 "$$s_{jt}(\\alpha, \\beta, \\theta) = \\int \\frac{\\exp(\\alpha_i p_{jt} + x_{jt} \\beta_i^\\text{ex} + \\xi_{jt})}{1 + \\sum_k \\exp(\\alpha_i p_{jt} + x_{kt} \\beta_i^\\text{ex} + \\xi_{kt})} f(\\alpha_i, \\beta_i \\mid \\theta).$$\n",
                 "\n",
-                "In general, this integral needs to be calculated numerically. This also means that we can't directly linearize the model. It is common to re-parametrize the model to separate the aspects of mean utility that all individuals agree on, $\\delta_{jt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt}$, from the individual specific heterogeneity, $\\mu_{jti}(\\theta)$. This gives us\n",
+                "In general, this integral needs to be calculated numerically. This also means that we can't directly linearize the model. It is common to re-parametrize the model to separate the aspects of mean utility that all individuals agree on, $\\delta_{jt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt}$, from the individual specific heterogeneity, $\\mu_{ijt}(\\theta)$. This gives us\n",
                 "\n",
-                "$$s_{jt}(\\delta_{jt}, \\theta) = \\int \\frac{\\exp(\\delta_{jt} + \\mu_{jti})}{1 + \\sum_k \\exp(\\delta_{kt} + \\mu_{kti})} f(\\mu_{it} | \\theta).$$\n",
+                "$$s_{jt}(\\delta_{jt}, \\theta) = \\int \\frac{\\exp(\\delta_{jt} + \\mu_{ijt})}{1 + \\sum_k \\exp(\\delta_{kt} + \\mu_{ikt})} f(\\mu_{it} | \\theta).$$\n",
                 "\n",
-                "Given a guess of $\\theta$ we can solve the system of nonlinear equations for the vector $\\delta$ which equates observed and predicted marketshares $s_{jt} = s_{jt}(\\delta, \\theta)$. Now we can perform a linear IV GMM regression of the form\n",
+                "Given a guess of $\\theta$ we can solve the system of nonlinear equations for the vector $\\delta$ which equates observed and predicted market share $s_{jt} = s_{jt}(\\delta, \\theta)$. Now we can perform a linear IV GMM regression of the form\n",
                 "\n",
                 "$$\\delta_{jt}(\\theta) = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt}.$$\n",
                 "\n",
                 "The moments are constructed by interacting the predicted residuals $\\hat{\\xi}_{jt}(\\theta)$ with instruments $z_{jt}$ to form\n",
                 "\n",
                 "$$\\bar{g}(\\theta) =\\frac{1}{N} \\sum_{j,t} z_{jt}' \\hat{\\xi}_{jt}(\\theta).$$"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## Application of Random Coefficients Logit with the Fake Cereal Data\n",
+                "## Random Coefficients\n",
                 "\n",
                 "To include random coefficients we need to add a :class:`Formulation` for the demand-side nonlinear characteristics $X_2$.\n",
                 "\n",
                 "Just like in the logit case we have the same reserved field names in `product_data`:\n",
                 "\n",
                 "- `market_ids` are the unique market identifiers which we subscript $t$.\n",
-                "- `shares` specifies the marketshares which need to be between zero and one, and within a market ID, $\\sum_{j} s_{jt} < 1$.\n",
+                "- `shares` specifies the market share which need to be between zero and one, and within a market ID, $\\sum_{j} s_{jt} < 1$.\n",
                 "- `prices` are prices $p_{jt}$. These have some special properties and are _always_ treated as endogenous.\n",
                 "- `demand_instruments0`, `demand_instruments1`, and so on are numbered demand instruments. These represent only the _excluded_ instruments. The exogenous regressors in $X_1$ (of which $X_2$ is typically a subset) will be automatically added to the set of instruments.\n",
                 "\n",
                 "We proceed with the following steps:\n",
                 "\n",
                 "1. Load the `product data` which at a minimum consists of `market_ids`, `shares`, `prices`, and at least a single column of demand instruments, `demand_instruments0`.\n",
                 "2. Define a :class:`Formulation` for the $X_1$ (linear) demand model.\n",
@@ -108,36 +108,34 @@
                 "    - It is required to specify an initial guess of the nonlinear parameters. This serves two primary purposes: speeding up estimation and indicating to the solver through initial values of zero which parameters are restricted to be always zero."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Specification of Random Taste Parameters\n",
+                "## Specification of Random Taste Parameters\n",
                 "\n",
                 "It is common to assume that $f(\\beta_i \\mid \\theta)$ follows a multivariate normal distribution, and to break it up into three parts:\n",
                 "\n",
                 "1. A mean $K_1 \\times 1$ taste which all individuals agree on, $\\beta$.\n",
-                "2. A $K_2 \\times K_2$ covariance matrix, $\\Sigma$ .\n",
+                "2. A $K_2 \\times K_2$ covariance matrix, $V$. As is common with multivariate normal distributions, $V$ is not estimated directly. Rather, its matrix square (Cholesky) root $\\Sigma$ is estimated where $\\Sigma\\Sigma' = V$.\n",
                 "3. Any $K_2 \\times D$ interactions, $\\Pi$, with observed $D \\times 1$ demographic data, $d_i$.\n",
                 "\n",
                 "Together this gives us that\n",
                 "\n",
-                "$$\\beta_i \\sim N(\\beta + \\Pi d_i, \\Sigma).$$\n",
+                "$$\\beta_i \\sim N(\\beta + \\Pi d_i, \\Sigma\\Sigma').$$\n",
                 "\n",
-                ":meth:`Problem.solve` takes an initial guess $\\Sigma_0$ of $\\Sigma$. It guarantees that $\\hat{\\Sigma}$ (the estimated parameters) will have the same sparsity structure as $\\Sigma_0$. So any zero element of $\\Sigma$ is restricted to be zero in the solution $\\hat{\\Sigma}$. For example, a popular restriction is that $\\Sigma$ is diagonal, this can be achieved by passing a diagonal matrix as $\\Sigma_0$.\n",
-                "\n",
-                "As is common with multivariate normal distributions, $\\Sigma$ is not estimated directly. Rather, its matrix square (Cholesky) root $L$ is estimated where $LL' = \\Sigma$."
+                ":meth:`Problem.solve` takes an initial guess $\\Sigma_0$ of $\\Sigma$. It guarantees that $\\hat{\\Sigma}$ (the estimated parameters) will have the same sparsity structure as $\\Sigma_0$. So any zero element of $\\Sigma$ is restricted to be zero in the solution $\\hat{\\Sigma}$. For example, a popular restriction is that $\\Sigma$ is diagonal, this can be achieved by passing a diagonal matrix as $\\Sigma_0$."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Loading the Data\n",
+                "## Loading Data\n",
                 "\n",
                 "The `product_data` argument of :class:`Problem` should be a structured array-like object with fields that store data. Product data can be a structured [NumPy](https://numpy.org/) array, a [pandas](https://pandas.pydata.org/) DataFrame, or other similar objects."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
@@ -371,15 +369,15 @@
                 "For more information about the instruments and the example data as a whole, refer to the :mod:`data` module."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Setting Up and Solving the Problem Without Demographics\n",
+                "## Setting Up and Solving the Problem Without Demographics\n",
                 "\n",
                 "Formulations, product data, and an integration configuration are collectively used to initialize a :class:`Problem`. Once initialized, :meth:`Problem.solve` runs the estimation routine. The arguments to :meth:`Problem.solve` configure how estimation is performed. For example, `optimization` and `iteration` arguments configure the optimization and iteration routines that are used by the outer and inner loops of estimation.\n",
                 "\n",
                 "We'll specify :class:`Formulation` configurations for $X_1$, the demand-side linear characteristics, and $X_2$, the nonlinear characteristics.\n",
                 "\n",
                 "- The formulation for $X_1$ consists of `prices` and fixed effects constructed from `product_ids`, which we will absorb using `absorb` argument of :class:`Formulation`.\n",
                 "- If we were interested in reporting estimates for each fixed effect, we could replace the formulation for $X_1$ with `Formulation('prices + C(product_ids)')`.\n",
@@ -411,15 +409,15 @@
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We also need to specify an :class:`Integration` configuration. We consider two alternatives:\n",
                 "\n",
-                "1. Monte Carlo draws: we simulate 50 individuals from a random normal distribution.\n",
+                "1. Monte Carlo draws: we simulate 50 individuals from a random normal distribution. This is just for simplicity. In practice quasi-Monte Carlo sequences such as Halton draws are preferable, and there should generally be many more simulated individuals than just 50.\n",
                 "2. Product rules: we construct nodes and weights according to a product rule that exactly integrates polynomials of degree $5 \\times 2 - 1 = 9$ or less."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 4,
             "metadata": {},
@@ -529,91 +527,91 @@
                 "pr_problem"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "As an illustration of how to configure the optimization routine, we'll use a simpler, non-default :class:`Optimization` configuration that doesn't support parameter bounds."
+                "As an illustration of how to configure the optimization routine, we'll use a simpler, non-default :class:`Optimization` configuration that doesn't support parameter bounds, and use a relatively loose tolerance so the problems are solved quickly. In practice along with more integration draws, it's a good idea to use a tighter termination tolerance."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 8,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "Configured to optimize using the BFGS algorithm implemented in SciPy with analytic gradients and options {gtol: +1.0E-10}."
+                            "Configured to optimize using the BFGS algorithm implemented in SciPy with analytic gradients and options {gtol: +1.0E-04}."
                         ]
                     },
                     "execution_count": 8,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "bfgs = pyblp.Optimization('bfgs', {'gtol': 1e-10})\n",
+                "bfgs = pyblp.Optimization('bfgs', {'gtol': 1e-4})\n",
                 "bfgs"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We estimate three versions of the model:\n",
                 "\n",
                 "1. An unrestricted covariance matrix for random tastes using Monte Carlo integration.\n",
                 "2. An unrestricted covariance matrix for random tastes using the product rule.\n",
                 "3. A restricted diagonal matrix for random tastes using Monte Carlo Integration.\n",
                 "\n",
-                "Notice that the only thing that changes when we estimate the restricted covariance is our initial guess of $\\Sigma_0$. The upper diagonal in this initial guess is ignored because we are optimizing over the lower-triangular Cholesky root of $\\Sigma$."
+                "Notice that the only thing that changes when we estimate the restricted covariance is our initial guess of $\\Sigma_0$. The upper diagonal in this initial guess is ignored because we are optimizing over the lower-triangular Cholesky root of $V = \\Sigma\\Sigma'$."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 9,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 2    +1.5E+02   +3.1E-06     +8.5E-02        +6.5E+03         +5.2E+07          +8.3E+05     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 2    +1.5E+02   +8.7E-05     +8.5E-02        +6.5E+03        0         +5.2E+07          +8.3E+05     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:04:04         60           173        143806       445313   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:01:37       Yes          58           75          83853       257686   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
-                            "======================================================\n",
-                            "Sigma:      1         prices      sugar       mushy   \n",
-                            "------  ----------  ----------  ----------  ----------\n",
-                            "  1      +1.2E+00                                     \n",
-                            "        (+3.0E+00)                                    \n",
-                            "                                                      \n",
-                            "prices   -1.1E+01    +8.4E+00                         \n",
-                            "        (+1.8E+01)  (+1.2E+01)                        \n",
-                            "                                                      \n",
-                            "sugar    +6.1E-02    -9.1E-02    +3.8E-02             \n",
-                            "        (+2.5E-01)  (+2.3E-01)  (+8.3E-02)            \n",
-                            "                                                      \n",
-                            "mushy    -5.9E-01    -6.2E-01    -2.3E-02    +4.8E-01 \n",
-                            "        (+2.1E+00)  (+1.5E+00)  (+2.5E+00)  (+1.3E+00)\n",
-                            "======================================================\n",
+                            "=========================================================================================================================\n",
+                            "Sigma:      1         prices      sugar       mushy     |  Sigma Squared:      1         prices      sugar       mushy   \n",
+                            "------  ----------  ----------  ----------  ----------  |  --------------  ----------  ----------  ----------  ----------\n",
+                            "  1      +1.2E+00                                       |        1          +1.5E+00    -1.4E+01    +7.3E-02    -7.1E-01 \n",
+                            "        (+3.0E+00)                                      |                  (+7.2E+00)  (+5.2E+01)  (+2.2E-01)  (+2.3E+00)\n",
+                            "                                                        |                                                                \n",
+                            "prices   -1.1E+01    +8.4E+00                           |      prices       -1.4E+01    +2.0E+02    -1.5E+00    +1.5E+00 \n",
+                            "        (+1.8E+01)  (+1.2E+01)                          |                  (+5.2E+01)  (+3.1E+02)  (+1.2E+00)  (+1.5E+01)\n",
+                            "                                                        |                                                                \n",
+                            "sugar    +6.1E-02    -9.1E-02    +3.8E-02               |      sugar        +7.3E-02    -1.5E+00    +1.3E-02    +2.0E-02 \n",
+                            "        (+2.5E-01)  (+2.3E-01)  (+8.3E-02)              |                  (+2.2E-01)  (+1.2E+00)  (+2.8E-02)  (+2.7E-01)\n",
+                            "                                                        |                                                                \n",
+                            "mushy    -5.9E-01    -6.2E-01    -2.3E-02    +4.8E-01   |      mushy        -7.1E-01    +1.5E+00    +2.0E-02    +9.6E-01 \n",
+                            "        (+2.1E+00)  (+1.5E+00)  (+2.5E+00)  (+1.3E+00)  |                  (+2.3E+00)  (+1.5E+01)  (+2.7E-01)  (+4.0E+00)\n",
+                            "=========================================================================================================================\n",
                             "\n",
                             "Beta Estimates (Robust SEs in Parentheses):\n",
                             "==========\n",
                             "  prices  \n",
                             "----------\n",
                             " -3.1E+01 \n",
                             "(+6.0E+00)\n",
@@ -635,52 +633,52 @@
             "execution_count": 10,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 2    +1.6E+02   +1.4E-06     +1.6E-02        +5.3E+03         +5.3E+07          +3.9E+33     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 2    +1.6E+02   +3.2E-05     +1.6E-02        +5.3E+03        0         +5.3E+07          +1.2E+21     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:09:37         68           202        131877       409484   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:03:02       Yes          64           78          68670       212085   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
-                            "======================================================\n",
-                            "Sigma:      1         prices      sugar       mushy   \n",
-                            "------  ----------  ----------  ----------  ----------\n",
-                            "  1      -7.4E-01                                     \n",
-                            "        (+2.0E+00)                                    \n",
-                            "                                                      \n",
-                            "prices   +1.3E+01    -3.8E-07                         \n",
-                            "        (+6.1E+00)  (+2.3E-07)                        \n",
-                            "                                                      \n",
-                            "sugar    -1.1E-01    +4.7E-09    -1.4E-10             \n",
-                            "        (+9.5E-02)  (+5.9E-06)  (+6.7E-07)            \n",
-                            "                                                      \n",
-                            "mushy    +1.5E-01    +2.0E-08    +9.7E-10    -4.6E-10 \n",
-                            "        (+6.8E-01)  (+1.9E-07)  (+2.6E-08)  (+7.3E-09)\n",
-                            "======================================================\n",
+                            "=========================================================================================================================\n",
+                            "Sigma:      1         prices      sugar       mushy     |  Sigma Squared:      1         prices      sugar       mushy   \n",
+                            "------  ----------  ----------  ----------  ----------  |  --------------  ----------  ----------  ----------  ----------\n",
+                            "  1      -7.4E-01                                       |        1          +5.5E-01    -9.4E+00    +8.3E-02    -1.1E-01 \n",
+                            "        (+2.1E+00)                                      |                  (+3.1E+00)  (+3.2E+01)  (+1.7E-01)  (+6.3E-01)\n",
+                            "                                                        |                                                                \n",
+                            "prices   +1.3E+01    -8.8E-06                           |      prices       -9.4E+00    +1.6E+02    -1.4E+00    +1.9E+00 \n",
+                            "        (+7.7E+00)  (+2.3E+03)                          |                  (+3.2E+01)  (+2.0E+02)  (+7.8E-01)  (+8.8E+00)\n",
+                            "                                                        |                                                                \n",
+                            "sugar    -1.1E-01    +1.1E-07    -3.0E-09               |      sugar        +8.3E-02    -1.4E+00    +1.2E-02    -1.7E-02 \n",
+                            "        (+2.1E-01)  (+1.9E+05)  (+7.3E+02)              |                  (+1.7E-01)  (+7.8E-01)  (+2.1E-02)  (+1.5E-01)\n",
+                            "                                                        |                                                                \n",
+                            "mushy    +1.5E-01    +4.6E-07    +1.9E-08    -1.3E-08   |      mushy        -1.1E-01    +1.9E+00    -1.7E-02    +2.2E-02 \n",
+                            "        (+6.8E-01)  (+3.5E+03)  (+6.3E+02)  (+1.8E+02)  |                  (+6.3E-01)  (+8.8E+00)  (+1.5E-01)  (+2.0E-01)\n",
+                            "=========================================================================================================================\n",
                             "\n",
                             "Beta Estimates (Robust SEs in Parentheses):\n",
                             "==========\n",
                             "  prices  \n",
                             "----------\n",
                             " -3.1E+01 \n",
-                            "(+3.9E+00)\n",
+                            "(+4.4E+00)\n",
                             "=========="
                         ]
                     },
                     "execution_count": 10,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
@@ -695,28 +693,28 @@
             "execution_count": 11,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 2    +1.8E+02   +1.3E-06     +1.1E+00        +6.0E+03         +5.8E+07          +4.8E+04     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 2    +1.8E+02   +1.3E-06     +1.1E+00        +6.0E+03        0         +5.8E+07          +4.8E+04     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:01:21         18           138         67387       212090   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:00:23       Yes          16           24          19540        60462   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
                             "======================================================\n",
                             "Sigma:      1         prices      sugar       mushy   \n",
                             "------  ----------  ----------  ----------  ----------\n",
                             "  1      +5.2E-02                                     \n",
                             "        (+1.1E+00)                                    \n",
@@ -757,26 +755,26 @@
                 "We see that all three models give similar estimates of the price coefficient $\\hat{\\alpha} \\approx -30$. Note a few of the estimated terms on the diagonal of $\\Sigma$ are negative. Since the diagonal consists of standard deviations, negative values are unrealistic. When using another optimization routine that supports bounds (like the default L-BFGS-B routine), these diagonal elements are by default bounded from below by zero."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Adding Demographics to the Problem\n",
+                "## Adding Demographics to the Problem\n",
                 "\n",
                 "To add demographic data we need to make two changes:\n",
                 "\n",
                 "1. We need to load `agent_data`, which for this cereal problem contains pre-computed Monte Carlo draws and demographics.\n",
                 "2. We need to add an `agent_formulation` to the model.\n",
                 "\n",
                 "The `agent data` has several reserved column names.\n",
                 "\n",
                 "- `market_ids` are the index that link the `agent data` to the `market_ids` in `product data`.\n",
                 "- `weights` are the weights $w_{it}$ attached to each agent. In each market, these should sum to one so that $\\sum_i w_{it} = 1$. It is often the case the $w_{it} = 1 / I_t$ where $I_t$ is the number of agents in market $t$, so that each agent gets equal weight. Other possibilities include quadrature nodes and weights.\n",
-                "- `nodes0`, `nodes1`, and so on are the nodes at which the unobserved agent tastes $\\mu_{jti}$ are evaluated. The nodes should be labeled from $0, \\ldots, K_2 - 1$ where $K_2$ is the number of random coefficients.\n",
+                "- `nodes0`, `nodes1`, and so on are the nodes at which the unobserved agent tastes $\\mu_{ijt}$ are evaluated. The nodes should be labeled from $0, \\ldots, K_2 - 1$ where $K_2$ is the number of random coefficients.\n",
                 "- Other fields are the realizations of the demographics $d$ themselves."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 12,
             "metadata": {},
@@ -1001,42 +999,42 @@
                 "nevo_problem"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "The initialized problem can be solved with :meth:`Problem.solve`. We'll use the same starting values as :ref:`references:Nevo (2000)`. By passing a diagonal matrix as starting values for $\\Sigma$, we're choosing to ignore covariance terms. Similarly, zeros in the starting values for $\\Pi$ mean that those parameters will be fixed at zero.\n",
+                "The initialized problem can be solved with :meth:`Problem.solve`. We'll use the same starting values as :ref:`references:Nevo (2000a)`. By passing a diagonal matrix as starting values for $\\Sigma$, we're choosing to ignore covariance terms. Similarly, zeros in the starting values for $\\Pi$ mean that those parameters will be fixed at zero.\n",
                 "\n",
-                "To replicate common estimates, we'll use the non-default BFGS optimization routine, and we'll configure :meth:`Problem.solve` to use 1-step GMM instead of 2-step GMM."
+                "To replicate common estimates, we'll use the non-default BFGS optimization routine (with a slightly tighter tolerance to avoid getting stuck at a spurious local minimum), and we'll configure :meth:`Problem.solve` to use 1-step GMM instead of 2-step GMM."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 15,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 1    +4.6E+00   +4.1E-10     +2.6E-05        +1.6E+04         +6.9E+07          +8.4E+08     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 1    +4.6E+00   +6.9E-06     +3.2E-05        +1.6E+04        0         +6.9E+07          +8.4E+08     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:02:17         56           180        155590       482541   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:01:00       Yes          51           57          46386       143970   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
                             "=====================================================================================================================\n",
                             "Sigma:      1         prices      sugar       mushy     |   Pi:      income    income_squared     age        child   \n",
                             "------  ----------  ----------  ----------  ----------  |  ------  ----------  --------------  ----------  ----------\n",
                             "  1      +5.6E-01                                       |    1      +2.3E+00      +0.0E+00      +1.3E+00    +0.0E+00 \n",
                             "        (+1.6E-01)                                      |          (+1.2E+00)                  (+6.3E-01)            \n",
@@ -1069,35 +1067,36 @@
                 "initial_sigma = np.diag([0.3302, 2.4526, 0.0163, 0.2441])\n",
                 "initial_pi = np.array([\n",
                 "  [ 5.4819,  0,      0.2037,  0     ],\n",
                 "  [15.8935, -1.2000, 0,       2.6342],\n",
                 "  [-0.2506,  0,      0.0511,  0     ],\n",
                 "  [ 1.2650,  0,     -0.8091,  0     ]\n",
                 "])\n",
+                "tighter_bfgs = pyblp.Optimization('bfgs', {'gtol': 1e-5})\n",
                 "nevo_results = nevo_problem.solve(\n",
                 "    initial_sigma,\n",
                 "    initial_pi,\n",
-                "    optimization=bfgs,\n",
+                "    optimization=tighter_bfgs,\n",
                 "    method='1s'\n",
                 ")\n",
                 "nevo_results"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Results are similar to those in the original paper with a (non-scaled) objective value of $q(\\hat{\\theta}) = \\text{8.96E-7} = 4.65 / N^2$ and a price coefficient of $\\hat{\\alpha} = -62.7$. "
+                "Results are similar to those in the original paper with a (scaled) objective value of $q(\\hat{\\theta}) = 4.65$ and a price coefficient of $\\hat{\\alpha} = -62.7$. "
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Restricting Parameters\n",
+                "## Restricting Parameters\n",
                 "\n",
                 "Because the interactions between `price`, `income`, and `income_squared` are potentially collinear, we might worry that $\\hat{\\Pi}_{21} = 588$ and  $\\hat{\\Pi}_{22} = -30.2$ are pushing the price coefficient in opposite directions. Both are large in magnitude but statistically insignficant. One way of dealing with this is to restrict $\\Pi_{22} = 0$.\n",
                 "\n",
                 "There are two ways we can do this:\n",
                 "\n",
                 "1. Change the initial $\\Pi_0$ values to make this term zero.\n",
                 "2. Change the agent formula to drop `income_squared`.\n",
@@ -1110,28 +1109,28 @@
             "execution_count": 16,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 1    +1.5E+01   +2.7E-06     +4.7E-02        +1.7E+04         +6.9E+07          +5.7E+05     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 1    +1.5E+01   +5.2E-06     +4.7E-02        +1.7E+04        0         +6.9E+07          +5.7E+05     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:01:17         36           99          77185       239212   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:00:45       Yes          34           40          34392       106492   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
                             "=====================================================================================================================\n",
                             "Sigma:      1         prices      sugar       mushy     |   Pi:      income    income_squared     age        child   \n",
                             "------  ----------  ----------  ----------  ----------  |  ------  ----------  --------------  ----------  ----------\n",
                             "  1      +3.7E-01                                       |    1      +3.1E+00      +0.0E+00      +1.2E+00    +0.0E+00 \n",
                             "        (+1.2E-01)                                      |          (+1.1E+00)                  (+1.0E+00)            \n",
@@ -1162,15 +1161,15 @@
             ],
             "source": [
                 "restricted_pi = initial_pi.copy()\n",
                 "restricted_pi[1, 1] = 0\n",
                 "nevo_problem.solve(\n",
                 "    initial_sigma,\n",
                 "    restricted_pi,\n",
-                "    optimization=bfgs,\n",
+                "    optimization=tighter_bfgs,\n",
                 "    method='1s'\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -1183,28 +1182,28 @@
             "execution_count": 17,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 1    +1.5E+01   +2.3E-07     +4.7E-02        +1.7E+04         +6.9E+07          +5.7E+05     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 1    +1.5E+01   +5.2E-06     +4.7E-02        +1.7E+04        0         +6.9E+07          +5.7E+05     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:01:06         35           78          61695       191191   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:00:45       Yes          34           40          34403       106539   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
                             "=====================================================================================================\n",
                             "Sigma:      1         prices      sugar       mushy     |   Pi:      income       age        child   \n",
                             "------  ----------  ----------  ----------  ----------  |  ------  ----------  ----------  ----------\n",
                             "  1      +3.7E-01                                       |    1      +3.1E+00    +1.2E+00    +0.0E+00 \n",
                             "        (+1.2E-01)                                      |          (+1.1E+00)  (+1.0E+00)            \n",
@@ -1241,44 +1240,44 @@
                 "    product_data,\n",
                 "    restricted_formulation,\n",
                 "    agent_data\n",
                 ")\n",
                 "restricted_problem.solve(\n",
                 "    initial_sigma,\n",
                 "    deleted_pi,\n",
-                "    optimization=bfgs,\n",
+                "    optimization=tighter_bfgs,\n",
                 "    method='1s'\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "The parameter estimates and standard errors are identical for both approaches. Based on the number of fixed point iterations, there is some evidence that the solver took a slightly different path for each problem, but both restricted problems arrived at identical answers. The $\\hat{\\Pi}_{12}$ interaction term is still insignificant."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         },
         "pycharm": {
             "stem_cell": {
                 "cell_type": "raw",
                 "metadata": {
                     "collapsed": false
                 },
```

### Comparing `pyblp-0.9.0/docs/notebooks/tutorial/.ipynb_checkpoints/simulation-checkpoint.ipynb` & `pyblp-1.0.0/docs/notebooks/tutorial/.ipynb_checkpoints/simulation-checkpoint.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9916145174753437%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 8: {\'source\': '*

 * *            "{insert: [(4, 'At this stage, simulated variables are not consistent with true "*

 * *            'parameters, so we still need to solve the simulation with '*

 * *            ':meth:`Simulation.replace_endogenous`. This method replaced simulated prices and '*

 * *            'market shares with values that are consistent with the true parameters. Just like '*

 * *            ':meth:`ProblemResults.compute_prices`,  […]*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -163,32 +163,32 @@
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "When :class:`Simulation` is initialized, it constructs :attr:`Simulation.agent_data` and simulates :attr:`Simulation.product_data`.\n",
                 "\n",
                 "The :class:`Simulation` can be further configured with other arguments that determine how unobserved product characteristics are simulated and how marginal costs are specified.\n",
                 "\n",
-                "At this stage, simulated variables are not consistent with true parameters, so we still need to solve the simulation with :meth:`Simulation.replace_endogenous`. This method replaced simulated prices and marketshares with values that are consistent with the true parameters. Just like :meth:`ProblemResults.compute_prices`, to do so it iterates over the $\\zeta$-markup equation from :ref:`references:Morrow and Skerlos (2011)`."
+                "At this stage, simulated variables are not consistent with true parameters, so we still need to solve the simulation with :meth:`Simulation.replace_endogenous`. This method replaced simulated prices and market shares with values that are consistent with the true parameters. Just like :meth:`ProblemResults.compute_prices`, to do so it iterates over the $\\zeta$-markup equation from :ref:`references:Morrow and Skerlos (2011)`."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 5,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Simulation Results Summary:\n",
-                            "=====================================\n",
-                            "Computation  Fixed Point  Contraction\n",
-                            "   Time      Iterations   Evaluations\n",
-                            "-----------  -----------  -----------\n",
-                            " 00:00:00        721          721    \n",
-                            "====================================="
+                            "======================================================================================================\n",
+                            "Computation  Fixed Point  Fixed Point  Contraction  Profit Gradients  Profit Hessians  Profit Hessians\n",
+                            "   Time       Failures    Iterations   Evaluations      Max Norm      Min Eigenvalue   Max Eigenvalue \n",
+                            "-----------  -----------  -----------  -----------  ----------------  ---------------  ---------------\n",
+                            " 00:00:01         0           721          721          +1.3E-13         -8.4E-01         -9.6E-06    \n",
+                            "======================================================================================================"
                         ]
                     },
                     "execution_count": 5,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -253,28 +253,28 @@
             "execution_count": 7,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "=====================================================================================================\n",
-                            "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Condition Number  Condition Number \n",
-                            "----  ---------  -------------  ---------------  ---------------  ----------------  -----------------\n",
-                            " 2    +6.4E+00     +2.1E-11        +7.2E+00         +3.8E+03          +3.7E+04          +1.5E+04     \n",
-                            "=====================================================================================================\n",
+                            "==============================================================================================================\n",
+                            "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
+                            "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
+                            " 2    +6.4E+00     +6.9E-08        +7.2E+00         +3.8E+03         0         +3.7E+04          +1.5E+04     \n",
+                            "==============================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:00:56         25           79          22958        72258   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:00:13       Yes          23           30          8295         26314   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
                             "==================\n",
                             "Sigma:      x     \n",
                             "------  ----------\n",
                             "  x      +7.8E-01 \n",
                             "        (+5.2E-01)\n",
@@ -300,17 +300,18 @@
                     "execution_count": 7,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "results = problem.solve(\n",
-                "   sigma=0.5 * simulation.sigma, \n",
-                "   pi=0.5 * simulation.pi,\n",
-                "   beta=[None, 0.5 * simulation.beta[1], None]\n",
+                "    sigma=0.5 * simulation.sigma, \n",
+                "    pi=0.5 * simulation.pi,\n",
+                "    beta=[None, 0.5 * simulation.beta[1], None],\n",
+                "    optimization=pyblp.Optimization('l-bfgs-b', {'gtol': 1e-5})\n",
                 ")\n",
                 "results"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -324,15 +325,15 @@
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "array([[ 1.        ,  0.96223514],\n",
                             "       [-2.        , -2.00792431],\n",
-                            "       [ 2.        ,  2.10032016]])"
+                            "       [ 2.        ,  2.10032015]])"
                         ]
                     },
                     "execution_count": 8,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -387,27 +388,27 @@
             "source": [
                 "In addition to checking that the configuration for a model based on actual data makes sense, the :class:`Simulation` class can also be a helpful tool for better understanding under what general conditions BLP models can be accurately estimated. Simulations are also used extensively in pyblp's test suite."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/notebooks/tutorial/blp.ipynb` & `pyblp-1.0.0/docs/notebooks/tutorial/blp.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9897808878354342%*

 * *Differences: {"'cells'": "{0: {'source': ['# Supply Side Tutorial with Automobile Data']}, 1: {'outputs': {0: "*

 * *            '{\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': {insert: [(0, "In this '*

 * *            "tutorial, we'll use data from :ref:`references:Berry, Levinsohn, and Pakes (1995)` to "*

 * *            "solve the paper's automobile problem. This tutorial is similar to the [fake cereal "*

 * *            'tutorial](nevo.ipynb), but exhibits some other features of pyblp:\\n"), (7, \'## '*

 * *            "Load […]*

```diff
@@ -1,25 +1,25 @@
 {
     "cells": [
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "# Random Coefficients Logit Tutorial with the Automobile Data"
+                "# Supply Side Tutorial with Automobile Data"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -33,27 +33,22 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this tutorial, we'll use data from :ref:`references:Berry, Levinsohn, and Pakes (1995)` to solve the paper's automobile problem.\n",
-                "\n",
-                "\n",
-                "## Application of Random Coefficients Logit with the Automobile Data\n",
-                "\n",
-                "This tutorial is similar to the [fake cereal tutorial](nevo.ipynb), but exhibits some other features of pyblp:\n",
+                "In this tutorial, we'll use data from :ref:`references:Berry, Levinsohn, and Pakes (1995)` to solve the paper's automobile problem. This tutorial is similar to the [fake cereal tutorial](nevo.ipynb), but exhibits some other features of pyblp:\n",
                 "\n",
                 "- Incorporating a supply side into demand estimation.\n",
                 "- Allowing for simple price-income demographic effects.\n",
                 "- Calculating clustered standard errors.\n",
                 "\n",
                 "\n",
-                "### Loading the Data\n",
+                "## Loading Data\n",
                 "\n",
                 "We'll use [pandas](https://pandas.pydata.org/) to load two sets of data:\n",
                 "\n",
                 "1. `product_data`, which contains prices, shares, and other product characteristics.\n",
                 "2. `agent_data`, which contains draws from the distribution of heterogeneity."
             ]
         },
@@ -414,15 +409,15 @@
                 "agent_data.head()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Setting up the Problem\n",
+                "## Setting up the Problem\n",
                 "\n",
                 "Unlike the fake cereal problem, we won't absorb any fixed effects in the automobile problem, so the linear part of demand $X_1$ has more components. We also need to specify a formula for the random coefficients $X_2$, including a random coefficient on the constant, which captures correlation among all inside goods.\n",
                 "\n",
                 "The primary new addition to the model relative to the fake cereal problem is that we add a supply side formula for product characteristics that contribute to marginal costs, $X_3$. The [patsy](https://patsy.readthedocs.io/en/stable/)-style formulas support functions of regressors such as the `log` function used below.\n",
                 "\n",
                 "We stack the three product formulations in order: $X_1$, $X_2$, and $X_3$."
             ]
@@ -484,41 +479,50 @@
                 "agent_formulation"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "As in the cereal example, the :class:`Problem` can be constructed by combining the `product_formulations`, `product_data`, `agent_formulation`, and `agent_data`. We'll also choose the functional form of marginal costs $c_{jt}$. A linear marginal cost specification is the default setting, so we'll need to use the `costs_type` argument of :meth:`Problem` to employ the log-linear specification used by :ref:`references:Berry, Levinsohn, and Pakes (1995)`."
+                "As in the cereal example, the :class:`Problem` can be constructed by combining the `product_formulations`, `product_data`, `agent_formulation`, and `agent_data`. We'll also choose the functional form of marginal costs $c_{jt}$. A linear marginal cost specification is the default setting, so we'll need to use the `costs_type` argument of :meth:`Problem` to employ the log-linear specification used by :ref:`references:Berry, Levinsohn, and Pakes (1995)`.\n",
+                "\n",
+                "When initializing the problem, we get a warning about integration weights not summing to one. This is because the above product data were created by the original paper with importance sampling. To disable this warning, we could increase `pyblp.options.weights_tol`."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 6,
             "metadata": {},
             "outputs": [
                 {
+                    "name": "stderr",
+                    "output_type": "stream",
+                    "text": [
+                        "Integration weights in the following markets sum to a value that differs from 1 by more than options.weights_tol: all markets. Sometimes this is fine, for example when weights were built with importance sampling. Otherwise, it is a sign that there is a data problem.\n"
+                    ]
+                },
+                {
                     "data": {
                         "text/plain": [
                             "Dimensions:\n",
                             "=======================================================\n",
                             " T    N     F    I     K1    K2    K3    D    MD    MS \n",
                             "---  ----  ---  ----  ----  ----  ----  ---  ----  ----\n",
                             "20   2217  26   4000   5     6     6     1    13    18 \n",
                             "=======================================================\n",
                             "\n",
                             "Formulations:\n",
-                            "=====================================================================================\n",
-                            "       Column Indices:            0          1       2       3          4         5  \n",
-                            "-----------------------------  --------  ---------  ----  --------  ----------  -----\n",
-                            " X1: Linear Characteristics       1        hpwt     air     mpd       space          \n",
-                            "X2: Nonlinear Characteristics     1       prices    hpwt    air        mpd      space\n",
-                            "X3: Log Cost Characteristics      1      log(hpwt)  air   log(mpg)  log(space)  trend\n",
-                            "       d: Demographics         1/income                                              \n",
-                            "====================================================================================="
+                            "=======================================================================================\n",
+                            "       Column Indices:             0           1       2       3          4         5  \n",
+                            "-----------------------------  ----------  ---------  ----  --------  ----------  -----\n",
+                            " X1: Linear Characteristics        1         hpwt     air     mpd       space          \n",
+                            "X2: Nonlinear Characteristics      1        prices    hpwt    air        mpd      space\n",
+                            "X3: Log Cost Characteristics       1       log(hpwt)  air   log(mpg)  log(space)  trend\n",
+                            "       d: Demographics         1*1/income                                              \n",
+                            "======================================================================================="
                         ]
                     },
                     "execution_count": 6,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -547,15 +551,15 @@
                 "The formulations table describes all four formulas for demand-side linear characteristics, demand-side nonlinear characteristics, supply-side characteristics, and demographics."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Solving the Problem"
+                "## Solving the Problem"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "The only remaining decisions are:\n",
@@ -564,35 +568,26 @@
                 "- Potentially choosing bounds for $\\Sigma$ and $\\Pi$.\n",
                 "\n",
                 "The decisions we will use are:\n",
                 "\n",
                 "- Use published estimates as our starting values in $\\Sigma_0$.\n",
                 "- Interact the inverse of income, $1 / y_i$, only with prices, and use the published estimate on $\\log(y_i - p_j)$ as our starting value for $\\alpha$ in $\\Pi_0$.\n",
                 "- Bound $\\Sigma_0$ to be positive since it is a diagonal matrix where the diagonal consists of standard deviations.\n",
-                "- Constrain the $p_j / y_i$ coefficient to be negative. Specifically, we'll use a bound that's slightly smaller than zero because when the parameter is exactly zero, there are matrix inversion problems with estimating marginal costs.\n",
                 "\n",
                 "When using a routine that supports bounds, it's usually a good idea to set your own more bounds so that the routine doesn't try out large parameter values that create numerical issues."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 7,
             "metadata": {},
             "outputs": [],
             "source": [
                 "initial_sigma = np.diag([3.612, 0, 4.628, 1.818, 1.050, 2.056])\n",
-                "initial_pi = np.c_[[0, -43.501, 0, 0, 0, 0]]\n",
-                "sigma_bounds = (\n",
-                "   np.zeros_like(initial_sigma),\n",
-                "   np.diag([100, 0, 100, 100, 100, 100])\n",
-                ")\n",
-                "pi_bounds = (\n",
-                "   np.c_[[0, -100, 0, 0, 0, 0]],\n",
-                "   np.c_[[0, -0.1, 0, 0, 0, 0]]\n",
-                ")"
+                "initial_pi = np.c_[[0, -43.501, 0, 0, 0, 0]]"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Note that there are only 5 nonzeros on the diagonal of $\\Sigma$, which means that we only need 5 columns of integration nodes to integrate over these 5 dimensions of unobserved heterogeneity. Indeed, `agent_data` contains exactly 5 columns of nodes. If we were to ignore the $\\log(y_i - p_j)$ term (by not configuring $\\Pi$) and include a term on prices in $\\Sigma$ instead, we would have needed 6 columns of integration nodes in our `agent_data`.\n",
@@ -607,32 +602,32 @@
             "execution_count": 8,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================================\n",
-                            "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue    Costs   Condition Number  Condition Number \n",
-                            "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
-                            " 2    +5.0E+02     +1.9E-06        +4.9E-01         +5.1E+02         0         +4.2E+09          +3.8E+08     \n",
-                            "==============================================================================================================\n",
+                            "=======================================================================================================================\n",
+                            "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares    Costs   Condition Number  Condition Number \n",
+                            "----  ---------  -------------  ---------------  ---------------  -------  -------  ----------------  -----------------\n",
+                            " 2    +5.0E+02     +4.1E-08        +4.9E-01         +5.1E+02         0        0         +4.2E+09          +3.8E+08     \n",
+                            "=======================================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:14:08         59           157         42290       129955   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:03:29       No           55           112         34229       104904   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs Adjusted for 999 Clusters in Parentheses):\n",
                             "===================================================================================================\n",
-                            "Sigma:      1        prices      hpwt        air         mpd        space     |   Pi:     1/income \n",
+                            "Sigma:      1        prices      hpwt        air         mpd        space     |   Pi:    1*1/income\n",
                             "------  ----------  --------  ----------  ----------  ----------  ----------  |  ------  ----------\n",
                             "  1      +2.0E+00                                                             |    1      +0.0E+00 \n",
                             "        (+6.1E+00)                                                            |                    \n",
                             "                                                                              |                    \n",
                             "prices   +0.0E+00   +0.0E+00                                                  |  prices   -4.5E+01 \n",
                             "                                                                              |          (+9.2E+00)\n",
                             "                                                                              |                    \n",
@@ -671,16 +666,14 @@
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "results = problem.solve(\n",
                 "    initial_sigma,\n",
                 "    initial_pi,\n",
-                "    sigma_bounds=sigma_bounds,\n",
-                "    pi_bounds=pi_bounds,\n",
                 "    costs_bounds=(0.001, None),\n",
                 "    W_type='clustered',\n",
                 "    se_type='clustered',\n",
                 "    initial_update=True,\n",
                 ")\n",
                 "results"
             ]
@@ -691,29 +684,29 @@
             "source": [
                 "There are some discrepancies between our results and the original paper, but results are similar."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         },
         "pycharm": {
             "stem_cell": {
                 "cell_type": "raw",
                 "metadata": {
                     "collapsed": false
                 },
```

### Comparing `pyblp-0.9.0/docs/notebooks/tutorial/logit_nested.ipynb` & `pyblp-1.0.0/docs/notebooks/tutorial/logit_nested.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.994614054565705%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': '*

 * *            '{insert: [(0, "In this tutorial, we\'ll use data from :ref:`references:Nevo (2000a)` '*

 * *            "to solve the paper's fake cereal problem. Locations of CSV files that contain the "*

 * *            'data are in the :mod:`data` module.\\n"), (2, \'We will compare two simple models, '*

 * *            'the plain (IIA) logit model and the nested logit (GEV) model using the fake cereal '*

 * *            "datase […]*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -33,25 +33,25 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this tutorial, we'll use data from :ref:`references:Nevo (2000)` to solve the paper's fake cereal problem. Locations of CSV files that contain the data are in the :mod:`data` module.\n",
+                "In this tutorial, we'll use data from :ref:`references:Nevo (2000a)` to solve the paper's fake cereal problem. Locations of CSV files that contain the data are in the :mod:`data` module.\n",
                 "\n",
-                "We will compare two simple models, the plain (IIA) logit model and the nested logit (GEV) model using the fake cereal dataset of :ref:`references:Nevo (2000)`.\n",
+                "We will compare two simple models, the plain (IIA) logit model and the nested logit (GEV) model using the fake cereal dataset of :ref:`references:Nevo (2000a)`.\n",
                 "\n",
                 "## Theory of Plain Logit\n",
                 "\n",
                 "Let's start with the plain logit model under independence of irrelevant alternatives (IIA). In this  model (indirect) utility is given by\n",
                 "\n",
-                "$$U_{jti} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt} + \\epsilon_{jti},$$\n",
+                "$$U_{ijt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt} + \\epsilon_{ijt},$$\n",
                 "\n",
-                "where $\\epsilon_{jti}$ is distributed IID with the Type I Extreme Value (Gumbel) distribution. It is common to normalize the mean utility of the outside good to zero so that $U_{0ti} = \\epsilon_{0ti}$. This gives us aggregate marketshares\n",
+                "where $\\epsilon_{ijt}$ is distributed IID with the Type I Extreme Value (Gumbel) distribution. It is common to normalize the mean utility of the outside good to zero so that $U_{i0t} = \\epsilon_{i0t}$. This gives us aggregate market shares\n",
                 "\n",
                 "$$s_{jt} = \\frac{\\exp(\\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt})}{1 + \\sum_k \\exp(\\alpha p_{kt} + x_{kt} \\beta^\\text{ex} + \\xi_{kt})}.$$\n",
                 "\n",
                 "If we take logs we get\n",
                 "\n",
                 "$$\\log s_{jt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt} - \\log \\sum_k \\exp(\\alpha p_{kt} + x_{kt} \\beta^\\text{ex} + \\xi_{kt})$$\n",
                 "\n",
@@ -70,20 +70,20 @@
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Application of Plain Logit\n",
                 "\n",
                 "A Logit :class:`Problem` can be created by simply excluding the formulation for the nonlinear parameters, $X_2$, along with any agent information. In other words, it requires only specifying the _linear component_ of demand.\n",
                 "\n",
-                "We'll set up and solve a simple version of the fake data cereal problem from :ref:`references:Nevo (2000)`. Since we won't include any demand-side nonlinear characteristics or parameters, we don't have to worry about configuring an optimization routine.\n",
+                "We'll set up and solve a simple version of the fake data cereal problem from :ref:`references:Nevo (2000a)`. Since we won't include any demand-side nonlinear characteristics or parameters, we don't have to worry about configuring an optimization routine.\n",
                 "\n",
                 "There are some important reserved variable names:\n",
                 "\n",
                 "- `market_ids` are the unique market identifiers which we subscript with $t$.\n",
-                "- `shares` specifies the marketshares which need to be between zero and one, and within a market ID, $\\sum_{j} s_{jt} \\leq 1$.\n",
+                "- `shares` specifies the market shares which need to be between zero and one, and within a market ID, $\\sum_{j} s_{jt} \\leq 1$.\n",
                 "- `prices` are prices $p_{jt}$. These have some special properties and are _always_ treated as endogenous.\n",
                 "- `demand_instruments0`, `demand_instruments1`, and so on are numbered demand instruments. These represent only the _excluded_ instruments. The exogenous regressors in $X_1$ will be automatically added to the set of instruments.\n",
                 "\n",
                 "We begin with two steps:\n",
                 "\n",
                 "1. Load the product data which at a minimum consists of `market_ids`, `shares`, `prices`, and at least a single column of demand instruments, `demand_instruments0`.\n",
                 "2. Define a :class:`Formulation` for the $X_1$ (linear) demand model.\n",
@@ -439,20 +439,20 @@
             "execution_count": 5,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "=================================\n",
-                            "GMM   Objective  Weighting Matrix\n",
-                            "Step    Value    Condition Number\n",
-                            "----  ---------  ----------------\n",
-                            " 2    +1.9E+02       +5.7E+07    \n",
-                            "=================================\n",
+                            "==========================================\n",
+                            "GMM   Objective  Clipped  Weighting Matrix\n",
+                            "Step    Value    Shares   Condition Number\n",
+                            "----  ---------  -------  ----------------\n",
+                            " 2    +1.9E+02      0         +5.7E+07    \n",
+                            "==========================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
                             "========================\n",
                             "Computation   Objective \n",
                             "   Time      Evaluations\n",
                             "-----------  -----------\n",
                             " 00:00:00         2     \n",
@@ -481,19 +481,19 @@
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Theory of Nested Logit\n",
                 "\n",
                 "We can extend the logit model to allow for correlation within a group $h$ so that\n",
                 "\n",
-                "$$U_{jti} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt} + \\bar{\\epsilon}_{h(j)ti} + (1 - \\rho) \\bar{\\epsilon}_{jti}.$$\n",
+                "$$U_{ijt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt} + \\bar{\\epsilon}_{h(j)ti} + (1 - \\rho) \\bar{\\epsilon}_{ijt}.$$\n",
                 "\n",
                 "Now, we require that $\\epsilon_{jti} = \\bar{\\epsilon}_{h(j)ti} + (1 - \\rho) \\bar{\\epsilon}_{jti}$ is distributed IID with the Type I Extreme Value (Gumbel) distribution. As $\\rho \\rightarrow 1$, all consumers stay within their group. As $\\rho \\rightarrow 0$, this collapses to the IIA logit. Note that if we wanted, we could allow $\\rho$ to differ between groups with the notation $\\rho_{h(j)}$.\n",
                 "\n",
-                "This gives us aggregate marketshares as the product of two logits, the within group logit and the across group logit:\n",
+                "This gives us aggregate market shares as the product of two logits, the within group logit and the across group logit:\n",
                 "\n",
                 "$$s_{jt} = \\frac{\\exp[V_{jt} / (1 - \\rho)]}{\\exp[V_{h(j)t} / (1 - \\rho)]}\\cdot\\frac{\\exp V_{h(j)t}}{1 + \\sum_h \\exp V_{ht}},$$\n",
                 "\n",
                 "where $V_{jt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt}$.\n",
                 "\n",
                 "After some work we again obtain the linear estimating equation:\n",
                 "\n",
@@ -555,28 +555,28 @@
             "execution_count": 7,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "=============================================================================\n",
-                            "GMM   Objective    Projected    Reduced   Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Gradient Norm  Hessian   Condition Number  Condition Number \n",
-                            "----  ---------  -------------  --------  ----------------  -----------------\n",
-                            " 2    +2.0E+02     +7.9E-10     +1.1E+04      +2.0E+09          +3.0E+04     \n",
-                            "=============================================================================\n",
+                            "======================================================================================\n",
+                            "GMM   Objective    Projected    Reduced   Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Gradient Norm  Hessian   Shares   Condition Number  Condition Number \n",
+                            "----  ---------  -------------  --------  -------  ----------------  -----------------\n",
+                            " 2    +2.0E+02     +3.0E-09     +1.1E+04     0         +2.0E+09          +3.0E+04     \n",
+                            "======================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "======================================\n",
-                            "Computation  Optimization   Objective \n",
-                            "   Time       Iterations   Evaluations\n",
-                            "-----------  ------------  -----------\n",
-                            " 00:00:04         3             8     \n",
-                            "======================================\n",
+                            "=================================================\n",
+                            "Computation  Optimizer  Optimization   Objective \n",
+                            "   Time      Converged   Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------\n",
+                            " 00:00:02       Yes          3             8     \n",
+                            "=================================================\n",
                             "\n",
                             "Rho Estimates (Robust SEs in Parentheses):\n",
                             "==========\n",
                             "All Groups\n",
                             "----------\n",
                             " +9.8E-01 \n",
                             "(+1.4E-02)\n",
@@ -654,28 +654,28 @@
             "execution_count": 9,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "=============================================================================\n",
-                            "GMM   Objective    Projected    Reduced   Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Gradient Norm  Hessian   Condition Number  Condition Number \n",
-                            "----  ---------  -------------  --------  ----------------  -----------------\n",
-                            " 2    +6.9E+02     +8.2E-09     +5.6E+03      +5.1E+08          +2.0E+04     \n",
-                            "=============================================================================\n",
+                            "======================================================================================\n",
+                            "GMM   Objective    Projected    Reduced   Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Gradient Norm  Hessian   Shares   Condition Number  Condition Number \n",
+                            "----  ---------  -------------  --------  -------  ----------------  -----------------\n",
+                            " 2    +6.9E+02     +5.5E-09     +5.6E+03     0         +5.1E+08          +2.0E+04     \n",
+                            "======================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "======================================\n",
-                            "Computation  Optimization   Objective \n",
-                            "   Time       Iterations   Evaluations\n",
-                            "-----------  ------------  -----------\n",
-                            " 00:00:05         4            13     \n",
-                            "======================================\n",
+                            "=================================================\n",
+                            "Computation  Optimizer  Optimization   Objective \n",
+                            "   Time      Converged   Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------\n",
+                            " 00:00:02       Yes          3             8     \n",
+                            "=================================================\n",
                             "\n",
                             "Rho Estimates (Robust SEs in Parentheses):\n",
                             "==========\n",
                             "All Groups\n",
                             "----------\n",
                             " +8.9E-01 \n",
                             "(+1.9E-02)\n",
@@ -790,20 +790,20 @@
             "execution_count": 13,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "====================================================\n",
-                            "GMM   Objective  Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Condition Number  Condition Number \n",
-                            "----  ---------  ----------------  -----------------\n",
-                            " 2    +2.0E+02       +2.1E+09          +1.1E+04     \n",
-                            "====================================================\n",
+                            "=============================================================\n",
+                            "GMM   Objective  Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Shares   Condition Number  Condition Number \n",
+                            "----  ---------  -------  ----------------  -----------------\n",
+                            " 2    +2.0E+02      0         +2.1E+09          +1.1E+04     \n",
+                            "=============================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
                             "========================\n",
                             "Computation   Objective \n",
                             "   Time      Evaluations\n",
                             "-----------  -----------\n",
                             " 00:00:00         2     \n",
@@ -840,20 +840,20 @@
             "execution_count": 14,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "====================================================\n",
-                            "GMM   Objective  Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Condition Number  Condition Number \n",
-                            "----  ---------  ----------------  -----------------\n",
-                            " 2    +7.0E+02       +5.5E+08          +7.7E+03     \n",
-                            "====================================================\n",
+                            "=============================================================\n",
+                            "GMM   Objective  Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Shares   Condition Number  Condition Number \n",
+                            "----  ---------  -------  ----------------  -----------------\n",
+                            " 2    +7.0E+02      0         +5.5E+08          +7.7E+03     \n",
+                            "=============================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
                             "========================\n",
                             "Computation   Objective \n",
                             "   Time      Evaluations\n",
                             "-----------  -----------\n",
                             " 00:00:00         2     \n",
@@ -889,15 +889,15 @@
             "cell_type": "code",
             "execution_count": 15,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "array([-86.37368446])"
+                            "array([-86.37368445])"
                         ]
                     },
                     "execution_count": 15,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -924,29 +924,29 @@
             "source": [
                 "nl2_results2.beta[0] / (1 - nl2_results2.beta[1])"
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         },
         "pycharm": {
             "stem_cell": {
                 "cell_type": "raw",
                 "metadata": {
                     "collapsed": false
                 },
```

### Comparing `pyblp-0.9.0/docs/notebooks/tutorial/nevo.ipynb` & `pyblp-1.0.0/docs/notebooks/tutorial/nevo.ipynb`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9912587015925616%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 2: {\'source\': '*

 * *            '{insert: [(0, "In this tutorial, we\'ll use data from :ref:`references:Nevo (2000a)` '*

 * *            "to solve the paper's fake cereal problem. Locations of CSV files that contain the "*

 * *            'data are in the :mod:`data` module.\\n"), (7, \'$$u_{ijt} = \\\\alpha_i p_{jt} + '*

 * *            "x_{jt} \\\\beta_i^\\\\text{ex} + \\\\xi_{jt} + \\\\epsilon_{ijt}$$\\n'), (11, "*

 * *            "'Conditional  […]*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -33,54 +33,54 @@
                 "pyblp.__version__"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this tutorial, we'll use data from :ref:`references:Nevo (2000)` to solve the paper's fake cereal problem. Locations of CSV files that contain the data are in the :mod:`data` module.\n",
+                "In this tutorial, we'll use data from :ref:`references:Nevo (2000a)` to solve the paper's fake cereal problem. Locations of CSV files that contain the data are in the :mod:`data` module.\n",
                 "\n",
                 "## Theory of Random Coefficients Logit\n",
                 "\n",
                 "The random coefficients model extends the plain logit model by allowing for correlated tastes for different product characteristics.\n",
                 "In this  model (indirect) utility is given by\n",
                 "\n",
-                "$$u_{jti} = \\alpha_i p_{jt} + x_{jt} \\beta_i^\\text{ex} + \\xi_{jt} + \\epsilon_{jti}$$\n",
+                "$$u_{ijt} = \\alpha_i p_{jt} + x_{jt} \\beta_i^\\text{ex} + \\xi_{jt} + \\epsilon_{ijt}$$\n",
                 "\n",
                 "The main addition is that $\\beta_i = (\\alpha_i, \\beta_i^\\text{ex})$ have individual specific subscripts $i$.\n",
                 "\n",
-                "Conditional on $\\beta_i$, the individual marketshares follow the same logit form as before. But now we must integrate over heterogeneous individuals to get the aggregate marketshares:\n",
+                "Conditional on $\\beta_i$, the individual market share follow the same logit form as before. But now we must integrate over heterogeneous individuals to get the aggregate market share:\n",
                 "\n",
                 "$$s_{jt}(\\alpha, \\beta, \\theta) = \\int \\frac{\\exp(\\alpha_i p_{jt} + x_{jt} \\beta_i^\\text{ex} + \\xi_{jt})}{1 + \\sum_k \\exp(\\alpha_i p_{jt} + x_{kt} \\beta_i^\\text{ex} + \\xi_{kt})} f(\\alpha_i, \\beta_i \\mid \\theta).$$\n",
                 "\n",
-                "In general, this integral needs to be calculated numerically. This also means that we can't directly linearize the model. It is common to re-parametrize the model to separate the aspects of mean utility that all individuals agree on, $\\delta_{jt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt}$, from the individual specific heterogeneity, $\\mu_{jti}(\\theta)$. This gives us\n",
+                "In general, this integral needs to be calculated numerically. This also means that we can't directly linearize the model. It is common to re-parametrize the model to separate the aspects of mean utility that all individuals agree on, $\\delta_{jt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt}$, from the individual specific heterogeneity, $\\mu_{ijt}(\\theta)$. This gives us\n",
                 "\n",
-                "$$s_{jt}(\\delta_{jt}, \\theta) = \\int \\frac{\\exp(\\delta_{jt} + \\mu_{jti})}{1 + \\sum_k \\exp(\\delta_{kt} + \\mu_{kti})} f(\\mu_{it} | \\theta).$$\n",
+                "$$s_{jt}(\\delta_{jt}, \\theta) = \\int \\frac{\\exp(\\delta_{jt} + \\mu_{ijt})}{1 + \\sum_k \\exp(\\delta_{kt} + \\mu_{ikt})} f(\\mu_{it} | \\theta).$$\n",
                 "\n",
-                "Given a guess of $\\theta$ we can solve the system of nonlinear equations for the vector $\\delta$ which equates observed and predicted marketshares $s_{jt} = s_{jt}(\\delta, \\theta)$. Now we can perform a linear IV GMM regression of the form\n",
+                "Given a guess of $\\theta$ we can solve the system of nonlinear equations for the vector $\\delta$ which equates observed and predicted market share $s_{jt} = s_{jt}(\\delta, \\theta)$. Now we can perform a linear IV GMM regression of the form\n",
                 "\n",
                 "$$\\delta_{jt}(\\theta) = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt}.$$\n",
                 "\n",
                 "The moments are constructed by interacting the predicted residuals $\\hat{\\xi}_{jt}(\\theta)$ with instruments $z_{jt}$ to form\n",
                 "\n",
                 "$$\\bar{g}(\\theta) =\\frac{1}{N} \\sum_{j,t} z_{jt}' \\hat{\\xi}_{jt}(\\theta).$$"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## Application of Random Coefficients Logit with the Fake Cereal Data\n",
+                "## Random Coefficients\n",
                 "\n",
                 "To include random coefficients we need to add a :class:`Formulation` for the demand-side nonlinear characteristics $X_2$.\n",
                 "\n",
                 "Just like in the logit case we have the same reserved field names in `product_data`:\n",
                 "\n",
                 "- `market_ids` are the unique market identifiers which we subscript $t$.\n",
-                "- `shares` specifies the marketshares which need to be between zero and one, and within a market ID, $\\sum_{j} s_{jt} < 1$.\n",
+                "- `shares` specifies the market share which need to be between zero and one, and within a market ID, $\\sum_{j} s_{jt} < 1$.\n",
                 "- `prices` are prices $p_{jt}$. These have some special properties and are _always_ treated as endogenous.\n",
                 "- `demand_instruments0`, `demand_instruments1`, and so on are numbered demand instruments. These represent only the _excluded_ instruments. The exogenous regressors in $X_1$ (of which $X_2$ is typically a subset) will be automatically added to the set of instruments.\n",
                 "\n",
                 "We proceed with the following steps:\n",
                 "\n",
                 "1. Load the `product data` which at a minimum consists of `market_ids`, `shares`, `prices`, and at least a single column of demand instruments, `demand_instruments0`.\n",
                 "2. Define a :class:`Formulation` for the $X_1$ (linear) demand model.\n",
@@ -108,36 +108,34 @@
                 "    - It is required to specify an initial guess of the nonlinear parameters. This serves two primary purposes: speeding up estimation and indicating to the solver through initial values of zero which parameters are restricted to be always zero."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Specification of Random Taste Parameters\n",
+                "## Specification of Random Taste Parameters\n",
                 "\n",
                 "It is common to assume that $f(\\beta_i \\mid \\theta)$ follows a multivariate normal distribution, and to break it up into three parts:\n",
                 "\n",
                 "1. A mean $K_1 \\times 1$ taste which all individuals agree on, $\\beta$.\n",
-                "2. A $K_2 \\times K_2$ covariance matrix, $\\Sigma$ .\n",
+                "2. A $K_2 \\times K_2$ covariance matrix, $V$. As is common with multivariate normal distributions, $V$ is not estimated directly. Rather, its matrix square (Cholesky) root $\\Sigma$ is estimated where $\\Sigma\\Sigma' = V$.\n",
                 "3. Any $K_2 \\times D$ interactions, $\\Pi$, with observed $D \\times 1$ demographic data, $d_i$.\n",
                 "\n",
                 "Together this gives us that\n",
                 "\n",
-                "$$\\beta_i \\sim N(\\beta + \\Pi d_i, \\Sigma).$$\n",
+                "$$\\beta_i \\sim N(\\beta + \\Pi d_i, \\Sigma\\Sigma').$$\n",
                 "\n",
-                ":meth:`Problem.solve` takes an initial guess $\\Sigma_0$ of $\\Sigma$. It guarantees that $\\hat{\\Sigma}$ (the estimated parameters) will have the same sparsity structure as $\\Sigma_0$. So any zero element of $\\Sigma$ is restricted to be zero in the solution $\\hat{\\Sigma}$. For example, a popular restriction is that $\\Sigma$ is diagonal, this can be achieved by passing a diagonal matrix as $\\Sigma_0$.\n",
-                "\n",
-                "As is common with multivariate normal distributions, $\\Sigma$ is not estimated directly. Rather, its matrix square (Cholesky) root $L$ is estimated where $LL' = \\Sigma$."
+                ":meth:`Problem.solve` takes an initial guess $\\Sigma_0$ of $\\Sigma$. It guarantees that $\\hat{\\Sigma}$ (the estimated parameters) will have the same sparsity structure as $\\Sigma_0$. So any zero element of $\\Sigma$ is restricted to be zero in the solution $\\hat{\\Sigma}$. For example, a popular restriction is that $\\Sigma$ is diagonal, this can be achieved by passing a diagonal matrix as $\\Sigma_0$."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Loading the Data\n",
+                "## Loading Data\n",
                 "\n",
                 "The `product_data` argument of :class:`Problem` should be a structured array-like object with fields that store data. Product data can be a structured [NumPy](https://numpy.org/) array, a [pandas](https://pandas.pydata.org/) DataFrame, or other similar objects."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
@@ -371,15 +369,15 @@
                 "For more information about the instruments and the example data as a whole, refer to the :mod:`data` module."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Setting Up and Solving the Problem Without Demographics\n",
+                "## Setting Up and Solving the Problem Without Demographics\n",
                 "\n",
                 "Formulations, product data, and an integration configuration are collectively used to initialize a :class:`Problem`. Once initialized, :meth:`Problem.solve` runs the estimation routine. The arguments to :meth:`Problem.solve` configure how estimation is performed. For example, `optimization` and `iteration` arguments configure the optimization and iteration routines that are used by the outer and inner loops of estimation.\n",
                 "\n",
                 "We'll specify :class:`Formulation` configurations for $X_1$, the demand-side linear characteristics, and $X_2$, the nonlinear characteristics.\n",
                 "\n",
                 "- The formulation for $X_1$ consists of `prices` and fixed effects constructed from `product_ids`, which we will absorb using `absorb` argument of :class:`Formulation`.\n",
                 "- If we were interested in reporting estimates for each fixed effect, we could replace the formulation for $X_1$ with `Formulation('prices + C(product_ids)')`.\n",
@@ -411,15 +409,15 @@
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We also need to specify an :class:`Integration` configuration. We consider two alternatives:\n",
                 "\n",
-                "1. Monte Carlo draws: we simulate 50 individuals from a random normal distribution.\n",
+                "1. Monte Carlo draws: we simulate 50 individuals from a random normal distribution. This is just for simplicity. In practice quasi-Monte Carlo sequences such as Halton draws are preferable, and there should generally be many more simulated individuals than just 50.\n",
                 "2. Product rules: we construct nodes and weights according to a product rule that exactly integrates polynomials of degree $5 \\times 2 - 1 = 9$ or less."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 4,
             "metadata": {},
@@ -529,91 +527,91 @@
                 "pr_problem"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "As an illustration of how to configure the optimization routine, we'll use a simpler, non-default :class:`Optimization` configuration that doesn't support parameter bounds."
+                "As an illustration of how to configure the optimization routine, we'll use a simpler, non-default :class:`Optimization` configuration that doesn't support parameter bounds, and use a relatively loose tolerance so the problems are solved quickly. In practice along with more integration draws, it's a good idea to use a tighter termination tolerance."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 8,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "Configured to optimize using the BFGS algorithm implemented in SciPy with analytic gradients and options {gtol: +1.0E-10}."
+                            "Configured to optimize using the BFGS algorithm implemented in SciPy with analytic gradients and options {gtol: +1.0E-04}."
                         ]
                     },
                     "execution_count": 8,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "bfgs = pyblp.Optimization('bfgs', {'gtol': 1e-10})\n",
+                "bfgs = pyblp.Optimization('bfgs', {'gtol': 1e-4})\n",
                 "bfgs"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We estimate three versions of the model:\n",
                 "\n",
                 "1. An unrestricted covariance matrix for random tastes using Monte Carlo integration.\n",
                 "2. An unrestricted covariance matrix for random tastes using the product rule.\n",
                 "3. A restricted diagonal matrix for random tastes using Monte Carlo Integration.\n",
                 "\n",
-                "Notice that the only thing that changes when we estimate the restricted covariance is our initial guess of $\\Sigma_0$. The upper diagonal in this initial guess is ignored because we are optimizing over the lower-triangular Cholesky root of $\\Sigma$."
+                "Notice that the only thing that changes when we estimate the restricted covariance is our initial guess of $\\Sigma_0$. The upper diagonal in this initial guess is ignored because we are optimizing over the lower-triangular Cholesky root of $V = \\Sigma\\Sigma'$."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 9,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 2    +1.5E+02   +3.1E-06     +8.5E-02        +6.5E+03         +5.2E+07          +8.3E+05     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 2    +1.5E+02   +8.7E-05     +8.5E-02        +6.5E+03        0         +5.2E+07          +8.3E+05     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:04:04         60           173        143806       445313   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:01:37       Yes          58           75          83853       257686   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
-                            "======================================================\n",
-                            "Sigma:      1         prices      sugar       mushy   \n",
-                            "------  ----------  ----------  ----------  ----------\n",
-                            "  1      +1.2E+00                                     \n",
-                            "        (+3.0E+00)                                    \n",
-                            "                                                      \n",
-                            "prices   -1.1E+01    +8.4E+00                         \n",
-                            "        (+1.8E+01)  (+1.2E+01)                        \n",
-                            "                                                      \n",
-                            "sugar    +6.1E-02    -9.1E-02    +3.8E-02             \n",
-                            "        (+2.5E-01)  (+2.3E-01)  (+8.3E-02)            \n",
-                            "                                                      \n",
-                            "mushy    -5.9E-01    -6.2E-01    -2.3E-02    +4.8E-01 \n",
-                            "        (+2.1E+00)  (+1.5E+00)  (+2.5E+00)  (+1.3E+00)\n",
-                            "======================================================\n",
+                            "=========================================================================================================================\n",
+                            "Sigma:      1         prices      sugar       mushy     |  Sigma Squared:      1         prices      sugar       mushy   \n",
+                            "------  ----------  ----------  ----------  ----------  |  --------------  ----------  ----------  ----------  ----------\n",
+                            "  1      +1.2E+00                                       |        1          +1.5E+00    -1.4E+01    +7.3E-02    -7.1E-01 \n",
+                            "        (+3.0E+00)                                      |                  (+7.2E+00)  (+5.2E+01)  (+2.2E-01)  (+2.3E+00)\n",
+                            "                                                        |                                                                \n",
+                            "prices   -1.1E+01    +8.4E+00                           |      prices       -1.4E+01    +2.0E+02    -1.5E+00    +1.5E+00 \n",
+                            "        (+1.8E+01)  (+1.2E+01)                          |                  (+5.2E+01)  (+3.1E+02)  (+1.2E+00)  (+1.5E+01)\n",
+                            "                                                        |                                                                \n",
+                            "sugar    +6.1E-02    -9.1E-02    +3.8E-02               |      sugar        +7.3E-02    -1.5E+00    +1.3E-02    +2.0E-02 \n",
+                            "        (+2.5E-01)  (+2.3E-01)  (+8.3E-02)              |                  (+2.2E-01)  (+1.2E+00)  (+2.8E-02)  (+2.7E-01)\n",
+                            "                                                        |                                                                \n",
+                            "mushy    -5.9E-01    -6.2E-01    -2.3E-02    +4.8E-01   |      mushy        -7.1E-01    +1.5E+00    +2.0E-02    +9.6E-01 \n",
+                            "        (+2.1E+00)  (+1.5E+00)  (+2.5E+00)  (+1.3E+00)  |                  (+2.3E+00)  (+1.5E+01)  (+2.7E-01)  (+4.0E+00)\n",
+                            "=========================================================================================================================\n",
                             "\n",
                             "Beta Estimates (Robust SEs in Parentheses):\n",
                             "==========\n",
                             "  prices  \n",
                             "----------\n",
                             " -3.1E+01 \n",
                             "(+6.0E+00)\n",
@@ -635,52 +633,52 @@
             "execution_count": 10,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 2    +1.6E+02   +1.4E-06     +1.6E-02        +5.3E+03         +5.3E+07          +3.9E+33     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 2    +1.6E+02   +3.2E-05     +1.6E-02        +5.3E+03        0         +5.3E+07          +1.2E+21     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:09:37         68           202        131877       409484   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:03:02       Yes          64           78          68670       212085   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
-                            "======================================================\n",
-                            "Sigma:      1         prices      sugar       mushy   \n",
-                            "------  ----------  ----------  ----------  ----------\n",
-                            "  1      -7.4E-01                                     \n",
-                            "        (+2.0E+00)                                    \n",
-                            "                                                      \n",
-                            "prices   +1.3E+01    -3.8E-07                         \n",
-                            "        (+6.1E+00)  (+2.3E-07)                        \n",
-                            "                                                      \n",
-                            "sugar    -1.1E-01    +4.7E-09    -1.4E-10             \n",
-                            "        (+9.5E-02)  (+5.9E-06)  (+6.7E-07)            \n",
-                            "                                                      \n",
-                            "mushy    +1.5E-01    +2.0E-08    +9.7E-10    -4.6E-10 \n",
-                            "        (+6.8E-01)  (+1.9E-07)  (+2.6E-08)  (+7.3E-09)\n",
-                            "======================================================\n",
+                            "=========================================================================================================================\n",
+                            "Sigma:      1         prices      sugar       mushy     |  Sigma Squared:      1         prices      sugar       mushy   \n",
+                            "------  ----------  ----------  ----------  ----------  |  --------------  ----------  ----------  ----------  ----------\n",
+                            "  1      -7.4E-01                                       |        1          +5.5E-01    -9.4E+00    +8.3E-02    -1.1E-01 \n",
+                            "        (+2.1E+00)                                      |                  (+3.1E+00)  (+3.2E+01)  (+1.7E-01)  (+6.3E-01)\n",
+                            "                                                        |                                                                \n",
+                            "prices   +1.3E+01    -8.8E-06                           |      prices       -9.4E+00    +1.6E+02    -1.4E+00    +1.9E+00 \n",
+                            "        (+7.7E+00)  (+2.3E+03)                          |                  (+3.2E+01)  (+2.0E+02)  (+7.8E-01)  (+8.8E+00)\n",
+                            "                                                        |                                                                \n",
+                            "sugar    -1.1E-01    +1.1E-07    -3.0E-09               |      sugar        +8.3E-02    -1.4E+00    +1.2E-02    -1.7E-02 \n",
+                            "        (+2.1E-01)  (+1.9E+05)  (+7.3E+02)              |                  (+1.7E-01)  (+7.8E-01)  (+2.1E-02)  (+1.5E-01)\n",
+                            "                                                        |                                                                \n",
+                            "mushy    +1.5E-01    +4.6E-07    +1.9E-08    -1.3E-08   |      mushy        -1.1E-01    +1.9E+00    -1.7E-02    +2.2E-02 \n",
+                            "        (+6.8E-01)  (+3.5E+03)  (+6.3E+02)  (+1.8E+02)  |                  (+6.3E-01)  (+8.8E+00)  (+1.5E-01)  (+2.0E-01)\n",
+                            "=========================================================================================================================\n",
                             "\n",
                             "Beta Estimates (Robust SEs in Parentheses):\n",
                             "==========\n",
                             "  prices  \n",
                             "----------\n",
                             " -3.1E+01 \n",
-                            "(+3.9E+00)\n",
+                            "(+4.4E+00)\n",
                             "=========="
                         ]
                     },
                     "execution_count": 10,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
@@ -695,28 +693,28 @@
             "execution_count": 11,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 2    +1.8E+02   +1.3E-06     +1.1E+00        +6.0E+03         +5.8E+07          +4.8E+04     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 2    +1.8E+02   +1.3E-06     +1.1E+00        +6.0E+03        0         +5.8E+07          +4.8E+04     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:01:21         18           138         67387       212090   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:00:23       Yes          16           24          19540        60462   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
                             "======================================================\n",
                             "Sigma:      1         prices      sugar       mushy   \n",
                             "------  ----------  ----------  ----------  ----------\n",
                             "  1      +5.2E-02                                     \n",
                             "        (+1.1E+00)                                    \n",
@@ -757,26 +755,26 @@
                 "We see that all three models give similar estimates of the price coefficient $\\hat{\\alpha} \\approx -30$. Note a few of the estimated terms on the diagonal of $\\Sigma$ are negative. Since the diagonal consists of standard deviations, negative values are unrealistic. When using another optimization routine that supports bounds (like the default L-BFGS-B routine), these diagonal elements are by default bounded from below by zero."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Adding Demographics to the Problem\n",
+                "## Adding Demographics to the Problem\n",
                 "\n",
                 "To add demographic data we need to make two changes:\n",
                 "\n",
                 "1. We need to load `agent_data`, which for this cereal problem contains pre-computed Monte Carlo draws and demographics.\n",
                 "2. We need to add an `agent_formulation` to the model.\n",
                 "\n",
                 "The `agent data` has several reserved column names.\n",
                 "\n",
                 "- `market_ids` are the index that link the `agent data` to the `market_ids` in `product data`.\n",
                 "- `weights` are the weights $w_{it}$ attached to each agent. In each market, these should sum to one so that $\\sum_i w_{it} = 1$. It is often the case the $w_{it} = 1 / I_t$ where $I_t$ is the number of agents in market $t$, so that each agent gets equal weight. Other possibilities include quadrature nodes and weights.\n",
-                "- `nodes0`, `nodes1`, and so on are the nodes at which the unobserved agent tastes $\\mu_{jti}$ are evaluated. The nodes should be labeled from $0, \\ldots, K_2 - 1$ where $K_2$ is the number of random coefficients.\n",
+                "- `nodes0`, `nodes1`, and so on are the nodes at which the unobserved agent tastes $\\mu_{ijt}$ are evaluated. The nodes should be labeled from $0, \\ldots, K_2 - 1$ where $K_2$ is the number of random coefficients.\n",
                 "- Other fields are the realizations of the demographics $d$ themselves."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 12,
             "metadata": {},
@@ -1001,42 +999,42 @@
                 "nevo_problem"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "The initialized problem can be solved with :meth:`Problem.solve`. We'll use the same starting values as :ref:`references:Nevo (2000)`. By passing a diagonal matrix as starting values for $\\Sigma$, we're choosing to ignore covariance terms. Similarly, zeros in the starting values for $\\Pi$ mean that those parameters will be fixed at zero.\n",
+                "The initialized problem can be solved with :meth:`Problem.solve`. We'll use the same starting values as :ref:`references:Nevo (2000a)`. By passing a diagonal matrix as starting values for $\\Sigma$, we're choosing to ignore covariance terms. Similarly, zeros in the starting values for $\\Pi$ mean that those parameters will be fixed at zero.\n",
                 "\n",
-                "To replicate common estimates, we'll use the non-default BFGS optimization routine, and we'll configure :meth:`Problem.solve` to use 1-step GMM instead of 2-step GMM."
+                "To replicate common estimates, we'll use the non-default BFGS optimization routine (with a slightly tighter tolerance to avoid getting stuck at a spurious local minimum), and we'll configure :meth:`Problem.solve` to use 1-step GMM instead of 2-step GMM."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 15,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 1    +4.6E+00   +4.1E-10     +2.6E-05        +1.6E+04         +6.9E+07          +8.4E+08     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 1    +4.6E+00   +6.9E-06     +3.2E-05        +1.6E+04        0         +6.9E+07          +8.4E+08     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:02:17         56           180        155590       482541   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:01:00       Yes          51           57          46386       143970   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
                             "=====================================================================================================================\n",
                             "Sigma:      1         prices      sugar       mushy     |   Pi:      income    income_squared     age        child   \n",
                             "------  ----------  ----------  ----------  ----------  |  ------  ----------  --------------  ----------  ----------\n",
                             "  1      +5.6E-01                                       |    1      +2.3E+00      +0.0E+00      +1.3E+00    +0.0E+00 \n",
                             "        (+1.6E-01)                                      |          (+1.2E+00)                  (+6.3E-01)            \n",
@@ -1069,35 +1067,36 @@
                 "initial_sigma = np.diag([0.3302, 2.4526, 0.0163, 0.2441])\n",
                 "initial_pi = np.array([\n",
                 "  [ 5.4819,  0,      0.2037,  0     ],\n",
                 "  [15.8935, -1.2000, 0,       2.6342],\n",
                 "  [-0.2506,  0,      0.0511,  0     ],\n",
                 "  [ 1.2650,  0,     -0.8091,  0     ]\n",
                 "])\n",
+                "tighter_bfgs = pyblp.Optimization('bfgs', {'gtol': 1e-5})\n",
                 "nevo_results = nevo_problem.solve(\n",
                 "    initial_sigma,\n",
                 "    initial_pi,\n",
-                "    optimization=bfgs,\n",
+                "    optimization=tighter_bfgs,\n",
                 "    method='1s'\n",
                 ")\n",
                 "nevo_results"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Results are similar to those in the original paper with a (non-scaled) objective value of $q(\\hat{\\theta}) = \\text{8.96E-7} = 4.65 / N^2$ and a price coefficient of $\\hat{\\alpha} = -62.7$. "
+                "Results are similar to those in the original paper with a (scaled) objective value of $q(\\hat{\\theta}) = 4.65$ and a price coefficient of $\\hat{\\alpha} = -62.7$. "
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Restricting Parameters\n",
+                "## Restricting Parameters\n",
                 "\n",
                 "Because the interactions between `price`, `income`, and `income_squared` are potentially collinear, we might worry that $\\hat{\\Pi}_{21} = 588$ and  $\\hat{\\Pi}_{22} = -30.2$ are pushing the price coefficient in opposite directions. Both are large in magnitude but statistically insignficant. One way of dealing with this is to restrict $\\Pi_{22} = 0$.\n",
                 "\n",
                 "There are two ways we can do this:\n",
                 "\n",
                 "1. Change the initial $\\Pi_0$ values to make this term zero.\n",
                 "2. Change the agent formula to drop `income_squared`.\n",
@@ -1110,28 +1109,28 @@
             "execution_count": 16,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 1    +1.5E+01   +2.7E-06     +4.7E-02        +1.7E+04         +6.9E+07          +5.7E+05     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 1    +1.5E+01   +5.2E-06     +4.7E-02        +1.7E+04        0         +6.9E+07          +5.7E+05     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:01:17         36           99          77185       239212   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:00:45       Yes          34           40          34392       106492   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
                             "=====================================================================================================================\n",
                             "Sigma:      1         prices      sugar       mushy     |   Pi:      income    income_squared     age        child   \n",
                             "------  ----------  ----------  ----------  ----------  |  ------  ----------  --------------  ----------  ----------\n",
                             "  1      +3.7E-01                                       |    1      +3.1E+00      +0.0E+00      +1.2E+00    +0.0E+00 \n",
                             "        (+1.2E-01)                                      |          (+1.1E+00)                  (+1.0E+00)            \n",
@@ -1162,15 +1161,15 @@
             ],
             "source": [
                 "restricted_pi = initial_pi.copy()\n",
                 "restricted_pi[1, 1] = 0\n",
                 "nevo_problem.solve(\n",
                 "    initial_sigma,\n",
                 "    restricted_pi,\n",
-                "    optimization=bfgs,\n",
+                "    optimization=tighter_bfgs,\n",
                 "    method='1s'\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -1183,28 +1182,28 @@
             "execution_count": 17,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "==============================================================================================\n",
-                            "GMM   Objective  Gradient      Hessian         Hessian     Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Condition Number  Condition Number \n",
-                            "----  ---------  --------  --------------  --------------  ----------------  -----------------\n",
-                            " 1    +1.5E+01   +2.3E-07     +4.7E-02        +1.7E+04         +6.9E+07          +5.7E+05     \n",
-                            "==============================================================================================\n",
+                            "=======================================================================================================\n",
+                            "GMM   Objective  Gradient      Hessian         Hessian     Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value      Norm    Min Eigenvalue  Max Eigenvalue  Shares   Condition Number  Condition Number \n",
+                            "----  ---------  --------  --------------  --------------  -------  ----------------  -----------------\n",
+                            " 1    +1.5E+01   +5.2E-06     +4.7E-02        +1.7E+04        0         +6.9E+07          +5.7E+05     \n",
+                            "=======================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:01:06         35           78          61695       191191   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:00:45       Yes          34           40          34403       106539   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
                             "=====================================================================================================\n",
                             "Sigma:      1         prices      sugar       mushy     |   Pi:      income       age        child   \n",
                             "------  ----------  ----------  ----------  ----------  |  ------  ----------  ----------  ----------\n",
                             "  1      +3.7E-01                                       |    1      +3.1E+00    +1.2E+00    +0.0E+00 \n",
                             "        (+1.2E-01)                                      |          (+1.1E+00)  (+1.0E+00)            \n",
@@ -1241,44 +1240,44 @@
                 "    product_data,\n",
                 "    restricted_formulation,\n",
                 "    agent_data\n",
                 ")\n",
                 "restricted_problem.solve(\n",
                 "    initial_sigma,\n",
                 "    deleted_pi,\n",
-                "    optimization=bfgs,\n",
+                "    optimization=tighter_bfgs,\n",
                 "    method='1s'\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "The parameter estimates and standard errors are identical for both approaches. Based on the number of fixed point iterations, there is some evidence that the solver took a slightly different path for each problem, but both restricted problems arrived at identical answers. The $\\hat{\\Pi}_{12}$ interaction term is still insignificant."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         },
         "pycharm": {
             "stem_cell": {
                 "cell_type": "raw",
                 "metadata": {
                     "collapsed": false
                 },
```

### Comparing `pyblp-0.9.0/docs/notebooks/tutorial/simulation.ipynb` & `pyblp-1.0.0/docs/notebooks/tutorial/simulation.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9916145174753437%*

 * *Differences: {"'cells'": '{1: {\'outputs\': {0: {\'data\': {\'text/plain\': ["\'1.0.0\'"]}}}}, 8: {\'source\': '*

 * *            "{insert: [(4, 'At this stage, simulated variables are not consistent with true "*

 * *            'parameters, so we still need to solve the simulation with '*

 * *            ':meth:`Simulation.replace_endogenous`. This method replaced simulated prices and '*

 * *            'market shares with values that are consistent with the true parameters. Just like '*

 * *            ':meth:`ProblemResults.compute_prices`,  […]*

```diff
@@ -11,15 +11,15 @@
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "'0.9.0'"
+                            "'1.0.0'"
                         ]
                     },
                     "execution_count": 1,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -163,32 +163,32 @@
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "When :class:`Simulation` is initialized, it constructs :attr:`Simulation.agent_data` and simulates :attr:`Simulation.product_data`.\n",
                 "\n",
                 "The :class:`Simulation` can be further configured with other arguments that determine how unobserved product characteristics are simulated and how marginal costs are specified.\n",
                 "\n",
-                "At this stage, simulated variables are not consistent with true parameters, so we still need to solve the simulation with :meth:`Simulation.replace_endogenous`. This method replaced simulated prices and marketshares with values that are consistent with the true parameters. Just like :meth:`ProblemResults.compute_prices`, to do so it iterates over the $\\zeta$-markup equation from :ref:`references:Morrow and Skerlos (2011)`."
+                "At this stage, simulated variables are not consistent with true parameters, so we still need to solve the simulation with :meth:`Simulation.replace_endogenous`. This method replaced simulated prices and market shares with values that are consistent with the true parameters. Just like :meth:`ProblemResults.compute_prices`, to do so it iterates over the $\\zeta$-markup equation from :ref:`references:Morrow and Skerlos (2011)`."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 5,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Simulation Results Summary:\n",
-                            "=====================================\n",
-                            "Computation  Fixed Point  Contraction\n",
-                            "   Time      Iterations   Evaluations\n",
-                            "-----------  -----------  -----------\n",
-                            " 00:00:00        721          721    \n",
-                            "====================================="
+                            "======================================================================================================\n",
+                            "Computation  Fixed Point  Fixed Point  Contraction  Profit Gradients  Profit Hessians  Profit Hessians\n",
+                            "   Time       Failures    Iterations   Evaluations      Max Norm      Min Eigenvalue   Max Eigenvalue \n",
+                            "-----------  -----------  -----------  -----------  ----------------  ---------------  ---------------\n",
+                            " 00:00:01         0           721          721          +1.3E-13         -8.4E-01         -9.6E-06    \n",
+                            "======================================================================================================"
                         ]
                     },
                     "execution_count": 5,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -253,28 +253,28 @@
             "execution_count": 7,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "Problem Results Summary:\n",
-                            "=====================================================================================================\n",
-                            "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Weighting Matrix  Covariance Matrix\n",
-                            "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Condition Number  Condition Number \n",
-                            "----  ---------  -------------  ---------------  ---------------  ----------------  -----------------\n",
-                            " 2    +6.4E+00     +2.1E-11        +7.2E+00         +3.8E+03          +3.7E+04          +1.5E+04     \n",
-                            "=====================================================================================================\n",
+                            "==============================================================================================================\n",
+                            "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
+                            "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
+                            "----  ---------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
+                            " 2    +6.4E+00     +6.9E-08        +7.2E+00         +3.8E+03         0         +3.7E+04          +1.5E+04     \n",
+                            "==============================================================================================================\n",
                             "\n",
                             "Cumulative Statistics:\n",
-                            "================================================================\n",
-                            "Computation  Optimization   Objective   Fixed Point  Contraction\n",
-                            "   Time       Iterations   Evaluations  Iterations   Evaluations\n",
-                            "-----------  ------------  -----------  -----------  -----------\n",
-                            " 00:00:56         25           79          22958        72258   \n",
-                            "================================================================\n",
+                            "===========================================================================\n",
+                            "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
+                            "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
+                            "-----------  ---------  ------------  -----------  -----------  -----------\n",
+                            " 00:00:13       Yes          23           30          8295         26314   \n",
+                            "===========================================================================\n",
                             "\n",
                             "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
                             "==================\n",
                             "Sigma:      x     \n",
                             "------  ----------\n",
                             "  x      +7.8E-01 \n",
                             "        (+5.2E-01)\n",
@@ -300,17 +300,18 @@
                     "execution_count": 7,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "results = problem.solve(\n",
-                "   sigma=0.5 * simulation.sigma, \n",
-                "   pi=0.5 * simulation.pi,\n",
-                "   beta=[None, 0.5 * simulation.beta[1], None]\n",
+                "    sigma=0.5 * simulation.sigma, \n",
+                "    pi=0.5 * simulation.pi,\n",
+                "    beta=[None, 0.5 * simulation.beta[1], None],\n",
+                "    optimization=pyblp.Optimization('l-bfgs-b', {'gtol': 1e-5})\n",
                 ")\n",
                 "results"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -324,15 +325,15 @@
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "array([[ 1.        ,  0.96223514],\n",
                             "       [-2.        , -2.00792431],\n",
-                            "       [ 2.        ,  2.10032016]])"
+                            "       [ 2.        ,  2.10032015]])"
                         ]
                     },
                     "execution_count": 8,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -387,27 +388,27 @@
             "source": [
                 "In addition to checking that the configuration for a model based on actual data makes sense, the :class:`Simulation` class can also be a helpful tool for better understanding under what general conditions BLP models can be accurately estimated. Simulations are also used extensively in pyblp's test suite."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "Python 3 (ipykernel)",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.3"
+            "version": "3.9.15"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `pyblp-0.9.0/docs/references.rst` & `pyblp-1.0.0/docs/references.rst`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,23 @@
 References
 ==========
 
-This page contains a full list of references cited in the documentation, including the original work of :ref:`references:Berry, Levinsohn, and Pakes (1995)`. If you use PyBLP in your research, we ask that you also cite the below :ref:`references:Conlon and Gortmaker (2020)`, which describes the advances implemented in the package.
+This page contains a full list of references cited in the documentation, including the original work of :ref:`references:Berry, Levinsohn, and Pakes (1995)`. If you use PyBLP in your research, we ask that you also cite the below :ref:`references:Conlon and Gortmaker (2020)`, which describes the advances implemented in the package. If you use PyBLP's micro moments functionality, we ask that you also cite :ref:`references:Conlon and Gortmaker (2023)`, which describes the standardized framework implemented by PyBLP for incorporating micro data into BLP-style estimation.
 
 
 Conlon and Gortmaker (2020)
 ---------------------------
 
-Conlon, Christopher T., and Jeff Gortmaker (2020). `Best practices for differentiated products demand estimation with PyBLP <https://jeffgortmaker.com/files/pyblp.pdf>`_. Working paper.
+Conlon, Christopher, and Jeff Gortmaker (2020). `Best practices for differentiated products demand estimation with PyBLP <https://ideas.repec.org/a/bla/randje/v51y2020i4p1108-1161.html>`_. *RAND Journal of Economics, 51* (4), 1108-1161.
+
+
+Conlon and Gortmaker (2023)
+---------------------------
+
+Conlon, Christopher, and Jeff Gortmaker (2023). `Incorporating micro data into differentiated products demand estimation with PyBLP <https://jeffgortmaker.com/files/micro.pdf>`_. Working paper.
 
 
 Other References
 ----------------
 
 Amemiya (1977)
 ~~~~~~~~~~~~~~
@@ -51,14 +57,20 @@
 
 Berry, Levinsohn, and Pakes (2004)
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Berry, Steven, James Levinsohn, and Ariel Pakes (2004). `Differentiated products demand systems from a combination of micro and macro data: The new car market <https://ideas.repec.org/a/ucp/jpolec/v112y2004i1p68-105.html>`_. *Journal of Political Economy, 112* (1), 68-105.
 
 
+Berry and Pakes (2007)
+~~~~~~~~~~~~~~~~~~~~~~
+
+Berry, Steven, and Ariel Pakes (2007). `The pure characteristics demand model <https://ideas.repec.org/a/ier/iecrev/v48y2007i4p1193-1225.html>`_. *International Economic Review, 48* (4), 1193-1225.
+
+
 Brenkers and Verboven (2006)
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Brenkers, Randy, and Frank Verboven (2006). `Liberalizing a distribution system: The European car market <https://ideas.repec.org/a/tpr/jeurec/v4y2006i1p216-251.html>`_. *Journal of the European Economic Association, 4* (1), 216-251.
 
 
 Brunner, Heiss, Romahn, and Weiser (2017)
@@ -99,14 +111,20 @@
 
 Grigolon and Verboven (2014)
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Grigolon, Laura, and Frank Verboven (2014). `Nested logit or random coefficients logit? A comparison of alternative discrete choice models of product differentiation <https://ideas.repec.org/a/tpr/restat/v96y2014i5p916-935.html>`_. *Review of Economics and Statistics, 96* (5), 916-935.
 
 
+Hausman, Leonard, and Zona (1994)
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Hausman, Jerry, Gregory Leonard, and J. Douglas Zona (1994). `Competitive analysis with differentiated products <https://ideas.repec.org/a/adr/anecst/y1994i34p143-157.html>`_. *Annals of Economics and Statistics, 34*, 143-157.
+
+
 Hansen (1982)
 ~~~~~~~~~~~~~
 
 Hansen, Lars Peter (1982). `Large sample properties of generalized method of moments estimators <https://ideas.repec.org/a/ecm/emetrp/v50y1982i4p1029-54.html>`_. *Econometrica, 50* (4), 1029-1054.
 
 
 Heiss and Winschel (2008)
@@ -147,36 +165,36 @@
 
 Morrow and Skerlos (2011)
 ~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Morrow, W. Ross, and Steven J. Skerlos (2011). `Fixed-point approaches to computing Bertrand-Nash equilibrium prices under mixed-logit demand <https://ideas.repec.org/a/inm/oropre/v59y2011i2p328-345.html>`_. *Operations Research, 59* (2), 328-345.
 
 
-Nevo (1997)
-~~~~~~~~~~~
+Nevo (2000a)
+~~~~~~~~~~~~
 
-Nevo, Aviv (1997). `Mergers with differentiated products: The case of the ready-to-eat cereal industry <https://ideas.repec.org/p/cdl/compol/qt1d53t6ts.html>`_. Competition Policy Center, Working Paper Series qt1d53t6ts, Competition Policy Center, Institute for Business and Economic Research, UC Berkeley.
+Nevo, Aviv (2000). `A practitioner's guide to estimation of random‐coefficients logit models of demand <https://ideas.repec.org/a/bla/jemstr/v9y2000i4p513-548.html>`_. *Journal of Economics & Management Strategy, 9* (4), 513-548.
 
 
-Nevo (2000)
-~~~~~~~~~~~
+Nevo (2000b)
+~~~~~~~~~~~~
 
-Nevo, Aviv (2000). `A practitioner's guide to estimation of random‐coefficients logit models of demand <https://ideas.repec.org/a/bla/jemstr/v9y2000i4p513-548.html>`_. *Journal of Economics & Management Strategy, 9* (4), 513-548.
+Nevo, Aviv (2000). `Mergers with differentiated products: The case of the ready-to-eat cereal industry <https://ideas.repec.org/a/rje/randje/v31y2000iautumnp395-421.html>`_. *RAND Journal of Economics, 31* (3), 395-421.
 
 
 Newey and West (1987)
 ~~~~~~~~~~~~~~~~~~~~~
 
 Newey, Whitney K., and Kenneth D. West (1987). `Hypothesis testing with efficient method of moments estimation <https://ideas.repec.org/a/ier/iecrev/v28y1987i3p777-87.html>`_. *International Economic Review, 28* (3), 777-787.
 
 
 Owen (2013)
 ~~~~~~~~~~~
 
-Owen, Art B. (2013). `Monte Carlo theory, methods and examples <https://statweb.stanford.edu/~owen/mc/>`_.
+Owen, Art B. (2013). `Monte Carlo theory, methods and examples <https://artowen.su.domains/mc/>`_.
 
 
 Owen (2017)
 ~~~~~~~~~~~
 
 Owen, Art B. (2017). `A randomized Halton algorithm in R <https://arxiv.org/pdf/1706.02808.pdf>`_.
 
@@ -195,17 +213,17 @@
 
 Reynaerts, Varadhan, and Nash (2012)
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Reynaerts, Jo, Ravi Varadhan, and John C. Nash (2012). `Enhancing the convergence properties of the BLP (1995) contraction mapping <https://ideas.repec.org/p/ete/vivwps/35.html>`_. VIVES discussion paper 35.
 
 
-Skrainka (2012)
-~~~~~~~~~~~~~~~
-
-Skrainka, Benjamin S. (2012). `A large scale study of the small sample performance of random coefficient models of demand <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1942627>`_.
-
-
 Varadhan and Roland (2008)
 ~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Varadhan, Ravi, and Christophe Roland (2008). `Simple and globally convergent methods for accelerating the convergence of any EM algorithm <https://ideas.repec.org/a/bla/scjsta/v35y2008i2p335-353.html>`_. *Scandinavian Journal of Statistics, 35* (2), 335-353.
+
+
+Werden (1997)
+~~~~~~~~~~~~~
+
+Werden, Gregory J. (1997). `Simulating the effects of differentiated products mergers: A practitioners' guide <https://ideas.repec.org/p/ags/rpssiw/25942.html>`_. Economic Analysis Group, Proceedings of NE-165 Conference, Washington, D.C., June 20–21, 1996, 1997.
```

### Comparing `pyblp-0.9.0/docs/static/override.css` & `pyblp-1.0.0/docs/static/override.css`

 * *Files identical despite different names*

### Comparing `pyblp-0.9.0/docs/testing.rst` & `pyblp-1.0.0/docs/testing.rst`

 * *Files 8% similar despite different names*

```diff
@@ -1,30 +1,31 @@
 Testing
 =======
 
-Testing is done with the `tox <https://tox.readthedocs.io/en/latest/>`_ automation tool, which runs a `pytest <https://docs.pytest.org/en/latest/>`_-backed test suite in the ``tests/`` directory. This `FAQ <https://tox.readthedocs.io/en/latest/developers.html>`_ contains some useful information about how to use tox on Windows.
+Testing is done with the `tox <https://tox.wiki/en/latest/>`_ automation tool, which runs a `pytest <https://docs.pytest.org/en/latest/>`_-backed test suite in the ``tests/`` directory.
 
 
 Testing Requirements
 --------------------
 
 In addition to the installation requirements for the package itself, running tests and building documentation requires additional packages specified by the ``tests`` and ``docs`` extras in ``setup.py``, along with any other explicitly specified ``deps`` in ``tox.ini``.
 
 The full suite of tests also requires installation of the following software:
 
 - `Artleys Knitro <https://www.artelys.com/solvers/knitro/>`_ version 10.3 or newer: testing optimization routines.
 - `MATLAB <https://www.mathworks.com/products/matlab.html>`_: comparing sparse grids with those created by the function `nwspgr <http://www.sparse-grids.de/>`_ created by Florian Heiss and Viktor Winschel, which must be included in a directory on the MATLAB path.
+- `R <https://www.r-project.org/>`_: simulating nested logit errors created by the package `evd <https://cran.r-project.org/web/packages/evd/index.html>`_ created by Alec Stephenson, which must be installed.
 
 If software is not installed, its associated tests will be skipped. Additionally, some tests that require support for extended precision will be skipped if on the platform running the tests, ``numpy.longdouble`` has the same precision as ``numpy.float64``. This tends to be the case on Windows.
 
 
 Running Tests
 -------------
 
-Defined in ``tox.ini`` are environments that test the package under different python versions, check types, enforce style guidelines, verify the integrity of the documentation, and release the package. The following command can be run in the top-level ``pyblp`` directory to run all testing environments::
+Defined in ``tox.ini`` are environments that test the package under different python versions, check types, enforce style guidelines, verify the integrity of the documentation, and release the package. First, `tox <https://tox.wiki/en/latest/>`_ should be installed on top of an Anaconda installation. The following command can be run in the top-level ``pyblp`` directory to run all testing environments::
 
     tox
 
 You can choose to run only one environment, such as the one that builds the documentation, with the ``-e`` flag::
 
     tox -e docs
```

### Comparing `pyblp-0.9.0/LICENSE.txt` & `pyblp-1.0.0/LICENSE.txt`

 * *Files 14% similar despite different names*

```diff
@@ -1,7 +1,7 @@
-Copyright 2019 Jeff Gortmaker
+Copyright 2021 Jeff Gortmaker and Christopher Conlon
 
 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
 
 The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
 
 THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
```

### Comparing `pyblp-0.9.0/pyblp/configurations/formulation.py` & `pyblp-1.0.0/pyblp/configurations/formulation.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Formulation of data matrices and absorption of fixed effects."""
 
 import functools
 import numbers
 import token
-from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Set, Tuple, Union
+from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Set, Tuple, Type, Union
 
 import numpy as np
 import patsy
 import patsy.builtins
 import patsy.contrasts
 import patsy.desc
 import patsy.design_info
@@ -147,37 +147,55 @@
         if absorb_options is None:
             absorb_options = {}
         elif not isinstance(absorb_options, dict):
             raise TypeError("absorb_options must be None or a dict.")
         self._absorb_method = absorb_method
         self._absorb_options = absorb_options
 
+    def __reduce__(self) -> Tuple[Type['Formulation'], Tuple]:
+        """Handle pickling."""
+        return self.__class__, (self._formula, self._absorb, self._absorb_method, self._absorb_options)
+
     def __str__(self) -> str:
         """Format the terms as a string."""
         names: List[str] = []
         for term in self._terms:
             names.append('1' if term == patsy.desc.INTERCEPT else term.name())
         for absorbed_term in self._absorbed_terms:
             names.append(f'Absorb[{absorbed_term.name()}]')
         return ' + '.join(names)
 
-    def _build_matrix(self, data: Mapping) -> Tuple[Array, List['ColumnFormulation'], Data]:
+    def _build_matrix(
+            self, data: Mapping, fallback_index: Optional[int] = None, ignore_na: bool = False) -> (
+            Tuple[Array, List['ColumnFormulation'], Data]):
         """Convert a mapping from variable names to arrays into the designed matrix, a list of column formulations that
         describe the columns of the matrix, and a mapping from variable names to arrays of data underlying the matrix,
-        which include unchanged continuous variables and indicators constructed from categorical variables.
+        which include unchanged continuous variables and indicators constructed from categorical variables. If there is
+        a fallback index, allow variables to optionally have this index.
         """
 
         # normalize the data
         data_mapping: Data = {}
         for name in self._names:
             try:
                 data_mapping[name] = np.asarray(data[name]).flatten()
             except Exception as exception:
-                origin = patsy.origin.Origin(self._formula, 0, len(self._formula))
-                raise patsy.PatsyError(f"Failed to load data for '{name}'.", origin) from exception
+                fallback = False
+                if fallback_index is not None:
+                    try:
+                        data_mapping[name] = np.asarray(data[f'{name}{fallback_index}']).flatten()
+                    except Exception:
+                        pass
+                    else:
+                        fallback = True
+
+                if not fallback:
+                    origin = patsy.origin.Origin(self._formula, 0, len(self._formula))
+                    message = f"Failed to load data for '{name}' because of the above exception."
+                    raise patsy.PatsyError(message, origin) from exception
 
         # always have at least one column to represent the size of the data
         if not data_mapping:
             data_mapping = {'': np.zeros(extract_size(data))}
 
         # design the matrix (adding an intercept term if there are absorbed terms gets Patsy to use reduced coding)
         if self._absorbed_terms:
@@ -209,29 +227,30 @@
                 indicator_design = design_matrix([patsy.desc.Term([factor])], data_mapping)
                 indicator_matrix = build_matrix(indicator_design, data_mapping)
                 for name, indicator in zip(indicator_design.column_names, indicator_matrix.T):
                     symbol = CategoricalTreatment.parse_full_symbol(name)
                     if symbol.name in underlying_data:
                         underlying_data[symbol.name] = indicator
 
-        matrix = build_matrix(matrix_design, data_mapping)
+        matrix = build_matrix(matrix_design, data_mapping, ignore_na)
         return matrix[:, column_indices], column_formulations, underlying_data
 
     def _build_ids(self, data: Mapping) -> Array:
         """Convert a mapping from variable names to arrays into the designed matrix of IDs to be absorbed."""
 
         # normalize the data
         data_mapping: Data = {}
         for name in self._absorbed_names:
             try:
                 data_mapping[name] = np.asarray(data[name]).flatten()
             except Exception as exception:
                 assert self._absorb is not None
                 origin = patsy.origin.Origin(self._absorb, 0, len(self._absorb))
-                raise patsy.PatsyError(f"Failed to load data for '{name}'.", origin) from exception
+                message = f"Failed to load data for '{name}' because of the above exception."
+                raise patsy.PatsyError(message, origin) from exception
 
         # build columns of absorbed IDs
         ids_columns: List[Array] = []
         for term in self._absorbed_terms:
             factor_columns: List[Array] = []
             term_design = design_matrix([term], data_mapping)
             for factor, info in term_design.factor_infos.items():
@@ -239,42 +258,46 @@
                     raise patsy.PatsyError("Only categorical variables can be absorbed.", factor.origin)
                 symbol = parse_expression(factor.name())
                 factor_columns.append(data_mapping[symbol.name])
             ids_columns.append(interact_ids(*factor_columns))
 
         return np.column_stack(ids_columns)
 
-    def _build_absorb(self, ids: Array) -> Callable[[Array], Tuple[Array, List[Error]]]:
+    def _build_absorb(self, ids: Array) -> 'Absorb':
         """Build a function used to absorb fixed effects defined by columns of IDs."""
         import pyhdfe
-
-        # initialize the algorithm for repeated absorption
-        algorithm = pyhdfe.create(
+        return Absorb(pyhdfe.create(
             ids, drop_singletons=False, compute_degrees=False, residualize_method=self._absorb_method,
             options=self._absorb_options
-        )
+        ))
 
-        def absorb(matrix: Array) -> Tuple[Array, List[Error]]:
-            """Handle any absorption errors."""
-            errors: List[Error] = []
-            try:
-                matrix = algorithm.residualize(matrix)
-            except Exception as exception:
-                errors.append(exceptions.AbsorptionError(exception))
-            return matrix, errors
 
-        return absorb
+class Absorb(object):
+    """Wrapper for PyHDFE fixed effect absorption."""
+
+    def __init__(self, algorithm: Any) -> None:
+        """Store the PyHDFE algorithm."""
+        self.algorithm = algorithm
+
+    def __call__(self, matrix: Array) -> Tuple[Array, List[Error]]:
+        """Handle any absorption errors."""
+        errors: List[Error] = []
+        try:
+            matrix = self.algorithm.residualize(matrix)
+        except Exception as exception:
+            errors.append(exceptions.AbsorptionError(exception))
+        return matrix, errors
 
 
 class ColumnFormulation(object):
     """Information about a single column in a matrix formulation."""
 
     names: Set[str]
     expression: sp.Expr
-    derivatives: Dict[str, sp.Expr]
+    derivatives: Dict[Tuple[str, int], sp.Expr]
 
     def __init__(self, formula: str, expression: sp.Expr) -> None:
         """Parse the column name into a patsy term and replace any categorical variables in its SymPy expression with
         the correct indicator variable symbols.
         """
         self.expression = expression
         self.names = {str(s) for s in expression.free_symbols}
@@ -283,15 +306,15 @@
         for factor in parse_terms(formula)[-1].factors:
             name = factor.name()
             base_symbol = CategoricalTreatment.parse_base_symbol(name)
             if base_symbol is not None:
                 self.expression = self.expression.replace(base_symbol, CategoricalTreatment.parse_full_symbol(name))
 
         # cache evaluated derivatives
-        self.derivatives: Dict[str, sp.Expr] = {}
+        self.derivatives: Dict[Tuple[str, int], sp.Expr] = {}
 
     def __str__(self) -> str:
         """Format the expression as a string."""
         return str(self.expression)
 
     def __repr__(self) -> str:
         """Defer to the string representation."""
@@ -305,27 +328,29 @@
         """Defer to the string representation."""
         return str(self) == str(other)
 
     def evaluate(self, data: Mapping, data_override: Optional[Mapping] = None) -> Array:
         """Evaluate the SymPy column expression at the values supplied by mappings from variable names to arrays."""
         return evaluate_expression(self.expression, data, data_override)
 
-    def evaluate_derivative(self, name: str, data: Mapping, data_override: Optional[Mapping] = None) -> Array:
+    def evaluate_derivative(
+            self, name: str, data: Mapping, data_override: Optional[Mapping] = None, order: int = 1) -> Array:
         """Differentiate the SymPy column expression with respect to a variable name and evaluate the derivative at
         values supplied by mappings from variable names to arrays.
         """
-        return evaluate_expression(self.differentiate(name), data, data_override)
+        return evaluate_expression(self.differentiate(name, order), data, data_override)
 
-    def differentiate(self, name: str) -> sp.Expr:
+    def differentiate(self, name: str, order: int = 1) -> sp.Expr:
         """Differentiate the SymPy column expression with respect to a variable name. Cache calls for speed."""
-        derivative = self.derivatives.get(name)
-        if derivative is None:
-            derivative = self.expression.diff(sp.Symbol(name))
-            self.derivatives[name] = derivative
-        return derivative
+        if (name, order) not in self.derivatives:
+            self.derivatives[(name, order)] = self.expression
+            for _ in range(order):
+                self.derivatives[(name, order)] = self.derivatives[(name, order)].diff(sp.Symbol(name))
+
+        return self.derivatives[(name, order)]
 
 
 class EvaluationEnvironment(patsy.eval.EvalEnvironment):
     """Execution environment that parses SymPy expressions from strings and evaluates them at values from a data mapping
     represented as a namespace.
     """
 
@@ -431,39 +456,41 @@
 
 
 def design_matrix(terms: Sequence[patsy.desc.Term], data: Mapping) -> patsy.design_info.DesignInfo:
     """Design a patsy matrix."""
     return patsy.build.design_matrix_builders([terms], lambda: iter([data]), EvaluationEnvironment([data]))[0]
 
 
-def build_matrix(design: patsy.design_info.DesignInfo, data: Mapping) -> Array:
+def build_matrix(design: patsy.design_info.DesignInfo, data: Mapping, ignore_na: bool = False) -> Array:
     """Build a matrix according to its design and data mapping variable names to arrays."""
 
     # identify the number of rows in the data
     size = next(iter(data.values())).shape[0]
 
     # if the design lacks factors, it must consist of only an intercept term
     if not design.factor_infos:
         return np.ones((size, 1))
 
     # build the matrix and raise an exception if there are any null values
-    matrix = patsy.build.build_design_matrices([design], data, NA_action='raise')[0].base
+    na_action = patsy.NAAction(NA_types=[]) if ignore_na else 'raise'
+    matrix = patsy.build.build_design_matrices([design], data, NA_action=na_action)[0].base
 
     # if the design did not use any data, the matrix may be a single row that needs to be stacked to the proper height
     return matrix if matrix.shape[0] == size else np.repeat(matrix[[0]], size, axis=0)
 
 
 def parse_term_expression(term: patsy.desc.Term) -> sp.Expr:
     """Multiply the SymPy expressions parsed from each factor in a patsy term."""
     expression = sp.Integer(1)
     for factor in term.factors:
         try:
             expression *= parse_expression(factor.name())
         except Exception as exception:
-            raise patsy.PatsyError("Failed to parse a term.", factor.origin) from exception
+            message = "Failed to parse a term because of the above exception."
+            raise patsy.PatsyError(message, factor.origin) from exception
 
     return expression
 
 
 def parse_expression(string: str, mark_categorical: bool = False) -> sp.Expr:
     """Parse a SymPy expression from a string. Optionally, preserve the categorical marker function instead of treating
     it like the identify function.
@@ -515,15 +542,15 @@
 
     # parse the expression, validate it by attempting to represent it as a string, and validate categorical markers
     try:
         expression = sympy.parsing.sympy_parser.parse_expr(string, mapping, [transform_tokens], evaluate=False)
         str(expression)
         validate_categorical(expression)
     except (TypeError, ValueError) as exception:
-        raise ValueError(f"The expression '{string}' is malformed.") from exception
+        raise ValueError(f"The expression '{string}' is malformed because of the above exception.") from exception
 
     # replace patsy functions with the identity function, unless categorical variables are to be explicitly marked
     for name in patsy_function_names:
         if name != 'C' or not mark_categorical:
             expression = expression.replace(mapping[name], sp.Id)
 
     return expression
```

### Comparing `pyblp-0.9.0/pyblp/configurations/integration.py` & `pyblp-1.0.0/pyblp/configurations/integration.py`

 * *Files 0% similar despite different names*

```diff
@@ -59,15 +59,15 @@
     size : `int`
         The number of draws if ``specification`` is ``'monte_carlo'``, ``'halton'``, ``'lhs'``, or ``'mlhs'``, and the
         level of the quadrature rule otherwise.
     specification_options : `dict, optional`
         Options for the integration specification. The ``'monte_carlo'``, ``'halton'``, ``'lhs'``, and ``'mlhs'``
         specifications support the following option:
 
-            - **seed** : (`int`) - Passed to :class:`numpy.random.mtrand.RandomState` to seed the random number
+            - **seed** : (`int`) - Passed to :class:`numpy.random.RandomState` to seed the random number
               generator before building integration nodes. By default, a seed is not passed to the random number
               generator. For ``'halton'`` draws, this is only relevant if ``scramble`` is ``True`` (which is the
               default).
 
         The ``'halton'`` specification supports the following options:
 
             - **discard** : (`int`) - How many values at the beginning of each dimension's Halton sequence to discard.
```

### Comparing `pyblp-0.9.0/pyblp/configurations/iteration.py` & `pyblp-1.0.0/pyblp/configurations/iteration.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,31 +7,31 @@
 import scipy.optimize
 
 from ..utilities.basics import Array, Options, SolverStats, StringRepresentation, format_options
 
 
 # define contraction function types
 ContractionResults = Tuple[Array, Optional[Array], Optional[Array]]
-ContractionFunction = Callable[[Array], ContractionResults]
+ContractionFunction = Callable[[Array, int, int], ContractionResults]
+ContractionWrapper = Callable[[Array], ContractionResults]
 
 
 class Iteration(StringRepresentation):
     r"""Configuration for solving fixed point problems.
 
     Parameters
     ----------
     method : `str or callable`
         The fixed point iteration routine that will be used. The following routines do not use analytic Jacobians:
 
             - ``'simple'`` - Non-accelerated iteration.
 
             - ``'squarem'`` - SQUAREM acceleration method of :ref:`references:Varadhan and Roland (2008)` and considered
               in the context of the BLP problem in :ref:`references:Reynaerts, Varadhan, and Nash (2012)`. This
-              implementation uses a first-order squared non-monotone extrapolation scheme. If there are any errors
-              during the acceleration step, it uses the last values for the next iteration of the algorithm.
+              implementation uses a first-order squared non-monotone extrapolation scheme.
 
             - ``'broyden1'`` - Use the :func:`scipy.optimize.root` Broyden's first Jacobian approximation method, known
               as Broyden's good method.
 
             - ``'broyden2'`` - Use the :func:`scipy.optimize.root` Broyden's second Jacobian approximation method, known
               as Broyden's bad method.
 
@@ -110,17 +110,20 @@
 
             - **step_factor** : (`float`) - When the step length exceeds ``step_max``, it is set equal to ``step_max``,
               but ``step_max`` is scaled by this factor. Similarly, if ``step_min`` is negative and the step length is
               below ``step_min``, it is set equal to ``step_min`` and ``step_min`` is scaled by this factor. The default
               value is ``4.0``.
 
     compute_jacobian : `bool, optional`
-        Whether to compute an analytic Jacobian during iteration, which must be ``False`` if ``method`` does not use
-        analytic Jacobians. By default, analytic Jacobians are not computed, and if a ``method`` is selected that
-        supports analytic Jacobians, they will by default be numerically approximated.
+        Whether to compute an analytic Jacobian during iteration. By default, analytic Jacobians are not computed, and
+        if a ``method`` is selected that supports analytic Jacobians, they will by default be numerically approximated.
+    universal_display : `bool, optional`
+        Whether to format iteration progress such that the display looks the same for all routines. By default, the
+        universal display is not used and no iteration progress is displayed. Setting this to ``True`` can be helpful
+        for debugging iteration issues. For example, iteration may get stuck above the configured termination tolerance.
 
     Examples
     --------
     .. raw:: latex
 
        \begin{examplenotebook}
 
@@ -134,17 +137,18 @@
 
     """
 
     _iterator: functools.partial
     _description: str
     _method_options: Options
     _compute_jacobian: bool
+    _universal_display: bool
 
     def __init__(self, method: Union[str, Callable], method_options: Optional[Options] = None,
-                 compute_jacobian: bool = False) -> None:
+                 compute_jacobian: bool = False, universal_display: bool = False) -> None:
         """Validate the method and configure default options."""
         simple_methods = {
             'simple': (functools.partial(simple_iterator), "no acceleration"),
             'squarem': (functools.partial(squarem_iterator), "the SQUAREM acceleration method"),
             'broyden1': (functools.partial(scipy_iterator), "Broyden's good method implemented in SciPy"),
             'broyden2': (functools.partial(scipy_iterator), "Broyden's bad method implemented in SciPy"),
             'anderson': (functools.partial(scipy_iterator), "Anderson's method implemented in SciPy"),
@@ -171,14 +175,15 @@
         if method_options is not None and not isinstance(method_options, dict):
             raise ValueError("method_options must be None or a dict.")
         if method in simple_methods and compute_jacobian:
             raise ValueError(f"compute_jacobian must be False when method is '{method}'.")
 
         # initialize class attributes
         self._compute_jacobian = compute_jacobian
+        self._universal_display = universal_display
 
         # options are by default empty
         if method_options is None:
             method_options = {}
 
         # options are simply passed along to custom methods
         if callable(method):
@@ -261,15 +266,15 @@
             evaluations.
             """
             nonlocal evaluations
             evaluations += 1
             if not isinstance(raw_values, np.ndarray):
                 raw_values = np.asarray(raw_values)
             values = raw_values.reshape(initial.shape).astype(initial.dtype, copy=False)
-            values, weights, jacobian = contraction(values)
+            values, weights, jacobian = contraction(values, iterations, evaluations)
             return (
                 values.astype(raw_values.dtype, copy=False).reshape(raw_values.shape),
                 None if weights is None else weights.astype(raw_values.dtype, copy=False).reshape(raw_values.shape),
                 None if jacobian is None else jacobian.astype(raw_values.dtype, copy=False)
             )
 
         # normalize the starting values
@@ -292,15 +297,15 @@
 def return_iterator(initial: Array, *_: Any, **__: Any) -> Tuple[Array, bool]:
     """Assume the initial values are the optimal ones."""
     success = True
     return initial, success
 
 
 def scipy_iterator(
-        initial: Array, contraction: ContractionFunction, iteration_callback: Callable[[], None], method: str,
+        initial: Array, contraction: ContractionWrapper, iteration_callback: Callable[[], None], method: str,
         compute_jacobian: bool, **scipy_options: Any) -> Tuple[Array, bool]:
     """Apply a SciPy root finding method."""
 
     # define method-specific options so iteration callbacks and norm weighting works properly
     weights_cache = np.ones_like(initial)
     scipy_options = scipy_options.copy()
     if method in {'hybr', 'lm'}:
@@ -348,15 +353,15 @@
         contraction_wrapper, initial, method=method, jac=compute_jacobian or None, callback=callback,
         options=scipy_options
     )
     return results.x, not failed and results.success
 
 
 def simple_iterator(
-        initial: Array, contraction: ContractionFunction, iteration_callback: Callable[[], None], max_evaluations: int,
+        initial: Array, contraction: ContractionWrapper, iteration_callback: Callable[[], None], max_evaluations: int,
         atol: float, rtol: float, norm: Callable[[Array], float]) -> Tuple[Array, bool]:
     """Apply simple fixed point iteration with no acceleration."""
     x = initial
     failed = False
     evaluations = 0
     while True:
         # contraction step
@@ -376,15 +381,15 @@
 
     # determine whether there was convergence
     converged = not failed and evaluations < max_evaluations
     return x, converged
 
 
 def squarem_iterator(
-        initial: Array, contraction: ContractionFunction, iteration_callback: Callable[[], None], max_evaluations: int,
+        initial: Array, contraction: ContractionWrapper, iteration_callback: Callable[[], None], max_evaluations: int,
         atol: float, rtol: float, norm: Callable[[Array], float], scheme: int, step_min: float, step_max: float,
         step_factor: float) -> Tuple[Array, bool]:
     """Apply the SQUAREM acceleration method for fixed point iteration."""
     x = initial
     failed = False
     evaluations = 0
     while True:
@@ -413,20 +418,21 @@
         evaluations += 1
         if evaluations >= max_evaluations or termination_check(x, g1, weights, atol, rtol, norm):
             break
 
         # compute the step length
         r = g0
         v = g1 - g0
-        if scheme == 1:
-            alpha = (r.T @ v) / (v.T @ v)
-        elif scheme == 2:
-            alpha = (r.T @ r) / (r.T @ v)
-        else:
-            alpha = -np.sqrt((r.T @ r) / (v.T @ v))
+        with np.errstate(divide='ignore'):
+            if scheme == 1:
+                alpha = (r.T @ v) / (v.T @ v)
+            elif scheme == 2:
+                alpha = (r.T @ r) / (r.T @ v)
+            else:
+                alpha = -np.sqrt((r.T @ r) / (v.T @ v))
 
         # bound the step length and update its bounds
         alpha = -np.maximum(step_min, np.minimum(step_max, -alpha))
         if -alpha == step_max:
             step_max *= step_factor
         if -alpha == step_min and step_min < 0:
             step_min *= step_factor
```

### Comparing `pyblp-0.9.0/pyblp/configurations/optimization.py` & `pyblp-1.0.0/pyblp/configurations/optimization.py`

 * *Files 4% similar despite different names*

```diff
@@ -80,16 +80,17 @@
 
         where ``gradient`` is ``None`` if ``compute_gradient is ``False``.
 
     method_options : `dict, optional`
         Options for the optimization routine.
 
         For any non-custom ``method`` other than ``'knitro'`` and ``'return'``, these options will be passed to
-        ``options`` in :func:`scipy.optimize.minimize`. Refer to the SciPy documentation for information about which
-        options are available for each optimization routine.
+        ``options`` in :func:`scipy.optimize.minimize`, with the exception of ``'keep_feasible'``, which is by default
+        ``True`` and is passed to any ``scipy.optimize.Bounds``. Refer to the SciPy documentation for information about
+        which options are available for each optimization routine.
 
         If ``method`` is ``'knitro'``, these options should be
         `Knitro user options <https://www.artelys.com/docs/knitro//3_referenceManual/userOptions.html>`_. The
         non-standard ``knitro_dir`` option can also be specified. The following options have non-standard default
         values:
 
             - **knitro_dir** : (`str`) - By default, the KNITRODIR environment variable is used. Otherwise, this
@@ -110,17 +111,21 @@
             - **honorbnds** : (`int`) - Whether to enforce satisfaction of simple variable bounds. The default value is
               ``1``, which corresponds to enforcing that the initial point and all subsequent solution estimates satisfy
               the bounds.
 
     compute_gradient : `bool, optional`
         Whether to compute an analytic objective gradient during optimization, which must be ``False`` if ``method``
         does not use analytic gradients, and must be ``True`` if ``method`` is ``'newton-cg'``, which requires an
-        analytic gradient. By default, analytic gradients are computed. Not using an analytic gradient will likely slow
+        analytic gradient.
+
+        By default, analytic gradients are computed. Not using an analytic gradient will likely slow
         down estimation a good deal. If ``False``, an analytic gradient may still be computed once at the end of
-        optimization to compute optimization results.
+        optimization to compute optimization results. To always use finite differences, ``finite_differences`` in
+        :meth:`Problem.solve` can be set to ``True``.
+
     universal_display : `bool, optional`
         Whether to format optimization progress such that the display looks the same for all routines. By default, the
         universal display is used and some ``method_options`` are used to prevent default displays from showing up.
 
     Examples
     --------
     .. raw:: latex
@@ -316,14 +321,22 @@
         if cache is None or not np.array_equal(values, cache[0]):
             cache = (values.copy(), objective_function(values))
         return cache[1][1]
 
     # by default use the BFGS approximation for the Hessian
     hess = scipy_options.get('hess', scipy.optimize.BFGS() if method == 'trust-constr' else None)
 
+    # extract and configure any bound feasibility
+    if 'keep_feasible' in scipy_options:
+        if bounds is not None:
+            lb, ub = zip(*bounds)
+            bounds = scipy.optimize.Bounds(lb, ub, scipy_options['keep_feasible'])
+        scipy_options = scipy_options.copy()
+        del scipy_options['keep_feasible']
+
     # call the SciPy function
     callback = lambda *_: iteration_callback()
     results = scipy.optimize.minimize(
         objective_wrapper, initial_values, method=method, jac=gradient_wrapper if compute_gradient else False,
         hess=hess, bounds=bounds, callback=callback, options=scipy_options
     )
     return results.x, results.success
@@ -415,15 +428,16 @@
 
         # solve the problem
         values_store = np.zeros_like(initial_values)
         with warnings.catch_warnings():
             warnings.simplefilter('ignore')
             return_code = knitro.KTR_solve(
                 kc=knitro_context, x=values_store, lambda_=np.zeros_like(initial_values), evalStatus=0,
-                obj=np.array([0]), c=None, objGrad=None, jac=None, hess=None, hessVector=None, userParams=None
+                obj=np.array([0], np.float64), c=None, objGrad=None, jac=None, hess=None, hessVector=None,
+                userParams=None
             )
 
         # Knitro was only successful if its return code was 0 (final solution satisfies the termination conditions for
         #   verifying optimality) or between -100 and -199 (a feasible approximate solution was found)
         return values_store, return_code > -200
 
 
@@ -448,15 +462,20 @@
         knitro.KTR_array_handler._cToUserArray = knitroNumPy._cToUserArray
     except ImportError:
         pass
 
     # create the Knitro context and attempt to free it if anything goes wrong
     knitro_context = None
     try:
-        knitro_context = knitro.KTR_new()
+        knitro_context = None
+        try:
+            knitro_context = knitro.KTR_new()
+        except RuntimeError as exception:
+            if 'Error while initializing parameter' not in str(exception):
+                raise
         if not knitro_context:
             raise OSError(
                 "Failed to find a Knitro license. Make sure that Knitro is properly installed. You may have to create "
                 "the environment variable ARTELYS_LICENSE and set it to the location of the directory with the license "
                 "file."
             )
         yield knitro, knitro_context
```

### Comparing `pyblp-0.9.0/pyblp/construction.py` & `pyblp-1.0.0/pyblp/construction.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,11 @@
 """Data construction."""
 
+from pathlib import Path
+import pickle
 from typing import Any, Callable, Dict, Iterator, List, Mapping, Optional, Union
 
 import numpy as np
 
 from . import exceptions, options
 from .configurations.formulation import Formulation
 from .configurations.integration import Integration
@@ -50,26 +52,26 @@
 
     """
     if not isinstance(T, int) or not isinstance(F, int) or T < 1 or F < 1:
         raise ValueError("Both T and F must be positive ints.")
     if not isinstance(J, int) or J < F:
         raise ValueError("J must be an int that is at least F.")
     return structure_matrices({
-        'market_ids': (np.repeat(np.arange(T), J).astype(np.int), np.object),
-        'firm_ids': (np.floor(np.tile(np.arange(J), T) * F / J).astype(np.int), np.object)
+        'market_ids': (np.repeat(np.arange(T), J).astype(np.int64), np.object_),
+        'firm_ids': (np.floor(np.tile(np.arange(J), T) * F / J).astype(np.int64), np.object_)
     })
 
 
 def build_ownership(
         product_data: Mapping, kappa_specification: Optional[Union[str, Callable[[Any, Any], float]]] = None) -> Array:
     r"""Build ownership matrices, :math:`O`.
 
     Ownership or product holding matrices are defined by their cooperation matrix counterparts, :math:`\kappa`. For each
-    market :math:`t`, :math:`\mathscr{H}_{jk} = \kappa_{fg}` where :math:`j \in \mathscr{J}_{ft}`, the set of products
-    produced by firm :math:`f` in the market, and similarly, :math:`g \in \mathscr{J}_{gt}`.
+    market :math:`t`, :math:`\mathscr{H}_{jk} = \kappa_{fg}` where :math:`j \in J_{ft}`, the set of products
+    produced by firm :math:`f` in the market, and similarly, :math:`g \in J_{gt}`.
 
     Parameters
     ----------
     product_data : `structured array-like`
         Each row corresponds to a product. Markets can have differing numbers of products. The following fields are
         required (except for ``firm_ids`` when ``kappa_specification`` is one of the special cases):
 
@@ -180,20 +182,20 @@
 
     .. note::
 
        To construct simpler, firm-agnostic instruments that are sums over characteristics of other goods, specify a
        constant column of firm IDs and keep only the first half of the instrument columns.
 
     Let :math:`x_{jt}` be the vector of characteristics in :math:`X` for product :math:`j` in market :math:`t`, which is
-    produced by firm :math:`f`. That is, :math:`j \in \mathscr{J}_{ft}`. Then,
+    produced by firm :math:`f`. That is, :math:`j \in J_{ft}`. Then,
 
     .. math::
 
-       Z_{jt}^\text{BLP,Other}(X) = \sum_{k \in \mathscr{J}_{ft} \setminus \{j\}} x_{kt}, \\
-       Z_{jt}^\text{BLP,Rival}(X) = \sum_{k \notin \mathscr{J}_{ft}} x_{kt}.
+       Z_{jt}^\text{BLP,Other}(X) = \sum_{k \in J_{ft} \setminus \{j\}} x_{kt}, \\
+       Z_{jt}^\text{BLP,Rival}(X) = \sum_{k \notin J_{ft}} x_{kt}.
 
     .. note::
 
        Usually, any supply or demand shifters are added to these excluded instruments, depending on whether they are
        meant to be used for demand- or supply-side estimation.
 
     Parameters
@@ -269,39 +271,39 @@
 
     .. note::
 
        To construct simpler, firm-agnostic instruments that are sums over functions of differences between all different
        goods, specify a constant column of firm IDs and keep only the first half of the instrument columns.
 
     Let :math:`x_{jt\ell}` be characteristic :math:`\ell` in :math:`X` for product :math:`j` in market :math:`t`, which
-    is produced by firm :math:`f`. That is, :math:`j \in \mathscr{J}_{ft}`. Then in the "local" version of
+    is produced by firm :math:`f`. That is, :math:`j \in J_{ft}`. Then in the "local" version of
     :math:`Z^\text{Diff}(X)`,
 
     .. math::
        :label: local_instruments
 
        Z_{jt\ell}^\text{Local,Other}(X) =
-       \sum_{k \in \mathscr{J}_{ft} \setminus \{j\}} 1(|d_{jkt\ell}| < \text{SD}_\ell), \\
+       \sum_{k \in J_{ft} \setminus \{j\}} 1(|d_{jkt\ell}| < \text{SD}_\ell), \\
        Z_{jt\ell}^\text{Local,Rival}(X) =
-       \sum_{k \notin \mathscr{J}_{ft}} 1(|d_{jkt\ell}| < \text{SD}_\ell),
+       \sum_{k \notin J_{ft}} 1(|d_{jkt\ell}| < \text{SD}_\ell),
 
     where :math:`d_{jkt\ell} = x_{kt\ell} - x_{jt\ell}` is the difference between products :math:`j` and :math:`k` in
     terms of characteristic :math:`\ell`, :math:`\text{SD}_\ell` is the standard deviation of these pairwise differences
     computed across all markets, and :math:`1(|d_{jkt\ell}| < \text{SD}_\ell)` indicates that products :math:`j` and
     :math:`k` are close to each other in terms of characteristic :math:`\ell`.
 
     The intuition behind this "local" version is that demand for products is often most influenced by a small number of
     other goods that are very similar. For the "quadratic" version of :math:`Z^\text{Diff}(X)`, which uses a more
     continuous measure of the distance between goods,
 
     .. math::
        :label: quadratic_instruments
 
-       Z_{jtk}^\text{Quad,Other}(X) = \sum_{k \in \mathscr{J}_{ft} \setminus\{j\}} d_{jkt\ell}^2, \\
-       Z_{jtk}^\text{Quad,Rival}(X) = \sum_{k \notin \mathscr{J}_{ft}} d_{jkt\ell}^2.
+       Z_{jtk}^\text{Quad,Other}(X) = \sum_{k \in J_{ft} \setminus\{j\}} d_{jkt\ell}^2, \\
+       Z_{jtk}^\text{Quad,Rival}(X) = \sum_{k \notin J_{ft}} d_{jkt\ell}^2.
 
     With interaction terms, which reflect covariances between different characteristics, the summands for the "local"
     versions are :math:`1(|d_{jkt\ell}| < \text{SD}_\ell) \times d_{jkt\ell'}` for all characteristics :math:`\ell'`,
     and the summands for the "quadratic" versions are :math:`d_{jkt\ell} \times d_{jkt\ell'}` for all
     :math:`\ell' \geq \ell`.
 
     .. note::
@@ -410,15 +412,15 @@
                 elif version == 'local':
                     with np.errstate(invalid='ignore'):
                         close = (np.abs(distances_mapping[k1]) < sd_mapping[k1]).astype(np.float64)
                     if not interact:
                         yield close
                     else:
                         for k2 in range(K):
-                            yield close * distances_mapping[k2]
+                            yield close * np.nan_to_num(distances_mapping[k2])
                 else:
                     raise ValueError("version must be 'local' or 'quadratic'.")
 
         # append instrument blocks
         other_blocks.append([])
         rival_blocks.append([])
         ownership = (firm_ids[indices_t] == firm_ids[indices_t].T).astype(np.float64)
@@ -568,14 +570,17 @@
        \end{examplenotebook}
 
     """
     if not isinstance(data, np.recarray):
         raise TypeError("data must be a NumPy record array.")
 
     mapping: Dict[str, Array] = {}
+    if data.dtype.names is None:
+        return mapping
+
     for key in data.dtype.names:
         if len(data[key].shape) > 2:
             raise ValueError("Arrays with more than two dimensions are not supported.")
         if ignore_empty and data[key].size == 0:
             continue
         if len(data[key].shape) == 1 or data[key].shape[1] == 1 or data[key].size == 0:
             mapping[key] = data[key].flatten()
@@ -583,7 +588,44 @@
         for index in range(data[key].shape[1]):
             new_key = f'{key}{index}'
             if new_key in data.dtype.names:
                 raise KeyError(f"'{key}' cannot be split into columns because '{new_key}' is already a field.")
             mapping[new_key] = data[key][:, index].flatten()
 
     return mapping
+
+
+def save_pickle(x: object, path: Union[str, Path]) -> None:
+    """Save an object as a pickle file.
+
+    This is a simple wrapper around `pickle.dump`.
+
+    Parameters
+    ----------
+    x : `object`
+        Object to be pickled.
+    path : `str or Path`
+        File path to which the object will be saved.
+
+    """
+    with open(path, 'wb') as handle:
+        pickle.dump(x, handle)
+
+
+def read_pickle(path: Union[str, Path]) -> object:
+    """Load a pickled object into memory.
+
+    This is a simple wrapper around `pickle.load`.
+
+    Parameters
+    ----------
+    path : `str or Path`
+        File path of a pickled object.
+
+    Returns
+    -------
+    `object`
+        The unpickled object.
+
+    """
+    with open(path, 'rb') as handle:
+        return pickle.load(handle)
```

### Comparing `pyblp-0.9.0/pyblp/data/blp_agents.csv` & `pyblp-1.0.0/pyblp/data/blp_agents.csv`

 * *Files identical despite different names*

### Comparing `pyblp-0.9.0/pyblp/data/blp_products.csv` & `pyblp-1.0.0/pyblp/data/blp_products.csv`

 * *Files identical despite different names*

### Comparing `pyblp-0.9.0/pyblp/data/nevo_agents.csv` & `pyblp-1.0.0/pyblp/data/nevo_agents.csv`

 * *Files identical despite different names*

### Comparing `pyblp-0.9.0/pyblp/data/nevo_products.csv` & `pyblp-1.0.0/pyblp/data/nevo_products.csv`

 * *Files identical despite different names*

### Comparing `pyblp-0.9.0/pyblp/economies/economy.py` & `pyblp-1.0.0/pyblp/economies/economy.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,69 +1,77 @@
 """Economy underlying the BLP model."""
 
 import abc
-import collections
-import functools
-from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence
+import collections.abc
+from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple
 
 import numpy as np
 
 from .. import exceptions, options
-from ..configurations.formulation import Formulation
+from ..configurations.formulation import Formulation, Absorb
 from ..configurations.iteration import Iteration
 from ..primitives import Container
-from ..utilities.algebra import precisely_identify_collinearity, precisely_identify_psd
-from ..utilities.basics import Array, Error, Groups, RecArray, StringRepresentation, format_table, get_indices, output
+from ..utilities.algebra import precisely_identify_collinearity, precisely_identify_singularity, precisely_identify_psd
+from ..utilities.basics import (
+    Array, Bounds, Error, Groups, RecArray, StringRepresentation, format_number, format_table, get_indices, output, warn
+)
 
 
 class Economy(Container, StringRepresentation):
     """An abstract economy underlying the BLP model."""
 
     product_formulations: Sequence[Optional[Formulation]]
     agent_formulation: Optional[Formulation]
-    distributions: List[str]
+    rc_types: List[str]
+    epsilon_scale: float
     costs_type: str
     unique_market_ids: Array
     unique_firm_ids: Array
     unique_nesting_ids: Array
+    unique_product_ids: Array
+    unique_agent_ids: Array
     T: int
     N: int
     F: int
     I: int
     K1: int
     K2: int
     K3: int
     D: int
     MD: int
     MS: int
     ED: int
     ES: int
     H: int
+    _market_indices: Dict[Hashable, int]
     _product_market_indices: Dict[Hashable, Array]
     _agent_market_indices: Dict[Hashable, Array]
     _max_J: int
     _max_I: int
-    _absorb_demand_ids: Optional[functools.partial]
-    _absorb_supply_ids: Optional[functools.partial]
+    _absorb_demand_ids: Optional[Absorb]
+    _absorb_supply_ids: Optional[Absorb]
 
     @abc.abstractmethod
     def __init__(
             self, product_formulations: Sequence[Optional[Formulation]], agent_formulation: Optional[Formulation],
-            products: RecArray, agents: RecArray, distributions: Optional[Sequence[str]], costs_type: str) -> None:
+            products: RecArray, agents: RecArray, rc_types: Optional[Sequence[str]], epsilon_scale: float,
+            costs_type: str) -> None:
         """Store information about formulations and data. Any fixed effects should be absorbed after initialization."""
 
         # store data and formulations
         super().__init__(products, agents)
         self.product_formulations = product_formulations
         self.agent_formulation = agent_formulation
 
-        # identify unique markets and nests
+        # identify unique markets, nests, products, and agents
         self.unique_market_ids = np.unique(self.products.market_ids.flatten())
         self.unique_firm_ids = np.unique(self.products.firm_ids.flatten())
-        self.unique_nesting_ids = np.unique(self.products.nesting_ids).flatten()
+        self.unique_nesting_ids = np.unique(self.products.nesting_ids.flatten())
+        self.unique_product_ids = np.unique(self.products.product_ids.flatten())
+        self.unique_agent_ids = np.unique(self.agents.agent_ids.flatten())
 
         # count dimensions
         self.N = self.products.shape[0]
         self.T = self.unique_market_ids.size
         self.F = self.unique_firm_ids.size
         self.I = self.agents.shape[0] if self.products.X2.shape[1] > 0 else 0
         self.K1 = self.products.X1.shape[1]
@@ -73,43 +81,51 @@
         self.MD = self.products.ZD.shape[1]
         self.MS = self.products.ZS.shape[1]
         self.ED = self.products.demand_ids.shape[1]
         self.ES = self.products.supply_ids.shape[1]
         self.H = self.unique_nesting_ids.size
 
         # identify market indices
+        self._market_indices = {t: i for i, t in enumerate(self.unique_market_ids)}
         self._product_market_indices = get_indices(self.products.market_ids)
         self._agent_market_indices = get_indices(self.agents.market_ids)
 
         # identify the largest number of products and agents in a market
         self._max_J = max(i.size for i in self._product_market_indices.values())
         self._max_I = max(i.size for i in self._agent_market_indices.values())
 
         # construct fixed effect absorption functions
         self._absorb_demand_ids = self._absorb_supply_ids = None
         if self.ED > 0:
             assert product_formulations[0] is not None
-            self._absorb_demand_ids = functools.partial(product_formulations[0]._build_absorb(self.products.demand_ids))
+            self._absorb_demand_ids = product_formulations[0]._build_absorb(self.products.demand_ids)
         if self.ES > 0:
             assert product_formulations[2] is not None
-            self._absorb_supply_ids = functools.partial(product_formulations[2]._build_absorb(self.products.supply_ids))
+            self._absorb_supply_ids = product_formulations[2]._build_absorb(self.products.supply_ids)
 
-        # validate and store random coefficient distributions
-        if distributions is None:
-            self.distributions = ['normal'] * self.K2
+        # validate random coefficient types
+        if rc_types is None:
+            self.rc_types = ['linear'] * self.K2
         else:
-            if not isinstance(distributions, collections.abc.Sequence):
-                raise TypeError("distributions must be None or a sequence.")
-            if len(distributions) != self.K2:
-                raise ValueError(f"distributions must be None or a sequence of length {self.K2}.")
-            if any(d not in {'normal', 'lognormal'} for d in distributions):
-                raise TypeError("distributions must be None or a sequence of 'normal' or 'lognormal' strings.")
-            self.distributions = list(distributions)
+            if not isinstance(rc_types, collections.abc.Sequence):
+                raise TypeError("rc_types must be None or a sequence.")
+            if len(rc_types) != self.K2:
+                raise ValueError(f"rc_types must be None or a sequence of length {self.K2}.")
+            if any(d not in {'linear', 'log', 'logit'} for d in rc_types):
+                raise TypeError("rc_types must be None or a sequence of 'linear', 'log', or 'logit' strings.")
+            self.rc_types = list(rc_types)
+
+        # validate the scale of epsilon
+        if not isinstance(epsilon_scale, (int, float)) or epsilon_scale <= 0:
+            raise ValueError("epsilon_scale must be a positive float.")
+        if epsilon_scale != 1 and self.H > 0:
+            raise ValueError("epsilon_scale must equal 1 when there are nesting groups.")
+        self.epsilon_scale = float(epsilon_scale)
 
-        # validate th type of marginal costs
+        # validate the type of marginal costs
         if costs_type not in {'linear', 'log'}:
             raise ValueError("costs_type must be 'linear' or 'log'.")
         self.costs_type = costs_type
 
     def __str__(self) -> str:
         """Format economy information as a string."""
         return "\n\n".join([self._format_dimensions(), self._format_formulations()])
@@ -143,55 +159,69 @@
 
         # construct the header
         max_formulations = max(len(r[1:]) for r in data)
         header = ["Column Indices:"] + [f" {i} " for i in range(max_formulations)]
 
         return format_table(header, *data, title="Formulations")
 
-    def _detect_collinearity(self) -> None:
+    def _detect_collinearity(self, added_exogenous: bool) -> None:
         """Detect any collinearity issues in product data matrices."""
 
         # skip collinearity checking when it is disabled via zero tolerances
         if max(options.collinear_atol, options.collinear_rtol) <= 0:
             return
 
         # collect labels for columns of matrices that will be checked for collinearity issues
         matrix_labels = {
             'X1': [str(f) for f in self._X1_formulations],
             'X2': [str(f) for f in self._X2_formulations],
             'X3': [str(f) for f in self._X3_formulations],
-            'ZD': [str(f) for f in self._X1_formulations if 'prices' not in f.names],
-            'ZS': [str(f) for f in self._X3_formulations]
+            'ZD': [str(f) for f in self._X1_formulations if 'prices' not in f.names] if added_exogenous else [],
+            'ZS': [str(f) for f in self._X3_formulations if 'shares' not in f.names] if added_exogenous else [],
         }
         matrix_labels.update({
             'ZD': [f'demand_instruments{i}' for i in range(self.MD - len(matrix_labels['ZD']))] + matrix_labels['ZD'],
-            'ZS': [f'demand_instruments{i}' for i in range(self.MD - len(matrix_labels['ZS']))] + matrix_labels['ZS']
+            'ZS': [f'demand_instruments{i}' for i in range(self.MS - len(matrix_labels['ZS']))] + matrix_labels['ZS']
         })
 
         # check each matrix for collinearity
         for name, labels in matrix_labels.items():
             collinear, successful = precisely_identify_collinearity(self.products[name])
             common_message = "To disable collinearity checks, set options.collinear_atol = options.collinear_rtol = 0."
             if (self.ED > 0 and name in {'X1', 'ZD'}) or (self.ES > 0 and name in {'X3', 'ZS'}):
                 common_message = f"Absorbed fixed effects may be creating collinearity problems. {common_message}"
             if not successful:
-                raise ValueError(
+                warn(
                     f"Failed to compute the QR decomposition of {name} while checking for collinearity issues. "
                     f"{common_message}"
                 )
             if collinear.any():
                 collinear_labels = ", ".join(l for l, c in zip(labels, collinear) if c)
-                raise ValueError(
+                warn(
                     f"Detected collinearity issues with [{collinear_labels}] and at least one other column in {name}. "
                     f"{common_message}"
                 )
 
     @staticmethod
-    def _detect_psd(matrix: Array, name: str) -> None:
-        """Detect whether a matrix is PSD."""
+    def _detect_singularity(matrix: Array, name: str) -> None:
+        """Detect any singularity issues in a matrix."""
+        singular, successful, condition = precisely_identify_singularity(matrix)
+        common_message = "To disable singularity checks, set options.singular_tol = numpy.inf."
+        if not successful:
+            warn(f"Failed to compute the condition number of {name} while checking for singularity. {common_message}")
+        if singular:
+            prefix = "nearly " if condition < np.inf else ""
+            warn(
+                f"Detected that {name} is {prefix}singular with condition number {format_number(condition).strip()}. "
+                f"{common_message}"
+            )
+
+    @staticmethod
+    def _require_psd(matrix: Array, name: str) -> None:
+        """Require that a matrix is PSD."""
         psd, successful = precisely_identify_psd(matrix)
         common_message = "To disable PSD checks, set options.psd_atol = options.psd_rtol = numpy.inf."
         if not successful:
             raise ValueError(f"Failed to compute the SVD of {name} while checking that it is PSD. {common_message}")
         if not psd:
             raise ValueError(f"{name} must be a PSD matrix. {common_message}")
 
@@ -201,49 +231,68 @@
         if errors:
             if error_behavior == 'raise':
                 raise exceptions.MultipleErrors(errors)
             output("")
             output(exceptions.MultipleErrors(errors))
             output("")
 
-    def _validate_name(self, name: str) -> None:
-        """Validate that a name corresponds to a variable in X1, X2, or X3."""
+    def _validate_name(self, name: Optional[str], none_valid: bool = True) -> None:
+        """Validate that a name is either None or corresponds to a variable in X1, X2, or X3."""
+        if name is None and none_valid:
+            return
         formulations = self._X1_formulations + self._X2_formulations + self._X3_formulations
         names = {n for f in formulations for n in f.names}
         if name not in names:
-            raise NameError(f"The name '{name}' is not one of the underlying variables, {list(sorted(names))}.")
+            raise NameError(f"'{name}' is not None or one of the underlying variables, {list(sorted(names))}.")
+
+    def _validate_product_ids_index(self, product_ids_index: int) -> None:
+        """Validate that a product IDs index is valid."""
+        if not isinstance(product_ids_index, int) or product_ids_index < 0:
+            raise ValueError("The product IDs index must be a non-negative int.")
+        if self.products.product_ids.size == 0:
+            raise ValueError("Since the product IDs index is not None, product_data must have product_ids.")
+        max_index = self.products.product_ids.shape[1] - 1
+        if not 0 <= product_ids_index <= max_index:
+            raise ValueError(f"The product IDs index should be at most {max_index}.")
 
     def _coerce_optional_firm_ids(self, firm_ids: Optional[Any], market_ids: Optional[Array] = None) -> Array:
         """Coerce optional array-like firm IDs into a column vector and validate it. By default, assume that firm IDs
         are for all markets.
         """
         if firm_ids is None:
             return None
-        firm_ids = np.c_[np.asarray(firm_ids, options.dtype)]
+        firm_ids = np.c_[np.asarray(firm_ids, np.object_)]
         rows = self.N
         if market_ids is not None:
             rows = sum(i.size for t, i in self._product_market_indices.items() if t in market_ids)
         if firm_ids.shape != (rows, 1):
             raise ValueError(f"firm_ids must be None or a {rows}-vector.")
         return firm_ids
 
     def _coerce_optional_ownership(self, ownership: Optional[Any], market_ids: Optional[Array] = None) -> Array:
         """Coerce optional array-like ownership matrices into a stacked matrix and validate it. By default, assume that
         ownership matrices are for all markets.
         """
         if ownership is None:
             return None
+
         ownership = np.c_[np.asarray(ownership, options.dtype)]
+
         rows = self.N
         columns = self._max_J
         if market_ids is not None:
-            rows = sum(i.size for t, i in self._product_market_indices.items() if t in market_ids)
-            columns = max(i.size for t, i in self._product_market_indices.items() if t in market_ids)
+            rows = columns = 0
+            for t in market_ids:
+                size = self._product_market_indices[t].size
+                rows += size
+                columns = max(columns, size)
+
         if ownership.shape != (rows, columns):
             raise ValueError(f"ownership must be None or a {rows} by {columns} matrix.")
+
         return ownership
 
     @staticmethod
     def _coerce_optional_delta_iteration(iteration: Optional[Iteration]) -> Iteration:
         """Validate or choose a default configuration for iterating over the mean utility."""
         if iteration is None:
             iteration = Iteration('squarem', {'atol': 1e-14})
@@ -254,28 +303,45 @@
     @staticmethod
     def _coerce_optional_prices_iteration(iteration: Optional[Iteration]) -> Iteration:
         """Validate or choose a default configuration for iteration over prices."""
         if iteration is None:
             iteration = Iteration('simple', {'atol': 1e-12})
         elif not isinstance(iteration, Iteration):
             raise ValueError("iteration must be None or an Iteration.")
-        elif iteration._compute_jacobian:
-            raise ValueError("Analytic Jacobians are not supported for solving this system.")
         return iteration
 
-    @staticmethod
-    def _validate_fp_type(fp_type: str) -> None:
+    def _validate_fp_type(self, fp_type: str) -> None:
         """Validate that the delta fixed point type is supported."""
         if fp_type not in {'safe_linear', 'linear', 'safe_nonlinear', 'nonlinear'}:
             raise ValueError("fp_type must be 'safe_linear', 'linear', 'safe_nonlinear', or 'nonlinear'.")
+        if fp_type in {'safe_nonlinear', 'nonlinear'} and self.epsilon_scale != 1:
+            raise ValueError("When epsilon_scale is not 1, fp_type must be 'safe_linear' or 'linear'.")
+
+    @staticmethod
+    def _coerce_optional_bounds(bounds: Optional[Tuple[Any, Any]], name: str) -> Bounds:
+        """Validate or choose default bounds for some object."""
+        if bounds is None:
+            return -np.inf, +np.inf
+        if len(bounds) != 2:
+            raise ValueError(f"{name} must be a tuple of the form (lb, ub).")
+        bounds = (np.asarray(bounds[0], options.dtype), np.asarray(bounds[1], options.dtype))
+        bounds[0][np.isnan(bounds[0])] = -np.inf
+        bounds[1][np.isnan(bounds[1])] = +np.inf
+        if bounds[0].size != 1:
+            raise ValueError(f"The lower bound in {name} must be None or a float.")
+        if bounds[1].size != 1:
+            raise ValueError(f"The upper bound in {name} must be None or a float.")
+        if bounds[0] > bounds[1]:
+            raise ValueError(f"The lower bound in {name} cannot be larger than the upper bound.")
+        return bounds
 
     def _compute_true_X1(self, data_override: Optional[Mapping] = None, index: Optional[Array] = None) -> Array:
         """Compute X1 or columns of X1 without any absorbed demand-side fixed effects."""
         if index is None:
-            index = np.ones(self.K1, np.bool)
+            index = np.ones(self.K1, np.bool_)
         if self.ED == 0 and not data_override:
             return self.products.X1[:, index]
 
         # compute X1 column-by-column
         columns = []
         for include, formulation in zip(index, self._X1_formulations):
             if include:
@@ -283,15 +349,15 @@
                 columns.append(np.broadcast_to(column, (self.N, 1)).astype(options.dtype))
 
         return np.column_stack(columns)
 
     def _compute_true_X3(self, data_override: Optional[Mapping] = None, index: Optional[Array] = None) -> Array:
         """Compute X3 or columns of X3 without any absorbed supply-side fixed effects."""
         if index is None:
-            index = np.ones(self.K3, np.bool)
+            index = np.ones(self.K3, np.bool_)
         if self.ES == 0 and not data_override:
             return self.products.X3[:, index]
 
         # compute X3 column-by-column
         columns = []
         for include, formulation in zip(index, self._X3_formulations):
             if include:
@@ -315,8 +381,13 @@
                 log_group_shares_t = np.log(groups_t.expand(groups_t.sum(shares_t)))
                 if rho.size == 1:
                     rho_t = np.full_like(shares_t, float(rho))
                 else:
                     rho_t = groups_t.expand(rho[np.searchsorted(self.unique_nesting_ids, groups_t.unique)])
                 delta[indices_t] -= rho_t * (log_shares_t - log_group_shares_t)
 
+        # delta needs to be scaled if the error term is scaled
+        if self.epsilon_scale != 1:
+            assert self.H == 0
+            delta *= self.epsilon_scale
+
         return delta
```

### Comparing `pyblp-0.9.0/pyblp/economies/problem.py` & `pyblp-1.0.0/pyblp/economies/problem.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,77 +1,81 @@
 """Economy-level BLP problem functionality."""
 
 import abc
-import collections
+import collections.abc
 import functools
 import time
-from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union
+from typing import Any, Callable, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union
 
 import numpy as np
 import scipy.linalg
+import scipy.sparse
 
 from .economy import Economy
 from .. import exceptions, options
 from ..configurations.formulation import Formulation
 from ..configurations.integration import Integration
 from ..configurations.iteration import Iteration
 from ..configurations.optimization import ObjectiveResults, Optimization
 from ..markets.problem_market import ProblemMarket
-from ..moments import Moment, EconomyMoments
+from ..micro import MicroDataset, MicroMoment, Moments
 from ..parameters import Parameters
 from ..primitives import Agents, Products
 from ..results.problem_results import ProblemResults
-from ..utilities.algebra import precisely_invert
+from ..utilities.algebra import precisely_invert, precisely_identify_collinearity
 from ..utilities.basics import (
-    Array, Bounds, Error, RecArray, SolverStats, format_number, format_seconds, format_table, generate_items, output,
-    update_matrices
+    Array, Bounds, Error, RecArray, SolverStats, format_number, format_seconds, format_table, generate_items,
+    get_indices, output, update_matrices, compute_finite_differences, warn
 )
 from ..utilities.statistics import IV, compute_gmm_moments_mean, compute_gmm_moments_jacobian_mean
 
 
 class ProblemEconomy(Economy):
     """An abstract BLP problem."""
 
     @abc.abstractmethod
     def __init__(
             self, product_formulations: Sequence[Optional[Formulation]], agent_formulation: Optional[Formulation],
-            products: RecArray, agents: RecArray, distributions: Optional[Sequence[str]], costs_type: str) -> None:
+            products: RecArray, agents: RecArray, rc_types: Optional[Sequence[str]], epsilon_scale: float,
+            costs_type: str) -> None:
         """Initialize the underlying economy with product and agent data."""
-        super().__init__(product_formulations, agent_formulation, products, agents, distributions, costs_type)
+        super().__init__(product_formulations, agent_formulation, products, agents, rc_types, epsilon_scale, costs_type)
 
     def solve(
             self, sigma: Optional[Any] = None, pi: Optional[Any] = None, rho: Optional[Any] = None,
             beta: Optional[Any] = None, gamma: Optional[Any] = None, sigma_bounds: Optional[Tuple[Any, Any]] = None,
             pi_bounds: Optional[Tuple[Any, Any]] = None, rho_bounds: Optional[Tuple[Any, Any]] = None,
             beta_bounds: Optional[Tuple[Any, Any]] = None, gamma_bounds: Optional[Tuple[Any, Any]] = None,
-            delta: Optional[Any] = None, method: str = '2s', initial_update: bool = False,
+            delta: Optional[Any] = None, method: str = '2s', initial_update: Optional[bool] = None,
             optimization: Optional[Optimization] = None, scale_objective: bool = True, check_optimality: str = 'both',
-            error_behavior: str = 'revert', error_punishment: float = 1, delta_behavior: str = 'first',
-            iteration: Optional[Iteration] = None, fp_type: str = 'safe_linear',
-            costs_bounds: Optional[Tuple[Any, Any]] = None, W: Optional[Any] = None, center_moments: bool = True,
-            W_type: str = 'robust', se_type: str = 'robust', micro_moments: Sequence[Moment] = (),
-            extra_micro_covariances: Optional[Any] = None) -> ProblemResults:
+            finite_differences: bool = False, error_behavior: str = 'revert', error_punishment: float = 1,
+            delta_behavior: str = 'first', iteration: Optional[Iteration] = None, fp_type: str = 'safe_linear',
+            shares_bounds: Optional[Tuple[Any, Any]] = (1e-300, None), costs_bounds: Optional[Tuple[Any, Any]] = None,
+            W: Optional[Any] = None, center_moments: bool = True, W_type: str = 'robust', se_type: str = 'robust',
+            micro_moments: Sequence[MicroMoment] = (), micro_sample_covariances: Optional[Any] = None,
+            resample_agent_data: Optional[Callable[[int], Optional[Mapping]]] = None) -> (
+            ProblemResults):
         r"""Solve the problem.
 
-        The problem is solved in one or more GMM steps. During each step, any parameters in :math:`\hat{\theta}` are
-        optimized to minimize the GMM objective value. If there are no parameters in :math:`\hat{\theta}` (for example,
-        in the logit model there are no nonlinear parameters and all linear parameters can be concentrated out), the
-        objective is evaluated once during the step.
-
-        If there are nonlinear parameters, the mean utility, :math:`\delta(\hat{\theta})` is computed market-by-market
-        with fixed point iteration. Otherwise, it is computed analytically according to the solution of the logit model.
-        If a supply side is to be estimated, marginal costs, :math:`c(\hat{\theta})`, are also computed
-        market-by-market. Linear parameters are then estimated, which are used to recover structural error terms, which
-        in turn are used to form the objective value. By default, the objective gradient is computed as well.
+        The problem is solved in one or more GMM steps. During each step, any parameters in :math:`\theta` are optimized
+        to minimize the GMM objective value, giving the estimated :math:`\hat{\theta}`. If there are no parameters in
+        :math:`\theta` (for example, in the logit model there are no nonlinear parameters and all linear parameters can
+        be concentrated out), the objective is evaluated once during the step.
+
+        If there are nonlinear parameters, the mean utility, :math:`\delta(\theta)` is computed market-by-market with
+        fixed point iteration. Otherwise, it is computed analytically according to the solution of the logit model. If a
+        supply side is to be estimated, marginal costs, :math:`c(\theta)`, are also computed market-by-market. Linear
+        parameters are then estimated, which are used to recover structural error terms, which in turn are used to form
+        the objective value. By default, the objective gradient is computed as well.
 
         .. note::
 
            This method supports :func:`parallel` processing. If multiprocessing is used, market-by-market computation of
-           :math:`\delta(\hat{\theta})` (and :math:`\tilde{c}(\hat{\theta})` if a supply side is estimated), along with
-           associated Jacobians, will be distributed among the processes.
+           :math:`\delta(\theta)` (and :math:`\tilde{c}(\theta)` if a supply side is estimated), along with associated
+           Jacobians, will be distributed among the processes.
 
         Parameters
         ----------
         sigma : `array-like, optional`
             Configuration for which elements in the lower-triangular Cholesky root of the covariance matrix for
             unobserved taste heterogeneity, :math:`\Sigma`, are fixed at zero and starting values for the other
             elements, which, if not fixed by ``sigma_bounds``, are in the vector of unknown elements, :math:`\theta`.
@@ -80,14 +84,18 @@
             ``product_formulations`` in :class:`Problem`. If :math:`X_2` was not formulated, this should not be
             specified, since the logit model will be estimated.
 
             Values above the diagonal are ignored. Zeros are assumed to be zero throughout estimation and nonzeros are,
             if not fixed by ``sigma_bounds``, starting values for unknown elements in :math:`\theta`. If any columns are
             fixed at zero, only the first few columns of integration nodes (specified in :class:`Problem`) will be used.
 
+            To have nonzero covariances for only a subset of the random coefficients, the characteristics for those
+            random coefficients with zero covariances should come first in :math:`X_2`. This can be seen by looking at
+            the expression for :math:`\Sigma\Sigma'`, the actual covariance matrix of the random coefficients.
+
         pi : `array-like, optional`
             Configuration for which elements in the matrix of parameters that measures how agent tastes vary with
             demographics, :math:`\Pi`, are fixed at zero and starting values for the other elements, which, if not fixed
             by ``pi_bounds``, are in the vector of unknown elements, :math:`\theta`.
 
             Rows correspond to the same product characteristics as in ``sigma``. Columns correspond to columns in
             :math:`d`, which is formulated according to ``agent_formulation`` in :class:`Problem`. If :math:`d` was not
@@ -214,17 +222,28 @@
                 - ``'2s'`` (default) - Two-step GMM.
 
             Iterated GMM can be manually implemented by executing single GMM steps in a loop, in which after the first
             iteration, nonlinear parameters and weighting matrices from the last :class:`ProblemResults` are passed as
             arguments.
 
         initial_update : `bool, optional`
-            Whether to update starting values for the mean utility :math:`\delta`, and the weighting matrix, :math:`W`,
-            at the initial parameter values before the first GMM step (this initial update will be called a zeroth
-            step). By default, initial values are not updated because this requires an additional objective evaluation.
+            Whether to update starting values for the mean utility :math:`\delta` and the weighting matrix :math:`W` at
+            the initial parameter values before the first GMM step. This initial update will be called a zeroth step.
+
+            By default, an initial update will not be used unless ``micro_moments`` are specified without an initial
+            weighting matrix ``W``.
+
+            .. note::
+
+               When trying multiple parameter starting values to verify that the optimization routine converges to the
+               same optimum, using ``initial_update`` is not recommended because different weighting matrices will be
+               used for these different runs. A better option is to use ``optimization=Optimization('return')`` at the
+               best guess for parameter values and pass :attr:`ProblemResults.updated_W` to ``W`` for each set of
+               different parameter starting values.
+
         optimization : `Optimization, optional`
             :class:`Optimization` configuration for how to solve the optimization problem in each GMM step, which is
             only used if there are unfixed nonlinear parameters over which to optimize. By default,
             ``Optimization('l-bfgs-b', {'ftol': 0, 'gtol': 1e-8})`` is used. If available, ``Optimization('knitro')``
             may be preferable. Generally, it is recommended to consider a number of different optimization routines and
             starting values, verifying that :math:`\hat{\theta}` satisfies both the first and second order conditions.
             Choosing a routine that supports bounds (and configuring bounds) is typically a good idea. Choosing a
@@ -243,104 +262,131 @@
 
                 - ``'gradient'`` - Analytically compute the gradient after optimization finishes, but do not compute the
                   Hessian. Since Jacobians needed to compute standard errors will already be computed, gradient
                   computation will not take a long time. This option may be useful if Hessian computation takes a long
                   time when, for example, there are a large number of parameters.
 
                 - ``'both'`` (default) - Also compute the Hessian with central finite differences after optimization
-                  finishes. Specifically, analytically compute the gradient :math:`2P` times, perturbing each of the
-                  :math:`P` parameters by :math:`\pm\sqrt{\epsilon^\text{mach}} / 2` where :math:`\epsilon^\text{mach}`
-                  is the machine precision.
+                  finishes.
+
+        finite_differences : `bool, optional`
+            Whether to use finite differences to compute Jacobians and the gradient instead of analytic expressions.
+            Since finite differences comes with numerical approximation error and is typically slower, analytic
+            expressions are used by default.
+
+            One situation in which finite differences may be preferable is when there are a sufficiently large number of
+            products and integration nodes in individual markets to make computing analytic Jacobians infeasible because
+            of memory requirements. Note that an analytic expression for the Hessian has not been implemented, so when
+            computed it is always approximated with finite differences.
 
         error_behavior : `str, optional`
             How to handle any errors. For example, there can sometimes be overflow or underflow when computing
-            :math:`\delta(\hat{\theta})` at a large :math:`\hat{\theta}`. The following behaviors are supported:
+            :math:`\delta(\theta)` at a large :math:`\hat{\theta}`. The following behaviors are supported:
 
                 - ``'revert'`` (default) - Revert problematic values to their last computed values. If there are
-                  problematic values during the first objective evaluation, revert values in
-                  :math:`\delta(\hat{\theta})` to their starting values; in :math:`\tilde{c}(\hat{\theta})`, to prices;
-                  in the objective, to ``1e10``; and in other matrices such as Jacobians, to zeros.
+                  problematic values during the first objective evaluation, revert values in :math:`\delta(\theta)` to
+                  their starting values; in :math:`\tilde{c}(\hat{\theta})`, to prices; in the objective, to ``1e10``;
+                  and in other matrices such as Jacobians, to zeros.
 
                 - ``'punish'`` - Set the objective to ``1`` and its gradient to all zeros. This option along with a
                   large ``error_punishment`` can be helpful for routines that do not use analytic gradients.
 
                 - ``'raise'`` - Raise an exception.
 
         error_punishment : `float, optional`
             How to scale the GMM objective value after an error. By default, the objective value is not scaled.
         delta_behavior : `str, optional`
-            Configuration for the values at which the fixed point computation of :math:`\delta(\hat{\theta})` in each
-            market will start. This configuration is only relevant if there are unfixed nonlinear parameters over which
-            to optimize. The following behaviors are supported:
+            Configuration for the values at which the fixed point computation of :math:`\delta(\theta)` in each market
+            will start. This configuration is only relevant if there are unfixed nonlinear parameters over which to
+            optimize. The following behaviors are supported:
 
                 - ``'first'`` (default) - Start at the values configured by ``delta`` during the first GMM step, and at
                   the values computed by the last GMM step for each subsequent step.
 
-                - ``'last'`` - Start at the values of :math:`\delta(\hat{\theta})` computed during the last objective
+                - ``'logit'`` - Start at the solution to the logit model in :eq:`logit_delta`, or if :math:`\rho` is
+                  specified, the solution to the nested logit model in :eq:`nested_logit_delta`. If the initial
+                  ``delta`` is left unspecified and there is no nesting parameter being optimized over, this will
+                  generally be equivalent to ``'first'``.
+
+                - ``'last'`` - Start at the values of :math:`\delta(\theta)` computed during the last objective
                   evaluation, or, if this is the first evaluation, at the values configured by ``delta``. This behavior
                   tends to speed up computation but may introduce some instability into estimation.
 
         iteration : `Iteration, optional`
             :class:`Iteration` configuration for how to solve the fixed point problem used to compute
-            :math:`\delta(\hat{\theta})` in each market. This configuration is only relevant if there are nonlinear
+            :math:`\delta(\theta)` in each market. This configuration is only relevant if there are nonlinear
             parameters, since :math:`\delta` can be estimated analytically in the logit model. By default,
             ``Iteration('squarem', {'atol': 1e-14})`` is used. Newton-based routines such as ``Iteration('lm'`)`` that
             compute the Jacobian can often be faster (especially when there are nesting parameters), but the
-            non-Jacobian SQUAREM routine is used by default because it speed is often comparable and in practice it can
+            Jacobian-free SQUAREM routine is used by default because it speed is often comparable and in practice it can
             be slightly more stable.
         fp_type : `str, optional`
-            Configuration for the type of contraction mapping used to compute :math:`\delta(\hat{\theta})`. The
-            following types are supported:
+            Configuration for the type of contraction mapping used to compute :math:`\delta(\theta)`. The following
+            types are supported:
 
                 - ``'safe_linear'`` (default) - The standard linear contraction mapping in :eq:`contraction` (or
                   :eq:`nested_contraction` when there is nesting) with safeguards against numerical overflow.
-                  Specifically, :math:`\max_j V_{jti}` (or :math:`\max_j V_{jti} / (1 - \rho_{h(j)})` when there is
-                  nesting) is subtracted from :math:`V_{jti}` and the logit expression for choice probabilities in
+                  Specifically, :math:`\max_j V_{ijt}` (or :math:`\max_j V_{ijt} / (1 - \rho_{h(j)})` when there is
+                  nesting) is subtracted from :math:`V_{ijt}` and the logit expression for choice probabilities in
                   :eq:`probabilities` (or :eq:`nested_probabilities`) is re-scaled accordingly. Such re-scaling is known
                   as the log-sum-exp trick.
 
                 - ``'linear'`` - The standard linear contraction mapping without safeguards against numerical overflow.
                   This option may be preferable to ``'safe_linear'`` if utilities are reasonably small and unlikely to
                   create overflow problems.
 
-                - ``'nonlinear'`` - Iteration over :math:`\exp(\delta_{jt})` instead of :math:`\delta_{jt}`. This can be
+                - ``'nonlinear'`` - Iteration over :math:`\exp \delta_{jt}` instead of :math:`\delta_{jt}`. This can be
                   faster than ``'linear'`` because it involves fewer logarithms. Also, following
-                  :ref:`references:Brunner, Heiss, Romahn, and Weiser (2017)`, the :math:`\exp(\delta_{jt})` term can be
+                  :ref:`references:Brunner, Heiss, Romahn, and Weiser (2017)`, the :math:`\exp \delta_{jt}` term can be
                   cancelled out of the expression because it also appears in the numerator of :eq:`probabilities` in the
-                  definition of :math:`s_{jt}(\delta, \hat{\theta})`. This second trick only works when there are no
+                  definition of :math:`s_{jt}(\delta, \theta)`. This second trick only works when there are no
                   nesting parameters.
 
                 - ``'safe_nonlinear'`` - Exponentiated version with minimal safeguards against numerical overflow.
-                  Specifically, :math:`\max_j \mu_{jti}` is subtracted from :math:`\mu_{jti}`. This helps with stability
-                  but is less helpful than subtracting from the full :math:`V_{jti}`, so this version is less stable
+                  Specifically, :math:`\max_j \mu_{ijt}` is subtracted from :math:`\mu_{ijt}`. This helps with stability
+                  but is less helpful than subtracting from the full :math:`V_{ijt}`, so this version is less stable
                   than ``'safe_linear'``.
 
             This option is only relevant if ``sigma`` or ``pi`` are specified because :math:`\delta` can be estimated
             analytically in the logit model with :eq:`logit_delta` and in the nested logit model with
             :eq:`nested_logit_delta`.
 
+        shares_bounds : `tuple, optional`
+            Configuration for :math:`s_{jt}(\delta, \theta)` bounds in the contraction in :eq:`contraction` of the form
+            ``(lb, ub)``, in which both ``lb`` and ``ub`` are floats or ``None``. By default, simulated shares are
+            bounded from below by ``1e-300``. This is only relevant if ``fp_type`` is ``'safe_linear'`` or ``'linear'``.
+            Bounding shares in the contraction does nothing with a nonlinear fixed point.
+
+            It can be particularly helpful to bound shares in the contraction from below by a small number to prevent
+            the contraction from failing when there are issues with zero or negative simulated shares. Zero shares can
+            occur when there are underflow issues and negative shares can occur when there are issues with the numerical
+            integration routine having negative integration weights (e.g., for sparse grid integration).
+
+            The idea is that a small lower bound will allow the contraction to converge even when it encounters some
+            issues with small or negative shares. However, if these issues are unlikely, disabling this behavior can
+            speed up the iteration routine because fewer checks will be done.
+
+            Both ``None`` and ``numpy.nan`` are converted to ``-numpy.inf`` in ``lb`` and to ``numpy.inf`` in ``ub``.
+
         costs_bounds : `tuple, optional`
-            Configuration for :math:`c` bounds of the form ``(lb, ub)``, in which both ``lb`` and ``ub`` are floats.
-            This is only relevant if :math:`X_3` was formulated by ``product_formulations`` in :class:`Problem`. By
-            default, marginal costs are unbounded.
-
-            When ``costs_type`` in :class:`Problem` is ``'log'``, nonpositive :math:`c(\hat{\theta})` values can create
-            problems when computing :math:`\tilde{c}(\hat{\theta}) = \log c(\hat{\theta})`. One solution is to set
-            ``lb`` to a small number. Rows in Jacobians associated with clipped marginal costs will be zero.
+            Configuration for :math:`c_{jt}(\theta)` bounds of the form ``(lb, ub)``, in which both ``lb`` and ``ub``
+            are floats or ``None``. This is only relevant if :math:`X_3` was formulated by ``product_formulations`` in
+            :class:`Problem`. By default, marginal costs are unbounded.
+
+            When ``costs_type`` in :class:`Problem` is ``'log'``, nonpositive :math:`c(\theta)` values can create
+            problems when computing :math:`\tilde{c}(\theta) = \log c(\theta)`. One solution is to set ``lb`` to a small
+            number. Rows in Jacobians associated with clipped marginal costs will be zero.
 
             Both ``None`` and ``numpy.nan`` are converted to ``-numpy.inf`` in ``lb`` and to ``numpy.inf`` in ``ub``.
 
         W : `array-like, optional`
             Starting values for the weighting matrix, :math:`W`. By default, the 2SLS weighting matrix in :eq:`2sls_W`
-            is used.
-
-            If there are any ``micro_moments``, the initial weighting matrix will by default be block-diagonal with an
-            identity matrix for the micro moment block. This micro moment block should usually be replaced by a matrix
-            that better reflects micro moment covariances and the size of the micro dataset relative to :math:`N`.
-
+            is used, unless there are any ``micro_moments``, in which case an ``initial_update`` will be used to update
+            starting values :math:`W` and the mean utility :math:`\delta` at the initial parameter values before the
+            first GMM step.
         center_moments : `bool, optional`
             Whether to center each column of the demand- and supply-side moments :math:`g` before updating the weighting
             matrix :math:`W` according to :eq:`W`. By default, the moments are centered. This has no effect if
             ``W_type`` is ``'unadjusted'``.
         W_type : `str, optional`
             How to update the weighting matrix. This has no effect if ``method`` is ``'1s'``. Usually, ``se_type``
             should be the same. The following types are supported:
@@ -351,15 +397,15 @@
                 - ``'clustered'`` - Clustered weighting matrix defined in :eq:`W` and :eq:`clustered_S`. Clusters must
                   be defined by the ``clustering_ids`` field of ``product_data`` in :class:`Problem`.
 
                 - ``'unadjusted'`` - Homoskedastic weighting matrix defined in :eq:`W` and :eq:`unadjusted_S`.
 
             This only affects the standard demand- and supply-side block of the updated weighting matrix. If there are
             micro moments, this matrix will be block-diagonal with a micro moment block equal to the inverse of the
-            covariance matrix defined in :eq:`averaged_micro_moment_covariances` plus any ``extra_micro_covariances``.
+            scaled covariance matrix defined in :eq:`scaled_micro_moment_covariances`.
 
         se_type : `str, optional`
             How to compute parameter covariances and standard errors. Usually, ``W_type`` should be the same. The
             following types are supported:
 
                 - ``'robust'`` (default) - Heteroscedasticity robust covariances defined in :eq:`covariances` and
                   :eq:`robust_S`.
@@ -368,32 +414,52 @@
                   must be defined by the ``clustering_ids`` field of ``product_data`` in :class:`Problem`.
 
                 - ``'unadjusted'`` - Homoskedastic covariances defined in :eq:`unadjusted_covariances`, which are
                   computed under the assumption that the weighting matrix is optimal.
 
             This only affects the standard demand- and supply-side block of the matrix of averaged moment covariances.
             If there are micro moments, the :math:`S` matrix defined in the expressions referenced above will be
-            block-diagonal with a micro moment block equal to the covariance matrix defined in
-            :eq:`averaged_micro_moment_covariances` plus any ``extra_micro_covariances``.
+            block-diagonal with a micro moment block equal to the scaled covariance matrix defined in
+            :eq:`scaled_micro_moment_covariances`.
 
-        micro_moments : `sequence of FirstChoiceCovarianceMoment, optional`
-            Configurations for the :math:`M_M` micro moments that will be added to the standard set of moments. The only
-            type of micro moment currently supported is the :class:`FirstChoiceCovarianceMoment`. By default, no micro
-            moments are used, so :math:`M_M = 0`.
-
-            If micro moments are specified, the micro moment block in ``W`` should usually be replaced by a matrix that
-            better reflects micro moment covariances and the size of the micro dataset relative to :math:`N`. If micro
-            moments were computed with substantial sampling error, ``extra_micro_covariances`` can be specified to
-            account for this additional source of error.
-
-        extra_micro_covariances : `array-like, optional`
-            Covariance matrix that is added on to the :math:`M_M \times M_M` matrix of micro moments covariances defined
-            in :eq:`averaged_micro_moment_covariances`, which is used to update the weighting matrix and compute
-            standard errors. By default, this matrix is assumed to be zero. It should be specified if, for example,
-            micro moments were computed with substantial sampling error.
+        micro_moments : `sequence of MicroMoment, optional`
+            Configurations for the :math:`M_M` :class:`MicroMoment` instances that will be added to the standard set of
+            moments. By default, no micro moments are used, so :math:`M_M = 0`.
+
+            When micro moments are specified, unless an initial weighting matrix ``W`` is specified as well (with a
+            lower right micro moment block that reflects micro moment covariances), an ``initial_update`` will be used
+            to update starting values :math:`W` and the mean utility :math:`\delta` at the initial parameter values
+            before the first GMM step.
+
+            .. note::
+
+               When trying multiple parameter starting values to verify that the optimization routine converges to the
+               same optimum, using ``initial_update`` is not recommended because different weighting matrices will be
+               used for these different runs. A better option is to use ``optimization=Optimization('return')`` at the
+               best guess for parameter values and pass :attr:`ProblemResults.updated_W` to ``W`` for each set of
+               different parameter starting values.
+
+        micro_sample_covariances : `array-like, optional`
+            Sample covariance matrix for the :math:`M_M` micro moments. By default, their asymptotic covariance matrix
+            is computed according to :eq:`scaled_micro_moment_covariances`. This override could be used, for example, if
+            instead of estimating covariances at some estimated :math:`\hat{\theta}`, one wanted to use a boostrap
+            procedure to compute their covariances directly from the micro data.
+
+        resample_agent_data : `callable, optional`
+            If specified, simulation error in moment covariances will be accounted for by resampling
+            :math:`r = 1, \dots, R` sets of agents by iteratively calling this function, which should be of the
+            following form::
+
+                resample_agent_data(index) --> agent_data or None
+
+            where ``index`` increments from ``0`` to ``1`` and so on and ``agent_data`` is the corresponding resampled
+            agent data, which should be a resampled version of the ``agent_data`` passed to :class:`Problem`. Each
+            ``index`` should correspond to a different set of randomly drawn agent data, with different integration
+            nodes and demographics. If ``index`` is larger than :math:`R - 1`, this function should return ``None``,
+            at which point agents will stop being resampled.
 
         Returns
         -------
         `ProblemResults`
             :class:`ProblemResults` of the solved problem.
 
         Examples
@@ -415,51 +481,61 @@
             raise TypeError("optimization must be None or an Optimization instance.")
         if check_optimality not in {'gradient', 'both'}:
             raise ValueError("check_optimality must be 'gradient' or 'both'.")
         if error_behavior not in {'revert', 'punish', 'raise'}:
             raise ValueError("error_behavior must be 'revert', 'punish', or 'raise'.")
         if not isinstance(error_punishment, (float, int)) or error_punishment < 0:
             raise ValueError("error_punishment must be a positive float.")
-        if delta_behavior not in {'last', 'first'}:
-            raise ValueError("delta_behavior must be 'last' or 'first'.")
+        if delta_behavior not in {'last', 'logit', 'first'}:
+            raise ValueError("delta_behavior must be 'last', 'logit', or 'first'.")
         iteration = self._coerce_optional_delta_iteration(iteration)
         self._validate_fp_type(fp_type)
         if W_type not in {'robust', 'unadjusted', 'clustered'}:
             raise ValueError("W_type must be 'robust', 'unadjusted', or 'clustered'.")
         if se_type not in {'robust', 'unadjusted', 'clustered'}:
             raise ValueError("se_type must be 'robust', 'unadjusted', or 'clustered'.")
-        if 'clustered' in {W_type, se_type} and 'clustering_ids' not in self.products.dtype.names:
-            raise ValueError("W_type or se_type is 'clustered' but clustering_ids were not specified in product_data.")
+        if 'clustered' in {W_type, se_type}:
+            if 'clustering_ids' not in self.products.dtype.names or self.products.clustering_ids.size == 0:
+                raise ValueError(
+                    "W_type or se_type is 'clustered' but clustering_ids were not specified in product_data."
+                )
 
-        # configure or validate costs bounds
-        if costs_bounds is None:
-            costs_bounds = (-np.inf, +np.inf)
-        else:
-            if len(costs_bounds) != 2:
-                raise ValueError("costs_bounds must be a tuple of the form (lb, ub).")
-            costs_bounds = (np.asarray(costs_bounds[0], options.dtype), np.asarray(costs_bounds[1], options.dtype))
-            costs_bounds[0][np.isnan(costs_bounds[0])] = -np.inf
-            costs_bounds[1][np.isnan(costs_bounds[1])] = +np.inf
-            if costs_bounds[0].size != 1:
-                raise ValueError(f"The lower bound in costs_bounds must be None or a float.")
-            if costs_bounds[1].size != 1:
-                raise ValueError(f"The upper bound in costs_bounds must be None or a float.")
-            if costs_bounds[0] > costs_bounds[1]:
-                raise ValueError("The lower bound in costs_bounds cannot be larger than the upper bound.")
+        # configure or validate bounds on shares and costs
+        shares_bounds = self._coerce_optional_bounds(shares_bounds, 'shares_bounds')
+        costs_bounds = self._coerce_optional_bounds(costs_bounds, 'costs_bounds')
 
         # validate and structure micro moments before outputting related information
-        moments = EconomyMoments(self, micro_moments)
+        moments = Moments(micro_moments, self)
+        micro_moment_covariances = None
         if moments.MM > 0:
             output("")
             output(moments.format("Micro Moments"))
-            if extra_micro_covariances is not None:
-                extra_micro_covariances = np.c_[np.asarray(extra_micro_covariances, options.dtype)]
-                if extra_micro_covariances.shape != (moments.MM, moments.MM):
-                    raise ValueError(f"extra_micro_moments must be a square {moments.MM} by {moments.MM} matrix.")
-                self._detect_psd(extra_micro_covariances, "extra_micro_moments")
+            if micro_sample_covariances is not None:
+                micro_moment_covariances = np.c_[np.asarray(micro_sample_covariances, options.dtype)]
+                if micro_moment_covariances.shape != (moments.MM, moments.MM):
+                    raise ValueError(f"micro_sample_covariances must be a square {moments.MM} by {moments.MM} matrix.")
+                self._require_psd(micro_moment_covariances, "micro_sample_covariances")
+                self._detect_singularity(micro_moment_covariances, "micro_sample_covariances")
+
+        # determine whether to check micro moment collinearity
+        detect_micro_collinearity = (
+            moments.MM > 0 and
+            options.detect_micro_collinearity and
+            (options.collinear_atol > 0 or options.collinear_rtol > 0)
+        )
+
+        # validate any agent data resampler
+        if resample_agent_data is not None and not callable(resample_agent_data):
+            raise TypeError("resample_agent_data must be None or a function.")
+
+        # choose whether to do an initial update
+        if initial_update is None:
+            initial_update = bool(moments.MM > 0 and W is None)
+        elif not initial_update and moments.MM > 0 and W is None:
+            raise ValueError("initial_update cannot be False with micro_moments and no initial W specified.")
 
         # validate parameters before compressing unfixed parameters into theta and outputting related information
         parameters = Parameters(
             self, sigma, pi, rho, beta, gamma, sigma_bounds, pi_bounds, rho_bounds, beta_bounds, gamma_bounds,
             bounded=optimization._supports_bounds, allow_linear_nans=True
         )
         theta = parameters.compress()
@@ -470,30 +546,36 @@
             if parameters.fixed or optimization._supports_bounds:
                 output("")
                 output(parameters.format_lower_bounds("Lower Bounds"))
                 output("")
                 output(parameters.format_upper_bounds("Upper Bounds"))
                 output("")
 
-        # compute or load the weighting matrix
-        if W is None:
-            W, successful = precisely_invert(scipy.linalg.block_diag(
+        # load or compute the weighting matrix
+        if W is not None:
+            W = np.c_[np.asarray(W, options.dtype)]
+            M = self.MD + self.MS + moments.MM
+            if W.shape != (M, M):
+                raise ValueError(f"W must be a square {M} by {M} matrix.")
+            self._require_psd(W, "W")
+            self._detect_singularity(W, "W")
+        else:
+            S = scipy.linalg.block_diag(
                 self.products.ZD.T @ self.products.ZD / self.N,
                 self.products.ZS.T @ self.products.ZS / self.N,
-            ))
+            )
+            self._detect_singularity(S, "the 2SLS weighting matrix")
+            W, successful = precisely_invert(S)
             if not successful:
                 raise ValueError("Failed to compute the 2SLS weighting matrix. There may be instrument collinearity.")
+
+            # an initial update will be used when there are micro moments, so this initial block does not matter
             if moments.MM > 0:
-                W = scipy.linalg.block_diag(W, np.eye(moments.MM, dtype=options.dtype))
-        else:
-            W = np.c_[np.asarray(W, options.dtype)]
-            M = self.MD + self.MS + moments.MM
-            if W.shape != (M, M):
-                raise ValueError(f"W must be a square {M} by {M} matrix.")
-            self._detect_psd(W, "W")
+                assert initial_update
+                W = scipy.linalg.block_diag(W, np.zeros((moments.MM, moments.MM), options.dtype))
 
         # compute or load initial delta values
         if delta is None:
             delta = self._compute_logit_delta(parameters.rho)
         else:
             delta = np.c_[np.asarray(delta, options.dtype)]
             if delta.shape != (self.N, 1):
@@ -539,41 +621,46 @@
             # initialize an IV model for linear parameter estimation
             iv = IV(X_list, Z_list, W[:self.MD + self.MS, :self.MD + self.MS])
             self._handle_errors(iv.errors, error_behavior)
 
             # wrap computation of progress information with step-specific information
             compute_step_progress = functools.partial(
                 self._compute_progress, parameters, moments, iv, W, scale_objective, error_behavior, error_punishment,
-                delta_behavior, iteration, fp_type, costs_bounds
+                delta_behavior, iteration, fp_type, shares_bounds, costs_bounds, finite_differences, resample_agent_data
             )
 
             # initialize optimization progress
             iteration_stats: List[Dict[Hashable, SolverStats]] = []
             smallest_objective = np.inf
             progress = InitialProgress(
                 self, parameters, moments, W, theta, objective, gradient, hessian, delta, delta, tilde_costs, micro,
                 xi_jacobian, omega_jacobian, micro_jacobian
             )
 
             # define the objective function
             def wrapper(new_theta: Array, iterations: int, evaluations: int) -> ObjectiveResults:
                 """Compute and output progress associated with a single objective evaluation."""
-                nonlocal iteration_stats, smallest_objective, progress
-                assert optimization is not None and costs_bounds is not None
+                nonlocal iteration_stats, smallest_objective, progress, detect_micro_collinearity
+                assert optimization is not None and shares_bounds is not None and costs_bounds is not None
+                progress_start_time = time.time()
                 progress = compute_step_progress(
                     new_theta, progress, optimization._compute_gradient, compute_hessian=False,
-                    compute_micro_covariances=False
+                    compute_micro_covariances=False, detect_micro_collinearity=detect_micro_collinearity,
+                    compute_simulation_covariances=False,
                 )
                 iteration_stats.append(progress.iteration_stats)
+                progress_time = time.time() - progress_start_time
                 formatted_progress = progress.format(
-                    optimization, costs_bounds, step, iterations, evaluations, smallest_objective
+                    optimization, shares_bounds, costs_bounds, step, iterations, evaluations, progress_time,
+                    smallest_objective
                 )
                 if formatted_progress:
                     output(formatted_progress)
                 smallest_objective = min(smallest_objective, progress.objective)
+                detect_micro_collinearity = False
                 return progress.objective, progress.gradient if optimization._compute_gradient else None
 
             # optimize theta if there are parameters to optimize and this isn't the initial update step
             optimization_stats = SolverStats()
             optimization_start_time = optimization_end_time = time.time()
             if parameters.P > 0 and step > 0:
                 output(f"Starting optimization ...")
@@ -588,91 +675,368 @@
                 output(f"Optimization {status} after {format_seconds(optimization_time)}.")
 
             # identify what will be done when computing results
             initial_step = step == 0
             last_step = step == 2 or (method == '1s' and step == 1)
             compute_gradient = parameters.P > 0
             compute_hessian = compute_gradient and check_optimality == 'both' and step > 0
-            compute_micro_covariances = moments.MM > 0
+            compute_micro_covariances = moments.MM > 0 and micro_moment_covariances is None
+            compute_simulation_covariances = resample_agent_data is not None
 
             # use progress information computed at the optimal theta to compute results for the step
             if initial_step:
-                output("Updating starting values for delta and the weighting matrix ...")
+                output("Updating starting values for the weighting matrix and delta ...")
             elif compute_hessian and not last_step:
                 output("Computing the Hessian and and updating the weighting matrix ...")
             elif compute_hessian:
                 output("Computing the Hessian and estimating standard errors ...")
             elif not last_step:
                 output("Updating the weighting matrix ...")
             else:
                 output("Estimating standard errors ...")
             final_progress = compute_step_progress(
-                theta, progress, compute_gradient, compute_hessian, compute_micro_covariances
+                theta, progress, compute_gradient, compute_hessian, compute_micro_covariances,
+                detect_micro_collinearity, compute_simulation_covariances
             )
+            iteration_stats.append(final_progress.iteration_stats)
             optimization_stats.evaluations += 1
+            detect_micro_collinearity = False
             results = ProblemResults(
                 final_progress, last_results, step, last_step, step_start_time, optimization_start_time,
-                optimization_end_time, optimization_stats, iteration_stats, scale_objective, iteration, fp_type,
-                costs_bounds, extra_micro_covariances, center_moments, W_type, se_type
+                optimization_end_time, optimization_stats, iteration_stats, scale_objective, shares_bounds,
+                costs_bounds, micro_moment_covariances, center_moments, W_type, se_type
             )
             self._handle_errors(results._errors, error_behavior)
             output(f"Computed results after {format_seconds(results.total_time - results.optimization_time)}.")
 
             # store the last results and return results from the final step
             last_results = results
             output("")
-            if not last_step:
-                output(results._format_summary())
-                output("")
-            else:
+            if last_step:
                 output(results)
                 return results
+            if step > 0:
+                output(results._format_summary())
+                output("")
 
             # update vectors and matrices
             delta = results.delta
             tilde_costs = results.tilde_costs
             xi_jacobian = results.xi_by_theta_jacobian
             omega_jacobian = results.omega_by_theta_jacobian
             W = results.updated_W
             step += 1
             step_start_time = time.time()
 
     def _compute_progress(
-            self, parameters: Parameters, moments: EconomyMoments, iv: IV, W: Array, scale_objective: bool,
+            self, parameters: Parameters, moments: Moments, iv: IV, W: Array, scale_objective: bool,
             error_behavior: str, error_punishment: float, delta_behavior: str, iteration: Iteration, fp_type: str,
-            costs_bounds: Bounds, theta: Array, progress: 'InitialProgress', compute_gradient: bool,
-            compute_hessian: bool, compute_micro_covariances: bool) -> 'Progress':
+            shares_bounds: Bounds, costs_bounds: Bounds, finite_differences: bool,
+            resample_agent_data: Optional[Callable[[int], Optional[Mapping]]], theta: Array,
+            progress: 'InitialProgress', compute_gradient: bool, compute_hessian: bool,
+            compute_micro_covariances: bool, detect_micro_collinearity: bool,
+            compute_simulation_covariances: bool, agents_override: Optional[RecArray] = None) -> 'Progress':
         """Compute demand- and supply-side contributions before recovering the linear parameters and structural error
         terms. Then, form the GMM objective value and its gradient. Finally, handle any errors that were encountered
         before structuring relevant progress information.
         """
         errors: List[Error] = []
 
         # expand theta
         sigma, pi, rho, beta, gamma = parameters.expand(theta)
 
-        # compute demand-side contributions
-        delta, micro, xi_jacobian, micro_jacobian, micro_covariances, iteration_stats, demand_errors = (
-            self._compute_demand_contributions(
-                parameters, moments, iteration, fp_type, sigma, pi, rho, progress, compute_gradient,
-                compute_micro_covariances
-            )
-        )
-        errors.extend(demand_errors)
+        # initialize delta, micro moments, their Jacobians, micro moment covariances, micro moment values, indices of
+        #   clipped shares, and fixed point statistics so that they can be filled
+        delta = np.zeros((self.N, 1), options.dtype)
+        micro = np.zeros((moments.MM, 1), options.dtype)
+        xi_jacobian = np.zeros((self.N, parameters.P), options.dtype)
+        micro_jacobian = np.zeros((moments.MM, parameters.P), options.dtype)
+        micro_covariances = np.zeros((moments.MM, moments.MM), options.dtype)
+        micro_values = np.full((moments.MM, 1), np.nan, options.dtype)
+        clipped_shares = np.zeros((self.N, 1), np.bool_)
+        iteration_stats: Dict[Hashable, SolverStats] = {}
 
-        # compute supply-side contributions
+        # initialize transformed marginal costs, their Jacobian, and indices of clipped costs so that they can be filled
         if self.K3 == 0:
             tilde_costs = np.full((self.N, 0), np.nan, options.dtype)
             omega_jacobian = np.full((self.N, parameters.P), np.nan, options.dtype)
-            clipped_costs = np.zeros((self.N, 1), np.bool)
+            clipped_costs = np.zeros((self.N, 1), np.bool_)
         else:
-            tilde_costs, omega_jacobian, clipped_costs, supply_errors = self._compute_supply_contributions(
-                parameters, costs_bounds, sigma, pi, rho, beta, delta, xi_jacobian, progress, compute_gradient
-            )
-            errors.extend(supply_errors)
+            tilde_costs = np.zeros((self.N, 1), options.dtype)
+            omega_jacobian = np.zeros((self.N, parameters.P), options.dtype)
+            clipped_costs = np.zeros((self.N, 1), np.bool_)
+
+        # only do market-by-market computation when necessary
+        compute_jacobians = compute_gradient and not finite_differences
+        if self.K2 == self.K3 == moments.MM == 0 and (parameters.P == 0 or not compute_jacobians):
+            delta = self._compute_logit_delta(rho)
+        else:
+            next_delta = progress.next_delta
+            if delta_behavior == 'logit':
+                next_delta = self._compute_logit_delta(rho)
+
+            # get market indices for any overridden agents
+            agent_market_indices_override = None
+            if agents_override is not None:
+                agent_market_indices_override = get_indices(agents_override.market_ids)
+
+            def market_factory(
+                    s: Hashable) -> (
+                    Tuple[
+                        ProblemMarket, Array, Array, Array, Moments, Iteration, str, Bounds, Bounds, bool, bool, bool
+                    ]):
+                """Build a market along with arguments used to compute delta, micro moment contributions, transformed
+                marginal costs, and Jacobians.
+                """
+                agents_override_s = None
+                if agents_override is not None:
+                    assert agent_market_indices_override is not None
+                    agents_override_s = agents_override[agent_market_indices_override[s]]
+
+                market_s = ProblemMarket(self, s, parameters, sigma, pi, rho, beta, agents_override=agents_override_s)
+                delta_s = next_delta[self._product_market_indices[s]]
+                last_delta_s = progress.delta[self._product_market_indices[s]]
+                last_tilde_costs_s = progress.tilde_costs[self._product_market_indices[s]]
+                return (
+                    market_s, delta_s, last_delta_s, last_tilde_costs_s, moments, iteration, fp_type, shares_bounds,
+                    costs_bounds, compute_jacobians, compute_micro_covariances, detect_micro_collinearity
+                )
+
+            # if necessary, identify micro datasets in which there could possibly be collinearity issues
+            parts_collinearity_candidates: Dict[MicroDataset, List[int]] = {}
+            if detect_micro_collinearity:
+                for p, part in enumerate(moments.micro_parts):
+                    parts_collinearity_candidates.setdefault(part.dataset, []).append(p)
+                parts_collinearity_candidates = {d: v for d, v in parts_collinearity_candidates.items() if len(v) > 1}
+
+            # compute delta, contributions to micro moment parts, transformed marginal costs, Jacobians, and
+            #   covariances market-by-market
+            parts_numerator_mapping: Dict[Hashable, Array] = {}
+            parts_denominator_mapping: Dict[Hashable, Array] = {}
+            parts_numerator_jacobian_mapping: Dict[Hashable, Array] = {}
+            parts_denominator_jacobian_mapping: Dict[Hashable, Array] = {}
+            parts_covariances_numerator_mapping: Dict[Hashable, Array] = {}
+            parts_collinearity_candidate_values: Dict[Hashable, Dict[MicroDataset, Array]] = {}
+            generator = generate_items(self.unique_market_ids, market_factory, ProblemMarket.solve)
+            for t, generated_t in generator:
+                (
+                    delta_t, xi_jacobian_t, parts_numerator_t, parts_denominator_t, parts_numerator_jacobian_t,
+                    parts_denominator_jacobian_t, parts_covariances_numerator_t, weights_mapping_t, values_mapping_t,
+                    clipped_shares_t, iteration_stats_t, tilde_costs_t, omega_jacobian_t, clipped_costs_t, errors_t
+                ) = generated_t
+
+                delta[self._product_market_indices[t]] = delta_t
+                xi_jacobian[self._product_market_indices[t], :parameters.P] = xi_jacobian_t
+                clipped_shares[self._product_market_indices[t]] = clipped_shares_t
+                iteration_stats[t] = iteration_stats_t
+                parts_numerator_mapping[t] = scipy.sparse.csr_matrix(parts_numerator_t)
+                parts_denominator_mapping[t] = scipy.sparse.csr_matrix(parts_denominator_t)
+                if compute_jacobians:
+                    parts_numerator_jacobian_mapping[t] = scipy.sparse.csr_matrix(parts_numerator_jacobian_t)
+                    parts_denominator_jacobian_mapping[t] = scipy.sparse.csr_matrix(parts_denominator_jacobian_t)
+                if compute_micro_covariances:
+                    parts_covariances_numerator_mapping[t] = scipy.sparse.csr_matrix(parts_covariances_numerator_t)
+                if detect_micro_collinearity:
+                    parts_collinearity_candidate_values[t] = {}
+                    for dataset, part_indices in parts_collinearity_candidates.items():
+                        if dataset in weights_mapping_t:
+                            nonzero = np.nonzero(weights_mapping_t[dataset])
+                            parts_collinearity_candidate_values[t][dataset] = np.column_stack(
+                                [values_mapping_t[p][nonzero].flatten() for p in part_indices]
+                            )
+                if self.K3 > 0:
+                    tilde_costs[self._product_market_indices[t]] = tilde_costs_t
+                    clipped_costs[self._product_market_indices[t]] = clipped_costs_t
+                    if compute_jacobians:
+                        omega_jacobian[self._product_market_indices[t], :parameters.P] = omega_jacobian_t
+
+                errors.extend(errors_t)
+
+            # aggregate micro moments, their Jacobian, and their covariances across all markets (this is done after
+            #   market-by-market computation to preserve numerical stability with different market orderings)
+            if moments.MM > 0:
+                with np.errstate(all='ignore'):
+                    # construct micro moment parts
+                    parts_numerator = scipy.sparse.csr_matrix((moments.PM, 1), dtype=options.dtype)
+                    parts_denominator = scipy.sparse.csr_matrix((moments.PM, 1), dtype=options.dtype)
+                    for t in self.unique_market_ids:
+                        parts_numerator += parts_numerator_mapping[t]
+                        parts_denominator += parts_denominator_mapping[t]
+
+                    parts_numerator = parts_numerator.toarray()
+                    parts_denominator = parts_denominator.toarray()
+                    parts_values = parts_numerator / parts_denominator
+
+                    # from the parts, construct micro moment values and if needed their gradient too
+                    micro_gradients = np.zeros((moments.MM, moments.PM), options.dtype)
+                    for m, moment in enumerate(moments.micro_moments):
+                        part_indices = [moments.micro_parts.index(p) for p in moment.parts]
+                        micro_value = moment.compute_value(parts_values[part_indices])
+                        micro_value = np.asarray(micro_value).flatten()
+                        if micro_value.size != 1:
+                            raise TypeError(f"compute_value of micro moment '{moment}' should return a float.")
+                        if not np.isfinite(micro_value):
+                            warn(
+                                f"compute_value of micro moment '{moment}' returned "
+                                f"{format_number(micro_value).strip()}."
+                            )
+
+                        micro_values[m] = micro_value
+
+                        if compute_jacobians or compute_micro_covariances:
+                            micro_gradient = moment.compute_gradient(parts_values[part_indices])
+                            micro_gradient = np.asarray(micro_gradient, options.dtype).flatten()
+                            if micro_gradient.size != len(moment.parts):
+                                raise ValueError(
+                                    f"compute_gradient of micro moment '{moment}' should return an array of size "
+                                    f"{len(moment.parts)}, but it returned one of size {micro_gradient.size}."
+                                )
+                            for p, part in enumerate(moment.parts):
+                                if not np.isfinite(micro_gradient[p]):
+                                    warn(
+                                        f"compute_gradient of micro moment '{moment}' returned "
+                                        f"{format_number(micro_gradient[p]).strip()} for part '{part}'."
+                                    )
+
+                            micro_gradients[m, part_indices] = micro_gradient
+
+                    # construct micro moments
+                    micro = moments.values - micro_values
+
+                    # construct the micro moment Jacobian
+                    if compute_jacobians:
+                        # construct the micro moment parts Jacobian
+                        parts_numerator_jacobian = scipy.sparse.csr_matrix(
+                            (moments.PM, parameters.P), dtype=options.dtype
+                        )
+                        parts_denominator_jacobian = scipy.sparse.csr_matrix(
+                            (moments.PM, parameters.P), dtype=options.dtype
+                        )
+                        for t in self.unique_market_ids:
+                            parts_numerator_jacobian += parts_numerator_jacobian_mapping[t]
+                            parts_denominator_jacobian += parts_denominator_jacobian_mapping[t]
+
+                        parts_numerator_jacobian = parts_numerator_jacobian.toarray()
+                        parts_denominator_jacobian = parts_denominator_jacobian.toarray()
+                        parts_jacobian = (
+                            (parts_numerator_jacobian - parts_values * parts_denominator_jacobian) / parts_denominator
+                        )
+
+                        # construc the micro moment Jacobian with the product rule
+                        micro_jacobian = -micro_gradients @ parts_jacobian
+
+                    # construct micro moment covariances from part covariances
+                    if compute_micro_covariances:
+                        # construct non-centered, non-scaled, non-symmetric part covariances
+                        parts_covariances_numerator = scipy.sparse.csr_matrix(
+                            (moments.PM, moments.PM), dtype=options.dtype
+                        )
+                        for t in self.unique_market_ids:
+                            parts_covariances_numerator += parts_covariances_numerator_mapping[t]
+
+                        parts_covariances_numerator = parts_covariances_numerator.toarray()
+                        parts_covariances = parts_covariances_numerator / parts_denominator
+
+                        # subtract away means from second moments and scale by observation counts
+                        for p1, (part1, value1) in enumerate(zip(moments.micro_parts, parts_values)):
+                            for p2, (part2, value2) in enumerate(zip(moments.micro_parts, parts_values)):
+                                if p2 <= p1 and part1.dataset == part2.dataset:
+                                    parts_covariances[p2, p1] -= value1 * value2
+                                    parts_covariances[p2, p1] /= part1.dataset.observations
+
+                        # fill the lower triangle
+                        lower_indices = np.tril_indices(moments.PM, -1)
+                        parts_covariances[lower_indices] = parts_covariances.T[lower_indices]
+
+                        # compute micro moment covariances with the delta method
+                        micro_covariances = micro_gradients @ parts_covariances @ micro_gradients.T
+                        self._detect_singularity(micro_covariances, "the estimated covariance matrix of micro moments")
+
+                    # detect collinearity between micro moment parts
+                    if detect_micro_collinearity:
+                        for dataset, part_indices in parts_collinearity_candidates.items():
+                            market_ids = self.unique_market_ids if dataset.market_ids is None else dataset.market_ids
+                            values = np.row_stack([parts_collinearity_candidate_values[t][dataset] for t in market_ids])
+                            collinear, successful = precisely_identify_collinearity(values)
+                            common_message = (
+                                "To disable collinearity checks, set "
+                                "options.collinear_atol = options.collinear_rtol = 0."
+                            )
+                            if not successful:
+                                warn(
+                                    f"Failed to compute the QR decomposition for micro dataset '{dataset.name}' while "
+                                    f"checking for collinearity issues. {common_message}"
+                                )
+                            if collinear.any():
+                                labels = [moments.micro_parts[p].name for p in part_indices]
+                                collinear_labels = ", ".join(f"'{l}'" for l, c in zip(labels, collinear) if c)
+                                warn(
+                                    f"Detected collinearity issues with the values of micro moment parts "
+                                    f"[{collinear_labels}] and at least one other micro moment part based on micro "
+                                    f"dataset '{dataset.name}'. {common_message}"
+                                )
+
+        # replace invalid elements in delta, micro moments, and transformed marginal costs with their last values
+        bad_delta_index = ~np.isfinite(delta)
+        if np.any(bad_delta_index):
+            delta[bad_delta_index] = progress.delta[bad_delta_index]
+            errors.append(exceptions.DeltaReversionError(bad_delta_index))
+        if moments.MM > 0:
+            bad_micro_index = ~np.isfinite(micro)
+            if np.any(bad_micro_index):
+                micro[bad_micro_index] = progress.micro[bad_micro_index]
+                errors.append(exceptions.MicroMomentsReversionError(bad_micro_index))
+        if self.K3 > 0:
+            bad_tilde_costs_index = ~np.isfinite(tilde_costs)
+            if np.any(bad_tilde_costs_index):
+                tilde_costs[bad_tilde_costs_index] = progress.tilde_costs[bad_tilde_costs_index]
+                errors.append(exceptions.CostsReversionError(bad_tilde_costs_index))
+
+        # replace invalid elements in the Jacobians with their last values
+        if compute_jacobians:
+            bad_xi_jacobian_index = ~np.isfinite(xi_jacobian)
+            if np.any(bad_xi_jacobian_index):
+                xi_jacobian[bad_xi_jacobian_index] = progress.xi_jacobian[bad_xi_jacobian_index]
+                errors.append(exceptions.XiByThetaJacobianReversionError(bad_xi_jacobian_index))
+            if moments.MM > 0:
+                bad_micro_jacobian_index = ~np.isfinite(micro_jacobian)
+                if np.any(bad_micro_jacobian_index):
+                    micro_jacobian[bad_micro_jacobian_index] = progress.micro_jacobian[bad_micro_jacobian_index]
+                    errors.append(exceptions.MicroMomentsByThetaJacobianReversionError(bad_micro_jacobian_index))
+            if self.K3 > 0:
+                bad_omega_jacobian_index = ~np.isfinite(omega_jacobian)
+                if np.any(bad_omega_jacobian_index):
+                    omega_jacobian[bad_omega_jacobian_index] = progress.omega_jacobian[bad_omega_jacobian_index]
+                    errors.append(exceptions.OmegaByThetaJacobianReversionError(bad_omega_jacobian_index))
+
+        # optionally compute Jacobians with central finite differences
+        if compute_gradient and finite_differences and parameters.P > 0:
+            def compute_perturbed_stack(perturbed_theta: Array) -> Array:
+                """Evaluate a stack of xi, micro moments, and omega at a perturbed parameter vector."""
+                perturbed_progress = self._compute_progress(
+                    parameters, moments, iv, W, scale_objective, error_behavior, error_punishment, delta_behavior,
+                    iteration, fp_type, shares_bounds, costs_bounds, finite_differences=False, resample_agent_data=None,
+                    theta=perturbed_theta, progress=progress, compute_gradient=False, compute_hessian=False,
+                    compute_micro_covariances=False, detect_micro_collinearity=False,
+                    compute_simulation_covariances=False,
+                )
+                perturbed_stack = perturbed_progress.iv_delta
+                if moments.MM > 0:
+                    perturbed_stack = np.r_[perturbed_stack, perturbed_progress.micro]
+                if self.K3 > 0:
+                    perturbed_stack = np.r_[perturbed_stack, perturbed_progress.iv_tilde_costs]
+                return perturbed_stack
+
+            # compute and unstack the Jacobians
+            stack_jacobian = compute_finite_differences(compute_perturbed_stack, theta)
+            xi_jacobian = stack_jacobian[:self.N]
+            if moments.MM > 0:
+                micro_jacobian = stack_jacobian[self.N:self.N + moments.MM]
+            if self.K3 > 0:
+                omega_jacobian = stack_jacobian[-self.N:]
 
         # subtract contributions of linear parameters in theta
         iv_delta = delta.copy()
         iv_tilde_costs = tilde_costs.copy()
         if not parameters.eliminated_beta_index.all():
             theta_beta = np.c_[beta[~parameters.eliminated_beta_index]]
             iv_delta -= self._compute_true_X1(index=~parameters.eliminated_beta_index.flatten()) @ theta_beta
@@ -715,22 +1079,26 @@
             objective = mean_g.T @ W @ mean_g
             if scale_objective:
                 objective *= self.N
         if not np.isfinite(np.squeeze(objective)):
             objective = progress.objective
             errors.append(exceptions.ObjectiveReversionError())
 
-        # compute the gradient and replace any invalid elements with their last values
+        # compute the gradient and replace any invalid elements with their last values (even if we concentrate out
+        #   linear parameters, it turns out that one can use orthogonality conditions to show that treating the linear
+        #   parameters as fixed is fine, so that we can treat xi and omega Jacobians as equal to delta and transformed
+        #   marginal cost Jacobians when computing the gradient)
         gradient = np.full_like(progress.gradient, np.nan)
         if compute_gradient:
             with np.errstate(all='ignore'):
                 mean_G = np.r_[compute_gmm_moments_jacobian_mean(jacobian_list, Z_list), micro_jacobian]
                 gradient = 2 * (mean_G.T @ W @ mean_g)
                 if scale_objective:
                     gradient *= self.N
+
             bad_gradient_index = ~np.isfinite(gradient)
             if np.any(bad_gradient_index):
                 gradient[bad_gradient_index] = progress.gradient[bad_gradient_index]
                 errors.append(exceptions.GradientReversionError(bad_gradient_index))
 
         # handle any errors
         if errors:
@@ -741,178 +1109,78 @@
             else:
                 assert error_behavior == 'punish'
                 objective = np.array(error_punishment)
                 if compute_gradient:
                     gradient = np.zeros_like(progress.gradient)
 
         # select the delta that will be used in the next objective evaluation
-        if delta_behavior == 'last':
-            next_delta = delta
-        else:
-            assert delta_behavior == 'first'
+        next_delta = delta
+        if delta_behavior == 'first':
             next_delta = progress.next_delta
 
-        # compute the hessian with central finite differences
+        # optionally compute the Hessian with central finite differences
         hessian = np.full_like(progress.hessian, np.nan)
         if compute_hessian:
-            compute_progress = lambda x: self._compute_progress(
-                parameters, moments, iv, W, scale_objective, error_behavior, error_punishment, delta_behavior,
-                iteration, fp_type, costs_bounds, x, progress, compute_gradient=True, compute_hessian=False,
-                compute_micro_covariances=False
-            )
-            change = np.sqrt(np.finfo(np.float64).eps)
-            for p in range(parameters.P):
-                theta1 = theta.copy()
-                theta2 = theta.copy()
-                theta1[p] += change / 2
-                theta2[p] -= change / 2
-                hessian[:, [p]] = (compute_progress(theta1).gradient - compute_progress(theta2).gradient) / change
+            def compute_perturbed_gradient(perturbed_theta: Array) -> Array:
+                """Evaluate the gradient at a perturbed parameter vector."""
+                perturbed_progress = self._compute_progress(
+                    parameters, moments, iv, W, scale_objective, error_behavior, error_punishment, delta_behavior,
+                    iteration, fp_type, shares_bounds, costs_bounds, finite_differences, resample_agent_data,
+                    perturbed_theta, progress, compute_gradient=True, compute_hessian=False,
+                    compute_micro_covariances=False, detect_micro_collinearity=False,
+                    compute_simulation_covariances=False,
+                )
+                return perturbed_progress.gradient
 
-            # enforce shape and symmetry
+            # compute the Hessian, enforcing shape and symmetry
+            hessian = compute_finite_differences(compute_perturbed_gradient, theta)
             hessian = np.c_[hessian + hessian.T] / 2
 
+        # optionally resample agents to compute simulation covariances
+        simulation_covariances = np.zeros((mean_g.size, mean_g.size), options.dtype)
+        if compute_simulation_covariances:
+            assert callable(resample_agent_data)
+            mean_g_samples: List[Array] = []
+            while True:
+                try:
+                    resampled_agent_data = resample_agent_data(len(mean_g_samples))
+                except Exception as exception:
+                    raise RuntimeError("Failed to resample agents because of the above exception.") from exception
+                if resampled_agent_data is None:
+                    break
+                try:
+                    resampled_agents = Agents(self.products, self.agent_formulation, resampled_agent_data)
+                except Exception as exception:
+                    raise RuntimeError("Failed to use resampled agents because of the above exception.") from exception
+
+                resampled_progress = self._compute_progress(
+                    parameters, moments, iv, W, scale_objective, error_behavior, error_punishment, delta_behavior,
+                    iteration, fp_type, shares_bounds, costs_bounds, finite_differences, resample_agent_data, theta,
+                    progress, compute_gradient=False, compute_hessian=False, compute_micro_covariances=False,
+                    detect_micro_collinearity=False, compute_simulation_covariances=False,
+                    agents_override=resampled_agents,
+                )
+                mean_g_samples.append(resampled_progress.mean_g.flatten())
+
+            if len(mean_g_samples) < 2:
+                raise ValueError(
+                    "There must be at least 2 resampled sets of agent data to estimate the contribution of simulation "
+                    "error to moment covariances."
+                )
+
+            simulation_covariances = np.cov(mean_g_samples, rowvar=False)
+
         # structure progress
         return Progress(
             self, parameters, moments, W, theta, objective, gradient, hessian, next_delta, delta, tilde_costs, micro,
-            xi_jacobian, omega_jacobian, micro_jacobian, micro_covariances, xi, omega, beta, gamma, iteration_stats,
+            xi_jacobian, omega_jacobian, micro_jacobian, micro_covariances, micro_values, mean_g,
+            simulation_covariances, iv_delta, iv_tilde_costs, xi, omega, beta, gamma, iteration_stats, clipped_shares,
             clipped_costs, errors
         )
 
-    def _compute_demand_contributions(
-            self, parameters: Parameters, moments: EconomyMoments, iteration: Iteration, fp_type: str, sigma: Array,
-            pi: Array, rho: Array, progress: 'InitialProgress', compute_jacobian: bool,
-            compute_micro_covariances: bool) -> (
-            Tuple[Array, Array, Array, Array, Array, Dict[Hashable, SolverStats], List[Error]]):
-        """Compute delta and the Jacobian of xi (equivalently, of delta) with respect to theta market-by-market. If
-        there are any micro moments, compute them (taking the average across relevant markets) along with their
-        Jacobian and covariances. Revert any problematic elements to their last values.
-        """
-        errors: List[Error] = []
-
-        # initialize delta, micro moments, their Jacobians, micro moment covariances, and fixed point statistics so that
-        #   they can be filled
-        delta = np.zeros((self.N, 1), options.dtype)
-        micro = np.zeros((moments.MM, 1), options.dtype)
-        xi_jacobian = np.zeros((self.N, parameters.P), options.dtype)
-        micro_jacobian = np.zeros((moments.MM, parameters.P), options.dtype)
-        micro_covariances = np.zeros((moments.MM, moments.MM), options.dtype)
-        iteration_stats: Dict[Hashable, SolverStats] = {}
-
-        # when possible and when a gradient isn't needed, compute delta with a closed-form solution
-        if self.K2 == 0 and moments.MM == 0 and (parameters.P == 0 or not compute_jacobian):
-            delta = self._compute_logit_delta(rho)
-        else:
-            def market_factory(s: Hashable) -> Tuple[ProblemMarket, Array, Array, Iteration, str, bool, bool]:
-                """Build a market along with arguments used to compute delta, micro moment values, and Jacobians."""
-                market_s = ProblemMarket(self, s, parameters, sigma, pi, rho, moments=moments)
-                delta_s = progress.next_delta[self._product_market_indices[s]]
-                last_delta_s = progress.delta[self._product_market_indices[s]]
-                return market_s, delta_s, last_delta_s, iteration, fp_type, compute_jacobian, compute_micro_covariances
-
-            # compute delta, micro moments, their Jacobians, and micro moment covariances market-by-market
-            micro_mapping: Dict[Hashable, Array] = {}
-            micro_jacobian_mapping: Dict[Hashable, Array] = {}
-            micro_covariances_mapping: Dict[Hashable, Array] = {}
-            generator = generate_items(self.unique_market_ids, market_factory, ProblemMarket.solve_demand)
-            for t, (delta_t, micro_t, xi_jacobian_t, micro_jacobian_t, covariances_t, stats_t, errors_t) in generator:
-                delta[self._product_market_indices[t]] = delta_t
-                xi_jacobian[self._product_market_indices[t], :parameters.P] = xi_jacobian_t
-                micro_mapping[t] = micro_t
-                micro_jacobian_mapping[t] = micro_jacobian_t
-                micro_covariances_mapping[t] = covariances_t
-                iteration_stats[t] = stats_t
-                errors.extend(errors_t)
-
-            # average micro moments, their Jacobian, and their covariances across all markets (this is done after
-            #   market-by-market computation to preserve numerical stability with different market orderings)
-            if moments.MM > 0:
-                with np.errstate(all='ignore'):
-                    for t in self.unique_market_ids:
-                        indices = moments.market_indices[t]
-                        if indices.size > 0:
-                            micro[indices] += micro_mapping[t] / moments.market_counts[indices]
-                            micro_jacobian[indices, :parameters.P] += (
-                                micro_jacobian_mapping[t] / moments.market_counts[indices]
-                            )
-                            if compute_micro_covariances:
-                                pairwise_indices = tuple(np.meshgrid(indices, indices))
-                                micro_covariances[pairwise_indices] += (
-                                    micro_covariances_mapping[t] / moments.pairwise_market_counts[pairwise_indices]
-                                )
-
-                    # enforce shape and symmetry of micro covariances
-                    if compute_micro_covariances:
-                        micro_covariances = np.c_[micro_covariances + micro_covariances.T] / 2
-
-        # replace invalid elements in delta and the micro moment values with their last values
-        bad_delta_index = ~np.isfinite(delta)
-        bad_micro_index = ~np.isfinite(micro)
-        if np.any(bad_delta_index):
-            delta[bad_delta_index] = progress.delta[bad_delta_index]
-            errors.append(exceptions.DeltaReversionError(bad_delta_index))
-        if np.any(bad_micro_index):
-            micro[bad_micro_index] = progress.micro[bad_micro_index]
-            errors.append(exceptions.MicroMomentsReversionError(bad_micro_index))
-
-        # replace invalid elements in the Jacobians with their last values
-        if compute_jacobian:
-            bad_xi_jacobian_index = ~np.isfinite(xi_jacobian)
-            bad_micro_jacobian_index = ~np.isfinite(micro_jacobian)
-            if np.any(bad_xi_jacobian_index):
-                xi_jacobian[bad_xi_jacobian_index] = progress.xi_jacobian[bad_xi_jacobian_index]
-                errors.append(exceptions.XiByThetaJacobianReversionError(bad_xi_jacobian_index))
-            if np.any(bad_micro_jacobian_index):
-                micro_jacobian[bad_micro_jacobian_index] = progress.micro_jacobian[bad_micro_jacobian_index]
-                errors.append(exceptions.MicroMomentsByThetaJacobianReversionError(bad_micro_jacobian_index))
-        return delta, micro, xi_jacobian, micro_jacobian, micro_covariances, iteration_stats, errors
-
-    def _compute_supply_contributions(
-            self, parameters: Parameters, costs_bounds: Bounds, sigma: Array, pi: Array, rho: Array, beta: Array,
-            delta: Array, xi_jacobian: Array, progress: 'InitialProgress', compute_jacobian: bool) -> (
-            Tuple[Array, Array, Array, List[Error]]):
-        """Compute transformed marginal costs and the Jacobian of omega (equivalently, of transformed marginal costs)
-        with respect to theta market-by-market. Revert any problematic elements to their last values.
-        """
-        errors: List[Error] = []
-
-        # initialize transformed marginal costs, their Jacobian, and indices of clipped costs so that they can be filled
-        tilde_costs = np.zeros((self.N, 1), options.dtype)
-        omega_jacobian = np.zeros((self.N, parameters.P), options.dtype)
-        clipped_costs = np.zeros((self.N, 1), np.bool)
-
-        def market_factory(s: Hashable) -> Tuple[ProblemMarket, Array, Array, Bounds, bool]:
-            """Build a market along with arguments used to compute transformed marginal costs and their Jacobian."""
-            market_s = ProblemMarket(self, s, parameters, sigma, pi, rho, beta, delta=delta)
-            last_tilde_costs_s = progress.tilde_costs[self._product_market_indices[s]]
-            xi_jacobian_s = xi_jacobian[self._product_market_indices[s]]
-            return market_s, last_tilde_costs_s, xi_jacobian_s, costs_bounds, compute_jacobian
-
-        # compute transformed marginal costs and their Jacobian market-by-market
-        generator = generate_items(self.unique_market_ids, market_factory, ProblemMarket.solve_supply)
-        for t, (tilde_costs_t, omega_jacobian_t, clipped_costs_t, errors_t) in generator:
-            tilde_costs[self._product_market_indices[t]] = tilde_costs_t
-            omega_jacobian[self._product_market_indices[t], :parameters.P] = omega_jacobian_t
-            clipped_costs[self._product_market_indices[t]] = clipped_costs_t
-            errors.extend(errors_t)
-
-        # replace invalid transformed marginal costs with their last values
-        bad_tilde_costs_index = ~np.isfinite(tilde_costs)
-        if np.any(bad_tilde_costs_index):
-            tilde_costs[bad_tilde_costs_index] = progress.tilde_costs[bad_tilde_costs_index]
-            errors.append(exceptions.CostsReversionError(bad_tilde_costs_index))
-
-        # replace invalid elements in their Jacobian with their last values
-        if compute_jacobian:
-            bad_omega_jacobian_index = ~np.isfinite(omega_jacobian)
-            if np.any(bad_omega_jacobian_index):
-                omega_jacobian[bad_omega_jacobian_index] = progress.omega_jacobian[bad_omega_jacobian_index]
-                errors.append(exceptions.OmegaByThetaJacobianReversionError(bad_omega_jacobian_index))
-
-        return tilde_costs, omega_jacobian, clipped_costs, errors
-
 
 class Problem(ProblemEconomy):
     r"""A BLP-type problem.
 
     This class is initialized with relevant data and solved with :meth:`Problem.solve`.
 
     Parameters
@@ -922,17 +1190,16 @@
         matrix of demand-side linear product characteristics, :math:`X_1`, for the matrix of demand-side nonlinear
         product characteristics, :math:`X_2`, and for the matrix of supply-side characteristics, :math:`X_3`,
         respectively. If the formulation for :math:`X_3` is not specified or is ``None``, a supply side will not be
         estimated. Similarly, if the formulation for :math:`X_2` is not specified or is ``None``, the logit (or nested
         logit) model will be estimated.
 
         Variable names should correspond to fields in ``product_data``. The ``shares`` variable should not be included
-        in the formulations for :math:`X_1` or :math:`X_2`. If ``shares`` is included in the formulation for
-        :math:`X_3`, care should be taken when solving for equilibrium prices in, for example,
-        :meth:`ProblemResults.compute_prices`, since this routine assumes that marginal costs remain constant.
+        in the formulations for :math:`X_1` or :math:`X_2`. The formulation for :math:`X_3` can include shares to allow
+        marginal costs to depend on quantity.
 
         The ``prices`` variable should not be included in the formulation for :math:`X_3`, but it should be included in
         the formulation for :math:`X_1` or :math:`X_2` (or both). The ``absorb`` argument of :class:`Formulation` can be
         used to absorb fixed effects into :math:`X_1` and :math:`X_3`, but not :math:`X_2`. Characteristics in
         :math:`X_2` should generally be included in :math:`X_1`. The typical exception is characteristics that are
         collinear with fixed effects that have been absorbed into :math:`X_1`.
 
@@ -955,15 +1222,15 @@
 
     product_data : `structured array-like`
         Each row corresponds to a product. Markets can have differing numbers of products. The following fields are
         required:
 
             - **market_ids** : (`object`) - IDs that associate products with markets.
 
-            - **shares** : (`numeric`) - Marketshares, :math:`s`, which should be between zero and one, exclusive.
+            - **shares** : (`numeric`) - Market shares, :math:`s`, which should be between zero and one, exclusive.
               Outside shares should also be between zero and one. Shares in each market should sum to less than one.
 
             - **prices** : (`numeric`) - Product prices, :math:`p`.
 
         If a formulation for :math:`X_3` is specified in ``product_formulations``, firm IDs are also required, since
         they will be used to estimate the supply side of the problem:
 
@@ -982,14 +1249,17 @@
               ``add_exogenous`` to ``False``.
 
         The recommendation in :ref:`references:Conlon and Gortmaker (2020)` is to start with differentiation instruments
         of :ref:`references:Gandhi and Houde (2017)`, which can be built with :func:`build_differentiation_instruments`,
         and then compute feasible optimal instruments with :func:`ProblemResults.compute_optimal_instruments` in the
         second stage.
 
+        For guidance on how to construct instruments and add them to product data, refer to the examples in the
+        documentation for the :func:`build_blp_instruments` and :func:`build_differentiation_instruments` functions.
+
         If ``firm_ids`` are specified, custom ownership matrices can be specified as well:
 
             - **ownership** : (`numeric, optional`) - Custom stacked :math:`J_t \times J_t` ownership or product
               holding matrices, :math:`\mathscr{H}`, for each market :math:`t`, which can be built with
               :func:`build_ownership`. By default, standard ownership matrices are built only when they are needed to
               reduce memory usage. If specified, there should be as many columns as there are products in the market
               with the most products. Rightmost columns in markets with fewer products will be ignored.
@@ -1003,30 +1273,37 @@
            ``demand_instruments0``, ``demand_instruments1``, and ``demand_instruments2``.
 
         To estimate a nested logit or random coefficients nested logit (RCNL) model, nesting groups must be specified:
 
             - **nesting_ids** (`object, optional`) - IDs that associate products with nesting groups. When these IDs are
               specified, ``rho`` must be specified in :meth:`Problem.solve` as well.
 
+        It may be convenient to define IDs for different products:
+
+            - **product_ids** (`object, optional`) - IDs that identify products within markets. There can be multiple
+              columns.
+
         Finally, clustering groups can be specified to account for within-group correlation while updating the weighting
         matrix and estimating standard errors:
 
             - **clustering_ids** (`object, optional`) - Cluster group IDs, which will be used if ``W_type`` or
               ``se_type`` in :meth:`Problem.solve` is ``'clustered'``.
 
-        Along with ``market_ids``, ``firm_ids``, ``nesting_ids``, ``clustering_ids``, and ``prices``, the names of any
-        additional fields can typically be used as variables in ``product_formulations``. However, there are a few
-        variable names such as ``'X1'``, which are reserved for use by :class:`Products`.
+        Along with ``market_ids``, ``firm_ids``, ``nesting_ids``, ``product_ids``, ``clustering_ids``, and ``prices``,
+        the names of any additional fields can typically be used as variables in ``product_formulations``. However,
+        there are a few variable names such as ``'X1'``, which are reserved for use by :class:`Products`.
 
     agent_formulation : `Formulation, optional`
         :class:`Formulation` configuration for the matrix of observed agent characteristics called demographics,
         :math:`d`, which will only be included in the model if this formulation is specified. Since demographics are
         only used if there are demand-side nonlinear product characteristics, this formulation should only be specified
         if :math:`X_2` is formulated in ``product_formulations``. Variable names should correspond to fields in
-        ``agent_data``.
+        ``agent_data``. See the information under ``agent_data`` for how to give fields for product-specific
+        demographics :math:`d_{ijt}`.
+
     agent_data : `structured array-like, optional`
         Each row corresponds to an agent. Markets can have differing numbers of agents. Since simulated agents are only
         used if there are demand-side nonlinear product characteristics, agent data should only be specified if
         :math:`X_2` is formulated in ``product_formulations``. If agent data are specified, market IDs are required:
 
             - **market_ids** : (`object`) - IDs that associate agents with markets. The set of distinct IDs should be
               the same as the set in ``product_data``. If ``integration`` is specified, there must be at least as many
@@ -1035,55 +1312,106 @@
         If ``integration`` is not specified, the following fields are required:
 
             - **weights** : (`numeric, optional`) - Integration weights, :math:`w`, for integration over agent choice
               probabilities.
 
             - **nodes** : (`numeric, optional`) - Unobserved agent characteristics called integration nodes,
               :math:`\nu`. If there are more than :math:`K_2` columns (the number of demand-side nonlinear product
-              characteristics), only the first :math:`K_2` will be retained.
+              characteristics), only the first :math:`K_2` will be retained. If any columns of ``sigma`` in
+              :meth:`Problem.solve` are fixed at zero, only the first few columns of these nodes will be used.
 
         The convenience function :func:`build_integration` can be useful when constructing custom nodes and weights.
 
         .. note::
 
            If ``nodes`` has multiple columns, it can be specified as a matrix or broken up into multiple one-dimensional
            fields with column index suffixes that start at zero. For example, if there are three columns of nodes, a
            ``nodes`` field with three columns can be replaced by three one-dimensional fields: ``nodes0``, ``nodes1``,
            and ``nodes2``.
 
-        Along with ``market_ids``, the names of any additional fields can be typically be used as variables in
-        ``agent_formulation``. The exception is the name ``'demographics'``, which is reserved for use by
+        It may be convenient to define IDs for different agents:
+
+            - **agent_ids** (`object, optional`) - IDs that identify agents within markets. There can be multiple of the
+              same ID within a market.
+
+        Along with ``market_ids`` and ``agent_ids``, the names of any additional fields can be typically be used as
+        variables in ``agent_formulation``. The exception is the name ``'demographics'``, which is reserved for use by
         :class:`Agents`.
 
+        In addition to standard demographic variables :math:`d_{it}`, it is also possible to specify product-specific
+        demographics :math:`d_{ijt}`. A typical example is geographic distance of agent :math:`i` from product
+        :math:`j`. If ``agent_formulation`` has, for example, ``'distance'``, instead of including a single
+        ``'distance'`` field in ``agent_data``, one should instead include ``'distance0'``, ``'distance1'``,
+        ``'distance2'`` and so on, where the index corresponds to the order in which products appear within market in
+        ``product_data``. For example, ``'distance5'`` should measure the distance of agents to the fifth product within
+        the market, as ordered in ``product_data``. The last index should be the number of products in the largest
+        market, minus one. For markets with fewer products than this maximum number, latter columns will be ignored.
+
     integration : `Integration, optional`
         :class:`Integration` configuration for how to build nodes and weights for integration over agent choice
         probabilities, which will replace any ``nodes`` and ``weights`` fields in ``agent_data``. This configuration is
         required if ``nodes`` and ``weights`` in ``agent_data`` are not specified. It should not be specified if
         :math:`X_2` is not formulated in ``product_formulations``.
 
         If this configuration is specified, :math:`K_2` columns of nodes (the number of demand-side nonlinear product
         characteristics) will be built. However, if ``sigma`` in :meth:`Problem.solve` is left unspecified or
         specified with columns fixed at zero, fewer columns will be used.
 
-    distributions : `sequence of str, optional`
-        Random coefficient distributions. By default, random coefficients in :eq:`mu` are assumed to be normally
-        distributed. Non-default distributions can be specified with a list of the following supported strings:
-
-            - ``'normal'`` (default) - The random coefficient is assumed to be normal.
-
-            - ``'lognormal'`` - The random coefficient is assumed to be lognormal. The coefficient's column in :eq:`mu`
-              is exponentiated before being pre-multiplied by :math:`X_2`.
-
-        The list should have as many strings as there are columns in :math:`X_2`. Each string determines the
-        distribution of the random coefficient on the corresponding product characteristic in :math:`X_2`.
-
-        A typical example of a lognormal coefficient is one on prices. Implementing this typically involves having a
-        ``I(-prices)`` in the formulation for :math:`X_2`, and instead of including ``prices`` in :math:`X_1`,
-        including a ``1`` in the ``agent_formulation``. Then the corresponding coefficient in :math:`\Pi` will serve as
-        the mean parameter for the lognormal random coefficient on negative prices, :math:`-p_{jt}`.
+    rc_types : `sequence of str, optional`
+        Random coefficient types:
+
+            - ``'linear'`` (default) - The random coefficient is as defined in :eq:`mu`. All elliptical distributions
+              are supported, including the normal distribution.
+
+            - ``'log'`` - The random coefficient's column in :eq:`mu` is exponentiated before being pre-multiplied by
+              :math:`X_2`. It will take on values bounded from below by zero. All log-elliptical distributions are
+              supported, including the lognormal distribution.
+
+            - ``'logit'`` - The random coefficient's column in :eq:`mu` is passed through the inverse logit function
+              before being pre-multiplied by :math:`X_2`. It will take on values bounded from below by zero and above by
+              one.
+
+        The list should have as many strings as there are columns in :math:`X_2`. Each string determines the type of the
+        random coefficient on the corresponding product characteristic in :math:`X_2`.
+
+        A typical example of when to use ``'log'`` is to have a lognormal coefficient on prices. Implementing this
+        typically involves having an ``I(-prices)`` in the formulation for :math:`X_2`, and instead of including
+        ``prices`` in :math:`X_1`, including a ``1`` in the ``agent_formulation``. Then the corresponding coefficient in
+        :math:`\Pi` will serve as the mean parameter for the lognormal random coefficient on negative
+        prices, :math:`-p_{jt}`.
+
+    epsilon_scale : `float, optional`
+        Factor by which the Type I Extreme Value idiosyncratic preference term, :math:`\epsilon_{ijt}`, is scaled. By
+        default, :math:`\epsilon_{ijt}` is not scaled. The typical use of this parameter is to approximate the pure
+        characteristics model of :ref:`references:Berry and Pakes (2007)` by choosing a value smaller than ``1.0``. As
+        this scaling factor approaches zero, the model approaches the pure characteristics model in which there is no
+        idiosyncratic preference term.
+
+        In practice, this is implemented by dividing :math:`V_{ijt} = \delta_{jt} + \mu_{ijt}` by the scaling factor
+        when solving for the mean utility :math:`\delta_{jt}`. For small scaling factors, this leads to large values
+        of :math:`V_{ijt}`, which when exponentiated in the logit expression can lead to overflow issues discussed in
+        :ref:`references:Berry and Pakes (2007)`. The safe versions of the contraction mapping discussed in the
+        documentation for ``fp_type`` in :meth:`Problem.solve` (which is used by default) eliminate overflow issues at
+        the cost of introducing fewer (but still common for a small scaling factor) underflow issues. Throughout the
+        contraction mapping, some values of the simulated shares :math:`s_{jt}(\delta, \theta)` can underflow to zero,
+        causing the contraction to fail when taking logs. By default, ``shares_bounds`` in :meth:`Problem.solve` bounds
+        these simulated shares from below by ``1e-300``, which eliminates these underflow issues at the cost of making
+        it more difficult for iteration routines to converge.
+
+        With this in mind, scaling epsilon is not supported for nonlinear contractions, and is also not supported when
+        there are nesting groups, since these further complicate the problem. In practice, if the goal is to approximate
+        the pure characteristics model, it is a good idea to slowly decrease the scale of epsilon (e.g., starting with
+        ``0.5``, trying ``0.1``, etc.) until the contraction begins to fail. To further decrease the scale, there are a
+        few things that can help. One is passing a different :class:`Iteration` configuration to ``iteration`` in
+        :meth:`Problem.solve`, such as ``'lm'``, which can be robust in this situation. Another is to set
+        ``pyblp.options.dtype = np.longdouble`` when on a system that supports extended precision (see
+        :mod:`~pyblp.options` for more information about this) and choose a smaller lower bound by configuring
+        ``shares_bounds`` in :meth:`Problem.solve`. Ultimately the model will stop being solvable at a certain point,
+        and this point will vary by problem, so approximating the pure characteristics model requires some degree of
+        experimentation.
 
     costs_type : `str, optional`
         Functional form of the marginal cost function :math:`\tilde{c} = f(c)` in :eq:`costs`. The following
         specifications are supported:
 
             - ``'linear'`` (default) - Linear specification: :math:`\tilde{c} = c`.
 
@@ -1126,16 +1454,22 @@
         :func:`data_to_dict` function can be used to convert this into a more usable data type.
     unique_market_ids : `ndarray`
         Unique market IDs in product and agent data.
     unique_firm_ids : `ndarray`
         Unique firm IDs in product data.
     unique_nesting_ids : `ndarray`
         Unique nesting group IDs in product data.
-    distributions : `list of str`
-        Random coefficient distributions.
+    unique_product_ids : `ndarray`
+        Unique product IDs in product data.
+    unique_agent_ids : `ndarray`
+        Unique agent IDs in agent data.
+    rc_types : `list of str`
+        Random coefficient types.
+    epsilon_scale : `float`
+        Factor by which the Type I Extreme Value idiosyncratic preference term, :math:`\epsilon_{ijt}`, is scaled.
     costs_type : `str`
         Functional form of the marginal cost function :math:`\tilde{c} = f(c)`.
     T : `int`
         Number of markets, :math:`T`.
     N : `int`
         Number of products across all markets, :math:`N`.
     F : `int`
@@ -1168,16 +1502,16 @@
         - :doc:`Tutorial </tutorial>`
 
     """
 
     def __init__(
             self, product_formulations: Union[Formulation, Sequence[Optional[Formulation]]], product_data: Mapping,
             agent_formulation: Optional[Formulation] = None, agent_data: Optional[Mapping] = None,
-            integration: Optional[Integration] = None, distributions: Optional[Sequence[str]] = None,
-            costs_type: str = 'linear', add_exogenous: bool = True) -> None:
+            integration: Optional[Integration] = None, rc_types: Optional[Sequence[str]] = None,
+            epsilon_scale: float = 1.0, costs_type: str = 'linear', add_exogenous: bool = True) -> None:
         """Initialize the underlying economy with product and agent data before absorbing fixed effects."""
 
         # keep track of long it takes to initialize the problem
         output("Initializing the problem ...")
         start_time = time.time()
 
         # validate and normalize product formulations
@@ -1188,15 +1522,15 @@
         else:
             raise TypeError("product_formulations must be a Formulation instance or a sequence of up to three of them.")
         product_formulations.extend([None] * (3 - len(product_formulations)))
 
         # initialize the underlying economy with structured product and agent data
         products = Products(product_formulations, product_data, add_exogenous=add_exogenous)
         agents = Agents(products, agent_formulation, agent_data, integration)
-        super().__init__(product_formulations, agent_formulation, products, agents, distributions, costs_type)
+        super().__init__(product_formulations, agent_formulation, products, agents, rc_types, epsilon_scale, costs_type)
 
         # absorb any demand-side fixed effects
         if self._absorb_demand_ids is not None:
             output("Absorbing demand-side fixed effects ...")
             self.products.X1, X1_errors = self._absorb_demand_ids(self.products.X1)
             self._handle_errors(X1_errors)
             if add_exogenous:
@@ -1209,15 +1543,15 @@
             self.products.X3, X3_errors = self._absorb_supply_ids(self.products.X3)
             self._handle_errors(X3_errors)
             if add_exogenous:
                 self.products.ZS, ZS_errors = self._absorb_supply_ids(self.products.ZS)
                 self._handle_errors(ZS_errors)
 
         # detect any problems with the product data
-        self._detect_collinearity()
+        self._detect_collinearity(add_exogenous)
 
         # output information about the initialized problem
         output(f"Initialized the problem after {format_seconds(time.time() - start_time)}.")
         output("")
         output(self)
 
 
@@ -1251,15 +1585,15 @@
             'ZD': (ZD, options.dtype),
             'ZS': (ZS, options.dtype)
         })
 
         # initialize the underlying economy with structured product and agent data
         super().__init__(
             problem.product_formulations, problem.agent_formulation, updated_products, problem.agents,
-            distributions=problem.distributions, costs_type=problem.costs_type
+            rc_types=problem.rc_types, epsilon_scale=problem.epsilon_scale, costs_type=problem.costs_type
         )
 
         # absorb any demand-side fixed effects, which have already been absorbed into X1
         if self._absorb_demand_ids is not None:
             output("Absorbing demand-side fixed effects ...")
             self.products.ZD, ZD_errors = self._absorb_demand_ids(self.products.ZD)
             if ZD_errors:
@@ -1269,15 +1603,15 @@
         if self._absorb_supply_ids is not None:
             output("Absorbing supply-side fixed effects ...")
             self.products.ZS, ZS_errors = self._absorb_supply_ids(self.products.ZS)
             if ZS_errors:
                 raise exceptions.MultipleErrors(ZS_errors)
 
         # detect any collinearity issues with the updated instruments
-        self._detect_collinearity()
+        self._detect_collinearity(added_exogenous=True)
 
         # output information about the re-created problem
         output(f"Re-created the problem after {format_seconds(time.time() - start_time)}.")
         output("")
         output(self)
 
 
@@ -1294,44 +1628,44 @@
         # keep track of long it takes to re-create the problem
         output("Re-creating the problem ...")
         start_time = time.time()
 
         # initialize the underlying economy with structured product and agent data
         super().__init__(
             problem.product_formulations, problem.agent_formulation, problem.products, sampled_agents,
-            distributions=problem.distributions, costs_type=problem.costs_type
+            rc_types=problem.rc_types, epsilon_scale=problem.epsilon_scale, costs_type=problem.costs_type
         )
 
         # output information about the re-created problem
         output(f"Re-created the problem after {format_seconds(time.time() - start_time)}.")
         output("")
         output(self)
 
 
 class InitialProgress(object):
     """Structured information about initial estimation progress."""
 
     problem: ProblemEconomy
     parameters: Parameters
-    moments: EconomyMoments
+    moments: Moments
     W: Array
     theta: Array
     objective: Array
     gradient: Array
     hessian: Array
     next_delta: Array
     delta: Array
     tilde_costs: Array
     micro: Array
     xi_jacobian: Array
     omega_jacobian: Array
     micro_jacobian: Array
 
     def __init__(
-            self, problem: ProblemEconomy, parameters: Parameters, moments: EconomyMoments, W: Array, theta: Array,
+            self, problem: ProblemEconomy, parameters: Parameters, moments: Moments, W: Array, theta: Array,
             objective: Array, gradient: Array, hessian: Array, next_delta: Array, delta: Array, tilde_costs: Array,
             micro: Array, xi_jacobian: Array, omega_jacobian: Array, micro_jacobian: Array) -> None:
         """Store initial progress information, computing the projected gradient and the reduced Hessian."""
         self.problem = problem
         self.parameters = parameters
         self.moments = moments
         self.W = W
@@ -1348,42 +1682,52 @@
         self.micro_jacobian = micro_jacobian
 
 
 class Progress(InitialProgress):
     """Structured information about estimation progress."""
 
     micro_covariances: Array
+    micro_values: Array
+    mean_g: Array
     xi: Array
     omega: Array
     beta: Array
     gamma: Array
     iteration_stats: Dict[Hashable, SolverStats]
+    clipped_shares: Array
     clipped_costs: Array
     errors: List[Error]
     projected_gradient: Array
     reduced_hessian: Array
     projected_gradient_norm: Array
 
     def __init__(
-            self, problem: ProblemEconomy, parameters: Parameters, moments: EconomyMoments, W: Array, theta: Array,
+            self, problem: ProblemEconomy, parameters: Parameters, moments: Moments, W: Array, theta: Array,
             objective: Array, gradient: Array, hessian: Array, next_delta: Array, delta: Array, tilde_costs: Array,
             micro: Array, xi_jacobian: Array, omega_jacobian: Array, micro_jacobian: Array, micro_covariances: Array,
+            micro_values: Array, mean_g: Array, simulation_covariances: Array, iv_delta: Array, iv_tilde_costs: Array,
             xi: Array, omega: Array, beta: Array, gamma: Array, iteration_stats: Dict[Hashable, SolverStats],
-            clipped_costs: Array, errors: List[Error]) -> None:
+            clipped_shares: Array, clipped_costs: Array, errors: List[Error]) -> None:
         """Store progress information, compute the projected gradient and its norm, and compute the reduced Hessian."""
         super().__init__(
             problem, parameters, moments, W, theta, objective, gradient, hessian, next_delta, delta, tilde_costs, micro,
             xi_jacobian, omega_jacobian, micro_jacobian
         )
         self.micro_covariances = micro_covariances
+        self.micro_values = micro_values
+        self.mean_g = mean_g
+        self.simulation_covariances = simulation_covariances
+        self.iv_delta = iv_delta
+        self.iv_tilde_costs = iv_tilde_costs
         self.xi = xi
         self.omega = omega
         self.beta = beta
         self.gamma = gamma
         self.iteration_stats = iteration_stats or {}
+        self.clipped_shares = clipped_shares
         self.clipped_costs = clipped_costs
         self.errors = errors or []
 
         # compute the projected gradient and the reduced Hessian
         self.projected_gradient = self.gradient.copy()
         self.reduced_hessian = self.hessian.copy()
         for p, (lb, ub) in enumerate(self.parameters.compress_bounds()):
@@ -1398,16 +1742,16 @@
         # compute the norm of the projected gradient
         self.projected_gradient_norm = np.array(np.nan, options.dtype)
         if gradient.size > 0:
             with np.errstate(invalid='ignore'):
                 self.projected_gradient_norm = np.abs(self.projected_gradient).max()
 
     def format(
-            self, optimization: Optimization, costs_bounds: Bounds, step: int, iterations: int, evaluations: int,
-            smallest_objective: Array) -> str:
+            self, optimization: Optimization, shares_bounds: Bounds, costs_bounds: Bounds, step: int, iterations: int,
+            evaluations: int, progress_time: float, smallest_objective: Array) -> str:
         """Format a universal display of optimization progress as a string. The first iteration will include the
         progress table header. If there are any errors, information about them will be formatted as well, regardless of
         whether or not a universal display is to be used. The smallest_objective is the smallest objective value
         encountered so far during optimization.
         """
         lines: List[str] = []
 
@@ -1423,26 +1767,30 @@
 
         # only output errors if the solver's display is being used
         if not optimization._universal_display:
             return "\n".join(lines)
 
         # construct the leftmost part of the table that always shows up
         header = [
-            ("GMM", "Step"), ("Optimization", "Iterations"), ("Objective", "Evaluations"),
+            ("GMM", "Step"), ("Computation", "Time"), ("Optimization", "Iterations"), ("Objective", "Evaluations"),
             ("Fixed Point", "Iterations"), ("Contraction", "Evaluations")
         ]
         values = [
             str(step),
+            format_seconds(progress_time),
             str(iterations),
             str(evaluations),
             str(sum(s.iterations for s in self.iteration_stats.values())),
             str(sum(s.evaluations for s in self.iteration_stats.values()))
         ]
 
-        # add a count of any clipped marginal costs
+        # add a count of any clipped shares or marginal costs
+        if np.isfinite(shares_bounds).any():
+            header.append(("Clipped", "Shares"))
+            values.append(str(self.clipped_shares.sum()))
         if np.isfinite(costs_bounds).any():
             header.append(("Clipped", "Costs"))
             values.append(str(self.clipped_costs.sum()))
 
         # add information about the objective
         header.extend([("Objective", "Value"), ("Objective", "Improvement")])
         values.append(format_number(self.objective))
@@ -1457,15 +1805,15 @@
             header.append(("Projected", "Gradient Norm") if self.parameters.any_bounds else ("Gradient", "Norm"))
             values.append(format_number(self.projected_gradient_norm))
 
         # add information about theta
         header.append(("", "Theta"))
         values.append(", ".join(format_number(x) for x in self.theta))
 
-        # add information about micro moments
-        if self.moments.MM > 0:
-            header.append(("Micro", "Moments"))
-            values.append(", ".join(format_number(x) for x in self.micro))
+        # add a space and an extra header every 50 evaluations
+        include_header = (evaluations - 1) % 50 == 0
+        if include_header and evaluations > 1:
+            lines.append("")
 
         # format the table
-        lines.append(format_table(header, values, include_border=False, include_header=evaluations == 1))
+        lines.append(format_table(header, values, include_border=False, include_header=include_header))
         return "\n".join(lines)
```

### Comparing `pyblp-0.9.0/pyblp/economies/simulation.py` & `pyblp-1.0.0/pyblp/economies/simulation.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,52 +1,55 @@
 """Economy-level simulation of synthetic BLP data."""
 
-import collections
+import collections.abc
 import time
 from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union
 
 import numpy as np
 
 from .economy import Economy
+from .. import exceptions
 from .. import options
 from ..configurations.formulation import Formulation
 from ..configurations.integration import Integration
 from ..configurations.iteration import Iteration
 from ..markets.simulation_market import SimulationMarket
 from ..parameters import Parameters
 from ..primitives import Agents, Products
 from ..results.simulation_results import SimulationResults
+from ..utilities.algebra import precisely_compute_eigenvalues
 from ..utilities.basics import (
-    Array, Error, Groups, SolverStats, RecArray, extract_matrix, format_seconds, generate_items, output,
+    Array, Bounds, Error, Groups, SolverStats, RecArray, extract_matrix, format_seconds, generate_items, output,
     output_progress, structure_matrices
 )
 
 
 class Simulation(Economy):
     r"""Simulation of data in BLP-type models.
 
     Any data left unspecified are simulated during initialization. Simulated prices and shares can be replaced by
     :meth:`Simulation.replace_endogenous` with equilibrium values that are consistent with true parameters. Less
-    commonly, simulated exogenous variables can be replaced instead by :meth:`Simulation.replace_exogenous`. Simulations
+    commonly, simulated exogenous variables can be replaced instead by :meth:`Simulation.replace_exogenous`. To choose
+    your own prices, refer to the first note in :meth:`Simulation.replace_endogenous`. Simulations
     are typically used for two purposes:
 
         1. Solving for equilibrium prices and shares under more complicated counterfactuals than is possible with
            :meth:`ProblemResults.compute_prices` and :meth:`ProblemResults.compute_shares`. For example, this class
            can be initialized with estimated parameters, structural errors, and marginal costs from a
            :meth:`ProblemResults`, but with changed data (fewer products, new products, different characteristics, etc.)
            and :meth:`Simulation.replace_endogenous` can be used to compute the corresponding prices and shares.
 
         2. Simulation of BLP-type models from scratch. For example, a model with fixed true parameters can be simulated
            many times, converted into problems with :meth:`SimulationResults.to_problem`, and solved with
            :meth:`Problem.solve` to evaluate in a Monte Carlo study how well the true parameters can be recovered.
 
     If data for variables (used to formulate product characteristics in :math:`X_1`, :math:`X_2`, and :math:`X_3`, as
-    well as agent demographics, :math:`d`, and endogenous prices and marketshares :math:`p` and :math:`s`) are not
+    well as agent demographics, :math:`d`, and endogenous prices and market shares :math:`p` and :math:`s`) are not
     provided, the values for each unspecified variable are drawn independently from the standard uniform distribution.
-    In each market :math:`t`, marketshares are divided by the number of products in the market :math:`J_t`. Typically,
+    In each market :math:`t`, market shares are divided by the number of products in the market :math:`J_t`. Typically,
     :meth:`Simulation.replace_endogenous` is used to replace prices and shares with equilibrium values that are
     consistent with true parameters.
 
     If data for unobserved demand-and supply-side product characteristics, :math:`\xi` and :math:`\omega`, are not
     provided, they are by default drawn from a mean-zero bivariate normal distribution.
 
     After variables are loaded or simulated, any unspecified integration nodes and weights, :math:`\nu` and :math:`w`,
@@ -58,17 +61,16 @@
         :class:`Formulation` configuration or a sequence of up to three :class:`Formulation` configurations for the
         matrix of demand-side linear product characteristics, :math:`X_1`, for the matrix of demand-side nonlinear
         product characteristics, :math:`X_2`, and for the matrix of supply-side characteristics, :math:`X_3`,
         respectively. If the formulation for :math:`X_2` is not specified or is ``None``, the logit (or nested logit)
         model will be simulated.
 
         The ``shares`` variable should not be included in the formulations for :math:`X_1` or :math:`X_2`. If ``shares``
-        is included in the formulation for :math:`X_3`, care should be taken when solving for equilibrium prices in,
-        for example, :meth:`Simulation.replace_endogenous`, since this routine assumes that marginal costs remain
-        constant.
+        is included in the formulation for :math:`X_3` and ``product_data`` does not include ``shares``, one will likely
+        want to set ``constant_costs=False`` in :meth:`Simulation.replace_endogenous`.
 
         The ``prices`` variable should not be included in the formulation for :math:`X_3`, but it should be included in
         the formulation for :math:`X_1` or :math:`X_2` (or both). Variables that cannot be loaded from ``product_data``
         will be drawn from independent standard uniform distributions. Unlike in :class:`Problem`, fixed effect
         absorption is not supported during simulation.
 
         .. warning::
@@ -85,35 +87,40 @@
 
             - **market_ids** : (`object`) - IDs that associate products with markets.
 
             - **firm_ids** : (`object`) - IDs that associate products with firms.
 
         Custom ownership matrices can be specified as well:
 
-            - **ownership** : (`numeric, optional') - Custom stacked :math:`J_t \times J_t` ownership or product holding
+            - **ownership** : (`numeric, optional`) - Custom stacked :math:`J_t \times J_t` ownership or product holding
               matrices, :math:`\mathscr{H}`, for each market :math:`t`, which can be built with :func:`build_ownership`.
               By default, standard ownership matrices are built only when they are needed to reduce memory usage. If
               specified, there should be as many columns as there are products in the market with the most products.
               Rightmost columns in markets with fewer products will be ignored.
 
         .. note::
 
            The ``ownership`` field can either be a matrix or can be broken up into multiple one-dimensional fields with
            column index suffixes that start at zero. For example, if there are three products in each market, a
            ``ownership`` field with three columns can be replaced by three one-dimensional fields: ``ownership0``,
            ``ownership1``, and ``ownership2``.
 
+        It may be convenient to define IDs for different products:
+
+            - **product_ids** (`object, optional`) - IDs that identify products within markets. There can be multiple
+              columns.
+
         To simulate a nested logit or random coefficients nested logit (RCNL) model, nesting groups must be specified:
 
             - **nesting_ids** (`object, optional`) - IDs that associate products with nesting groups. When these IDs are
               specified, ``rho`` must be specified as well.
 
-        Along with ``market_ids``, ``firm_ids``, and ``nesting_ids``, the names of any additional fields can typically
-        be used as variables in ``product_formulations``. However, there are a few variable names such as ``'X1'``,
-        which are reserved for use by :class:`Products`.
+        Along with ``market_ids``, ``firm_ids``, ``product_ids``, and ``nesting_ids``, the names of any additional
+        fields can typically be used as variables in ``product_formulations``. However, there are a few variable names
+        such as ``'X1'``, which are reserved for use by :class:`Products`.
 
     beta : `array-like`
         Vector of demand-side linear parameters, :math:`\beta`. Elements correspond to columns in :math:`X_1`, which
         is formulated by ``product_formulations``.
     sigma : `array-like, optional`
         Lower-triangular Cholesky root of the covariance matrix for unobserved taste heterogeneity, :math:`\Sigma`. Rows
         and columns correspond to columns in :math:`X_2`, which is formulated by ``product_formulations``. If
@@ -146,29 +153,44 @@
         If ``integration`` is not specified, the following fields are required:
 
             - **weights** : (`numeric, optional`) - Integration weights, :math:`w`, for integration over agent choice
               probabilities.
 
             - **nodes** : (`numeric, optional`) - Unobserved agent characteristics called integration nodes,
               :math:`\nu`. If there are more than :math:`K_2` columns (the number of demand-side nonlinear product
-              characteristics), only the first :math:`K_2` will be used.
+              characteristics), only the first :math:`K_2` will be used. If any columns of ``sigma`` are fixed at zero,
+              only the first few columns of these nodes will be used.
 
         The convenience function :func:`build_integration` can be useful when constructing custom nodes and weights.
 
         .. note::
 
            If ``nodes`` has multiple columns, it can be specified as a matrix or broken up into multiple one-dimensional
            fields with column index suffixes that start at zero. For example, if there are three columns of nodes, a
            ``nodes`` field with three columns can be replaced by three one-dimensional fields: ``nodes0``, ``nodes1``,
            and ``nodes2``.
 
-        Along with ``market_ids``, the names of any additional fields can typically be used as variables in
-        ``agent_formulation``. The exception is the name ``'demographics'``, which is reserved for use by
+        It may be convenient to define IDs for different agents:
+
+            - **agent_ids** (`object, optional`) - IDs that identify agents within markets. There can be multiple of the
+              same ID within a market.
+
+        Along with ``market_ids`` and ``agent_ids``, the names of any additional fields can typically be used as
+        variables in ``agent_formulation``. The exception is the name ``'demographics'``, which is reserved for use by
         :class:`Agents`.
 
+        In addition to standard demographic variables :math:`d_{it}`, it is also possible to specify product-specific
+        demographics :math:`d_{ijt}`. A typical example is geographic distance of agent :math:`i` from product
+        :math:`j`. If ``agent_formulation`` has, for example, ``'distance'``, instead of including a single
+        ``'distance'`` field in ``agent_data``, one should instead include ``'distance0'``, ``'distance1'``,
+        ``'distance2'`` and so on, where the index corresponds to the order in which products appear within market in
+        ``product_data``. For example, ``'distance5'`` should measure the distance of agents to the fifth product within
+        the market, as ordered in ``product_data``. The last index should be the number of products in the largest
+        market, minus one. For markets with fewer products than this maximum number, latter columns will be ignored.
+
     integration : `Integration, optional`
         :class:`Integration` configuration for how to build nodes and weights for integration over agent choice
         probabilities, which will replace any ``nodes`` and ``weights`` fields in ``agent_data``. This configuration is
         required if ``nodes`` and ``weights`` in ``agent_data`` are not specified. It should not be specified if
         :math:`X_2` is not formulated in ``product_formulations``.
 
         If this configuration is specified, :math:`K_2` columns of nodes (the number of demand-side nonlinear product
@@ -187,42 +209,58 @@
     xi_variance : `float, optional`
         Variance of :math:`\xi`. The default value is ``1.0``. This is ignored if ``xi`` or ``omega`` is specified.
     omega_variance : `float, optional`
         Variance of :math:`\omega`. The default value is ``1.0``. This is ignored if ``xi`` or ``omega`` is specified.
     correlation : `float, optional`
         Correlation between :math:`\xi` and :math:`\omega`. The default value is ``0.9``. This is ignored if ``xi`` or
         ``omega`` is specified.
-    distributions : `sequence of str, optional`
-        Random coefficient distributions. By default, random coefficients in :eq:`mu` are assumed to be normally
-        distributed. Non-default distributions can be specified with a list of the following supported strings:
-
-            - ``'normal'`` (default) - The random coefficient is assumed to be normal.
-
-            - ``'lognormal'`` - The random coefficient is assumed to be lognormal. The coefficient's column in :eq:`mu`
-              is exponentiated before being pre-multiplied by :math:`X_2`.
-
-        The list should have as many strings as there are columns in :math:`X_2`. Each string determines the
-        distribution of the random coefficient on the corresponding product characteristic in :math:`X_2`.
-
-        A typical example of a lognormal coefficient is one on prices. Implementing this typically involves having a
-        ``I(-prices)`` in the formulation for :math:`X_2`, and instead of including ``prices`` in :math:`X_1`,
-        including a ``1`` in the ``agent_formulation``. Then the corresponding coefficient in :math:`\Pi` will serve as
-        the mean parameter for the lognormal random coefficient on negative prices, :math:`-p_{jt}`.
+    rc_types : `sequence of str, optional`
+        Random coefficient types:
+
+            - ``'linear'`` (default) - The random coefficient is as defined in :eq:`mu`.
+
+            - ``'log'`` - The random coefficient's column in :eq:`mu` is exponentiated before being pre-multiplied by
+              :math:`X_2`. It will take on values bounded from below by zero.
+
+            - ``'logit'`` - The random coefficient's column in :eq:`mu` is passed through the inverse logit function
+              before being pre-multiplied by :math:`X_2`. It will take on values bounded from below by zero and above by
+              one.
+
+        The list should have as many strings as there are columns in :math:`X_2`. Each string determines the type of the
+        random coefficient on the corresponding product characteristic in :math:`X_2`.
+
+        A typical example of when to use ``'log'`` is to have a lognormal coefficient on prices. Implementing this
+        typically involves having an ``I(-prices)`` in the formulation for :math:`X_2`, and instead of including
+        ``prices`` in :math:`X_1`, including a ``1`` in the ``agent_formulation``. Then the corresponding coefficient in
+        :math:`\Pi` will serve as the mean parameter for the lognormal random coefficient on negative
+        prices, :math:`-p_{jt}`.
+
+    epsilon_scale : `float, optional`
+        Factor by which the Type I Extreme Value idiosyncratic preference term, :math:`\epsilon_{ijt}`, is scaled. By
+        default, :math:`\epsilon_{ijt}` is not scaled. The typical use of this parameter is to approximate the pure
+        characteristics model of :ref:`references:Berry and Pakes (2007)` by choosing a value smaller than ``1.0``. As
+        this scaling factor approaches zero, the model approaches the pure characteristics model in which there is no
+        idiosyncratic preference term.
+
+        For more information about choosing this parameter and estimating models where it is smaller than ``1.0``, refer
+        to the same argument in :meth:`Problem.solve`. In some situations, it may be easier to solve simulations with
+        small epsilon scaling factors by using :meth:`Simulation.replace_exogenous` rather than
+        :meth:`Simulation.replace_endogenous`.
 
     costs_type : `str, optional`
         Specification of the marginal cost function :math:`\tilde{c} = f(c)` in :eq:`costs`. The following
         specifications are supported:
 
             - ``'linear'`` (default) - Linear specification: :math:`\tilde{c} = c`.
 
             - ``'log'`` - Log-linear specification: :math:`\tilde{c} = \log c`.
 
     seed : `int, optional`
-        Passed to :class:`numpy.random.mtrand.RandomState` to seed the random number generator before data are
-        simulated. By default, a seed is not passed to the random number generator.
+        Passed to :class:`numpy.random.RandomState` to seed the random number generator before data are simulated. By
+        default, a seed is not passed to the random number generator.
 
     Attributes
     ----------
     product_formulations : `tuple`
         :class:`Formulation` configurations for :math:`X_1`, :math:`X_2`, and :math:`X_3`, respectively.
     agent_formulation : `tuple`
         :class:`Formulation` configuration for :math:`d`.
@@ -247,30 +285,36 @@
         usable data type.
     unique_market_ids : `ndarray`
         Unique market IDs in product and agent data.
     unique_firm_ids : `ndarray`
         Unique firm IDs in product data.
     unique_nesting_ids : `ndarray`
         Unique nesting IDs in product data.
+    unique_product_ids : `ndarray`
+        Unique product IDs in product data.
+    unique_agent_ids : `ndarray`
+        Unique agent IDs in agent data.
     beta : `ndarray`
         Demand-side linear parameters, :math:`\beta`.
     sigma : `ndarray`
         Cholesky root of the covariance matrix for unobserved taste heterogeneity, :math:`\Sigma`.
     gamma : `ndarray`
         Supply-side linear parameters, :math:`\gamma`.
     pi : `ndarray`
         Parameters that measures how agent tastes vary with demographics, :math:`\Pi`.
     rho : `ndarray`
         Parameters that measure within nesting group correlation, :math:`\rho`.
     xi : `ndarray`
         Unobserved demand-side product characteristics, :math:`\xi`.
     omega : `ndarray`
         Unobserved supply-side product characteristics, :math:`\omega`.
-    distributions : `list of str`
-        Random coefficient distributions.
+    rc_types : `list of str`
+        Random coefficient types.
+    epsilon_scale : `float`
+        Factor by which the Type I Extreme Value idiosyncratic preference term, :math:`\epsilon_{ijt}`, is scaled.
     costs_type : `str`
         Functional form of the marginal cost function :math:`\tilde{c} = f(c)`.
     T : `int`
         Number of markets, :math:`T`.
     N : `int`
         Number of products across all markets, :math:`N`.
     F : `int`
@@ -320,16 +364,16 @@
 
     def __init__(
             self, product_formulations: Union[Formulation, Sequence[Optional[Formulation]]], product_data: Mapping,
             beta: Any, sigma: Optional[Any] = None, pi: Optional[Any] = None, gamma: Optional[Any] = None,
             rho: Optional[Any] = None, agent_formulation: Optional[Formulation] = None,
             agent_data: Optional[Mapping] = None, integration: Optional[Integration] = None, xi: Optional[Any] = None,
             omega: Optional[Any] = None, xi_variance: float = 1, omega_variance: float = 1, correlation: float = 0.9,
-            distributions: Optional[Sequence[str]] = None, costs_type: str = 'linear', seed: Optional[int] = None) -> (
-            None):
+            rc_types: Optional[Sequence[str]] = None, epsilon_scale: float = 1.0, costs_type: str = 'linear',
+            seed: Optional[int] = None) -> None:
         """Load or simulate all data except for synthetic prices and shares."""
 
         # keep track of long it takes to initialize the simulation
         output("Initializing the simulation ...")
         start_time = time.time()
 
         # validate and normalize product formulations
@@ -350,14 +394,15 @@
             if agent_formulation._absorbed_terms:
                 raise ValueError("agent_formulation does not support fixed effect absorption.")
 
         # load IDs and ownership matrices
         market_ids = extract_matrix(product_data, 'market_ids')
         firm_ids = extract_matrix(product_data, 'firm_ids')
         nesting_ids = extract_matrix(product_data, 'nesting_ids')
+        product_ids = extract_matrix(product_data, 'product_ids')
         clustering_ids = extract_matrix(product_data, 'clustering_ids')
         ownership = extract_matrix(product_data, 'ownership')
         if market_ids is None:
             raise KeyError("product_data must have a market_ids field.")
         if firm_ids is None:
             raise KeyError("product_data must have a firm_ids field.")
         if market_ids.shape[1] > 1:
@@ -375,76 +420,83 @@
         if shares is None:
             shares = state.uniform(size=market_ids.size) / market_groups.expand(market_groups.counts)
         if prices is None:
             prices = state.uniform(size=market_ids.size)
 
         # load or simulate product variables in sorted order so that a seed always gives the same draws
         product_mapping = {
-            'market_ids': (market_ids, np.object),
-            'firm_ids': (firm_ids, np.object),
-            'nesting_ids': (nesting_ids, np.object),
-            'clustering_ids': (clustering_ids, np.object),
+            'market_ids': (market_ids, np.object_),
+            'firm_ids': (firm_ids, np.object_),
+            'nesting_ids': (nesting_ids, np.object_),
+            'product_ids': (product_ids, np.object_),
+            'clustering_ids': (clustering_ids, np.object_),
             'ownership': (ownership, options.dtype),
             'shares': (shares, options.dtype),
             'prices': (prices, options.dtype),
         }
         for formulation in product_formulations:
             if formulation is None:
                 continue
             for name in sorted(formulation._names - set(product_mapping)):
                 variable = extract_matrix(product_data, name)
                 if variable is None:
                     variable = state.uniform(size=market_ids.size)
                 elif variable.shape[1] > 1:
                     raise ValueError(f"The {name} variable has a field in product_data with more than one column.")
-                variable_dtype = options.dtype if np.issubdtype(variable.dtype, np.number) else np.object
+                variable_dtype = options.dtype if np.issubdtype(variable.dtype, np.number) else np.object_
                 product_mapping[name] = (variable, variable_dtype)
 
         # structure product data
         self.product_data = structure_matrices(product_mapping)
         products = Products(product_formulations, self.product_data, instruments=False)
 
         # load or build agent data
         agent_mapping = None
         if products.X2.shape[1] > 0:
             # determine the number of agents by loading market IDs or by building them along with nodes and weights
             if integration is not None:
                 if not isinstance(integration, Integration):
                     raise ValueError("integration must be None or an Integration instance.")
                 agent_market_ids, nodes, weights = integration._build_many(products.X2.shape[1], np.unique(market_ids))
+                agent_ids = None
             elif agent_data is not None:
                 agent_market_ids = extract_matrix(agent_data, 'market_ids')
+                agent_ids = extract_matrix(agent_data, 'agent_ids')
                 nodes = extract_matrix(agent_data, 'nodes')
                 weights = extract_matrix(agent_data, 'weights')
             else:
                 raise ValueError("At least one of agent_data or integration must be specified.")
 
             # load or simulate agent variables in sorted order so that a seed always gives the same draws
             agent_mapping = {
-                'market_ids': (agent_market_ids, np.object),
+                'market_ids': (agent_market_ids, np.object_),
+                'agent_ids': (agent_ids, np.object_),
                 'nodes': (nodes, options.dtype),
                 'weights': (weights, options.dtype)
             }
             if agent_formulation is not None:
                 for name in sorted(agent_formulation._names - set(agent_mapping)):
-                    variable = extract_matrix(agent_data, name) if agent_data is not None else None
-                    if variable is None:
-                        variable = state.uniform(size=agent_market_ids.size)
-                    elif variable.shape[1] > 1:
-                        raise ValueError(f"The {name} variable has a field in agent_data with more than one column.")
-                    variable_dtype = options.dtype if np.issubdtype(variable.dtype, np.number) else np.object
-                    agent_mapping[name] = (variable, variable_dtype)
+                    matrix = extract_matrix(agent_data, name) if agent_data is not None else None
+                    if matrix is None:
+                        agent_mapping[name] = (state.uniform(size=agent_market_ids.size), options.dtype)
+                    else:
+                        variable_dtype = options.dtype if np.issubdtype(matrix.dtype, np.number) else np.object_
+                        if matrix.shape[1] == 1:
+                            agent_mapping[name] = (matrix, variable_dtype)
+                        else:
+                            for index, variable in enumerate(matrix.T):
+                                agent_mapping[f'{name}{index}'] = (variable, variable_dtype)
 
         # structure agent data
         self.integration = integration
         self.agent_data = structure_matrices(agent_mapping) if agent_mapping is not None else None
         agents = Agents(products, agent_formulation, self.agent_data)
 
         # initialize the underlying economy
-        super().__init__(product_formulations, agent_formulation, products, agents, distributions, costs_type)
+        super().__init__(product_formulations, agent_formulation, products, agents, rc_types, epsilon_scale, costs_type)
 
         # load or simulate the structural errors
         self.xi = xi
         self.omega = omega
         if self.xi is not None:
             self.xi = np.c_[np.asarray(self.xi, options.dtype)]
             if self.xi.shape != (self.N, 1):
@@ -452,15 +504,15 @@
         if self.omega is not None:
             self.omega = np.c_[np.asarray(self.omega, options.dtype)]
             if self.omega.shape != (self.N, 1):
                 raise ValueError(f"omega must be a vector with {self.N} elements.")
         if self.xi is None and self.omega is None and self.K3 > 0:
             covariance = correlation * np.sqrt(xi_variance * omega_variance)
             covariances = np.array([[xi_variance, covariance], [covariance, omega_variance]], options.dtype)
-            self._detect_psd(covariances, "the covariance matrix from xi_variance, omega_variance, and correlation")
+            self._require_psd(covariances, "the covariance matrix from xi_variance, omega_variance, and correlation")
             xi_and_omega = state.multivariate_normal([0, 0], covariances, self.N, check_valid='ignore')
             self.xi = xi_and_omega[:, [0]].astype(options.dtype)
             self.omega = xi_and_omega[:, [1]].astype(options.dtype)
         if self.xi is None:
             raise ValueError("xi must be specified if X3 is not formulated or omega is specified.")
         if self.omega is None and self.K3 > 0:
             raise ValueError("omega must be specified if X3 is formulated and xi is specified.")
@@ -480,47 +532,65 @@
 
     def __str__(self) -> str:
         """Supplement general formatted information with other information about parameters."""
         return "\n\n".join([super().__str__(), self._parameters.format("True Values")])
 
     def replace_endogenous(
             self, costs: Optional[Any] = None, prices: Optional[Any] = None, iteration: Optional[Iteration] = None,
+            constant_costs: bool = True, compute_gradients: bool = True, compute_hessians: bool = True,
             error_behavior: str = 'raise') -> SimulationResults:
-        r"""Replace simulated prices and marketshares with equilibrium values that are consistent with true parameters.
+        r"""Replace simulated prices and market shares with equilibrium values that are consistent with true parameters.
 
-        This method is the standard way of solving the simulation. Prices and marketshares are computed in each market
+        This method is the standard way of solving the simulation. Prices and market shares are computed in each market
         by iterating over the :math:`\zeta`-markup contraction in :eq:`zeta_contraction`:
 
         .. math:: p \leftarrow c + \zeta(p).
 
         .. note::
 
-           To create a simulation under perfect (instead of Bertrand) competition, use an :class:`Iteration`
-           configuration with ``method='return'``. This will set prices equal to the default starting values for the
-           iteration routine, which are marginal costs.
+           To not replace prices, pass the desired prices to ``prices`` and use an :class:`Iteration` configuration with
+           ``method='return'``. This just uses the iteration "routine" that simply returns the the starting values,
+           which are ``prices``.
+
+           Using this same fake iteration routine and not setting prices will result in a simulation under perfect
+           (instead of Bertrand) competition because the default starting values for the iteration routine are marginal
+           costs.
 
         .. note::
 
            This method supports :func:`parallel` processing. If multiprocessing is used, market-by-market computation of
            prices and shares will be distributed among the processes.
 
         Parameters
         ----------
         costs : `array-like, optional`
             Marginal costs, :math:`c`. By default, :math:`c = X_3\gamma + \omega` if ``costs_type`` was ``'linear'`` in
             :class:`Simulation` (the default), and the exponential of this if it was ``'log'``. Marginal costs must be
             specified if :math:`X_3` was not formulated in :class:`Simulation`. If marginal costs depend on prices
-            through marketshares, they will be updated to reflect different prices during each iteration of the routine.
+            through market shares, they will be updated to reflect different prices during each iteration of the
+            routine.
         prices : `array-like, optional`
             Prices at which the fixed point iteration routine will start. By default, ``costs``, are used as starting
             values.
         iteration : `Iteration, optional`
             :class:`Iteration` configuration for how to solve the fixed point problem. By default,
-            ``Iteration('simple', {'atol': 1e-12})`` is used. Analytic Jacobians are not supported for solving this
-            system.
+            ``Iteration('simple', {'atol': 1e-12})`` is used.
+        constant_costs : `bool, optional`
+            Whether to assume that marginal costs, :math:`c`, remain constant as equilibrium prices and shares change.
+            By default this is ``True``, which means that firms treat marginal costs as constant (equal to ``costs``)
+            when setting prices. If set to ``False``, marginal costs will be allowed to adjust if ``shares`` was
+            included in the formulation for :math:`X_3`. When simulating fake data, it likely makes more sense to set
+            this to ``False`` since otherwise arbitrary ``shares`` simulated by :class:`Simulation` will be used in
+            marginal costs.
+        compute_gradients : `bool, optional`
+            Whether to compute profit gradients to verify first order conditions. This is by default ``True``. Setting
+            it to ``False`` will slightly speed up computation, but first order conditions will not be reported.
+        compute_hessians : `bool, optional`
+            Whether to compute profit Hessians to verify second order conditions. This is by default ``True``. Setting
+            it to ``False`` will slightly speed up computation, but second order conditions will not be reported.
         error_behavior : `str, optional`
             How to handle errors when computing prices and shares. For example, the fixed point routine may not converge
             if the effects of nonlinear parameters on price overwhelm the linear parameter on price, which should be
             sufficiently negative. The following behaviors are supported:
 
                 - ``'raise'`` (default) - Raise an exception.
 
@@ -567,61 +637,92 @@
         # validate other settings
         iteration = self._coerce_optional_prices_iteration(iteration)
         self._validate_error_behavior(error_behavior)
 
         # compute a baseline delta that will be updated when shares and prices are replaced
         delta = self.products.X1 @ self.beta + self.xi
 
-        def market_factory(s: Hashable) -> Tuple[SimulationMarket, Array, Array, Iteration]:
+        def market_factory(s: Hashable) -> Tuple[SimulationMarket, Array, Array, Iteration, bool, bool, bool]:
             """Build a market along with arguments used to compute prices and shares."""
             assert costs is not None and prices is not None and iteration is not None
             market_s = SimulationMarket(
                 self, s, self._parameters, self.sigma, self.pi, self.rho, self.beta, self.gamma, delta
             )
             costs_s = costs[self._product_market_indices[s]]
             prices_s = prices[self._product_market_indices[s]]
-            return market_s, costs_s, prices_s, iteration
+            return market_s, costs_s, prices_s, iteration, constant_costs, compute_gradients, compute_hessians
 
-        # compute prices and marketshares market-by-market, also collecting potentially updated delta and costs
+        # compute prices and market shares market-by-market, also collecting potentially updated delta and costs
         data_override = {
             'prices': np.zeros_like(self.products.prices),
             'shares': np.zeros_like(self.products.shares)
         }
         true_delta = np.zeros_like(delta)
         true_costs = np.zeros_like(costs)
         iteration_stats: Dict[Hashable, SolverStats] = {}
+        profit_gradients: Optional[Dict[Hashable, Dict[Hashable, Array]]] = {} if compute_gradients else None
+        profit_gradient_norms: Optional[Dict[Hashable, Dict[Hashable, Array]]] = {} if compute_gradients else None
+        profit_hessians: Optional[Dict[Hashable, Dict[Hashable, Array]]] = {} if compute_hessians else None
+        profit_hessian_eigenvalues: Optional[Dict[Hashable, Dict[Hashable, Array]]] = {} if compute_hessians else None
         generator = generate_items(self.unique_market_ids, market_factory, SimulationMarket.compute_endogenous)
         generator = output_progress(generator, self.T, start_time)
-        for t, (prices_t, shares_t, delta_t, costs_t, iteration_stats_t, errors_t) in generator:
+        for t, (prices_t, shares_t, delta_t, costs_t, stats_t, gradients_t, hessians_t, errors_t) in generator:
             data_override['prices'][self._product_market_indices[t]] = prices_t
             data_override['shares'][self._product_market_indices[t]] = shares_t
             true_delta[self._product_market_indices[t]] = delta_t
             true_costs[self._product_market_indices[t]] = costs_t
-            iteration_stats[t] = iteration_stats_t
+            iteration_stats[t] = stats_t
             errors.extend(errors_t)
 
+            # compute profit gradient norms to check first order conditions
+            if compute_gradients:
+                assert gradients_t is not None
+                assert profit_gradients is not None and profit_gradient_norms is not None
+                profit_gradients[t] = gradients_t
+                profit_gradient_norms[t] = {}
+                for f, profit_gradient in gradients_t.items():
+                    profit_gradient_norms[t][f] = np.nan
+                    if profit_gradient.size > 0:
+                        with np.errstate(invalid='ignore'):
+                            profit_gradient_norms[t][f] = np.abs(profit_gradient).max()
+
+            # compute profit Hessian eigenvalues to check second order conditions
+            if compute_hessians:
+                assert hessians_t is not None
+                assert profit_hessians is not None and profit_hessian_eigenvalues is not None
+                profit_hessians[t] = hessians_t
+                profit_hessian_eigenvalues[t] = {}
+                for f, profit_hessian in hessians_t.items():
+                    profit_hessian_eigenvalues[t][f], successful = precisely_compute_eigenvalues(profit_hessian)
+                    if not successful:
+                        errors.append(exceptions.ProfitHessianEigenvaluesError(profit_hessian))
+
         # structure the results
         self._handle_errors(errors, error_behavior)
         results = SimulationResults(
-            self, data_override, true_delta, true_costs, start_time, time.time(), iteration_stats
+            self, data_override, true_delta, true_costs, start_time, time.time(), iteration_stats, profit_gradients,
+            profit_gradient_norms, profit_hessians, profit_hessian_eigenvalues
         )
         output(f"Replaced prices and shares after {format_seconds(results.computation_time)}.")
         output("")
         output(results)
         return results
 
     def replace_exogenous(
             self, X1_name: str, X3_name: Optional[str] = None, delta: Optional[Any] = None,
-            iteration: Optional[Iteration] = None, fp_type: str = 'safe_linear', error_behavior: str = 'raise') -> (
+            iteration: Optional[Iteration] = None, fp_type: str = 'safe_linear',
+            shares_bounds: Optional[Tuple[Any, Any]] = (1e-300, None), error_behavior: str = 'raise') -> (
             SimulationResults):
         r"""Replace exogenous product characteristics with values that are consistent with true parameters.
 
         This method implements a less common way of solving the simulation. It may be preferable to
-        :meth:`Simulation.replace_endogenous` when for some reason it is desirable to retain the prices and marketshares
-        from :class:`Simulation`, which are assumed to be in equilibrium.
+        :meth:`Simulation.replace_endogenous` when for some reason it is desirable to retain the prices and market
+        shares from :class:`Simulation`, which are assumed to be in equilibrium. For example, it can be helpful when
+        approximating the pure characteristics model of :ref:`references:Berry and Pakes (2007)` by setting a small
+        ``epsilon_scale`` value in :class:`Simulation`.
 
         For this method of solving the simulation to be used, there must be an exogenous product characteristic
         :math:`v` that shows up only in :math:`X_1^\text{ex}`, and if there is a supply side, another product
         characteristic :math:`w` that shows up only in :math:`X_3^\text{ex}`. These characteristics will be replaced
         with values that are consistent with true parameters.
 
         First, the mean utility :math:`\delta` is computed in each market by iterating over the contraction in
@@ -657,14 +758,19 @@
             :math:`\delta` in each market. This configuration is only relevant if there are nonlinear parameters, since
             :math:`\delta` can be estimated analytically in the logit model. By default,
             ``Iteration('squarem', {'atol': 1e-14})`` is used. For more information, refer to the same argument in
             :meth:`Problem.solve`.
         fp_type : `str, optional`
             Configuration for the type of contraction mapping used to compute :math:`\delta`. For information about
             the different types, refer to the same argument in :meth:`Problem.solve`.
+        shares_bounds : `tuple, optional`
+            Configuration for :math:`s_{jt}(\delta, \theta)` bounds of the form ``(lb, ub)``, in which both ``lb`` and
+            ``ub`` are floats or ``None``. By default, simulated shares are bounded from below by ``1e-300``. This is
+            only relevant if ``fp_type`` is ``'safe_linear'`` or ``'linear'``. Bounding shares in the contraction does
+            nothing with a nonlinear fixed point. For more information, refer to :meth:`Problem.solve`.
         error_behavior : `str, optional`
             How to handle errors when computing :math:`\delta` and :math:`\tilde{c}`. The following behaviors are
             supported:
 
                 - ``'raise'`` (default) - Raise an exception.
 
                 - ``'warn'`` - Use the last computed :math:`\delta` and :math:`\tilde{c}`. If the fixed point routine
@@ -724,51 +830,54 @@
         else:
             delta = np.c_[np.asarray(delta, options.dtype)]
             if delta.shape != (self.N, 1):
                 raise ValueError(f"delta must None or a {self.N}-vector.")
 
         # validate other settings
         iteration = self._coerce_optional_delta_iteration(iteration)
+        shares_bounds = self._coerce_optional_bounds(shares_bounds, 'shares_bounds')
         self._validate_fp_type(fp_type)
         self._validate_error_behavior(error_behavior)
 
-        def market_factory(s: Hashable) -> Tuple[SimulationMarket, Array, Iteration, str]:
+        def market_factory(s: Hashable) -> Tuple[SimulationMarket, Array, Iteration, str, Bounds]:
             """Build a market along with arguments used to compute delta and marginal costs."""
-            assert delta is not None and iteration is not None
+            assert delta is not None and iteration is not None and shares_bounds is not None
             market_s = SimulationMarket(self, s, self._parameters, self.sigma, self.pi, self.rho, self.beta)
             delta_s = delta[self._product_market_indices[s]]
-            return market_s, delta_s, iteration, fp_type
+            return market_s, delta_s, iteration, fp_type, shares_bounds
 
         # compute delta and marginal costs market-by-market
         true_delta = np.zeros_like(self.xi)
-        true_tilde_costs = np.zeros_like(self.omega)
+        true_tilde_costs = None if self.omega is None else np.zeros_like(self.omega)
         iteration_stats: Dict[Hashable, SolverStats] = {}
         generator = generate_items(self.unique_market_ids, market_factory, SimulationMarket.compute_exogenous)
         for t, (delta_t, tilde_costs_t, iteration_stats_t, errors_t) in output_progress(generator, self.T, start_time):
             true_delta[self._product_market_indices[t]] = delta_t
-            true_tilde_costs[self._product_market_indices[t]] = tilde_costs_t
+            if true_tilde_costs is not None:
+                true_tilde_costs[self._product_market_indices[t]] = tilde_costs_t
             iteration_stats[t] = iteration_stats_t
             errors.extend(errors_t)
 
         # compute the exogenous variables, ignoring any numerical errors here that carry over from market computation
         data_override: Dict[str, Array] = {}
         with np.errstate(all='ignore'):
             data_override[X1_name] = (
                 self.products.X1[:, [X1_index]] +
                 (true_delta - self.xi - self.products.X1 @ self.beta) / self.beta[X1_index]
             )
             if X3_name is not None:
+                assert true_tilde_costs is not None
                 data_override[X3_name] = (
                     self.products.X3[:, [X3_index]] +
                     (true_tilde_costs - self.omega - self.products.X3 @ self.gamma) / self.gamma[X3_index]
                 )
 
         # compute non-transformed marginal costs
         true_costs = true_tilde_costs
-        if self.costs_type == 'log':
+        if self.costs_type == 'log' and true_costs is not None:
             true_costs = np.exp(true_costs)
 
         # structure the results
         self._handle_errors(errors, error_behavior)
         results = SimulationResults(
             self, data_override, true_delta, true_costs, start_time, time.time(), iteration_stats
         )
```

### Comparing `pyblp-0.9.0/pyblp/exceptions.py` & `pyblp-1.0.0/pyblp/exceptions.py`

 * *Files 2% similar despite different names*

```diff
@@ -50,20 +50,25 @@
     """Failed to compute standard errors because of invalid estimated covariances of GMM parameters."""
 
 
 class InvalidMomentCovariancesError(Error):
     """Failed to compute a weighting matrix because of invalid estimated covariances of GMM moments."""
 
 
+class GenericNumericalError(NumericalError):
+    """Encountered a numerical error."""
+
+
 class DeltaNumericalError(NumericalError):
     r"""Encountered a numerical error when computing :math:`\delta`.
 
     This problem is often due to prior problems, overflow, or nonpositive shares, and can sometimes be mitigated by
-    choosing smaller initial parameter values, setting more conservative bounds, rescaling data, removing outliers,
-    changing the floating point precision, or using different optimization, iteration, or integration configurations.
+    choosing smaller initial parameter values, setting more conservative bounds on parameters or shares, rescaling data,
+    removing outliers, changing the floating point precision, or using different optimization, iteration, or integration
+    configurations.
 
     """
 
 
 class CostsNumericalError(NumericalError):
     """Encountered a numerical error when computing marginal costs.
 
@@ -81,27 +86,27 @@
     choosing smaller initial parameter values, setting more conservative bounds, rescaling data, removing outliers,
     changing the floating point precision, or using different optimization, iteration, or integration configurations.
 
     """
 
 
 class XiByThetaJacobianNumericalError(NumericalError):
-    r"""Encountered a numerical error when computing the Jacobian of :math:`\xi` (equivalently, of :math:`\delta`)
-    with respect to :math:`\theta`.
+    r"""Encountered a numerical error when computing the Jacobian (holding :math:`\beta` fixed) of :math:`\xi`
+    (equivalently, of :math:`\delta`) with respect to :math:`\theta`.
 
     This problem is often due to prior problems, overflow, or nonpositive shares, and can sometimes be mitigated by
     choosing smaller initial parameter values, setting more conservative bounds, rescaling data, removing outliers,
     changing the floating point precision, or using different optimization, iteration, or integration configurations.
 
     """
 
 
 class OmegaByThetaJacobianNumericalError(NumericalError):
-    r"""Encountered a numerical error when computing the Jacobian of :math:`\omega` (equivalently, of transformed
-    marginal costs) with respect to :math:`\theta`.
+    r"""Encountered a numerical error when computing the Jacobian (holding :math:`\gamma` fixed) of :math:`\omega`
+    (equivalently, of transformed marginal costs) with respect to :math:`\theta`.
 
     This problem is often due to prior problems or overflow, and can sometimes be mitigated by choosing smaller initial
     parameter values, setting more conservative bounds, rescaling data, removing outliers, changing the floating point
     precision, or using different optimization or cost configurations.
 
     """
 
@@ -148,32 +153,34 @@
 
     This problem is often due to prior problems or overflow and can sometimes be mitigated by making sure that the
     specified parameters are reasonable.
 
     """
 
 
+class SyntheticMicroDataNumericalError(NumericalError):
+    """Encountered a numerical error when computing synthetic micro data."""
+
+
 class SyntheticMicroMomentsNumericalError(NumericalError):
     """Encountered a numerical error when computing synthetic micro moments."""
 
 
-class EquilibriumRealizationNumericalError(NumericalError):
-    """Encountered a numerical error when solving for a realization of equilibrium prices and shares."""
+class MicroScoresNumericalError(NumericalError):
+    """Encountered a numerical error when computing micro scores."""
 
 
-class XiByThetaJacobianRealizationNumericalError(NumericalError):
-    r"""Encountered a numerical error when computing a realization of the Jacobian of :math:`\xi` (equivalently, of
-    :math:`\delta`) with respect to :math:`\theta`.
-
-    """
+class EquilibriumRealizationNumericalError(NumericalError):
+    """Encountered a numerical error when solving for a realization of equilibrium prices and shares."""
 
 
-class OmegaByThetaJacobianRealizationNumericalError(NumericalError):
-    r"""Encountered a numerical error when computing a realization of the Jacobian of :math:`\omega` (equivalently, of
-    transformed marginal costs) with respect to :math:`\theta`.
+class JacobianRealizationNumericalError(NumericalError):
+    r"""Encountered a numerical error when computing a realization of the Jacobian (holding :math:`\beta` fixed) of
+    :math:`\xi` (equivalently, of :math:`\delta`) or :math:`\omega` (equivalently, of transformed marginal costs)
+    with respect to :math:`\theta`.
 
     """
 
 
 class PostEstimationNumericalError(NumericalError):
     """Encountered a numerical error when computing a post-estimation output."""
 
@@ -183,29 +190,33 @@
 
     Consider configuring absorption options or choosing a different absorption method. For information about absorption
     options and defaults, refer to the PyHDFE package's documentation.
 
     """
 
 
+class ClippedSharesError(Error):
+    r"""Shares were clipped during the final iteration of the fixed point routine for computing :math:`\delta`."""
+
+
 class ThetaConvergenceError(Error):
     """The optimization routine failed to converge.
 
     This problem can sometimes be mitigated by choosing more reasonable initial parameter values, setting more
     conservative bounds, or configuring other optimization settings.
 
     """
 
 
 class DeltaConvergenceError(Error):
     r"""The fixed point computation of :math:`\delta` failed to converge.
 
     This problem can sometimes be mitigated by increasing the maximum number of fixed point iterations, increasing the
-    fixed point tolerance, choosing more reasonable initial parameter values, setting more conservative bounds, or using
-    different iteration or optimization configurations.
+    fixed point tolerance, choosing more reasonable initial parameter values, setting more conservative parameter or
+    share bounds, or using different iteration or optimization configurations.
 
     """
 
 
 class SyntheticPricesConvergenceError(Error):
     """The fixed point computation of synthetic prices failed to converge.
 
@@ -251,52 +262,60 @@
 
 
 class MicroMomentsReversionError(MultipleReversionError):
     """Reverted problematic micro moments."""
 
 
 class XiByThetaJacobianReversionError(MultipleReversionError):
-    r"""Reverted problematic elements in the Jacobian of :math:`\xi` (equivalently, of :math:`\delta`) with respect to
-    :math:`\theta`.
+    r"""Reverted problematic elements in the Jacobian (holding :math:`\beta` fixed) of :math:`\xi` (equivalently, of
+    :math:`\delta`) with respect to :math:`\theta`.
 
     """
 
 
 class OmegaByThetaJacobianReversionError(MultipleReversionError):
-    r"""Reverted problematic elements in the Jacobian of :math:`\omega` (equivalently, of transformed marginal costs)
-    with respect to :math:`\theta`.
+    r"""Reverted problematic elements in the Jacobian (holding :math:`\gamma` fixed) of :math:`\omega` (equivalently, of
+    transformed marginal costs) with respect to :math:`\theta`.
 
     """
 
 
 class MicroMomentsByThetaJacobianReversionError(MultipleReversionError):
     r"""Reverted problematic elements in the Jacobian of micro moments with respect to :math:`\theta`."""
 
 
 class HessianEigenvaluesError(InversionError):
     """Failed to compute eigenvalues for the GMM objective's (reduced) Hessian matrix."""
 
 
+class ProfitHessianEigenvaluesError(InversionError):
+    """Failed to compute eigenvalues for a firm's profit Hessian."""
+
+
 class FittedValuesInversionError(InversionReplacementError):
     """Failed to invert an estimated covariance when computing fitted values.
 
     There are probably collinearity issues.
 
     """
 
 
 class SharesByXiJacobianInversionError(InversionReplacementError):
-    r"""Failed to invert a Jacobian of shares with respect to :math:`\xi` when computing the Jacobian of :math:`\xi`
-    (equivalently, of :math:`\delta`) with respect to :math:`\theta`.
+    r"""Failed to invert a Jacobian of shares with respect to :math:`\xi` when computing the Jacobian (holding
+    :math:`\beta` fixed) of :math:`\xi` (equivalently, of :math:`\delta`) with respect to :math:`\theta`.
 
     """
 
 
 class IntraFirmJacobianInversionError(InversionReplacementError):
-    r"""Failed to invert an intra-firm Jacobian of shares with respect to prices when computing :math:`\eta`."""
+    r"""Failed to invert an intra-firm Jacobian of shares with respect to prices."""
+
+
+class PassthroughInversionError(InversionReplacementError):
+    r"""Failed to invert the matrix to recover the passthrough matrix."""
 
 
 class LinearParameterCovariancesInversionError(InversionReplacementError):
     """Failed to invert an estimated covariance matrix of linear parameters.
 
     One or more data matrices may be highly collinear.
```

### Comparing `pyblp-0.9.0/pyblp/markets/simulation_market.py` & `pyblp-1.0.0/pyblp/markets/simulation_market.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,81 +1,118 @@
 """Market-level simulation of synthetic BLP data."""
 
-from typing import List, Tuple
+from typing import Dict, Hashable, List, Optional, Tuple
 
 import numpy as np
 
 from .market import Market
 from .. import exceptions, options
 from ..configurations.iteration import Iteration
-from ..utilities.basics import Array, Error, SolverStats, NumericalErrorHandler
+from ..utilities.basics import Array, Bounds, Error, SolverStats, NumericalErrorHandler
 
 
 class SimulationMarket(Market):
     """A market in a simulation of synthetic BLP data."""
 
     def compute_endogenous(
-            self, costs: Array, prices: Array, iteration: Iteration) -> (
-            Tuple[Array, Array, Array, Array, SolverStats, List[Error]]):
-        """Compute endogenous prices and shares, along with the associated delta and costs."""
+            self, costs: Array, prices: Array, iteration: Iteration, constant_costs: bool, compute_gradients: bool,
+            compute_hessians: bool) -> (
+            Tuple[
+                Array, Array, Array, Array, SolverStats, Optional[Dict[Hashable, Array]],
+                Optional[Dict[Hashable, Array]], List[Error]
+            ]):
+        """Compute endogenous prices and shares, along with the associated delta and costs. Optionally compute firms'
+        profit gradients and Hessians.
+        """
         errors: List[Error] = []
-        prices, stats, price_errors = self.safely_compute_equilibrium_prices(costs, iteration, prices)
+        prices, stats, price_errors = self.safely_compute_equilibrium_prices(costs, iteration, constant_costs, prices)
         shares, share_errors = self.safely_compute_shares(prices)
+        errors.extend(price_errors + share_errors)
+
+        # update mean utilities and marginal costs
         with np.errstate(all='ignore'):
             delta = self.update_delta_with_variable('prices', prices)
-            costs = self.update_costs_with_variable(costs, 'shares', shares)
-        errors.extend(price_errors + share_errors)
-        return prices, shares, delta, costs, stats, errors
+            if not constant_costs:
+                costs = self.update_costs_with_variable(costs, 'shares', shares)
+
+            # optionally compute profit gradients
+            profit_gradients = None
+            if compute_gradients:
+                profit_gradients = {}
+                ownership = self.get_ownership_matrix()
+                jacobian = self.compute_profit_jacobian(costs, prices)
+                firm_profit_gradient = (ownership * jacobian).sum(axis=0)
+                for firm_id in np.unique(self.products.firm_ids.flatten()):
+                    firm_index = self.products.firm_ids.flat == firm_id
+                    profit_gradients[firm_id] = firm_profit_gradient[firm_index]
+
+            # optionally compute profit Hessians
+            profit_hessians = None
+            if compute_hessians:
+                profit_hessians = {}
+                ownership = self.get_ownership_matrix()
+                hessian = self.compute_profit_hessian(costs, prices)
+                firm_profit_hessian = (ownership[..., None] * hessian).sum(axis=0)
+                for firm_id in np.unique(self.products.firm_ids.flatten()):
+                    firm_index = self.products.firm_ids.flat == firm_id
+                    profit_hessians[firm_id] = firm_profit_hessian[firm_index][:, firm_index]
+
+        return prices, shares, delta, costs, stats, profit_gradients, profit_hessians, errors
 
     def compute_exogenous(
-            self, initial_delta: Array, iteration: Iteration, fp_type: str) -> (
+            self, initial_delta: Array, iteration: Iteration, fp_type: str, shares_bounds: Bounds) -> (
             Tuple[Array, Array, SolverStats, List[Error]]):
         """Compute delta and transformed marginal costs, which map to the exogenous product characteristics."""
         errors: List[Error] = []
-        delta, stats, delta_errors = self.safely_compute_delta(initial_delta, iteration, fp_type)
+        delta, stats, delta_errors = self.safely_compute_delta(initial_delta, iteration, fp_type, shares_bounds)
         errors.extend(delta_errors)
         tilde_costs = np.zeros((self.J, 0), options.dtype)
         if self.K3 > 0:
             tilde_costs, tilde_costs_errors = self.safely_compute_tilde_costs(delta)
             errors.extend(tilde_costs_errors)
         return delta, tilde_costs, stats, errors
 
     @NumericalErrorHandler(exceptions.SyntheticPricesNumericalError)
     def safely_compute_equilibrium_prices(
-            self, costs: Array, iteration: Iteration, prices: Array) -> Tuple[Array, SolverStats, List[Error]]:
+            self, costs: Array, iteration: Iteration, constant_costs: bool, prices: Array) -> (
+            Tuple[Array, SolverStats, List[Error]]):
         """Compute equilibrium prices by iterating over the zeta-markup equation, handling any numerical errors."""
         errors: List[Error] = []
-        prices, stats = self.compute_equilibrium_prices(costs, iteration, prices)
+        prices, stats = self.compute_equilibrium_prices(costs, iteration, constant_costs, prices)
         if not stats.converged:
             errors.append(exceptions.SyntheticPricesConvergenceError())
         return prices, stats, errors
 
     @NumericalErrorHandler(exceptions.SyntheticSharesNumericalError)
     def safely_compute_shares(self, prices: Array) -> Tuple[Array, List[Error]]:
         """Compute equilibrium shares associated with prices, handling any numerical errors."""
         errors: List[Error] = []
         shares = self.compute_shares(prices)
         return shares, errors
 
     @NumericalErrorHandler(exceptions.SyntheticDeltaNumericalError)
     def safely_compute_delta(
-            self, initial_delta: Array, iteration: Iteration, fp_type: str) -> Tuple[Array, SolverStats, List[Error]]:
+            self, initial_delta: Array, iteration: Iteration, fp_type: str, shares_bounds: Bounds) -> (
+            Tuple[Array, SolverStats, List[Error]]):
         """Compute delta, handling any numerical errors."""
-        delta, stats, errors = self.compute_delta(initial_delta, iteration, fp_type)
+        delta, clipped_shares, stats, errors = self.compute_delta(initial_delta, iteration, fp_type, shares_bounds)
+        if clipped_shares.any():
+            errors.append(exceptions.ClippedSharesError())
         if not stats.converged:
             errors.append(exceptions.SyntheticDeltaConvergenceError())
         return delta, stats, errors
 
     @NumericalErrorHandler(exceptions.SyntheticCostsNumericalError)
     def safely_compute_tilde_costs(self, delta: Array) -> Tuple[Array, List[Error]]:
         """Compute transformed marginal costs, handling any numerical errors."""
         errors: List[Error] = []
 
         # compute marginal costs
-        eta, eta_errors = self.compute_eta(delta=delta)
+        probabilities, conditionals = self.compute_probabilities(delta)
+        eta, _, eta_errors = self.compute_eta(probabilities=probabilities, conditionals=conditionals)
         errors.extend(eta_errors)
         costs = self.products.prices - eta
 
         # take the log of marginal costs under a log-linear specification
         if self.costs_type == 'linear':
             tilde_costs = costs
         else:
```

### Comparing `pyblp-0.9.0/pyblp/parameters.py` & `pyblp-1.0.0/pyblp/parameters.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,22 +3,23 @@
 import abc
 from typing import Any, Type, Iterable, List, Optional, Sequence, Set, TYPE_CHECKING, Tuple, Union
 
 import numpy as np
 
 from . import options
 from .configurations.formulation import ColumnFormulation
-from .primitives import Container
-from .utilities.basics import Array, Bounds, Groups, format_number, format_se, format_table
+from .utilities.algebra import vech
+from .utilities.basics import Array, Bounds, format_number, format_se, format_table
 
 
 # only import objects that create import cycles when checking types
 if TYPE_CHECKING:
     from .economies.economy import Economy  # noqa
     from .markets.market import Market  # noqa
+    from .primitives import Container
 
 
 class Parameter(abc.ABC):
     """Information about a single parameter."""
 
     location: Sequence
     value: Optional[float]
@@ -29,36 +30,36 @@
         self.value = bounds[0][location] if bounds[0][location] == bounds[1][location] else None
 
 
 class Coefficient(Parameter):
     """Information about a single coefficient parameter in sigma, pi, beta, or gamma."""
 
     @abc.abstractmethod
-    def get_product_formulation(self, container: Container) -> ColumnFormulation:
+    def get_product_formulation(self, container: 'Container') -> ColumnFormulation:
         """Get the product formulation associated with the parameter."""
 
     @abc.abstractmethod
     def get_product_characteristic(self, market: 'Market') -> Array:
         """Get the product characteristic associated with the parameter."""
 
 
 class NonlinearCoefficient(Coefficient):
     """Information about a single nonlinear parameter in sigma or pi."""
 
-    def get_product_formulation(self, container: Container) -> ColumnFormulation:
+    def get_product_formulation(self, container: 'Container') -> ColumnFormulation:
         """Get the product formulation associated with the parameter."""
         return container._X2_formulations[self.location[0]]
 
     def get_product_characteristic(self, market: 'Market') -> Array:
         """Get the product characteristic associated with the parameter."""
         return market.products.X2[:, [self.location[0]]]
 
-    def get_distribution(self, market: 'Market') -> str:
-        """Get the random coefficient distribution associated with the parameter."""
-        return market.parameters.distributions[self.location[0]]
+    def get_rc_type(self, market: 'Market') -> str:
+        """Get the random coefficient type associated with the parameter."""
+        return market.parameters.rc_types[self.location[0]]
 
     @abc.abstractmethod
     def get_agent_characteristic(self, market: 'Market') -> Array:
         """Get the agent characteristic associated with the parameter."""
 
 
 class SigmaParameter(NonlinearCoefficient):
@@ -70,87 +71,96 @@
 
 
 class PiParameter(NonlinearCoefficient):
     """Information about a single parameter in pi."""
 
     def get_agent_characteristic(self, market: 'Market') -> Array:
         """Get the agent characteristic associated with the parameter."""
+        if len(market.agents.demographics.shape) == 3:
+            return market.agents.demographics[:, self.location[1]]
         return market.agents.demographics[:, [self.location[1]]]
 
 
 class RhoParameter(Parameter):
     """Information about a single parameter in rho."""
 
     @abc.abstractmethod
-    def get_group_associations(self, groups: Groups) -> Array:
+    def get_group_associations(self, market: 'Market') -> Array:
         """Get an indicator for which groups are associated with the parameter."""
 
 
 class AllGroupsRhoParameter(RhoParameter):
     """Information about a rho parameter for all groups."""
 
-    def get_group_associations(self, groups: Groups) -> Array:
+    def get_group_associations(self, market: 'Market') -> Array:
         """Get an indicator for all groups."""
-        return np.ones((groups.group_count, 1), options.dtype)
+        return np.ones((market.groups.group_count, 1), options.dtype)
 
 
 class OneGroupRhoParameter(RhoParameter):
     """Information about a rho parameter for a single group."""
 
-    def get_group_associations(self, groups: Groups) -> Array:
+    def get_group_associations(self, market: 'Market') -> Array:
         """Get an indicator for the group associated with the parameter."""
-        group_associations = np.zeros((groups.group_count, 1), options.dtype)
-        group_associations[self.location] = 1
+        group_associations = np.zeros((market.groups.group_count, 1), options.dtype)
+        group_associations[market.groups.unique == market.unique_nesting_ids[self.location[0]]] = 1
         return group_associations
 
 
 class LinearCoefficient(Coefficient):
     """Information about a single linear parameter in beta or gamma."""
 
+    def get_product_formulation(self, container: 'Container') -> ColumnFormulation:
+        """Get the product formulation associated with the parameter."""
+        return container._X2_formulations[self.location[0]]
+
     def get_product_characteristic(self, market: 'Market') -> Array:
         """Get the product characteristic associated with the parameter."""
         x = self.get_product_formulation(market).evaluate(market.products)
         return np.broadcast_to(x, (market.products.shape[0], 1)).astype(options.dtype)
 
 
 class BetaParameter(LinearCoefficient):
     """Information about a single linear parameter in beta."""
 
-    def get_product_formulation(self, container: Container) -> ColumnFormulation:
+    def get_product_formulation(self, container: 'Container') -> ColumnFormulation:
         """Get the product formulation associated with the parameter."""
         return container._X1_formulations[self.location[0]]
 
 
 class GammaParameter(LinearCoefficient):
     """Information about a single linear parameter in gamma."""
 
-    def get_product_formulation(self, container: Container) -> ColumnFormulation:
+    def get_product_formulation(self, container: 'Container') -> ColumnFormulation:
         """Get the product formulation associated with the parameter."""
         return container._X3_formulations[self.location[0]]
 
 
 class Parameters(object):
     """Information about sigma, pi, rho, beta, and gamma."""
 
     sigma_labels: List[str]
     pi_labels: List[str]
     rho_labels: List[str]
     beta_labels: List[str]
     gamma_labels: List[str]
-    distributions: List[str]
+    theta_labels: List[str]
+    rc_types: List[str]
     sigma: Array
+    sigma_squared: Array
     pi: Array
     rho: Array
     beta: Array
     gamma: Array
     sigma_bounds: Bounds
     pi_bounds: Bounds
     rho_bounds: Bounds
     beta_bounds: Bounds
     gamma_bounds: Bounds
+    diagonal_sigma: bool
     nonzero_sigma_index: Array
     alpha_index: Array
     endogenous_gamma_index: Array
     eliminated_beta_index: Array
     eliminated_gamma_index: Array
     eliminated_alpha_index: Array
     eliminated_endogenous_gamma_index: Array
@@ -161,52 +171,59 @@
     any_bounds: bool
 
     def __init__(
             self, economy: 'Economy', sigma: Optional[Any] = None, pi: Optional[Any] = None, rho: Optional[Any] = None,
             beta: Optional[Any] = None, gamma: Optional[Any] = None, sigma_bounds: Optional[Tuple[Any, Any]] = None,
             pi_bounds: Optional[Tuple[Any, Any]] = None, rho_bounds: Optional[Tuple[Any, Any]] = None,
             beta_bounds: Optional[Tuple[Any, Any]] = None, gamma_bounds: Optional[Tuple[Any, Any]] = None,
-            bounded: bool = False, allow_linear_nans: bool = False) -> None:
+            bounded: bool = False, allow_linear_nans: bool = False, check_alpha: bool = True) -> None:
         """Coerce parameters into usable formats before storing information about fixed (equal bounds) and unfixed
         (unequal bounds) elements of sigma, pi, rho, beta, and gamma. Also store information about eliminated
         (concentrated out) parameters in beta and gamma. If allow_linear_nans is True, allow null linear parameters in
-        order to denote those parameters that will be concentrated out.
+        order to denote those parameters that will be concentrated out. If check_alpha is True, check that alpha isn't
+        concentrated out when a supply side is included.
         """
 
         # store labels
         self.sigma_labels = [str(f) for f in economy._X2_formulations]
         self.pi_labels = [str(f) for f in economy._demographics_formulations]
         self.rho_labels = [str(i) for i in economy.unique_nesting_ids]
         self.beta_labels = [str(f) for f in economy._X1_formulations]
         self.gamma_labels = [str(f) for f in economy._X3_formulations]
 
-        # store distributions
-        self.distributions = economy.distributions
+        # store types
+        self.rc_types = economy.rc_types
 
         # validate and store parameters
         self.sigma = self.initialize_matrix("sigma", "X2 was formulated", sigma, [(economy.K2, economy.K2)])
         self.pi = self.initialize_matrix("pi", "demographics were formulated", pi, [(economy.K2, economy.D)])
         self.rho = self.initialize_matrix("rho", "nesting IDs were specified", rho, [(economy.H, 1), (1, 1)])
         self.beta = self.initialize_matrix("beta", "X1 was formulated", beta, [(economy.K1, 1)], allow_linear_nans)
         self.gamma = self.initialize_matrix("gamma", "X3 was formulated", gamma, [(economy.K3, 1)], allow_linear_nans)
 
         # fill the upper triangle of sigma with zeros
         self.sigma[np.triu_indices(economy.K2, 1)] = 0
 
+        # construct sigma squared (the underlying covariance matrix)
+        self.sigma_squared = self.sigma @ self.sigma.T
+
+        # identify whether sigma is a diagonal matrix
+        self.diagonal_sigma = not (np.tril(self.sigma, k=-1) != 0).any()
+
         # identify the index of nonzero columns in sigma
-        self.nonzero_sigma_index = np.sum(self.sigma, axis=0) > 0
+        self.nonzero_sigma_index = np.any(self.sigma != 0, axis=0)
 
         # identify the index of alpha in beta
-        self.alpha_index = np.zeros_like(self.beta, np.bool)
+        self.alpha_index = np.zeros_like(self.beta, np.bool_)
         for k, formulation in enumerate(economy._X1_formulations):
             if 'prices' in formulation.names:
                 self.alpha_index[k] = True
 
         # identify the index of analogous parameters in gamma
-        self.endogenous_gamma_index = np.zeros_like(self.gamma, np.bool)
+        self.endogenous_gamma_index = np.zeros_like(self.gamma, np.bool_)
         for k, formulation in enumerate(economy._X3_formulations):
             if 'shares' in formulation.names:
                 self.endogenous_gamma_index[k] = True
 
         # identify eliminated indexes
         self.eliminated_beta_index = np.isnan(self.beta)
         self.eliminated_gamma_index = np.isnan(self.gamma)
@@ -217,15 +234,15 @@
         if economy.agents.nodes.shape[1] < self.nonzero_sigma_index.sum():
             raise ValueError(
                 f"The number of columns of integration nodes, {economy.agents.nodes.shape[1]}, is smaller than the "
                 f"number of columns in sigma with at least one nonzero parameter, {self.nonzero_sigma_index.sum()}."
             )
 
         # alpha cannot be concentrated out if there's a supply side
-        if economy.K3 > 0:
+        if check_alpha and economy.K3 > 0:
             for formulation, eliminated in zip(economy._X1_formulations, self.eliminated_beta_index.flatten()):
                 if 'prices' in formulation.names and eliminated:
                     raise ValueError(
                         f"A supply side was specified, so alpha should not be concentrated out. That is, initial "
                         f"values should be specified for all parameters in beta on X1 characteristics involving prices."
                     )
 
@@ -261,14 +278,17 @@
                 elif isinstance(parameter, RhoParameter) and rho_bounds is None:
                     self.rho_bounds[0][location] = min(0, self.rho[location])
                     self.rho_bounds[1][location] = max(0.99, self.rho[location])
 
         # identify whether there are any bounds
         self.any_bounds = np.isfinite(self.compress_bounds()).any()
 
+        # define labels for theta
+        self.theta_labels = self.compress_labels()
+
     @staticmethod
     def initialize_matrix(
             name: str, condition_name: str, values: Optional[Any], shapes: Sequence[Tuple[int, int]],
             allow_nans: bool = False) -> Array:
         """Validate and structure a parameter matrix, which can be a number of different shapes."""
 
         # allow the matrix to be all nans if it is to be entirely concentrated out
@@ -334,15 +354,17 @@
             elif parameter.value is None:
                 self.unfixed.append(parameter)
             else:
                 self.fixed.append(parameter)
 
     def format(self, title: str) -> str:
         """Format fixed and unfixed parameter values as a string."""
-        return self.format_theta_parameters(title, self.sigma, self.pi, self.rho, self.beta, self.gamma)
+        return self.format_theta_parameters(
+            title, self.sigma, self.pi, self.rho, self.beta, self.gamma, self.sigma_squared
+        )
 
     def format_lower_bounds(self, title: str) -> str:
         """Format lower bounds for fixed and unfixed parameter values as a string."""
         return self.format_theta_parameters(
             title, self.sigma_bounds[0], self.pi_bounds[0], self.rho_bounds[0], self.beta_bounds[0],
             self.gamma_bounds[0]
         )
@@ -352,32 +374,37 @@
         return self.format_theta_parameters(
             title, self.sigma_bounds[1], self.pi_bounds[1], self.rho_bounds[1], self.beta_bounds[1],
             self.gamma_bounds[1]
         )
 
     def format_theta_parameters(
             self, title: str, sigma_like: Array, pi_like: Array, rho_like: Array, beta_like: Array,
-            gamma_like: Array) -> str:
+            gamma_like: Array, sigma_squared_like: Optional[Array] = None) -> str:
         """Format fixed and unfixed parameter-like values as a string. Skip sections of parameters without any that
         are in theta.
         """
         items = [
-            (NonlinearCoefficient, lambda: self.format_nonlinear_coefficients(title, sigma_like, pi_like)),
+            (NonlinearCoefficient, lambda: self.format_nonlinear_coefficients(
+                title, sigma_like, pi_like, sigma_squared_like
+            )),
             (RhoParameter, lambda: self.format_rho(title, rho_like)),
             (BetaParameter, lambda: self.format_beta(title, beta_like)),
             (GammaParameter, lambda: self.format_gamma(title, gamma_like))
         ]
         return "\n\n".join(f() for t, f in items if any(isinstance(p, t) for p in self.fixed + self.unfixed))
 
     def format_estimates(
-            self, title: str, sigma: Array, pi: Array, rho: Array, beta: Array, gamma: Array, sigma_se: Array,
-            pi_se: Array, rho_se: Array, beta_se: Array, gamma_se: Array) -> str:
+            self, title: str, sigma: Array, pi: Array, rho: Array, beta: Array, gamma: Array, sigma_squared: Array,
+            sigma_se: Array, pi_se: Array, rho_se: Array, beta_se: Array, gamma_se: Array,
+            sigma_squared_se: Array) -> str:
         """Format all estimates and their standard errors as a string."""
         items = [
-            (sigma, lambda: self.format_nonlinear_coefficients(title, sigma, pi, sigma_se, pi_se)),
+            (sigma, lambda: self.format_nonlinear_coefficients(
+                title, sigma, pi, sigma_squared, sigma_se, pi_se, sigma_squared_se
+            )),
             (rho, lambda: self.format_rho(title, rho, rho_se)),
             (beta, lambda: self.format_beta(title, beta, beta_se)),
             (gamma, lambda: self.format_gamma(title, gamma, gamma_se))
         ]
         return "\n\n".join(f() for e, f in items if e.size > 0)
 
     def format_rho(self, title: str, rho_like: Array, rho_se_like: Optional[Array] = None) -> str:
@@ -400,62 +427,78 @@
         data = [[format_number(x) for x in vector]]
         if vector_se is not None:
             fixed_indices = {p.location[0] for p in self.fixed if isinstance(p, parameter_type)}
             data.append(["" if i in fixed_indices else format_se(x) for i, x in enumerate(vector_se)])
         return format_table(header, *data, title=title)
 
     def format_nonlinear_coefficients(
-            self, title: str, sigma_like: Array, pi_like: Array, sigma_se_like: Optional[Array] = None,
-            pi_se_like: Optional[Array] = None) -> str:
+            self, title: str, sigma_like: Array, pi_like: Array, sigma_squared_like: Optional[Array] = None,
+            sigma_se: Optional[Array] = None, pi_se: Optional[Array] = None,
+            sigma_squared_se: Optional[Array] = None) -> str:
         """Format matrices (and optional standard errors) of the same size as sigma and pi as a string."""
 
-        # determine whether a distributions column is necessary
-        distributions_column = any(d != 'normal' for d in self.distributions)
+        # determine whether a types column is necessary
+        rc_types_column = any(d != 'linear' for d in self.rc_types)
+
+        # only add sigma squared columns if the covariance matrix has off-diagonal terms
+        if self.diagonal_sigma:
+            sigma_squared_like = sigma_squared_se = None
 
         # construct the header
-        line_indices: Set[int] = {0} if distributions_column else set()
-        header = (["Distributions:"] if distributions_column else []) + ["Sigma:"] + self.sigma_labels
+        line_indices: Set[int] = {0} if rc_types_column else set()
+        header = (["Types:"] if rc_types_column else []) + ["Sigma:"] + self.sigma_labels
+        if sigma_squared_like is not None:
+            line_indices.add(len(header) - 1)
+            header.extend(["Sigma Squared:"] + self.sigma_labels)
         if self.pi_labels:
             line_indices.add(len(header) - 1)
             header.extend(["Pi:"] + self.pi_labels)
 
         # construct the data
         data: List[List[str]] = []
-        for row_index, (row_distribution, row_label) in enumerate(zip(self.distributions, self.sigma_labels)):
+        for row_index, (row_rc_type, row_label) in enumerate(zip(self.rc_types, self.sigma_labels)):
             # add a row of values
-            values_row = ([row_distribution.title()] if distributions_column else []) + [row_label]
+            values_row = ([row_rc_type.title()] if rc_types_column else []) + [row_label]
             for column_index in range(row_index + 1):
                 values_row.append(format_number(sigma_like[row_index, column_index]))
             values_row.extend([""] * (sigma_like.shape[1] - row_index - 1))
+            if sigma_squared_like is not None:
+                values_row.append(row_label)
+                for column_index in range(sigma_squared_like.shape[1]):
+                    values_row.append(format_number(sigma_squared_like[row_index, column_index]))
             if pi_like.shape[1] > 0:
                 values_row.append(row_label)
                 for column_index in range(pi_like.shape[1]):
                     values_row.append(format_number(pi_like[row_index, column_index]))
             data.append(values_row)
 
             # only add a row of standard errors if standard errors are specified
-            if sigma_se_like is None:
+            if sigma_se is None:
                 continue
-            assert pi_se_like is not None
 
             # determine which columns in this row correspond to unfixed parameters
             relevant_unfixed = {p for p in self.unfixed if p.location[0] == row_index}
             unfixed_sigma_indices = {p.location[1] for p in relevant_unfixed if isinstance(p, SigmaParameter)}
             unfixed_pi_indices = {p.location[1] for p in relevant_unfixed if isinstance(p, PiParameter)}
 
             # add a row of standard errors
-            se_row = (2 if distributions_column else 1) * [""]
+            se_row = (2 if rc_types_column else 1) * [""]
             for column_index in range(row_index + 1):
-                se = sigma_se_like[row_index, column_index]
+                se = sigma_se[row_index, column_index]
                 se_row.append(format_se(se) if column_index in unfixed_sigma_indices else "")
             se_row.extend([""] * (sigma_like.shape[1] - row_index - 1))
-            if pi_se_like.shape[1] > 0:
+            if sigma_squared_se is not None:
+                se_row.append("")
+                for column_index in range(sigma_squared_se.shape[1]):
+                    se = sigma_squared_se[row_index, column_index]
+                    se_row.append(format_se(se))
+            if pi_se is not None and pi_se.shape[1] > 0:
                 se_row.append("")
-                for column_index in range(pi_se_like.shape[1]):
-                    se = pi_se_like[row_index, column_index]
+                for column_index in range(pi_se.shape[1]):
+                    se = pi_se[row_index, column_index]
                     se_row.append(format_se(se) if column_index in unfixed_pi_indices else "")
             data.append(se_row)
 
             # add a blank row to separate the standard errors from the next row of values
             if row_index < sigma_like.shape[1] - 1:
                 data.append([""] * len(se_row))
 
@@ -464,29 +507,40 @@
     def compress(self) -> Array:
         """Compress the initial parameters into theta."""
         items = [
             (SigmaParameter, self.sigma),
             (PiParameter, self.pi),
             (RhoParameter, self.rho),
             (BetaParameter, self.beta),
-            (GammaParameter, self.gamma)
+            (GammaParameter, self.gamma),
         ]
         return np.r_[[v[p.location] for t, v in items for p in self.unfixed if isinstance(p, t)]]
 
     def compress_bounds(self) -> List[Tuple[float, float]]:
         """Compress parameter bounds into a list of (lb, ub) tuples for theta."""
         items = [
             (SigmaParameter, self.sigma_bounds),
             (PiParameter, self.pi_bounds),
             (RhoParameter, self.rho_bounds),
             (BetaParameter, self.beta_bounds),
-            (GammaParameter, self.gamma_bounds)
+            (GammaParameter, self.gamma_bounds),
         ]
         return [(l[p.location], u[p.location]) for t, (l, u) in items for p in self.unfixed if isinstance(p, t)]
 
+    def compress_labels(self) -> List[str]:
+        """Compress labels into a list of labels for theta."""
+        items = [
+            (SigmaParameter, np.array([[f'{k1} x {k2}' for k2 in self.sigma_labels] for k1 in self.sigma_labels])),
+            (PiParameter, np.array([[f'{k1} x {k2}' for k2 in self.pi_labels] for k1 in self.sigma_labels])),
+            (RhoParameter, np.c_[np.array(self.rho_labels)]),
+            (BetaParameter, np.c_[np.array(self.beta_labels)]),
+            (GammaParameter, np.c_[np.array(self.gamma_labels)]),
+        ]
+        return [v[p.location] for t, v in items for p in self.unfixed if isinstance(p, t)]
+
     def expand(self, theta_like: Array, nullify: bool = False) -> Tuple[Array, Array, Array, Array, Array]:
         """Recover matrices of the same size as parameter matrices from a vector of the same size as theta. By default,
         fill elements corresponding to fixed parameters with their fixed values. Always fill concentrated out parameters
         with nulls.
         """
         sigma_like = np.full_like(self.sigma, np.nan)
         pi_like = np.full_like(self.pi, np.nan)
@@ -494,15 +548,15 @@
         beta_like = np.full_like(self.beta, np.nan)
         gamma_like = np.full_like(self.gamma, np.nan)
         items = [
             (SigmaParameter, sigma_like),
             (PiParameter, pi_like),
             (RhoParameter, rho_like),
             (BetaParameter, beta_like),
-            (GammaParameter, gamma_like)
+            (GammaParameter, gamma_like),
         ]
 
         # fill values of unfixed parameters
         for parameter, value in zip(self.unfixed, theta_like):
             for parameter_type, values in items:
                 if isinstance(parameter, parameter_type):
                     values[parameter.location] = value
@@ -514,7 +568,20 @@
             for parameter in self.fixed:
                 for parameter_type, values in items:
                     if isinstance(parameter, parameter_type):
                         values[parameter.location] = parameter.value
                         break
 
         return sigma_like, pi_like, rho_like, beta_like, gamma_like
+
+    def extract_sigma_vector_covariances(self, theta_covariances: Array) -> Array:
+        """Extract the sub-matrix of covariances for vech(sigma) from a full covariance matrix for theta."""
+        assert theta_covariances.shape == (self.P, self.P)
+
+        # identify indices in theta for elements in vech(sigma), with NaN representing fixed elements
+        sigma_indices = self.expand(np.arange(self.P), nullify=True)[0]
+        sigma_vector_indices = vech(sigma_indices)
+
+        # extract corresponding rows and columns from the theta covariances, taking zeros for fixed elements
+        padded_covariances = np.pad(theta_covariances, pad_width=(0, 1), mode='constant', constant_values=0)
+        indices = np.nan_to_num(sigma_vector_indices, nan=self.P).astype(np.int64)
+        return padded_covariances[indices, :][:, indices]
```

### Comparing `pyblp-0.9.0/pyblp/results/bootstrapped_results.py` & `pyblp-1.0.0/pyblp/results/bootstrapped_results.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,35 +1,58 @@
 """Economy-level structuring of bootstrapped BLP problem results."""
 
 import itertools
+from pathlib import Path
+import pickle
 import time
-from typing import Any, Callable, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple
+from typing import Any, Callable, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union
 
 import numpy as np
 
+from .economy_results import SimpleEconomyResults
 from .problem_results import ProblemResults
-from .results import Results
 from .. import exceptions, options
 from ..configurations.integration import Integration
-from ..markets.results_market import ResultsMarket
+from ..markets.economy_results_market import EconomyResultsMarket
 from ..primitives import Agents
 from ..utilities.basics import (
     Array, Error, SolverStats, format_seconds, format_table, generate_items, get_indices, output, output_progress
 )
 
 
-class BootstrappedResults(Results):
+class BootstrappedResults(SimpleEconomyResults):
     r"""Bootstrapped results of a solved problem.
 
-    This class has same methods as :class:`ProblemResults` that compute post-estimation outputs in one or more markets,
-    but not other methods like :meth:`ProblemResults.compute_optimal_instruments` that do not make sense in a
-    bootstrapped dataset. The only other difference is that methods return arrays with an extra first dimension along
-    which bootstrapped results are stacked (these stacked results can be used to construct, for example, confidence
-    intervals for post-estimation outputs). Similarly, arrays of data (except for firm IDs and ownership matrices)
-    passed as arguments to methods should have an extra first dimension of size :attr:`BootstrappedResults.draws`.
+    This class has slightly modified versions of the following :class:`ProblemResults` methods:
+
+        - :meth:`ProblemResults.compute_aggregate_elasticities`
+        - :meth:`ProblemResults.compute_elasticities`
+        - :meth:`ProblemResults.compute_demand_jacobians`
+        - :meth:`ProblemResults.compute_demand_hessians`
+        - :meth:`ProblemResults.compute_profit_hessians`
+        - :meth:`ProblemResults.compute_diversion_ratios`
+        - :meth:`ProblemResults.compute_long_run_diversion_ratios`
+        - :meth:`ProblemResults.compute_probabilities`
+        - :meth:`ProblemResults.extract_diagonals`
+        - :meth:`ProblemResults.extract_diagonal_means`
+        - :meth:`ProblemResults.compute_delta`
+        - :meth:`ProblemResults.compute_costs`
+        - :meth:`ProblemResults.compute_passthrough`
+        - :meth:`ProblemResults.compute_approximate_prices`
+        - :meth:`ProblemResults.compute_prices`
+        - :meth:`ProblemResults.compute_shares`
+        - :meth:`ProblemResults.compute_hhi`
+        - :meth:`ProblemResults.compute_markups`
+        - :meth:`ProblemResults.compute_profits`
+        - :meth:`ProblemResults.compute_consumer_surpluses`
+
+    The difference is that each method returns an array with an extra first dimension along which bootstrapped results
+    are stacked These stacked results can be used to construct, for example, confidence intervals for
+    post-estimation outputs. Similarly, arrays of data (except for firm IDs and ownership matrices) passed as arguments
+    to methods should have an extra first dimension of size :attr:`BootstrappedResults.draws`.
 
     Attributes
     ----------
     problem_results : `ProblemResults`
         :class:`ProblemResults` that was used to compute these bootstrapped results.
     bootstrapped_sigma : `ndarray`
         Bootstrapped Cholesky decomposition of the covariance matrix for unobserved taste heterogeneity, :math:`\Sigma`.
@@ -41,15 +64,15 @@
         Bootstrapped demand-side linear parameters, :math:`\beta`.
     bootstrapped_gamma : `ndarray`
         Bootstrapped supply-side linear parameters, :math:`\gamma`.
     bootstrapped_prices : `ndarray`
         Bootstrapped prices, :math:`p`. If a supply side was not estimated, these are unchanged prices. Otherwise, they
         are equilibrium prices implied by each draw.
     bootstrapped_shares : `ndarray`
-        Bootstrapped marketshares, :math:`s`, implied by each draw.
+        Bootstrapped market shares, :math:`s`, implied by each draw.
     bootstrapped_delta : `ndarray`
         Bootstrapped mean utility, :math:`\delta`, implied by each draw.
     computation_time : `float`
         Number of seconds it took to compute the bootstrapped results.
     draws : `int`
         Number of bootstrap draws.
     fp_converged : `ndarray`
@@ -86,98 +109,49 @@
 
     def __init__(
             self, problem_results: ProblemResults, bootstrapped_sigma: Array, bootstrapped_pi: Array,
             bootstrapped_rho: Array, bootstrapped_beta: Array, bootstrapped_gamma: Array, bootstrapped_prices: Array,
             bootstrapped_shares: Array, bootstrapped_delta: Array, start_time: float, end_time: float, draws: int,
             iteration_stats: Mapping[Hashable, SolverStats]) -> None:
         """Structure bootstrapped problem results."""
-        super().__init__(
-            problem_results.problem, problem_results._parameters, problem_results._moments, problem_results._iteration,
-            problem_results._fp_type
-        )
+        super().__init__(problem_results.problem, problem_results._parameters)
         self.problem_results = problem_results
         self.bootstrapped_sigma = bootstrapped_sigma
         self.bootstrapped_pi = bootstrapped_pi
         self.bootstrapped_rho = bootstrapped_rho
         self.bootstrapped_beta = bootstrapped_beta
         self.bootstrapped_gamma = bootstrapped_gamma
         self.bootstrapped_prices = bootstrapped_prices
         self.bootstrapped_shares = bootstrapped_shares
         self.bootstrapped_delta = bootstrapped_delta
         self.computation_time = end_time - start_time
         self.draws = draws
         unique_market_ids = problem_results.problem.unique_market_ids
         self.fp_converged = np.array(
-            [[iteration_stats[(d, t)].converged for d in range(self.draws)] for t in unique_market_ids], dtype=np.bool
+            [[iteration_stats[(d, t)].converged for d in range(self.draws)] for t in unique_market_ids],
+            dtype=np.bool_,
         )
         self.fp_iterations = np.array(
-            [[iteration_stats[(d, t)].iterations for d in range(self.draws)] for t in unique_market_ids], dtype=np.int
+            [[iteration_stats[(d, t)].iterations for d in range(self.draws)] for t in unique_market_ids],
+            dtype=np.int64,
         )
         self.contraction_evaluations = np.array(
-            [[iteration_stats[(d, t)].evaluations for d in range(self.draws)] for t in unique_market_ids], dtype=np.int
+            [[iteration_stats[(d, t)].evaluations for d in range(self.draws)] for t in unique_market_ids],
+            dtype=np.int64,
         )
 
     def __str__(self) -> str:
         """Format bootstrapped problem results as a string."""
         header = [("Computation", "Time"), ("Bootstrap", "Draws")]
         values = [format_seconds(self.computation_time), self.draws]
         if self.fp_iterations.sum() > 0 or self.contraction_evaluations.sum() > 0:
             header.extend([("Fixed Point", "Iterations"), ("Contraction", "Evaluations")])
             values.extend([self.fp_iterations.sum(), self.contraction_evaluations.sum()])
         return format_table(header, values, title="Bootstrapped Results Summary")
 
-    def _coerce_matrices(self, matrices: Any, market_ids: Array) -> Array:
-        """Coerce array-like stacked matrix tensors into a stacked matrix tensor and validate it."""
-        matrices = np.atleast_3d(np.asarray(matrices, options.dtype))
-        rows = sum(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        columns = max(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        if matrices.shape != (self.draws, rows, columns):
-            raise ValueError(f"matrices must be {self.draws} by {rows} by {columns}.")
-        return matrices
-
-    def _coerce_optional_delta(self, delta: Optional[Any], market_ids: Array) -> Array:
-        """Coerce optional array-like mean utilities into a column vector tensor and validate it."""
-        if delta is None:
-            return None
-        delta = np.atleast_3d(np.asarray(delta, options.dtype))
-        rows = sum(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        if delta.shape != (self.draws, rows, 1):
-            raise ValueError(f"delta must be None or {self.draws} by {rows}.")
-        return delta
-
-    def _coerce_optional_costs(self, costs: Optional[Any], market_ids: Array) -> Array:
-        """Coerce optional array-like costs into a column vector tensor and validate it."""
-        if costs is None:
-            return None
-        costs = np.atleast_3d(np.asarray(costs, options.dtype))
-        rows = sum(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        if costs.shape != (self.draws, rows, 1):
-            raise ValueError(f"costs must be None or {self.draws} by {rows}.")
-        return costs
-
-    def _coerce_optional_prices(self, prices: Optional[Any], market_ids: Array) -> Array:
-        """Coerce optional array-like prices into a column vector tensor and validate it."""
-        if prices is None:
-            return None
-        prices = np.atleast_3d(np.asarray(prices, options.dtype))
-        rows = sum(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        if prices.shape != (self.draws, rows, 1):
-            raise ValueError(f"prices must be None or {self.draws} by {rows}.")
-        return prices
-
-    def _coerce_optional_shares(self, shares: Optional[Any], market_ids: Array) -> Array:
-        """Coerce optional array-like shares into a column vector tensor and validate it."""
-        if shares is None:
-            return shares
-        shares = np.atleast_3d(np.asarray(shares, options.dtype))
-        rows = sum(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        if shares.shape != (self.draws, rows, 1):
-            raise ValueError(f"shares must be None or {self.draws} by {rows}.")
-        return shares
-
     def _combine_arrays(
             self, compute_market_results: Callable, market_ids: Array, fixed_args: Sequence = (),
             market_args: Sequence = (), agent_data: Optional[Mapping] = None,
             integration: Optional[Integration] = None) -> Array:
         """Compute arrays for one or all markets and stack them into a single tensor. An array for a single market is
         computed by passing fixed_args (identical for all markets) and market_args (matrices with as many rows as there
         are products that are restricted to the market) to compute_market_results, a ResultsMarket method that returns
@@ -187,106 +161,167 @@
         errors: List[Error] = []
 
         # keep track of how long it takes to compute the arrays
         start_time = time.time()
 
         # structure or construct different agent data
         if agent_data is None and integration is None:
-            agents = self.problem.agents
-            agents_market_indices = self.problem._agent_market_indices
+            agents = self._economy.agents
+            agents_market_indices = self._economy._agent_market_indices
         else:
-            agents = Agents(self.problem.products, self.problem.agent_formulation, agent_data, integration)
+            agents = Agents(self._economy.products, self._economy.agent_formulation, agent_data, integration)
             agents_market_indices = get_indices(agents.market_ids)
 
         def market_factory(pair: Tuple[int, Hashable]) -> tuple:
             """Build a market with bootstrapped data along with arguments used to compute arrays."""
             c, s = pair
             data_override_c = {
                 'prices': self.bootstrapped_prices[c],
                 'shares': self.bootstrapped_shares[c]
             }
-            market_cs = ResultsMarket(
-                self.problem, s, self._parameters, self.bootstrapped_sigma[c], self.bootstrapped_pi[c],
+            market_cs = EconomyResultsMarket(
+                self._economy, s, self._parameters, self.bootstrapped_sigma[c], self.bootstrapped_pi[c],
                 self.bootstrapped_rho[c], self.bootstrapped_beta[c], self.bootstrapped_gamma[c],
-                self.bootstrapped_delta[c], self._moments, data_override_c, agents[agents_market_indices[s]]
+                self.bootstrapped_delta[c], data_override=data_override_c,
+                agents_override=agents[agents_market_indices[s]]
             )
             args_cs: List[Optional[Array]] = []
             for market_arg in market_args:
                 if market_arg is None:
                     args_cs.append(market_arg)
                 elif len(market_arg.shape) == 2:
                     if market_ids.size == 1:
                         args_cs.append(market_arg)
                     else:
-                        args_cs.append(market_arg[self.problem._product_market_indices[s]])
+                        args_cs.append(market_arg[self._economy._product_market_indices[s]])
                 else:
                     assert len(market_arg.shape) == 3
                     if market_ids.size == 1:
                         args_cs.append(market_arg[c])
                     else:
-                        args_cs.append(market_arg[c, self.problem._product_market_indices[s]])
+                        args_cs.append(market_arg[c, self._economy._product_market_indices[s]])
             return (market_cs, *fixed_args, *args_cs)
 
         # construct a mapping from draws and market IDs to market-specific arrays and compute the full matrix size
-        matrix_mapping: Dict[Tuple[int, Hashable], Array] = {}
+        array_mapping: Dict[Tuple[int, Hashable], Array] = {}
         pairs = itertools.product(range(self.draws), market_ids)
         generator = generate_items(pairs, market_factory, compute_market_results)
         if self.draws > 1 or market_ids.size > 1:
             generator = output_progress(generator, self.draws * market_ids.size, start_time)
         for (d, t), (array_dt, errors_dt) in generator:
-            matrix_mapping[(d, t)] = np.c_[array_dt]
+            array_mapping[(d, t)] = np.c_[array_dt]
             errors.extend(errors_dt)
 
         # output a warning about any errors
         if errors:
             output("")
             output(exceptions.MultipleErrors(errors))
             output("")
 
-        # determine the number of rows and columns
-        row_count = sum(matrix_mapping[(0, t)].shape[0] for t in market_ids)
-        column_count = max(matrix_mapping[(0, t)].shape[1] for t in market_ids)
+        # determine the sizes of dimensions
+        dimension_sizes = []
+        for dimension in range(len(array_mapping[(0, market_ids[0])].shape)):
+            if dimension == 0:
+                dimension_sizes.append(sum(array_mapping[(0, t)].shape[dimension] for t in market_ids))
+            else:
+                dimension_sizes.append(max(array_mapping[(0, t)].shape[dimension] for t in market_ids))
 
         # preserve the original product order or the sorted market order when stacking the arrays
-        combined = np.full((self.draws, row_count, column_count), np.nan, options.dtype)
-        for (d, t), matrix_dt in matrix_mapping.items():
-            if row_count == market_ids.size:
-                combined[d, market_ids == t, :matrix_dt.shape[1]] = matrix_dt
-            elif row_count == self.problem.N:
-                combined[d, self.problem._product_market_indices[t], :matrix_dt.shape[1]] = matrix_dt
+        combined = np.full((self.draws, *dimension_sizes), np.nan, options.dtype)
+        for (d, t), array_dt in array_mapping.items():
+            slices = (slice(0, s) for s in array_dt.shape[1:])
+            if dimension_sizes[0] == market_ids.size:
+                combined[(d, market_ids == t, *slices)] = array_dt
+            elif dimension_sizes[0] == self._economy.N:
+                combined[(d, self._economy._product_market_indices[t], *slices)] = array_dt
             else:
                 assert market_ids.size == 1
-                combined[d] = matrix_dt
+                combined[d] = array_dt
 
         # output how long it took to compute the arrays
         end_time = time.time()
         output(f"Finished after {format_seconds(end_time - start_time)}.")
         output("")
         return combined
 
+    def _coerce_matrices(self, matrices: Any, market_ids: Array) -> Array:
+        """Coerce array-like stacked matrix tensors into a stacked matrix tensor and validate it."""
+        matrices = np.atleast_3d(np.asarray(matrices, options.dtype))
+        rows = sum(i.size for t, i in self._economy._product_market_indices.items() if t in market_ids)
+        columns = max(i.size for t, i in self._economy._product_market_indices.items() if t in market_ids)
+        if matrices.shape != (self.draws, rows, columns):
+            raise ValueError(f"matrices must be {self.draws} by {rows} by {columns}.")
+        return matrices
+
+    def _coerce_optional_delta(self, delta: Optional[Any], market_ids: Array) -> Array:
+        """Coerce optional array-like mean utilities into a column vector tensor and validate it."""
+        if delta is None:
+            return None
+        delta = np.atleast_3d(np.asarray(delta, options.dtype))
+        rows = sum(i.size for t, i in self._economy._product_market_indices.items() if t in market_ids)
+        if delta.shape != (self.draws, rows, 1):
+            raise ValueError(f"delta must be None or {self.draws} by {rows}.")
+        return delta
+
+    def _coerce_optional_costs(self, costs: Optional[Any], market_ids: Array) -> Array:
+        """Coerce optional array-like costs into a column vector tensor and validate it."""
+        if costs is None:
+            return None
+        costs = np.atleast_3d(np.asarray(costs, options.dtype))
+        rows = sum(i.size for t, i in self._economy._product_market_indices.items() if t in market_ids)
+        if costs.shape != (self.draws, rows, 1):
+            raise ValueError(f"costs must be None or {self.draws} by {rows}.")
+        return costs
+
+    def _coerce_optional_prices(self, prices: Optional[Any], market_ids: Array) -> Array:
+        """Coerce optional array-like prices into a column vector tensor and validate it."""
+        if prices is None:
+            return None
+        prices = np.atleast_3d(np.asarray(prices, options.dtype))
+        rows = sum(i.size for t, i in self._economy._product_market_indices.items() if t in market_ids)
+        if prices.shape != (self.draws, rows, 1):
+            raise ValueError(f"prices must be None or {self.draws} by {rows}.")
+        return prices
+
+    def _coerce_optional_shares(self, shares: Optional[Any], market_ids: Array) -> Array:
+        """Coerce optional array-like shares into a column vector tensor and validate it."""
+        if shares is None:
+            return shares
+        shares = np.atleast_3d(np.asarray(shares, options.dtype))
+        rows = sum(i.size for t, i in self._economy._product_market_indices.items() if t in market_ids)
+        if shares.shape != (self.draws, rows, 1):
+            raise ValueError(f"shares must be None or {self.draws} by {rows}.")
+        return shares
+
+    def to_pickle(self, path: Union[str, Path]) -> None:
+        """Save these results as a pickle file.
+
+        Parameters
+        ----------
+        path: `str or Path`
+            File path to which these results will be saved.
+
+        """
+        with open(path, 'wb') as handle:
+            pickle.dump(self, handle)
+
     def to_dict(
             self, attributes: Sequence[str] = (
                 'bootstrapped_sigma', 'bootstrapped_pi', 'bootstrapped_rho', 'bootstrapped_beta', 'bootstrapped_gamma',
                 'bootstrapped_prices', 'bootstrapped_shares', 'bootstrapped_delta', 'computation_time', 'draws',
                 'fp_converged', 'fp_iterations', 'contraction_evaluations'
             )) -> dict:
         """Convert these results into a dictionary that maps attribute names to values.
 
-        Once converted to a dictionary, these results can be saved to a file with :func:`pickle.dump`.
-
         Parameters
         ----------
         attributes : `sequence of str, optional`
             Name of attributes that will be added to the dictionary. By default, all :class:`BootstrappedResults`
             attributes are added except for :attr:`BootstrappedResults.problem_results`.
 
         Returns
         -------
         `dict`
             Mapping from attribute names to values.
 
-        Examples
-        --------
-            - :doc:`Tutorial </tutorial>`
-
         """
         return {k: getattr(self, k) for k in attributes}
```

### Comparing `pyblp-0.9.0/pyblp/results/importance_sampling_results.py` & `pyblp-1.0.0/pyblp/results/importance_sampling_results.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,12 @@
 """Economy-level structuring of importance sampling results."""
 
-from typing import Sequence, TYPE_CHECKING
+from pathlib import Path
+import pickle
+from typing import Sequence, TYPE_CHECKING, Union
 
 import numpy as np
 
 from .problem_results import ProblemResults
 from ..utilities.basics import Array, Groups, RecArray, StringRepresentation, format_seconds, format_table
 
 
@@ -35,19 +37,21 @@
         Number of importance sampling draws in each market.
     diagnostic_market_ids : `ndarray`
         Market IDs the correspond to the ordering of the following arrays of weight diagnostics.
     weight_sums : `ndarray`
         Sum of weights in each market: :math:`\sum_i w_{it}`. If importance sampling was successful, weights should not
         sum to numbers too far from one.
     effective_draws : `ndarray`
-        Effective sample sizes in each market: :math:`\frac{(\sum_i w_{it})^2}{\sum_i w_i^2}`.
+        Effective sample sizes in each market: :math:`\frac{(\sum_i w_{it})^2}{\sum_i w_{it}^2}`.
     effective_draws_for_variance : `ndarray`
-        Effective sample sizes for variance estimates in each market: :math:`\frac{(\sum_i w_i^2)^2}{\sum_i w_i^4}`.
+        Effective sample sizes for variance estimates in each market:
+        :math:`\frac{(\sum_i w_{it}^2)^2}{\sum_i w_{it}^4}`.
     effective_draws_for_skewness : `ndarray`
-        Effective sample sizes for gauging skewness in each market: :math:`\frac{(\sum_i w_i^2)^3}{(\sum_i w_i^3)^2}`.
+        Effective sample sizes for gauging skewness in each market:
+        :math:`\frac{(\sum_i w_{it}^2)^3}{(\sum_i w_{it}^3)^2}`.
 
     Examples
     --------
         - :doc:`Tutorial </tutorial>`
 
     """
 
@@ -86,28 +90,38 @@
         header = [
             ("Computation", "Time"), ("Total", "Sampling Draws"), ("Sampling Draws", "per Market"),
             ("Min", "Effective Draws"), ("Min", "Effective Draws", "for Variance"),
             ("Min", "Effective Draws", "for Skewness"), ("Min", "Weight Sum"), ("Max", "Weight Sum")
         ]
         values = [
             format_seconds(self.computation_time), self.sampled_agents.shape[0], self.draws,
-            np.min(self.effective_draws).astype(int), np.min(self.effective_draws_for_variance).astype(int),
-            np.min(self.effective_draws_for_skewness).astype(int), np.min(self.weight_sums), np.max(self.weight_sums)
+            int(np.min(self.effective_draws)), int(np.min(self.effective_draws_for_variance)),
+            int(np.min(self.effective_draws_for_skewness)), np.min(self.weight_sums), np.max(self.weight_sums)
         ]
         return format_table(header, values, title="Importance Sampling Results Summary")
 
+    def to_pickle(self, path: Union[str, Path]) -> None:
+        """Save these results as a pickle file.
+
+        Parameters
+        ----------
+        path: `str or Path`
+            File path to which these results will be saved.
+
+        """
+        with open(path, 'wb') as handle:
+            pickle.dump(self, handle)
+
     def to_dict(
             self, attributes: Sequence[str] = (
                 'sampled_agents', 'computation_time', 'draws', 'diagnostic_market_ids', 'weight_sums',
                 'effective_draws', 'effective_draws_for_variance', 'effective_draws_for_skewness'
             )) -> dict:
         """Convert these results into a dictionary that maps attribute names to values.
 
-        Once converted to a dictionary, these results can be saved to a file with :func:`pickle.dump`.
-
         Parameters
         ----------
         attributes : `sequence of str, optional`
             Names of attributes that will be added to the dictionary. By default, all :class:`ImportanceSamplingResults`
             attributes are added except for :attr:`ImportanceSamplingResults.problem_results`.
 
         Returns
```

### Comparing `pyblp-0.9.0/pyblp/results/optimal_instrument_results.py` & `pyblp-1.0.0/pyblp/results/optimal_instrument_results.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,15 @@
 """Economy-level structuring of optimal instrument results."""
 
-from typing import Hashable, Optional, Sequence, TYPE_CHECKING
+from pathlib import Path
+import pickle
+from typing import Hashable, Optional, Sequence, TYPE_CHECKING, Union
 
 import numpy as np
+import patsy
 
 from .problem_results import ProblemResults
 from ..configurations.formulation import Formulation
 from ..parameters import LinearCoefficient
 from ..utilities.basics import Array, Mapping, SolverStats, StringRepresentation, format_seconds, format_table
 
 
@@ -47,15 +50,15 @@
         Estimated :math:`E[\frac{\partial\xi}{\partial\theta} \mid Z]`.
     expected_omega_by_theta_jacobian : `ndarray`
         Estimated :math:`E[\frac{\partial\omega}{\partial\theta} \mid Z]`.
     expected_prices : `ndarray`
         Vector of expected prices conditional on all exogenous variables, :math:`E[p \mid Z]`, which may have been
         specified in :meth:`ProblemResults.compute_optimal_instruments`.
     expected_shares : `ndarray`
-        Vector of expected marketshares conditional on all exogenous variables, :math:`E[s \mid Z]`.
+        Vector of expected market shares conditional on all exogenous variables, :math:`E[s \mid Z]`.
     computation_time : `float`
         Number of seconds it took to compute optimal excluded instruments.
     draws : `int`
         Number of draws used to approximate the integral over the error term density.
     fp_converged : `ndarray`
         Flags for convergence of the iteration routine used to compute equilibrium prices in each market. Rows are in
         the same order as :attr:`Problem.unique_market_ids` and column indices correspond to draws.
@@ -106,59 +109,82 @@
         self.expected_omega_by_theta_jacobian = expected_omega_jacobian
         self.expected_prices = expected_prices
         self.expected_shares = expected_shares
         self.computation_time = end_time - start_time
         self.draws = draws
         unique_market_ids = problem_results.problem.unique_market_ids
         self.fp_converged = np.array(
-            [[m[t].converged if m else True for m in iteration_stats] for t in unique_market_ids], dtype=np.bool
+            [[m[t].converged if m else True for m in iteration_stats] for t in unique_market_ids], dtype=np.bool_
         )
         self.fp_iterations = np.array(
-            [[m[t].iterations if m else 0 for m in iteration_stats] for t in unique_market_ids], dtype=np.int
+            [[m[t].iterations if m else 0 for m in iteration_stats] for t in unique_market_ids], dtype=np.int64
         )
         self.contraction_evaluations = np.array(
-            [[m[t].evaluations if m else 0 for m in iteration_stats] for t in unique_market_ids], dtype=np.int
+            [[m[t].evaluations if m else 0 for m in iteration_stats] for t in unique_market_ids], dtype=np.int64
         )
 
         # construct default supply and demand shifter formulations
         self.supply_shifter_formulation = self.demand_shifter_formulation = None
         if self.problem_results.problem.K3 > 0:
             assert self.problem_results.problem.product_formulations[0] is not None
             assert self.problem_results.problem.product_formulations[2] is not None
+
+            X1_terms = self.problem_results.problem.product_formulations[0]._terms
+            X3_terms = self.problem_results.problem.product_formulations[2]._terms
             X1_expressions = self.problem_results.problem.product_formulations[0]._expressions
             X3_expressions = self.problem_results.problem.product_formulations[2]._expressions
-            supply_shifters = {str(e) for e in X3_expressions if all(str(s) != 'shares' for s in e.free_symbols)}
-            demand_shifters = {str(e) for e in X1_expressions if all(str(s) != 'prices' for s in e.free_symbols)}
+
+            supply_shifters = set()
+            for term, expression in zip(X3_terms, X3_expressions):
+                if all(str(s) != 'shares' for s in expression.free_symbols) and term.name() != 'Intercept':
+                    supply_shifters.add(term.name())
+
+            demand_shifters = set()
+            for term, expression in zip(X1_terms, X1_expressions):
+                if all(str(s) != 'prices' for s in expression.free_symbols) and term.name() != 'Intercept':
+                    demand_shifters.add(term.name())
+
             if supply_shifters - demand_shifters:
                 supply_shifter_formula = ' + '.join(sorted(supply_shifters - demand_shifters))
                 self.supply_shifter_formulation = Formulation(f'{supply_shifter_formula} - 1')
+
             if demand_shifters - supply_shifters:
                 demand_shifter_formula = ' + '.join(sorted(demand_shifters - supply_shifters))
                 self.demand_shifter_formulation = Formulation(f'{demand_shifter_formula} - 1')
 
     def __str__(self) -> str:
         """Format optimal instrument computation results as a string."""
         header = [("Computation", "Time"), ("Error Term", "Draws")]
         values = [format_seconds(self.computation_time), self.draws]
         if self.fp_iterations.sum() > 0 or self.contraction_evaluations.sum() > 0:
             header.extend([("Fixed Point", "Iterations"), ("Contraction", "Evaluations")])
             values.extend([self.fp_iterations.sum(), self.contraction_evaluations.sum()])
         return format_table(header, values, title="Optimal Instrument Results Summary")
 
+    def to_pickle(self, path: Union[str, Path]) -> None:
+        """Save these results as a pickle file.
+
+        Parameters
+        ----------
+        path: `str or Path`
+            File path to which these results will be saved.
+
+        """
+        with open(path, 'wb') as handle:
+            pickle.dump(self, handle)
+
     def to_dict(
             self, attributes: Sequence[str] = (
                 'demand_instruments', 'supply_instruments', 'inverse_covariance_matrix',
                 'expected_xi_by_theta_jacobian', 'expected_omega_by_theta_jacobian', 'expected_prices',
                 'expected_shares', 'computation_time', 'draws', 'fp_converged', 'fp_iterations',
                 'contraction_evaluations'
             )) -> dict:
         """Convert these results into a dictionary that maps attribute names to values.
 
-        Once converted to a dictionary, these results can be saved to a file with :func:`pickle.dump`.
-
         Parameters
         ----------
         attributes : `sequence of str, optional`
             Name of attributes that will be added to the dictionary. By default, all :class:`OptimalInstrumentResults`
             attributes are added except for :attr:`OptimalInstrumentResults.problem_results`,
             :attr:`OptimalInstrumentResults.supply_shifter_formulation`, and
             :attr:`OptimalInstrumentResults.demand_shifter_formulation`.
@@ -173,15 +199,16 @@
             - :doc:`Tutorial </tutorial>`
 
         """
         return {k: getattr(self, k) for k in attributes}
 
     def to_problem(
             self, supply_shifter_formulation: Optional[Formulation] = None,
-            demand_shifter_formulation: Optional[Formulation] = None) -> 'OptimalInstrumentProblem':
+            demand_shifter_formulation: Optional[Formulation] = None, product_data: Optional[Mapping] = None,
+            drop_indices: Optional[Sequence[int]] = None) -> 'OptimalInstrumentProblem':
         r"""Re-create the problem with estimated feasible optimal instruments.
 
         The re-created problem will be exactly the same, except that instruments will be replaced with estimated
         feasible optimal instruments.
 
         .. note::
 
@@ -213,15 +240,15 @@
                rather included in :math:`\theta` by :meth:`Problem.solve`.
 
             2. Optimal instruments for any linear supply-side parameters on endogenous product characteristics,
                :math:`\gamma^\text{en}`, which were concentrated out an hence not included in :math:`\theta`. This
                is only relevant if ``shares`` were included in the formulation for :math:`X_3` in :class:`Problem`.
                The corresponding optimal instruments are simply an integral of the endogenous product characteristics,
                :math:`X_3^\text{en}`, over the joint density of :math:`\xi` and :math:`\omega`. The approximation of
-               these optimal instruments is simply :math:`X_3^\text{en}` evaluated at the marketshares that arise under
+               these optimal instruments is simply :math:`X_3^\text{en}` evaluated at the market shares that arise under
                the constant vector of expected prices, :math:`E[p \mid Z]`, specified in
                :meth:`ProblemResults.compute_optimal_instruments`.
 
             2. If a supply side was estimated, any demand shifters, which are by default formulated by
                :attr:`OptimalInstrumentResults.demand_shifter_formulation`: all characteristics in :math:`X_1^\text{ex}`
                not in :math:`X_3^\text{ex}`.
 
@@ -240,27 +267,39 @@
             :class:`Formulation` configuration for supply shifters to be included in the set of optimal demand-side
             instruments. This is only used if a supply side was estimated. Intercepts will be ignored. By default,
             :attr:`OptimalInstrumentResults.supply_shifter_formulation` is used.
         demand_shifter_formulation : `Formulation, optional`
             :class:`Formulation` configuration for demand shifters to be included in the set of optimal supply-side
             instruments. This is only used if a supply side was estimated. Intercepts will be ignored. By default,
             :attr:`OptimalInstrumentResults.demand_shifter_formulation` is used.
+        product_data : `structured array-like, optional`
+            Product data used instead of what was saved from ``product_data`` when initializing the original
+            :class:`Problem`. This may need to be specified if either the supply or demand shifter formulation contains
+            some term that was not stored into memory, such as a categorical variable or a mathematical expression.
+        drop_indices : `sequence of int, optional`
+            Which column indices to drop from :attr:`OptimalInstrumentResults.demand_instruments` and
+            :attr:`OptimalInstrumentResults.supply_instruments`. By default, the only columns dropped are those that
+            correspond to parameters in :math:`\theta` on exogenous linear characteristics.
 
         Returns
         -------
         `OptimalInstrumentProblem`
             :class:`OptimalInstrumentProblem`, which is a :class:`Problem` updated to use the estimated optimal
             instruments.
 
         Examples
         --------
             - :doc:`Tutorial </tutorial>`
 
         """
 
+        # either use the stored variables as product data or any provided data
+        if product_data is None:
+            product_data = self.problem_results.problem.products
+
         # configure or validate the supply shifter formulation
         if self.problem_results.problem.K3 == 0:
             if supply_shifter_formulation is not None:
                 raise TypeError("A supply side was not estimated, so supply_shifter_formulation should be None.")
         elif supply_shifter_formulation is None:
             supply_shifter_formulation = self.supply_shifter_formulation
         elif not isinstance(supply_shifter_formulation, Formulation):
@@ -279,37 +318,49 @@
         elif not isinstance(demand_shifter_formulation, Formulation):
             raise TypeError("demand_shifter_formulation must be None or a Formulation instance.")
         elif demand_shifter_formulation._names:
             demand_shifter_formulation = Formulation(f'{demand_shifter_formulation._formula} - 1')
         else:
             demand_shifter_formulation = None
 
-        # identify which parameters in theta are are exogenous linear characteristics
-        dropped_index = np.zeros(self.problem_results._parameters.P, np.bool)
-        for p, parameter in enumerate(self.problem_results._parameters.unfixed):
-            if isinstance(parameter, LinearCoefficient):
-                names = parameter.get_product_formulation(self.problem_results.problem).names
-                if 'prices' not in names and 'shares' not in names:
-                    dropped_index[p] = True
+        # identify which parameters in theta that are on exogenous linear characteristics
+        dropped_index = np.zeros(self.problem_results._parameters.P, np.bool_)
+        if drop_indices is not None:
+            if not isinstance(drop_indices, Sequence) or not all(isinstance(i, int) for i in drop_indices):
+                raise TypeError("drop_indices must be a sequence of integers.")
+            for index in drop_indices:
+                dropped_index[index] = True
+        else:
+            for p, parameter in enumerate(self.problem_results._parameters.unfixed):
+                if isinstance(parameter, LinearCoefficient):
+                    names = parameter.get_product_formulation(self.problem_results.problem).names
+                    if 'prices' not in names and 'shares' not in names:
+                        dropped_index[p] = True
 
         # build excluded demand-side instruments
         demand_instruments = self.demand_instruments[:, ~dropped_index]
         if self.problem_results._parameters.eliminated_alpha_index.any():
             demand_instruments = np.c_[
                 demand_instruments,
                 self.problem_results.problem._compute_true_X1(
                     {'prices': self.expected_prices},
                     self.problem_results._parameters.eliminated_alpha_index.flatten()
                 )
             ]
         if supply_shifter_formulation is not None:
-            demand_instruments = np.c_[
-                demand_instruments,
-                supply_shifter_formulation._build_matrix(self.problem_results.problem.products)[0]
-            ]
+            try:
+                demand_instruments = np.c_[
+                    demand_instruments, supply_shifter_formulation._build_matrix(product_data)[0]
+                ]
+            except patsy.PatsyError as exception:
+                message = (
+                    "Failed to construct supply shifters from their formulation. You may need to specify "
+                    "product_data if not all variables in the formulation were saved when initializing the problem."
+                )
+                raise patsy.PatsyError(message) from exception
 
         # build excluded supply-side instruments
         if self.problem_results.problem.K3 == 0:
             supply_instruments = self.supply_instruments
         else:
             supply_instruments = self.supply_instruments[:, ~dropped_index]
             if self.problem_results._parameters.eliminated_endogenous_gamma_index.any():
@@ -317,15 +368,21 @@
                     supply_instruments,
                     self.problem_results.problem._compute_true_X3(
                         {'shares': self.expected_shares},
                         self.problem_results._parameters.eliminated_endogenous_gamma_index.flatten()
                     )
                 ]
             if demand_shifter_formulation is not None:
-                supply_instruments = np.c_[
-                    supply_instruments,
-                    demand_shifter_formulation._build_matrix(self.problem_results.problem.products)[0]
-                ]
+                try:
+                    supply_instruments = np.c_[
+                        supply_instruments, demand_shifter_formulation._build_matrix(product_data)[0]
+                    ]
+                except patsy.PatsyError as exception:
+                    message = (
+                        "Failed to construct demand shifters from their formulation. You may need to specify "
+                        "product_data if not all variables in the formulation were saved when initializing the problem."
+                    )
+                    raise patsy.PatsyError(message) from exception
 
         # initialize the problem
         from ..economies.problem import OptimalInstrumentProblem  # noqa
         return OptimalInstrumentProblem(self.problem_results.problem, demand_instruments, supply_instruments)
```

### Comparing `pyblp-0.9.0/pyblp/results/problem_results.py` & `pyblp-1.0.0/pyblp/results/problem_results.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,44 +1,47 @@
 """Economy-level structuring of BLP problem results."""
 
 import itertools
+from pathlib import Path
+import pickle
 import time
-from typing import Any, Callable, Dict, Hashable, List, Optional, Sequence, TYPE_CHECKING, Tuple
+from typing import Any, Dict, Hashable, List, Optional, Sequence, TYPE_CHECKING, Tuple, Union
 
 import numpy as np
 import scipy.linalg
 
-from .results import Results
+from .economy_results import EconomyResults
 from .. import exceptions, options
 from ..configurations.integration import Integration
 from ..configurations.iteration import Iteration
-from ..markets.results_market import ResultsMarket
+from ..markets.economy_results_market import EconomyResultsMarket
+from ..micro import Moments
 from ..primitives import Agents
 from ..utilities.algebra import (
-    approximately_invert, approximately_solve, compute_condition_number, precisely_compute_eigenvalues
+    approximately_invert, approximately_solve, compute_condition_number, precisely_compute_eigenvalues, vech_to_full
 )
 from ..utilities.basics import (
     Array, Bounds, Error, Mapping, RecArray, SolverStats, format_number, format_seconds, format_table, generate_items,
     get_indices, output, output_progress, update_matrices
 )
 from ..utilities.statistics import (
-    compute_gmm_moment_covariances, compute_gmm_moments_mean, compute_gmm_parameter_covariances,
-    compute_gmm_moments_jacobian_mean, compute_gmm_weights
+    compute_gmm_moment_covariances, compute_gmm_parameter_covariances, compute_gmm_moments_jacobian_mean,
+    compute_gmm_weights, compute_sigma_squared_vector_covariances
 )
 
 
 # only import objects that create import cycles when checking types
 if TYPE_CHECKING:
     from .bootstrapped_results import BootstrappedResults  # noqa
     from .importance_sampling_results import ImportanceSamplingResults  # noqa
     from .optimal_instrument_results import OptimalInstrumentResults  # noqa
     from ..economies.problem import Progress  # noqa
 
 
-class ProblemResults(Results):
+class ProblemResults(EconomyResults):
     r"""Results of a solved BLP problem.
 
     Many results are class attributes. Other post-estimation outputs be computed by calling class methods.
 
     .. note::
 
        Methods in this class that compute one or more post-estimation output per market support :func:`parallel`
@@ -71,53 +74,59 @@
     cumulative_optimization_iterations : `int`
         Sum of :attr:`ProblemResults.optimization_iterations` for this step and all prior steps.
     objective_evaluations : `int`
         Number of GMM objective evaluations.
     cumulative_objective_evaluations : `int`
         Sum of :attr:`ProblemResults.objective_evaluations` for this step and all prior steps.
     fp_converged : `ndarray`
-        Flags for convergence of the iteration routine used to compute :math:`\delta(\hat{\theta})` in each market
-        during each objective evaluation. Rows are in the same order as :attr:`Problem.unique_market_ids` and column
-        indices correspond to objective evaluations.
+        Flags for convergence of the iteration routine used to compute :math:`\delta(\theta)` in each market during each
+        objective evaluation. Rows are in the same order as :attr:`Problem.unique_market_ids` and column indices
+        correspond to objective evaluations.
     cumulative_fp_converged : `ndarray`
         Concatenation of :attr:`ProblemResults.fp_converged` for this step and all prior steps.
     fp_iterations : `ndarray`
-        Number of major iterations completed by the iteration routine used to compute :math:`\delta(\hat{\theta})` in
-        each market during each objective evaluation. Rows are in the same order as
-        :attr:`Problem.unique_market_ids` and column indices correspond to objective evaluations.
+        Number of major iterations completed by the iteration routine used to compute :math:`\delta(\theta)` in each
+        market during each objective evaluation. Rows are in the same order as :attr:`Problem.unique_market_ids` and
+        column indices correspond to objective evaluations.
     cumulative_fp_iterations : `ndarray`
         Concatenation of :attr:`ProblemResults.fp_iterations` for this step and all prior steps.
     contraction_evaluations : `ndarray`
-        Number of times the contraction used to compute :math:`\delta(\hat{\theta})` was evaluated in each market during
-        each objective evaluation. Rows are in the same order as :attr:`Problem.unique_market_ids` and column
-        indices correspond to objective evaluations.
+        Number of times the contraction used to compute :math:`\delta(\theta)` was evaluated in each market during each
+        objective evaluation. Rows are in the same order as :attr:`Problem.unique_market_ids` and column indices
+        correspond to objective evaluations.
     cumulative_contraction_evaluations : `ndarray`
         Concatenation of :attr:`ProblemResults.contraction_evaluations` for this step and all prior steps.
     parameters : `ndarray`
         Stacked parameters in the following order: :math:`\hat{\theta}`, concentrated out elements of
         :math:`\hat{\beta}`, and concentrated out elements of :math:`\hat{\gamma}`.
     parameter_covariances : `ndarray`
-        Estimated covariance matrix of the stacked parameters, from which standard errors are extracted. Parameter
-        covariances are not estimated during the first step of two-step GMM.
+        Estimated asymptotic covariance matrix for :math:`\sqrt{N}(\hat{\theta} - \theta_0)`, in which :math:`\theta`
+        are the stacked parameters. Standard errors are the square root of the diagonal of this matrix divided by
+        :math:`N`. Parameter covariances are not estimated during the first step of two-step GMM.
     theta : `ndarray`
         Estimated unfixed parameters, :math:`\hat{\theta}`, in the following order: :math:`\hat{\Sigma}`,
         :math:`\hat{\Pi}`, :math:`\hat{\rho}`, non-concentrated out elements from :math:`\hat{\beta}`, and
         non-concentrated out elements from :math:`\hat{\gamma}`.
     sigma : `ndarray`
         Estimated Cholesky root of the covariance matrix for unobserved taste heterogeneity, :math:`\hat{\Sigma}`.
+    sigma_squared : `ndarray`
+        Estimated covariance matrix for unobserved taste heterogeneity, :math:`\hat{\Sigma}\hat{\Sigma}'`.
     pi : `ndarray`
         Estimated parameters that measures how agent tastes vary with demographics, :math:`\hat{\Pi}`.
     rho : `ndarray`
         Estimated parameters that measure within nesting group correlations, :math:`\hat{\rho}`.
     beta : `ndarray`
         Estimated demand-side linear parameters, :math:`\hat{\beta}`.
     gamma : `ndarray`
         Estimated supply-side linear parameters, :math:`\hat{\gamma}`.
     sigma_se : `ndarray`
         Estimated standard errors for :math:`\hat{\Sigma}`, which are not estimated in the first step of two-step GMM.
+    sigma_squared_se : `ndarray`
+        Estimated standard errors for :math:`\hat{\Sigma}\hat{\Sigma}'`, which are computed with the delta method, and
+        are not estimated in the first step of two-step GMM.
     pi_se : `ndarray`
         Estimated standard errors for :math:`\hat{\Pi}`, which are not estimated in the first step of two-step GMM.
     rho_se : `ndarray`
         Estimated standard errors for :math:`\hat{\rho}`, which are not estimated in the first step of two-step GMM.
     beta_se : `ndarray`
         Estimated standard errors for :math:`\hat{\beta}`, which are not estimated in the first step of two-step GMM.
     gamma_se : `ndarray`
@@ -128,32 +137,67 @@
         Bounds for :math:`\Pi` that were used during optimization, which are of the form ``(lb, ub)``.
     rho_bounds : `tuple`
         Bounds for :math:`\rho` that were used during optimization, which are of the form ``(lb, ub)``.
     beta_bounds : `tuple`
         Bounds for :math:`\beta` that were used during optimization, which are of the form ``(lb, ub)``.
     gamma_bounds : `tuple`
         Bounds for :math:`\gamma` that were used during optimization, which are of the form ``(lb, ub)``.
+    sigma_labels : `list of str`
+        Variable labels for rows and columns of :math:`\Sigma`, which are derived from the formulation for :math:`X_2`.
+    pi_labels : `list of str`
+        Variable labels for columns of :math:`\Pi`, which are derived from the formulation for demographics.
+    rho_labels : `list of str`
+        Variable labels for :math:`\rho`. If :math:`\rho` is not a scalar, this is :attr:`Problem.unique_nesting_ids`.
+    beta_labels : `list of str`
+        Variable labels for :math:`\beta`, which are derived from the formulation for :math:`X_1`.
+    gamma_labels : `list of str`
+        Variable labels for :math:`\gamma`, which are derived from the formulation for :math:`X_3`.
+    theta_labels : `list of str`
+        Variable labels for :math:`\theta`, which are derived from the above labels.
     delta : `ndarray`
         Estimated mean utility, :math:`\delta(\hat{\theta})`.
+    clipped_shares : `ndarray`
+        Vector of booleans indicator whether the associated simulated shares were clipped during the last fixed point
+        iteration to compute :math:`\delta(\hat{\theta})`. All elements will be ``False`` if ``shares_bounds`` in
+        :meth:`Problem.solve` is disabled (by default shares are bounded from below by a small number to alleviate
+        issues with underflow and negative shares).
     tilde_costs : `ndarray`
         Estimated transformed marginal costs, :math:`\tilde{c}(\hat{\theta})` from :eq:`costs`. If ``costs_bounds`` were
         specified in :meth:`Problem.solve`, :math:`c` may have been clipped.
     clipped_costs : `ndarray`
         Vector of booleans indicating whether the associated marginal costs were clipped. All elements will be ``False``
         if ``costs_bounds`` in :meth:`Problem.solve` was not specified.
     xi : `ndarray`
         Estimated unobserved demand-side product characteristics, :math:`\xi(\hat{\theta})`, or equivalently, the
         demand-side structural error term. When there are demand-side fixed effects, this is
         :math:`\Delta\xi(\hat{\theta})` in :eq:`fe`. That is, fixed effects are not included.
     omega : `ndarray`
         Estimated unobserved supply-side product characteristics, :math:`\omega(\hat{\theta})`, or equivalently, the
         supply-side structural error term. When there are supply-side fixed effects, this is
         :math:`\Delta\omega(\hat{\theta})` in :eq:`fe`. That is, fixed effects are not included.
+    xi_fe : `ndarray`
+        Estimated demand-side fixed effects :math:`\xi_{k_1} + \cdots \xi_{k_{E_D}}` in :eq:`fe`, which are only
+        computed when there are demand-side fixed effects.
+    omega_fe : `ndarray`
+        Estimated supply-side fixed effects :math:`\omega_{k_1} + \cdots \omega_{k_{E_D}}` in :eq:`fe`, which are only
+        computed when there are supply-side fixed effects.
     micro : `ndarray`
-        Averaged micro moments, :math:`\bar{g}_M`, in :eq:`averaged_micro_moments`.
+        Micro moments, :math:`\bar{g}_M`, in :eq:`micro_moment`.
+    micro_values : `ndarray`
+        Estimated micro moment values, :math:`f_m(v)`. Rows are in the same order as :attr:`ProblemResults.micro`.
+    micro_covariances : `ndarray`
+        Estimated micro moment covariance matrix :math:`S_M` in :eq:`scaled_micro_moment_covariances` divided by
+        :math:`N`. Equal to ``micro_sample_covariances`` if overridden in :meth:`Problem.solve`.
+    moments : `ndarray`
+        Moments, :math:`\bar{g}`, in :eq:`averaged_moments`.
+    moments_jacobian : `ndarray`
+        Jacobian :math:`\bar{G}` of moments with respect to :math:`\theta`, in :eq:`averaged_moments_jacobian`.
+    simulation_covariances : `ndarray`
+        Adjustment in :eq:`simulation_S` to moment covariances to account for simulation error. This will be all zeros
+        unless ``resample_agent_data`` was specified in :meth:`Problem.solve`.
     objective : `float`
         GMM objective value, :math:`q(\hat{\theta})`, defined in :eq:`objective`. If ``scale_objective`` was ``True`` in
         :meth:`Problem.solve` (which is the default), this value was scaled by :math:`N` so that objective values are
         more comparable across different problem sizes. Note that in some of the BLP literature (and earlier versions of
         this package), this expression was previously scaled by :math:`N^2`.
     xi_by_theta_jacobian : `ndarray`
         Estimated :math:`\frac{\partial\xi}{\partial\theta} = \frac{\partial\delta}{\partial\theta}`, which is used to
@@ -211,83 +255,128 @@
     cumulative_fp_iterations: Array
     contraction_evaluations: Array
     cumulative_contraction_evaluations: Array
     parameters: Array
     parameter_covariances: Array
     theta: Array
     sigma: Array
+    sigma_squared: Array
     pi: Array
     rho: Array
     beta: Array
     gamma: Array
     sigma_se: Array
+    sigma_squared_se: Array
     pi_se: Array
     rho_se: Array
     beta_se: Array
     gamma_se: Array
     sigma_bounds: Bounds
     pi_bounds: Bounds
     rho_bounds: Bounds
     beta_bounds: Bounds
     gamma_bounds: Bounds
+    sigma_labels: List[str]
+    pi_labels: List[str]
+    rho_labels: List[str]
+    beta_labels: List[str]
+    gamma_labels: List[str]
+    theta_labels: List[str]
     delta: Array
+    clipped_shares: Array
     tilde_costs: Array
     clipped_costs: Array
     xi: Array
     omega: Array
+    xi_fe: Array
+    omega_fe: Array
     micro: Array
+    micro_values: Array
+    micro_covariances: Array
+    moments: Array
+    moments_jacobian: Array
+    simulation_covariances: Array
     objective: Array
     xi_by_theta_jacobian: Array
     omega_by_theta_jacobian: Array
     micro_by_theta_jacobian: Array
     gradient: Array
     projected_gradient: Array
     projected_gradient_norm: Array
     hessian: Array
     reduced_hessian: Array
     reduced_hessian_eigenvalues: Array
     W: Array
     updated_W: Array
     _scaled_objective: bool
+    _shares_bounds: Bounds
     _costs_bounds: Bounds
     _se_type: str
+    _formatted_moments: Optional[str]
     _errors: List[Error]
 
     def __init__(
             self, progress: 'Progress', last_results: Optional['ProblemResults'], step: int, last_step: bool,
             step_start_time: float, optimization_start_time: float, optimization_end_time: float,
             optimization_stats: SolverStats, iteration_stats: Sequence[Dict[Hashable, SolverStats]],
-            scaled_objective: bool, iteration: Iteration, fp_type: str, costs_bounds: Bounds,
-            extra_micro_covariances: Optional[Array], center_moments: bool, W_type: str, se_type: str) -> None:
+            scaled_objective: bool, shares_bounds: Bounds, costs_bounds: Bounds,
+            micro_moment_covariances: Optional[Array], center_moments: bool, W_type: str, se_type: str) -> None:
         """Compute cumulative progress statistics, update weighting matrices, and estimate standard errors."""
-        super().__init__(progress.problem, progress.parameters, progress.moments, iteration, fp_type)
+        self.sigma, self.pi, self.rho, _, _ = progress.parameters.expand(progress.theta)
         self._errors = progress.errors
         self.problem = progress.problem
         self.W = progress.W
         self.theta = progress.theta
         self.delta = progress.delta
         self.tilde_costs = progress.tilde_costs
         self.micro = progress.micro
+        self.micro_values = progress.micro_values
+        self.moments = progress.mean_g
+        self.simulation_covariances = progress.simulation_covariances
         self.xi_by_theta_jacobian = progress.xi_jacobian
         self.omega_by_theta_jacobian = progress.omega_jacobian
         self.micro_by_theta_jacobian = progress.micro_jacobian
         self.xi = progress.xi
         self.omega = progress.omega
         self.beta = progress.beta
         self.gamma = progress.gamma
         self.objective = progress.objective
         self.gradient = progress.gradient
         self.projected_gradient = progress.projected_gradient
         self.projected_gradient_norm = progress.projected_gradient_norm
         self.hessian = progress.hessian
         self.reduced_hessian = progress.reduced_hessian
+        self.clipped_shares = progress.clipped_shares
         self.clipped_costs = progress.clipped_costs
         self._scaled_objective = scaled_objective
+        self._shares_bounds = shares_bounds
         self._costs_bounds = costs_bounds
         self._se_type = se_type
+        super().__init__(
+            self.problem, progress.parameters, self.sigma, self.pi, self.rho, self.beta, self.gamma, self.delta
+        )
+
+        # if there are any fixed effects, compute their contributions to xi and omega
+        self.xi_fe = np.zeros_like(self.xi)
+        self.omega_fe = np.zeros_like(self.omega)
+        if self.problem.ED > 0:
+            self.xi_fe = self.delta - self.problem._compute_true_X1() @ self.beta - self.xi
+        if self.problem.ES > 0:
+            self.omega_fe = self.tilde_costs - self.problem._compute_true_X3() @ self.gamma - self.omega
+
+        # either use the manually-specified micro moment covariances or estimated ones
+        self.micro_covariances = micro_moment_covariances
+        if micro_moment_covariances is None:
+            self.micro_covariances = progress.micro_covariances
+
+        # format micro moments for displaying information (micro moments themselves often contain lambda functions and
+        #   are hence often not serializable)
+        self._formatted_moments = None
+        if progress.moments.MM > 0:
+            self._formatted_moments = progress.moments.format("Estimated Micro Moments", self.micro_values)
 
         # if the reduced Hessian was computed, compute its eigenvalues and the ratio of the smallest to largest ones
         self.reduced_hessian_eigenvalues = np.full(self._parameters.P, np.nan, options.dtype)
         if self._parameters.P > 0 and np.isfinite(self.reduced_hessian).all():
             self.reduced_hessian_eigenvalues, successful = precisely_compute_eigenvalues(self.reduced_hessian)
             if not successful:
                 self._errors.append(exceptions.HessianEigenvaluesError(self.reduced_hessian))
@@ -297,23 +386,23 @@
         self.total_time = self.cumulative_total_time = time.time() - step_start_time
         self.optimization_time = self.cumulative_optimization_time = optimization_end_time - optimization_start_time
         self.converged = self.cumulative_converged = optimization_stats.converged
         self.optimization_iterations = self.cumulative_optimization_iterations = optimization_stats.iterations
         self.objective_evaluations = self.cumulative_objective_evaluations = optimization_stats.evaluations
         self.fp_converged = self.cumulative_fp_converged = np.array(
             [[m[t].converged if m else True for m in iteration_stats] for t in self.problem.unique_market_ids],
-            dtype=np.int
+            dtype=np.int64
         )
         self.fp_iterations = self.cumulative_fp_iterations = np.array(
             [[m[t].iterations if m else 0 for m in iteration_stats] for t in self.problem.unique_market_ids],
-            dtype=np.int
+            dtype=np.int64
         )
         self.contraction_evaluations = self.cumulative_contraction_evaluations = np.array(
             [[m[t].evaluations if m else 0 for m in iteration_stats] for t in self.problem.unique_market_ids],
-            dtype=np.int
+            dtype=np.int64
         )
 
         # initialize last results and add to cumulative values
         self.last_results = last_results
         if last_results is not None:
             self.cumulative_total_time += last_results.cumulative_total_time
             self.cumulative_optimization_time += last_results.cumulative_optimization_time
@@ -327,71 +416,85 @@
                 last_results.cumulative_fp_iterations, self.cumulative_fp_iterations
             ]
             self.cumulative_contraction_evaluations = np.c_[
                 last_results.cumulative_contraction_evaluations, self.cumulative_contraction_evaluations
             ]
 
         # store estimated parameters and information about them (beta and gamma have already been stored above)
-        self.sigma, self.pi, self.rho, _, _ = self._parameters.expand(self.theta)
+        self.sigma_squared = self.sigma @ self.sigma.T
         self.parameters = np.c_[np.r_[
             self.theta,
             self.beta[self._parameters.eliminated_beta_index],
             self.gamma[self._parameters.eliminated_gamma_index]
         ]]
         self.sigma_bounds = self._parameters.sigma_bounds
         self.pi_bounds = self._parameters.pi_bounds
         self.rho_bounds = self._parameters.rho_bounds
         self.beta_bounds = self._parameters.beta_bounds
         self.gamma_bounds = self._parameters.gamma_bounds
+        self.sigma_labels = self._parameters.sigma_labels
+        self.pi_labels = self._parameters.pi_labels
+        self.rho_labels = self._parameters.rho_labels
+        self.beta_labels = self._parameters.beta_labels
+        self.gamma_labels = self._parameters.gamma_labels
+        self.theta_labels = self._parameters.theta_labels
 
         # ignore computational errors when updating the weighting matrix and computing covariances
         with np.errstate(all='ignore'):
             # update the weighting matrix
-            micro_covariances = progress.micro_covariances.copy()
-            if extra_micro_covariances is not None:
-                micro_covariances += extra_micro_covariances
-            S_for_weights = self._compute_S(micro_covariances, W_type, center_moments)
+            S_for_weights = self._compute_S(progress.moments, W_type, center_moments)
             self.updated_W, W_errors = compute_gmm_weights(S_for_weights)
             self._errors.extend(W_errors)
 
             # only compute parameter covariances and standard errors if this is the last step
+            self.moments_jacobian = np.full((self.moments.size, self.parameters.size), np.nan, options.dtype)
             self.parameter_covariances = np.full((self.parameters.size, self.parameters.size), np.nan, options.dtype)
             se = np.full((self.parameters.size, 1), np.nan, options.dtype)
+            sigma_squared_vector_se = np.full((self.problem.K2 * (self.problem.K2 + 1) // 2, 1), np.nan, options.dtype)
             if last_step:
                 S_for_covariances = S_for_weights
                 if se_type != W_type or center_moments:
-                    S_for_covariances = self._compute_S(micro_covariances, se_type)
+                    S_for_covariances = self._compute_S(progress.moments, se_type)
 
                 # if this is the first step, an unadjusted weighting matrix needs to be used when computing unadjusted
                 #   covariances so that they are scaled properly
                 W_for_covariances = self.W
                 if se_type == 'unadjusted' and self.step == 1:
                     W_for_covariances, W_for_covariances_errors = compute_gmm_weights(S_for_covariances)
                     self._errors.extend(W_for_covariances_errors)
 
                 # compute parameter covariances
-                mean_G = self._compute_mean_G()
+                self.moments_jacobian = self._compute_mean_G(progress.moments)
                 self.parameter_covariances, se_errors = compute_gmm_parameter_covariances(
-                    W_for_covariances, S_for_covariances, mean_G, se_type
+                    W_for_covariances, S_for_covariances, self.moments_jacobian, se_type
                 )
                 self._errors.extend(se_errors)
 
+                # use the delta method to compute covariances for the parameters in sigma squared
+                theta_covariances = self.parameter_covariances[:self._parameters.P, :self._parameters.P]
+                sigma_vector_covariances = self._parameters.extract_sigma_vector_covariances(theta_covariances)
+                sigma_squared_vector_covariances = compute_sigma_squared_vector_covariances(
+                    self.sigma, sigma_vector_covariances
+                )
+
                 # compute standard errors
                 se = np.sqrt(np.c_[self.parameter_covariances.diagonal()] / self.problem.N)
-                if np.isnan(se).any():
+                sigma_squared_vector_se = np.sqrt(np.c_[sigma_squared_vector_covariances.diagonal()] / self.problem.N)
+                if np.isnan(se).any() or np.isnan(sigma_squared_vector_se).any():
                     self._errors.append(exceptions.InvalidParameterCovariancesError())
 
         # expand standard errors
         theta_se, eliminated_beta_se, eliminated_gamma_se = np.split(se, [
             self._parameters.P,
             self._parameters.P + self._parameters.eliminated_beta_index.sum()
         ])
         self.sigma_se, self.pi_se, self.rho_se, self.beta_se, self.gamma_se = (
             self._parameters.expand(theta_se, nullify=True)
         )
+        self.sigma_squared_se = vech_to_full(sigma_squared_vector_se, self.problem.K2)
         self.beta_se[self._parameters.eliminated_beta_index] = eliminated_beta_se.flatten()
         self.gamma_se[self._parameters.eliminated_gamma_index] = eliminated_gamma_se.flatten()
 
     def __str__(self) -> str:
         """Format problem results as a string."""
         sections = [self._format_summary(), self._format_cumulative_statistics()]
 
@@ -403,33 +506,24 @@
         else:
             assert self._se_type == 'clustered'
             se_description = f'Robust SEs Adjusted for {np.unique(self.problem.products.clustering_ids).size} Clusters'
 
         # add sections formatting estimates and micro moments values
         sections.append(self._parameters.format_estimates(
             f"Estimates ({se_description} in Parentheses)", self.sigma, self.pi, self.rho, self.beta, self.gamma,
-            self.sigma_se, self.pi_se, self.rho_se, self.beta_se, self.gamma_se
+            self.sigma_squared, self.sigma_se, self.pi_se, self.rho_se, self.beta_se, self.gamma_se,
+            self.sigma_squared_se
         ))
-        if self._moments.MM > 0:
-            sections.append(self._moments.format("Micro Moment Values", self.micro))
+        if self._formatted_moments is not None:
+            sections.append(self._formatted_moments)
 
         # join the sections into a single string
         return "\n\n".join(sections)
 
-    def _compute_mean_g(self) -> Array:
-        """Compute moments."""
-        u_list = [self.xi]
-        Z_list = [self.problem.products.ZD]
-        if self.problem.K3 > 0:
-            u_list.append(self.omega)
-            Z_list.append(self.problem.products.ZS)
-        mean_g = np.r_[compute_gmm_moments_mean(u_list, Z_list), self.micro]
-        return mean_g
-
-    def _compute_mean_G(self) -> Array:
+    def _compute_mean_G(self, moments: Moments) -> Array:
         """Compute the Jacobian of moments with respect to parameters."""
         Z_list = [self.problem.products.ZD]
         jacobian_list = [np.c_[
             self.xi_by_theta_jacobian,
             -self.problem.products.X1[:, self._parameters.eliminated_beta_index.flat],
             np.zeros_like(self.problem.products.X3[:, self._parameters.eliminated_gamma_index.flat])
         ]]
@@ -440,46 +534,52 @@
                 np.zeros_like(self.problem.products.X1[:, self._parameters.eliminated_beta_index.flat]),
                 -self.problem.products.X3[:, self._parameters.eliminated_gamma_index.flat]
             ])
         mean_G = np.r_[
             compute_gmm_moments_jacobian_mean(jacobian_list, Z_list),
             np.c_[
                 self.micro_by_theta_jacobian,
-                np.zeros((self._moments.MM, self._parameters.eliminated_beta_index.sum()), options.dtype),
-                np.zeros((self._moments.MM, self._parameters.eliminated_gamma_index.sum()), options.dtype)
+                np.zeros((moments.MM, self._parameters.eliminated_beta_index.sum()), options.dtype),
+                np.zeros((moments.MM, self._parameters.eliminated_gamma_index.sum()), options.dtype)
             ]
         ]
         return mean_G
 
-    def _compute_S(self, micro_covariances: Array, S_type: str, center_moments: bool = False) -> Array:
+    def _compute_S(self, moments: Moments, S_type: str, center_moments: bool = False) -> Array:
         """Compute moment covariances."""
         u_list = [self.xi]
         Z_list = [self.problem.products.ZD]
         if self.problem.K3 > 0:
             u_list.append(self.omega)
             Z_list.append(self.problem.products.ZS)
+
         S = compute_gmm_moment_covariances(u_list, Z_list, S_type, self.problem.products.clustering_ids, center_moments)
-        if self._moments.MM > 0:
-            S = scipy.linalg.block_diag(S, micro_covariances)
+        self.problem._detect_singularity(S, "the estimated covariance matrix of aggregate GMM moments")
+        if moments.MM > 0:
+            S = scipy.linalg.block_diag(S, self.problem.N * self.micro_covariances)
+
+        S += self.simulation_covariances
         return S
 
     def _format_summary(self) -> str:
         """Format a summary table of problem results."""
 
         # construct the leftmost part of the table that always shows up
         header = [("GMM", "Step"), ("Objective", "Value")]
         values = [self.step, format_number(self.objective)]
 
-        # add information about first and second order conditions
+        # add information about first order conditions
         if np.isfinite(self.projected_gradient_norm):
             if self._parameters.any_bounds:
                 header.append(("Projected", "Gradient Norm"))
             else:
                 header.append(("Gradient", "Norm"))
             values.append(format_number(self.projected_gradient_norm))
+
+        # add information about second order conditions
         if np.isfinite(self.reduced_hessian_eigenvalues).any():
             hessian_type = "Reduced" if self._parameters.any_bounds else ""
             if self.reduced_hessian_eigenvalues.size == 1:
                 header.append((hessian_type, "Hessian"))
                 values.append(format_number(self.reduced_hessian))
             else:
                 header.extend([
@@ -487,22 +587,26 @@
                     (f"{hessian_type} Hessian", "Max Eigenvalue")
                 ])
                 values.extend([
                     format_number(self.reduced_hessian_eigenvalues.min()),
                     format_number(self.reduced_hessian_eigenvalues.max())
                 ])
 
-        # add a count of any clipped marginal costs
+        # add counts of any clipped shares or marginal costs
+        if np.isfinite(self._shares_bounds).any():
+            header.append(("Clipped", "Shares"))
+            values.append(self.clipped_shares.sum())
         if np.isfinite(self._costs_bounds).any():
             header.append(("Clipped", "Costs"))
             values.append(self.clipped_costs.sum())
 
-        # add information about the weighting matrix
-        header.append(("Weighting Matrix", "Condition Number"))
-        values.append(format_number(compute_condition_number(self.W)))
+        # add information about the weighting matrix if this isn't an initial update
+        if self.step > 0:
+            header.append(("Weighting Matrix", "Condition Number"))
+            values.append(format_number(compute_condition_number(self.W)))
 
         # add information about the covariance matrix
         if np.isfinite(self.parameter_covariances).any() and self.parameter_covariances.size > 1:
             header.append(("Covariance Matrix", "Condition Number"))
             values.append(format_number(compute_condition_number(self.parameter_covariances)))
 
         return format_table(header, values, title="Problem Results Summary")
@@ -510,48 +614,61 @@
     def _format_cumulative_statistics(self) -> str:
         """Format a table of cumulative statistics."""
 
         # construct the leftmost part of the top table that always shows up
         header = [("Computation", "Time")]
         values = [format_seconds(self.cumulative_total_time)]
 
-        # add optimization iterations
+        # add optimization convergence and iterations
         if self._parameters.P > 0:
-            header.append(("Optimization", "Iterations"))
-            values.append(str(self.cumulative_optimization_iterations))
+            header.extend([("Optimizer", "Converged"), ("Optimization", "Iterations")])
+            values.extend(["Yes" if self.cumulative_converged else "No", str(self.cumulative_optimization_iterations)])
 
         # add evaluations and iterations
         header.append(("Objective", "Evaluations"))
         values.append(str(self.cumulative_objective_evaluations))
         if np.any(self.cumulative_contraction_evaluations > 0):
             header.extend([("Fixed Point", "Iterations"), ("Contraction", "Evaluations")])
             values.extend([
                 str(self.cumulative_fp_iterations.sum()),
                 str(self.cumulative_contraction_evaluations.sum())]
             )
 
         return format_table(header, values, title="Cumulative Statistics")
 
+    def to_pickle(self, path: Union[str, Path]) -> None:
+        """Save these results as a pickle file.
+
+        Parameters
+        ----------
+        path: `str or Path`
+            File path to which these results will be saved.
+
+        """
+        with open(path, 'wb') as handle:
+            pickle.dump(self, handle)
+
     def to_dict(
             self, attributes: Sequence[str] = (
                 'step', 'optimization_time', 'cumulative_optimization_time', 'total_time', 'cumulative_total_time',
                 'converged', 'cumulative_converged', 'optimization_iterations', 'cumulative_optimization_iterations',
                 'objective_evaluations', 'cumulative_objective_evaluations', 'fp_converged', 'cumulative_fp_converged',
                 'fp_iterations', 'cumulative_fp_iterations', 'contraction_evaluations',
-                'cumulative_contraction_evaluations', 'parameters', 'parameter_covariances', 'theta', 'sigma', 'pi',
-                'rho', 'beta', 'gamma', 'sigma_se', 'pi_se', 'rho_se', 'beta_se', 'gamma_se', 'sigma_bounds',
-                'pi_bounds', 'rho_bounds', 'beta_bounds', 'gamma_bounds', 'delta', 'tilde_costs', 'clipped_costs', 'xi',
-                'omega', 'micro', 'objective', 'xi_by_theta_jacobian', 'omega_by_theta_jacobian',
-                'micro_by_theta_jacobian', 'gradient', 'projected_gradient', 'projected_gradient_norm', 'hessian',
-                'reduced_hessian', 'reduced_hessian_eigenvalues', 'W', 'updated_W'
+                'cumulative_contraction_evaluations', 'parameters', 'parameter_covariances', 'theta', 'sigma',
+                'sigma_squared', 'pi', 'rho', 'beta', 'gamma', 'sigma_se', 'sigma_squared_se', 'pi_se', 'rho_se',
+                'beta_se', 'gamma_se', 'sigma_bounds', 'pi_bounds', 'rho_bounds', 'beta_bounds', 'gamma_bounds',
+                'sigma_labels', 'pi_labels', 'rho_labels', 'beta_labels', 'gamma_labels', 'theta_labels', 'delta',
+                'tilde_costs', 'clipped_shares', 'clipped_costs', 'xi', 'omega', 'xi_fe', 'omega_fe', 'micro',
+                'micro_values', 'micro_covariances', 'moments', 'moments_jacobian', 'simulation_covariances',
+                'objective', 'xi_by_theta_jacobian', 'omega_by_theta_jacobian', 'micro_by_theta_jacobian', 'gradient',
+                'projected_gradient', 'projected_gradient_norm', 'hessian', 'reduced_hessian',
+                'reduced_hessian_eigenvalues', 'W', 'updated_W'
             )) -> dict:
         """Convert these results into a dictionary that maps attribute names to values.
 
-        Once converted to a dictionary, these results can be saved to a file with :func:`pickle.dump`.
-
         Parameters
         ----------
         attributes : `sequence of str, optional`
             Name of attributes that will be added to the dictionary. By default, all :class:`ProblemResults` attributes
             are added except for :attr:`ProblemResults.problem` and :attr:`ProblemResults.last_results`.
 
         Returns
@@ -605,23 +722,23 @@
         return (1 if self._scaled_objective else self.problem.N) * float(self.objective)
 
     def run_distance_test(self, unrestricted: 'ProblemResults') -> float:
         r"""Test the validity of model restrictions with the distance test.
 
         Following :ref:`references:Newey and West (1987)`, the distance or likelihood ratio-like statistic is
 
-        .. math:: \text{LR} = J(\hat{\theta^r}) - J(\hat{\theta^u})
+        .. math:: \text{LR} = J(\hat{\theta}^r) - J(\hat{\theta}^u)
 
-        where :math:`J(\hat{\theta^r})` is the :math:`J` statistic defined in :eq:`J` for this restricted model and
-        :math:`J(\hat{\theta^u})` is the :math:`J` statistic for the unrestricted model.
+        where :math:`J(\hat{\theta}^r)` is the :math:`J` statistic defined in :eq:`J` for this restricted model and
+        :math:`J(\hat{\theta}^u)` is the :math:`J` statistic for the unrestricted model.
 
         .. note::
 
            The statistic can equivalently be written as
-           :math:`\text{LR} = N[q(\hat{\theta^r}) - q(\hat{\theta^u})]` where the GMM objective value is defined in
+           :math:`\text{LR} = N[q(\hat{\theta}^r) - q(\hat{\theta}^u)]` where the GMM objective value is defined in
            :eq:`objective`, or the same but without the :math:`N` if the GMM objective value was scaled by :math:`N`,
            which is the default behavior.
 
         If the restrictions in this model are valid, the distance statistic is asymptotically :math:`\chi^2` with
         degrees of freedom equal to the number of restrictions.
 
         .. warning::
@@ -659,15 +776,15 @@
 
         .. math::
 
            \text{LM} = N\bar{g}(\hat{\theta})'W\bar{G}(\hat{\theta})V\bar{G}(\hat{\theta})'W\bar{g}(\hat{\theta})
 
         where :math:`\bar{g}(\hat{\theta})` is defined in :eq:`averaged_moments`, :math:`\bar{G}(\hat{\theta})` is
         defined in :eq:`averaged_moments_jacobian`, :math:`W` is the optimal weighting matrix in :eq:`W`, and :math:`V`
-        is the covariance matrix of parameters in :eq:`covariances`.
+        is the covariance matrix of :math:`\sqrt{N}(\hat{\theta} - \theta)` in :eq:`covariances`.
 
         If the restrictions in this model are valid, the Lagrange multiplier statistic is asymptotically :math:`\chi^2`
         with degrees of freedom equal to the number of restrictions.
 
         .. warning::
 
            This test requires :attr:`ProblemResults.W` to be an optimal weighting matrix, so it should typically be run
@@ -679,29 +796,27 @@
             The Lagrange multiplier statistic.
 
         Examples
         --------
             - :doc:`Tutorial </tutorial>`
 
         """
-        mean_g = self._compute_mean_g()
-        mean_G = self._compute_mean_G()
-        gradient = mean_G.T @ self.W @ mean_g
+        gradient = self.moments_jacobian.T @ self.W @ self.moments
         return self.problem.N * float(gradient.T @ self.parameter_covariances @ gradient)
 
     def run_wald_test(self, restrictions: Any, restrictions_jacobian: Any) -> float:
         r"""Test the validity of model restrictions with the Wald test.
 
         Following :ref:`references:Newey and West (1987)`, the Wald statistic is
 
         .. math:: \text{Wald} = Nr(\hat{\theta})'[R(\hat{\theta})VR(\hat{\theta})']^{-1}r(\hat{\theta})
 
         where the restrictions are :math:`r(\theta) = 0` under the test's null hypothesis, their Jacobian is
         :math:`R(\theta) = \frac{\partial r(\theta)}{\partial\theta}`, and :math:`V` is the covariance matrix of
-        parameters in :eq:`covariances`.
+        :math:`\sqrt{N}(\hat{\theta} - \theta)` in :eq:`covariances`.
 
         If the restrictions are valid, the Wald statistic is asymptotically :math:`\chi^2` with degrees of freedom equal
         to the number of restrictions.
 
         Parameters
         ----------
         restrictions : `array-like`
@@ -737,51 +852,58 @@
         matrix = restrictions_jacobian @ self.parameter_covariances @ restrictions_jacobian.T
         inverted, replacement = approximately_invert(matrix)
         if replacement:
             output(exceptions.WaldInversionError(matrix, replacement))
         return self.problem.N * float(restrictions.T @ inverted @ restrictions)
 
     def bootstrap(
-            self, draws: int = 1000, seed: Optional[int] = None, iteration: Optional[Iteration] = None) -> (
-            'BootstrappedResults'):
+            self, draws: int = 1000, seed: Optional[int] = None, iteration: Optional[Iteration] = None,
+            constant_costs: bool = True) -> 'BootstrappedResults':
         r"""Use a parametric bootstrap to create an empirical distribution of results.
 
         The constructed :class:`BootstrappedResults` can be used just like :class:`ProblemResults` to compute various
         post-estimation outputs for different markets. The only difference is that :class:`BootstrappedResults` methods
         return arrays with an extra first dimension, along which bootstrapped results are stacked. These stacked results
         can be used to construct, for example, confidence intervals for post-estimation outputs.
 
         For each bootstrap draw, parameters are drawn from the estimated multivariate normal distribution of all
-        parameters defined by :attr:`ProblemResults.parameters` and :attr:`ProblemResults.parameter_covariances`. Any
-        bounds configured in :meth:`Problem.solve` will also bound parameter draws. Each parameter draw is used to
-        compute the implied mean utility, :math:`\delta`, and shares, :math:`s`. If a supply side was estimated, the
-        implied marginal costs, :math:`c`, and prices, :math:`p`, are computed as well by iterating over the
-        :math:`\zeta`-markup contraction in :eq:`zeta_contraction`. If marginal costs depend on prices through
-        marketshares, they will be updated to reflect different prices during each iteration of the routine.
+        parameters defined by :attr:`ProblemResults.parameters` and :attr:`ProblemResults.parameter_covariances`
+        (where the second covariance matrix is divided by :math:`N`). Any bounds configured in :meth:`Problem.solve`
+        will also bound parameter draws. Each parameter draw is used to compute the implied mean utility,
+        :math:`\delta`, and shares, :math:`s`. If a supply side was estimated, the implied marginal costs, :math:`c`,
+        and prices, :math:`p`, are computed as well by iterating over the :math:`\zeta`-markup contraction in
+        :eq:`zeta_contraction`. If marginal costs depend on prices through market shares, they will be updated to
+        reflect different prices during each iteration of the routine.
 
         .. note::
 
            By default, parametric bootstrapping may use a lot of memory. This is because all bootstrapped results (for
            all ``draws``) are stored in memory at the same time. Memory usage can be reduced by calling this method in a
            loop with ``draws = 1``. In each iteration of the loop, compute the desired post-estimation output with the
            proper method of the returned :class:`BootstrappedResults` class and store these outputs.
 
         Parameters
         ----------
         draws : `int, optional`
             The number of draws that will be taken from the joint distribution of the parameters. The default value is
             ``1000``.
         seed : `int, optional`
-            Passed to :class:`numpy.random.mtrand.RandomState` to seed the random number generator before any draws are
+            Passed to :class:`numpy.random.RandomState` to seed the random number generator before any draws are
             taken. By default, a seed is not passed to the random number generator.
         iteration : `Iteration, optional`
             :class:`Iteration` configuration used to compute bootstrapped prices by iterating over the
             :math:`\zeta`-markup equation in :eq:`zeta_contraction`. By default, if a supply side was estimated, this
             is ``Iteration('simple', {'atol': 1e-12})``. Analytic Jacobians are not supported for solving this system.
             This configuration is not used if a supply side was not estimated.
+        constant_costs : `bool, optional`
+            Whether to assume that marginal costs, :math:`c`, remain constant as equilibrium prices and shares change.
+            By default this is ``True``, which means that firms treat marginal costs as constant when setting prices.
+            If set to ``False``, marginal costs will be allowed to adjust if ``shares`` was included in
+            the formulation for :math:`X_3` in :class:`Problem`. This is not relevant if a supply side was not
+            estimated.
 
         Returns
         -------
         `BootstrappedResults`
             Computed :class:`BootstrappedResults`.
 
         Examples
@@ -808,15 +930,15 @@
             raise TypeError("iteration must be None or an iteration instance.")
         elif iteration._compute_jacobian:
             raise ValueError("Analytic Jacobians are not supported for solving this system.")
 
         # draw from the asymptotic distribution implied by the estimated parameters
         state = np.random.RandomState(seed)
         bootstrapped_parameters = np.atleast_3d(state.multivariate_normal(
-            self.parameters.flatten(), self.parameter_covariances, draws
+            self.parameters.flatten(), self.parameter_covariances / self.problem.N, draws
         ))
 
         # extract the parameters
         bootstrapped_sigma = np.zeros((draws, self.sigma.shape[0], self.sigma.shape[1]), options.dtype)
         bootstrapped_pi = np.zeros((draws, self.pi.shape[0], self.pi.shape[1]), options.dtype)
         bootstrapped_rho = np.zeros((draws, self.rho.shape[0], self.rho.shape[1]), options.dtype)
         bootstrapped_beta = np.zeros((draws, self.beta.shape[0], self.beta.shape[1]), options.dtype)
@@ -841,35 +963,36 @@
             bootstrapped_gamma[d] = np.clip(bootstrapped_gamma[d], *self.gamma_bounds)
 
         # pre-compute X1 and X3 without any absorbed fixed effects
         true_X1 = self.problem._compute_true_X1()
         true_X3 = self.problem._compute_true_X3()
 
         def market_factory(
-                pair: Tuple[int, Hashable]) -> Tuple[ResultsMarket, Array, Optional[Array], Optional[Iteration]]:
+                pair: Tuple[int, Hashable]) -> (
+                Tuple[EconomyResultsMarket, Array, Optional[Array], Optional[Iteration], bool]):
             """Build a market along with arguments used to compute equilibrium prices and shares along with delta."""
             c, s = pair
             indices_s = self.problem._product_market_indices[s]
-            market_cs = ResultsMarket(
+            market_cs = EconomyResultsMarket(
                 self.problem, s, self._parameters, bootstrapped_sigma[c], bootstrapped_pi[c], bootstrapped_rho[c],
                 bootstrapped_beta[c], bootstrapped_gamma[c], self.delta + true_X1 @ (bootstrapped_beta[c] - self.beta)
             )
             costs_cs = self.tilde_costs[indices_s] + true_X3[indices_s] @ (bootstrapped_gamma[c] - self.gamma)
             if self.problem.costs_type == 'log':
                 costs_cs = np.exp(costs_cs)
             prices_s = self.problem.products.prices[indices_s] if iteration is None else None
-            return market_cs, costs_cs, prices_s, iteration
+            return market_cs, costs_cs, prices_s, iteration, constant_costs
 
         # compute bootstrapped prices, shares, and deltas
         bootstrapped_prices = np.zeros((draws, self.problem.N, 1), options.dtype)
         bootstrapped_shares = np.zeros((draws, self.problem.N, 1), options.dtype)
         bootstrapped_delta = np.zeros((draws, self.problem.N, 1), options.dtype)
         iteration_stats: Dict[Hashable, SolverStats] = {}
         pairs = itertools.product(range(draws), self.problem.unique_market_ids)
-        generator = generate_items(pairs, market_factory, ResultsMarket.safely_solve_equilibrium_realization)
+        generator = generate_items(pairs, market_factory, EconomyResultsMarket.safely_solve_equilibrium_realization)
         for (d, t), (prices_dt, shares_dt, delta_dt, iteration_stats_dt, errors_dt) in generator:
             bootstrapped_prices[d, self.problem._product_market_indices[t]] = prices_dt
             bootstrapped_shares[d, self.problem._product_market_indices[t]] = shares_dt
             bootstrapped_delta[d, self.problem._product_market_indices[t]] = delta_dt
             iteration_stats[(d, t)] = iteration_stats_dt
             errors.extend(errors_dt)
 
@@ -889,15 +1012,16 @@
         output(f"Bootstrapped results after {format_seconds(results.computation_time)}.")
         output("")
         output(results)
         return results
 
     def compute_optimal_instruments(
             self, method: str = 'approximate', draws: int = 1, seed: Optional[int] = None,
-            expected_prices: Optional[Any] = None, iteration: Optional[Iteration] = None) -> 'OptimalInstrumentResults':
+            expected_prices: Optional[Any] = None, iteration: Optional[Iteration] = None,
+            constant_costs: bool = True) -> 'OptimalInstrumentResults':
         r"""Estimate feasible optimal or efficient instruments, :math:`Z_D^\text{opt}` and :math:`Z_S^\text{opt}`.
 
         Optimal instruments have been shown, for example, by :ref:`references:Reynaert and Verboven (2014)` and
         :ref:`references:Conlon and Gortmaker (2020)`, to reduce bias, improve efficiency, and enhance stability of BLP
         estimates.
 
         Optimal instruments in the spirit of :ref:`references:Amemiya (1977)` or :ref:`references:Chamberlain (1987)`
@@ -919,15 +1043,15 @@
 
         in which :math:`Z` are all exogenous variables.
 
         Feasible optimal instruments are estimated by evaluating this expression at an estimated :math:`\hat{\theta}`.
         The expectation is taken by approximating an integral over the joint density of :math:`\xi` and :math:`\omega`.
         For each error term realization, if not already estimated, equilibrium prices and shares are computed by
         iterating over the :math:`\zeta`-markup contraction in :eq:`zeta_contraction`. If marginal costs depend on
-        prices through marketshares, they will be updated to reflect different prices during each iteration of the
+        prices through market shares, they will be updated to reflect different prices during each iteration of the
         routine.
 
         The expected Jacobians are estimated with the average over all computed Jacobian realizations. The
         :math:`2 \times 2` normalizing matrix :math:`\Sigma_{\xi\omega}` is estimated with the sample covariance matrix
         of the error terms.
 
         Optimal instruments for linear parameters not included in :math:`\theta` are simple product characteristics, so
@@ -959,26 +1083,32 @@
 
         draws : `int, optional`
             The number of draws that will be taken from the joint distribution of the error terms. This is ignored if
             ``method`` is ``'approximate'``. Because the default ``method`` is ``'approximate'``, the default number of
             draws is ``1``, even though it will be ignored. For ``'normal'`` or empirical, larger numbers such as
             ``100`` or ``1000`` are recommended.
         seed : `int, optional`
-            Passed to :class:`numpy.random.mtrand.RandomState` to seed the random number generator before any draws are
+            Passed to :class:`numpy.random.RandomState` to seed the random number generator before any draws are
             taken. By default, a seed is not passed to the random number generator.
         expected_prices : `array-like, optional`
             Vector of expected prices conditional on all exogenous variables, :math:`E[p \mid Z]`. By default, if a
             supply side was estimated and ``shares`` did not enter into the formulation for :math:`X_3` in
             :class:`Problem`, ``iteration`` is used. Otherwise, this is by default estimated with the fitted values from
             a reduced form regression of endogenous prices onto :math:`Z_D`.
         iteration : `Iteration, optional`
             :class:`Iteration` configuration used to estimate expected prices by iterating over the :math:`\zeta`-markup
             contraction in :eq:`zeta_contraction`. By default, if a supply side was estimated, this is
             ``Iteration('simple', {'atol': 1e-12})``. Analytic Jacobians are not supported for solving this system.
             This configuration is not used if ``expected_prices`` is specified.
+        constant_costs : `bool, optional`
+            Whether to assume that marginal costs, :math:`c`, remain constant as equilibrium prices and shares change.
+            By default this is ``True``, which means that firms treat marginal costs as constant when setting prices.
+            If set to ``False``, marginal costs will be allowed to adjust if ``shares`` was included in
+            the formulation for :math:`X_3` in :class:`Problem`. This is not relevant if a supply side was not
+            estimated.
 
         Returns
         -------
         `OptimalInstrumentResults`
            Computed :class:`OptimalInstrumentResults`.
 
         Examples
@@ -990,27 +1120,27 @@
 
         # keep track of long it takes to compute optimal instruments for theta
         output("Computing optimal instruments for theta ...")
         start_time = time.time()
 
         # validate the method and create a function that samples from the error distribution
         if method == 'approximate':
-            sample = lambda: (np.zeros_like(self.xi), np.zeros_like(self.omega))
+            sample = lambda: [np.zeros_like(self.xi), np.zeros_like(self.omega)]
         else:
             state = np.random.RandomState(seed)
             if method == 'normal':
                 if self.problem.K3 == 0:
                     variance = np.var(self.xi)
-                    sample = lambda: (np.c_[state.normal(0, variance, self.problem.N)], self.omega)
+                    sample = lambda: [np.c_[state.normal(0, variance, self.problem.N)], self.omega]
                 else:
                     covariance_matrix = np.cov(self.xi, self.omega, rowvar=False)
                     sample = lambda: np.hsplit(state.multivariate_normal([0, 0], covariance_matrix, self.problem.N), 2)
             elif method == 'empirical':
                 if self.problem.K3 == 0:
-                    sample = lambda: (self.xi[state.choice(self.problem.N, self.problem.N)], self.omega)
+                    sample = lambda: [self.xi[state.choice(self.problem.N, self.problem.N)], self.omega]
                 else:
                     joint = np.c_[self.xi, self.omega]
                     sample = lambda: np.hsplit(joint[state.choice(self.problem.N, self.problem.N)], 2)
             else:
                 raise ValueError("method must be 'approximate', 'normal', or 'empirical'.")
 
         # validate the number of draws (there will be only one for the approximate method)
@@ -1048,15 +1178,15 @@
         computed_expected_prices = np.zeros_like(self.problem.products.prices)
         expected_shares = np.zeros_like(self.problem.products.shares)
         expected_xi_jacobian = np.zeros_like(self.xi_by_theta_jacobian)
         expected_omega_jacobian = np.zeros_like(self.omega_by_theta_jacobian)
         iteration_stats: List[Dict[Hashable, SolverStats]] = []
         for _ in output_progress(range(draws), draws, start_time):
             prices_i, shares_i, xi_jacobian_i, omega_jacobian_i, iteration_stats_i, errors_i = (
-                self._compute_realizations(expected_prices, iteration, *sample())
+                self._compute_realizations(expected_prices, iteration, constant_costs, *sample())
             )
             computed_expected_prices += prices_i / draws
             expected_shares += shares_i / draws
             expected_xi_jacobian += xi_jacobian_i / draws
             expected_omega_jacobian += omega_jacobian_i / draws
             iteration_stats.append(iteration_stats_i)
             errors.extend(errors_i)
@@ -1088,188 +1218,141 @@
         )
         output(f"Computed optimal instruments after {format_seconds(results.computation_time)}.")
         output("")
         output(results)
         return results
 
     def _compute_realizations(
-            self, expected_prices: Optional[Array], iteration: Optional[Iteration], xi: Array, omega: Array) -> (
-            Tuple[Array, Array, Array, Array, Dict[Hashable, SolverStats], List[Error]]):
+            self, expected_prices: Optional[Array], iteration: Optional[Iteration], constant_costs: bool, xi: Array,
+            omega: Array) -> Tuple[Array, Array, Array, Array, Dict[Hashable, SolverStats], List[Error]]:
         """If they have not already been estimated, compute the equilibrium prices, shares, and delta associated with a
         realization of xi and omega market-by-market. Then, compute realizations of Jacobians of xi and omega with
         respect to theta.
         """
         errors: List[Error] = []
 
         # compute delta (which will change under equilibrium prices) and marginal costs (which won't change)
         delta = self.delta - self.xi + xi
         costs = tilde_costs = self.tilde_costs - self.omega + omega
         if self.problem.costs_type == 'log':
             costs = np.exp(costs)
 
-        def market_factory(s: Hashable) -> Tuple[ResultsMarket, Array, Optional[Array], Optional[Iteration]]:
+        def equilibrium_market_factory(
+                s: Hashable) -> Tuple[EconomyResultsMarket, Array, Optional[Array], Optional[Iteration], bool]:
             """Build a market along with arguments used to compute equilibrium prices and shares along with delta."""
-            market_s = ResultsMarket(
+            market_s = EconomyResultsMarket(
                 self.problem, s, self._parameters, self.sigma, self.pi, self.rho, self.beta, self.gamma, delta
             )
             costs_s = costs[self.problem._product_market_indices[s]]
             prices_s = expected_prices[self.problem._product_market_indices[s]] if expected_prices is not None else None
-            return market_s, costs_s, prices_s, iteration
+            return market_s, costs_s, prices_s, iteration, constant_costs
 
         # compute realizations of prices, shares, and delta market-by-market
         data_override = {
             'prices': np.zeros_like(self.problem.products.prices),
             'shares': np.zeros_like(self.problem.products.shares)
         }
         iteration_stats: Dict[Hashable, SolverStats] = {}
         generator = generate_items(
-            self.problem.unique_market_ids, market_factory, ResultsMarket.safely_solve_equilibrium_realization
+            self.problem.unique_market_ids, equilibrium_market_factory,
+            EconomyResultsMarket.safely_solve_equilibrium_realization
         )
         for t, (prices_t, shares_t, delta_t, iteration_stats_t, errors_t) in generator:
             data_override['prices'][self.problem._product_market_indices[t]] = prices_t
             data_override['shares'][self.problem._product_market_indices[t]] = shares_t
             delta[self.problem._product_market_indices[t]] = delta_t
             iteration_stats[t] = iteration_stats_t
             errors.extend(errors_t)
 
-        # compute the Jacobian of xi with respect to theta
-        xi_jacobian, demand_errors = self._compute_demand_realization(data_override, delta)
-        errors.extend(demand_errors)
-
-        # compute the Jacobian of omega with respect to theta
-        omega_jacobian = np.full((self.problem.N, self._parameters.P), np.nan, options.dtype)
-        if self.problem.K3 > 0:
-            omega_jacobian, supply_errors = self._compute_supply_realization(
-                data_override, delta, tilde_costs, xi_jacobian
-            )
-            errors.extend(supply_errors)
-
-        return data_override['prices'], data_override['shares'], xi_jacobian, omega_jacobian, iteration_stats, errors
-
-    def _compute_demand_realization(self, data_override: Dict[str, Array], delta: Array) -> Tuple[Array, List[Error]]:
-        """Compute a realization of the Jacobian of xi with respect to theta market-by-market. If necessary, revert
-        problematic elements to their estimated values.
-        """
-        errors: List[Error] = []
-
-        # check if the Jacobian does not need to be computed
+        # only compute Jacobians if necessary
         xi_jacobian = np.full((self.problem.N, self._parameters.P), np.nan, options.dtype)
-        if self._parameters.P == 0:
-            return xi_jacobian, errors
-
-        def market_factory(s: Hashable) -> Tuple[ResultsMarket]:
-            """Build a market with the data realization along with arguments used to compute the Jacobian."""
-            market_s = ResultsMarket(
-                self.problem, s, self._parameters, self.sigma, self.pi, self.rho, self.beta, delta=delta,
-                data_override=data_override
-            )
-            return market_s,
-
-        # compute the Jacobian market-by-market
-        generator = generate_items(
-            self.problem.unique_market_ids, market_factory,
-            ResultsMarket.safely_compute_xi_by_theta_jacobian_realization
-        )
-        for t, (xi_jacobian_t, errors_t) in generator:
-            xi_jacobian[self.problem._product_market_indices[t]] = xi_jacobian_t
-            errors.extend(errors_t)
-
-        # replace invalid elements
-        bad_jacobian_index = ~np.isfinite(xi_jacobian)
-        if np.any(bad_jacobian_index):
-            xi_jacobian[bad_jacobian_index] = self.xi_by_theta_jacobian[bad_jacobian_index]
-            errors.append(exceptions.XiByThetaJacobianReversionError(bad_jacobian_index))
-
-        return xi_jacobian, errors
-
-    def _compute_supply_realization(
-            self, data_override: Dict[str, Array], delta: Array, tilde_costs: Array, xi_jacobian: Array) -> (
-            Tuple[Array, List[Error]]):
-        """Compute a realization of the Jacobian of omega with respect to theta market-by-market. If necessary, revert
-        problematic elements to their estimated values.
-        """
-        errors: List[Error] = []
+        omega_jacobian = np.full((self.problem.N, self._parameters.P), np.nan, options.dtype)
+        if self._parameters.P > 0:
+            def jacobian_market_factory(s: Hashable) -> Tuple[EconomyResultsMarket, Array]:
+                """Build a market with the data realization along with arguments used to compute the Jacobians."""
+                market_s = EconomyResultsMarket(
+                    self.problem, s, self._parameters, self.sigma, self.pi, self.rho, self.beta, delta=delta,
+                    data_override=data_override
+                )
+                tilde_costs_s = tilde_costs[self.problem._product_market_indices[s]]
+                return market_s, tilde_costs_s
 
-        def market_factory(s: Hashable) -> Tuple[ResultsMarket, Array, Array]:
-            """Build a market with the data realization along with arguments used to compute the Jacobians."""
-            market_s = ResultsMarket(
-                self.problem, s, self._parameters, self.sigma, self.pi, self.rho, self.beta, delta=delta,
-                data_override=data_override
+            # compute the Jacobians market-by-market
+            generator = generate_items(
+                self.problem.unique_market_ids, jacobian_market_factory,
+                EconomyResultsMarket.safely_compute_jacobian_realizations
             )
-            tilde_costs_s = tilde_costs[self.problem._product_market_indices[s]]
-            xi_jacobian_s = xi_jacobian[self.problem._product_market_indices[s]]
-            return market_s, tilde_costs_s, xi_jacobian_s
+            for t, (xi_jacobian_t, omega_jacobian_t, errors_t) in generator:
+                xi_jacobian[self.problem._product_market_indices[t]] = xi_jacobian_t
 
-        # compute the Jacobian market-by-market
-        omega_jacobian = np.full((self.problem.N, self._parameters.P), np.nan, options.dtype)
-        generator = generate_items(
-            self.problem.unique_market_ids, market_factory,
-            ResultsMarket.safely_compute_omega_by_theta_jacobian_realization
-        )
-        for t, (omega_jacobian_t, errors_t) in generator:
-            omega_jacobian[self.problem._product_market_indices[t]] = omega_jacobian_t
-            errors.extend(errors_t)
+                if self.problem.K3 > 0:
+                    omega_jacobian[self.problem._product_market_indices[t]] = omega_jacobian_t
 
-        # the Jacobian should be zero for any clipped marginal costs
-        omega_jacobian[self.clipped_costs.flat] = 0
+                errors.extend(errors_t)
 
-        # replace invalid elements
-        bad_jacobian_index = ~np.isfinite(omega_jacobian)
-        if np.any(bad_jacobian_index):
-            omega_jacobian[bad_jacobian_index] = self.omega_by_theta_jacobian[bad_jacobian_index]
-            errors.append(exceptions.OmegaByThetaJacobianReversionError(bad_jacobian_index))
+            # replace invalid elements
+            bad_xi_jacobian_index = ~np.isfinite(xi_jacobian)
+            if np.any(bad_xi_jacobian_index):
+                xi_jacobian[bad_xi_jacobian_index] = self.xi_by_theta_jacobian[bad_xi_jacobian_index]
+                errors.append(exceptions.XiByThetaJacobianReversionError(bad_xi_jacobian_index))
+
+            if self.problem.K3 > 0:
+                bad_omega_jacobian_index = ~np.isfinite(omega_jacobian)
+                if np.any(bad_omega_jacobian_index):
+                    omega_jacobian[bad_omega_jacobian_index] = self.omega_by_theta_jacobian[bad_omega_jacobian_index]
+                    errors.append(exceptions.OmegaByThetaJacobianReversionError(bad_omega_jacobian_index))
 
-        return omega_jacobian, errors
+        return data_override['prices'], data_override['shares'], xi_jacobian, omega_jacobian, iteration_stats, errors
 
     def importance_sampling(
             self, draws: int, ar_constant: float = 1.0, seed: Optional[int] = None,
             agent_data: Optional[Mapping] = None, integration: Optional[Integration] = None,
             delta: Optional[Any] = None) -> 'ImportanceSamplingResults':
         r"""Use importance sampling to construct nodes and weights for integration.
 
         Importance sampling is done with the accept/reject procedure of
         :ref:`references:Berry, Levinsohn, and Pakes (1995)`. First, ``agent_data`` and/or ``integration`` are used to
-        provide a large number of candidate sampling nodes :math:`\nu_{it}` and any demographics :math:`d_{it}`.
+        provide a large number of candidate sampling nodes :math:`\nu` and any demographics :math:`d`.
 
         Out of these candidate agent data, each candidate agent :math:`i` in market :math:`t` is accepted with
-        probability :math:`\frac{1 - s_{0ti}}{M}` where :math:`M \geq 1` is some accept-reject constant. The probability
-        of choosing an inside good :math:`1 - s_{0ti}`, is evaluated at the estimated :math:`\hat{\theta}` and
-        :math:`\hat{\delta}(\hat{\theta})`.
+        probability :math:`\frac{1 - s_{i0t}}{M}` where :math:`M \geq 1` is some accept-reject constant. The probability
+        of choosing an inside good :math:`1 - s_{i0t}`, is evaluated at the estimated :math:`\hat{\theta}` and
+        :math:`\delta(\hat{\theta})`.
 
         Optionally, :meth:`ProblemResults.compute_delta` can be used to provide a more precise
-        :math:`\hat{\delta}(\hat{\theta})` than the estimated :attr:`ProblemResults.delta`. The idea is that more
-        precise agent data (i.e., more integration nodes) would be infeasible to use during estimation, but is feasible
-        here because :math:`\hat{\delta}(\hat{\theta})` only needs to be computed once given a :math:`\hat{\theta}`.
+        :math:`\delta(\hat{\theta})` than the estimated :attr:`ProblemResults.delta`. The idea is that more precise
+        agent data (i.e., more integration nodes) would be infeasible to use during estimation, but is feasible here
+        because :math:`\delta(\hat{\theta})` only needs to be computed once given a :math:`\hat{\theta}`.
 
         Out of the remaining accepted agents, :math:`I_t` equal to ``draws`` are randomly selected within each market
-        :math:`t` and assigned integration weights :math:`w_{it} = \frac{1}{I_t} \cdot \frac{1 - s_{0t}}{1 - s_{0ti}}`.
+        :math:`t` and assigned integration weights :math:`w_{it} = \frac{1}{I_t} \cdot \frac{1 - s_{0t}}{1 - s_{i0t}}`.
 
         If this procedure accepts fewer than ``draws`` agents in a market, an exception will be raised. A good rule of
         thumb is to provide more candidate draws in each market than :math:`\frac{M \times I_t}{1 - s_{0t}}`.
 
         Parameters
         ----------
         draws : `int, optional`
             Number of draws to take from ``sampling_agent_data`` in each market.
         ar_constant : `float, optional`
             Accept/reject constant :math:`M \geq 1`, which is by default, ``1.0``.
         seed : `int, optional`
-            Passed to :class:`numpy.random.mtrand.RandomState` to seed the random number generator before importance
-            sampling is done. By default, a seed is not passed to the random number generator.
+            Passed to :class:`numpy.random.RandomState` to seed the random number generator before importance sampling
+            is done. By default, a seed is not passed to the random number generator.
         agent_data : `structured array-like, optional`
             Agent data from which draws will be sampled, which should have the same structure as ``agent_data`` in
             :class:`Problem`. The ``weights`` field does not need to be specified, and if it is specified it will be
             ignored. By default, the same agent data used to solve the problem will be used.
         integration : `Integration, optional`
             :class:`Integration` configuration for how to build nodes from which draws will be sampled, which will
             replace any ``nodes`` field in ``sampling_agent_data``. This configuration is required if
             ``sampling_agent_data`` is specified without a ``nodes`` field.
         delta : `array-like, optional`
-            More precise :math:`\hat{\delta}(\hat{\theta})` than the estimated :attr:`ProblemResults.delta`, which can
-            be computed by passing a more precise integration rule to :meth:`ProblemResults.compute_delta`. By default,
+            More precise :math:`\delta(\hat{\theta})` than the estimated :attr:`ProblemResults.delta`, which can be
+            computed by passing a more precise integration rule to :meth:`ProblemResults.compute_delta`. By default,
             :attr:`ProblemResults.delta` is used.
 
         Returns
         -------
         `ImportanceSamplingResults`
            Computed :class:`ImportanceSamplingResults`.
 
@@ -1333,27 +1416,27 @@
     def _compute_importance_weights(
             self, agents: RecArray, delta: Array, draws: int, ar_constant: float, seed: Optional[int]) -> (
             Tuple[Array, List[Error]]):
         """Compute the importance sampling weights associated with a set of agents."""
         errors: List[Error] = []
         market_indices = get_indices(agents.market_ids)
 
-        def market_factory(s: Hashable) -> Tuple[ResultsMarket]:
+        def market_factory(s: Hashable) -> Tuple[EconomyResultsMarket]:
             """Build a market use to compute probabilities."""
-            market_s = ResultsMarket(
+            market_s = EconomyResultsMarket(
                 self.problem, s, self._parameters, self.sigma, self.pi, self.rho, delta=delta,
                 agents_override=agents[market_indices[s]]
             )
             return market_s,
 
         # compute weights market-by-market
         state = np.random.RandomState(seed)
         weights = np.zeros_like(agents.weights)
         generator = generate_items(
-            self.problem.unique_market_ids, market_factory, ResultsMarket.safely_compute_probabilities
+            self.problem.unique_market_ids, market_factory, EconomyResultsMarket.safely_compute_probabilities
         )
         for t, (probabilities_t, errors_t) in generator:
             errors.extend(errors_t)
             with np.errstate(all='ignore'):
                 inside_share_t = self.problem.products.shares[self.problem._product_market_indices[t]].sum()
                 inside_probabilities = probabilities_t.sum(axis=0)
                 probability_cutoffs_t = state.uniform(size=inside_probabilities.size)
@@ -1367,128 +1450,7 @@
                         f"sampling_agent_data and/or sampling_integration."
                     )
                 weights_t = np.zeros_like(inside_probabilities)
                 weights_t[sampled_indices_t] = inside_share_t / inside_probabilities[sampled_indices_t] / draws
                 weights[market_indices[t]] = weights_t[:, None]
 
         return weights, errors
-
-    def _coerce_matrices(self, matrices: Any, market_ids: Array) -> Array:
-        """Coerce array-like stacked matrices into a stacked matrix and validate it."""
-        matrices = np.c_[np.asarray(matrices, options.dtype)]
-        rows = sum(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        columns = max(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        if matrices.shape != (rows, columns):
-            raise ValueError(f"matrices must be {rows} by {columns}.")
-        return matrices
-
-    def _coerce_optional_delta(self, delta: Optional[Any], market_ids: Array) -> Array:
-        """Coerce optional array-like mean utilities into a column vector and validate it."""
-        if delta is None:
-            return None
-        delta = np.c_[np.asarray(delta, options.dtype)]
-        rows = sum(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        if delta.shape != (rows, 1):
-            raise ValueError(f"delta must be None or a {rows}-vector.")
-        return delta
-
-    def _coerce_optional_costs(self, costs: Optional[Any], market_ids: Array) -> Array:
-        """Coerce optional array-like costs into a column vector and validate it."""
-        if costs is None:
-            return None
-        costs = np.c_[np.asarray(costs, options.dtype)]
-        rows = sum(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        if costs.shape != (rows, 1):
-            raise ValueError(f"costs must be None or a {rows}-vector.")
-        return costs
-
-    def _coerce_optional_prices(self, prices: Optional[Any], market_ids: Array) -> Array:
-        """Coerce optional array-like prices into a column vector and validate it."""
-        if prices is None:
-            return None
-        prices = np.c_[np.asarray(prices, options.dtype)]
-        rows = sum(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        if prices.shape != (rows, 1):
-            raise ValueError(f"prices must be None or a {rows}-vector.")
-        return prices
-
-    def _coerce_optional_shares(self, shares: Optional[Any], market_ids: Array) -> Array:
-        """Coerce optional array-like shares into a column vector and validate it."""
-        if shares is None:
-            return None
-        shares = np.c_[np.asarray(shares, options.dtype)]
-        rows = sum(i.size for t, i in self.problem._product_market_indices.items() if t in market_ids)
-        if shares.shape != (rows, 1):
-            raise ValueError(f"shares must be None or a {rows}-vector.")
-        return shares
-
-    def _combine_arrays(
-            self, compute_market_results: Callable, market_ids: Array, fixed_args: Sequence = (),
-            market_args: Sequence = (), agent_data: Optional[Mapping] = None,
-            integration: Optional[Integration] = None) -> Array:
-        """Compute arrays for one or all markets and stack them into a single matrix. An array for a single market is
-        computed by passing fixed_args (identical for all markets) and market_args (matrices with as many rows as there
-        are products that are restricted to the market) to compute_market_results, a ResultsMarket method that returns
-        the output for the market any errors encountered during computation. Agent data and an integration configuration
-        can be optionally specified to override agent data.
-        """
-        errors: List[Error] = []
-
-        # keep track of how long it takes to compute the arrays
-        start_time = time.time()
-
-        # structure or construct different agent data
-        if agent_data is None and integration is None:
-            agents = self.problem.agents
-            agents_market_indices = self.problem._agent_market_indices
-        else:
-            agents = Agents(self.problem.products, self.problem.agent_formulation, agent_data, integration)
-            agents_market_indices = get_indices(agents.market_ids)
-
-        def market_factory(s: Hashable) -> tuple:
-            """Build a market along with arguments used to compute arrays."""
-            indices_s = self.problem._product_market_indices[s]
-            market_s = ResultsMarket(
-                self.problem, s, self._parameters, self.sigma, self.pi, self.rho, self.beta, self.gamma, self.delta,
-                self._moments, agents_override=agents[agents_market_indices[s]]
-            )
-            if market_ids.size == 1:
-                args_s = market_args
-            else:
-                args_s = [None if a is None else a[indices_s] for a in market_args]
-            return (market_s, *fixed_args, *args_s)
-
-        # construct a mapping from market IDs to market-specific arrays
-        matrix_mapping: Dict[Hashable, Array] = {}
-        generator = generate_items(market_ids, market_factory, compute_market_results)
-        if market_ids.size > 1:
-            generator = output_progress(generator, market_ids.size, start_time)
-        for t, (array_t, errors_t) in generator:
-            matrix_mapping[t] = np.c_[array_t]
-            errors.extend(errors_t)
-
-        # output a warning about any errors
-        if errors:
-            output("")
-            output(exceptions.MultipleErrors(errors))
-            output("")
-
-        # determine the number of rows and columns
-        row_count = sum(matrix_mapping[t].shape[0] for t in market_ids)
-        column_count = max(matrix_mapping[t].shape[1] for t in market_ids)
-
-        # preserve the original product order or the sorted market order when stacking the arrays
-        combined = np.full((row_count, column_count), np.nan, options.dtype)
-        for t, matrix_t in matrix_mapping.items():
-            if row_count == market_ids.size:
-                combined[market_ids == t, :matrix_t.shape[1]] = matrix_t
-            elif row_count == self.problem.N:
-                combined[self.problem._product_market_indices[t], :matrix_t.shape[1]] = matrix_t
-            else:
-                assert market_ids.size == 1
-                combined = matrix_t
-
-        # output how long it took to compute the arrays
-        end_time = time.time()
-        output(f"Finished after {format_seconds(end_time - start_time)}.")
-        output("")
-        return combined
```

### Comparing `pyblp-0.9.0/pyblp/utilities/basics.py` & `pyblp-1.0.0/pyblp/utilities/basics.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,55 +1,61 @@
 """Basic functionality."""
 
 import contextlib
+import copyreg
 import functools
 import inspect
 import multiprocessing.pool
 import re
+import sys
 import time
 import traceback
 from typing import (
     Any, Callable, Container, Dict, Hashable, Iterable, Iterator, List, Mapping, Optional, Set, Sequence, Type, Tuple,
     Union
 )
+import warnings
 
 import numpy as np
 
 from .. import options
 
 
 # define common types
 Array = Any
 RecArray = Any
 Data = Dict[str, Array]
 Options = Dict[str, Any]
 Bounds = Tuple[Array, Array]
 
-# define a pool managed by parallel and used by generate_items
-pool = None
+# define pools managed by parallel and used by generate_items
+pool: Any = None
 
 
 @contextlib.contextmanager
-def parallel(processes: int) -> Iterator[None]:
+def parallel(processes: int, use_pathos: bool = False) -> Iterator[None]:
     r"""Context manager used for parallel processing in a ``with`` statement context.
 
     This manager creates a context in which a pool of Python processes will be used by any method that requires
     market-by-market computation. These methods will distribute their work among the processes. After the context
-    created by the ``with`` statement ends, all worker processes in the pool will be terminated. Outside of this
-    context, such methods will not use multiprocessing.
+    created by the ``with`` statement ends, all worker processes in the pool will be terminated. Outside this context,
+    such methods will not use multiprocessing.
 
     Importantly, multiprocessing will only improve speed if gains from parallelization outweigh overhead from
     serializing and passing data between processes. For example, if computation for a single market is very fast and
     there is a lot of data in each market that must be serialized and passed between processes, using multiprocessing
     may reduce overall speed.
 
     Arguments
     ---------
     processes : `int`
         Number of Python processes that will be created and used by any method that supports parallel processing.
+    use_pathos : `bool, optional`
+        Whether to use `pathos <https://pathos.readthedocs.io/en/latest/>`_ (which will need to be installed) instead of
+        the default, built-in :mod:`multiprocessing` module.
 
     Examples
     --------
     .. raw:: latex
 
        \begin{examplenotebook}
 
@@ -62,45 +68,97 @@
        \end{examplenotebook}
 
     """
 
     # validate the number of processes
     if not isinstance(processes, int):
         raise TypeError("processes must be an int.")
-    if processes < 2:
-        raise ValueError("processes must be at least 2.")
+    if processes < 1:
+        raise ValueError("processes must be at least 1.")
+
+    # if there is only one process, do no multiprocessing
+    if processes == 1:
+        output("There is only one process, so there is no parallel processing to do.")
+        yield
+        return
+
+    # register hooks for pickling and unpickling SymPy's global parameters object
+    from sympy.core.parameters import _global_parameters
+    unpickle_global_parameters = lambda x: _global_parameters(**x)
+    pickle_global_parameters = lambda x: (unpickle_global_parameters, (x.__dict__,))
+    copyreg.pickle(_global_parameters, pickle_global_parameters)
 
     # start the process pool, wait for work to be done, and then terminate it
     output(f"Starting a pool of {processes} processes ...")
     start_time = time.time()
     global pool
-    try:
-        with multiprocessing.pool.Pool(processes) as pool:
+    if use_pathos:
+        try:
+            from pathos.multiprocessing import ProcessPool
+        except ImportError as exception:
+            if "pathos" not in str(exception):
+                raise
+            raise ImportError("pathos must be installed when use_pathos is True.") from exception
+        try:
+            pool = ProcessPool(nodes=processes)
             output(f"Started the process pool after {format_seconds(time.time() - start_time)}.")
             yield
+        finally:
             output(f"Terminating the pool of {processes} processes ...")
             terminate_time = time.time()
-    finally:
-        pool = None
+            try:
+                pool.close()
+            except Exception:
+                pass
+            try:
+                pool.join()
+            except Exception:
+                pass
+            try:
+                pool.clear()
+            except Exception:
+                pass
+            pool = None
+    else:
+        try:
+            with multiprocessing.pool.Pool(processes) as pool:
+                output(f"Started the process pool after {format_seconds(time.time() - start_time)}.")
+                yield
+                output(f"Terminating the pool of {processes} processes ...")
+                terminate_time = time.time()
+        except AttributeError as exception:
+            if "Can't pickle local object" not in str(exception) or "<lambda>" not in str(exception):
+                raise
+            pathos_message = (
+                "The built-in multiprocessing module does not support lambda functions. Consider setting "
+                "the use_pathos of parallel to True."
+            )
+            raise RuntimeError(pathos_message) from exception
+        finally:
+            pool = None
     output(f"Terminated the process pool after {format_seconds(time.time() - terminate_time)}.")
 
 
 def generate_items(keys: Iterable, factory: Callable[[Any], tuple], method: Callable) -> Iterator:
     """Generate (key, method(*factory(key))) tuples for each key. The first element returned by factory is an instance
     of the class to which method is attached. If a process pool has been initialized, use multiprocessing; otherwise,
     use serial processing.
     """
     if pool is None:
         return (generate_items_worker((k, factory(k), method)) for k in keys)
-    return pool.imap_unordered(generate_items_worker, ((k, factory(k), method) for k in keys))
+    try:
+        return pool.imap_unordered(generate_items_worker, ((k, factory(k), method) for k in keys))
+    except AttributeError:
+        # a pathos ProcessPool uses uimap instead of imap_unordered
+        return pool.uimap(generate_items_worker, ((k, factory(k), method) for k in keys))
 
 
 def generate_items_worker(args: Tuple[Any, tuple, Callable]) -> Tuple[Any, Any]:
-    """Call the the specified method of a class instance with any additional arguments. Return the associated key along
-    with the returned object.
+    """Call the specified method of a class instance with any additional arguments. Return the associated key along with
+    the returned object.
     """
     key, (instance, *method_args), method = args
     return key, method(instance, *method_args)
 
 
 def structure_matrices(mapping: Mapping) -> RecArray:
     """Structure a mapping of keys to (array or None, type) tuples as a record array in which each sub-array is
@@ -111,19 +169,19 @@
     size = next(a.shape[0] for a, _ in mapping.values() if a is not None)
 
     # collect matrices and data types
     matrices: List[Array] = []
     dtypes: List[Tuple[Union[str, Tuple[Hashable, str]], Any, Tuple[int]]] = []
     for key, (array, dtype) in mapping.items():
         matrix = np.zeros((size, 0)) if array is None else np.c_[array]
-        dtypes.append((key, dtype, (matrix.shape[1],)))
+        dtypes.append((key, dtype, matrix.shape[1:]))
         matrices.append(matrix)
 
     # build the record array
-    structured = np.recarray(size, dtypes)
+    structured: RecArray = np.recarray(size, dtypes)
     for dtype, matrix in zip(dtypes, matrices):
         structured[dtype[0] if isinstance(dtype[0], str) else dtype[0][1]] = matrix
     return structured
 
 
 def update_matrices(matrices: RecArray, update_mapping: Dict) -> RecArray:
     """Update fields in a record array created by structure_matrices by re-structuring the matrices."""
@@ -148,14 +206,22 @@
     except Exception:
         index = 0
         parts: List[Array] = []
         while True:
             try:
                 part = np.c_[structured_array_like[f'{key}{index}']]
             except Exception:
+                # warn if there's a 1 but no 0 (this is a common mistake)
+                if index == 0:
+                    try:
+                        structured_array_like[f'{key}1']
+                    except Exception:
+                        pass
+                    else:
+                        warn(f"'{key}1' was specified but not '{key}0'.")
                 break
             index += 1
             if part.size > 0:
                 parts.append(part)
 
         return np.hstack(parts) if parts else None
 
@@ -181,26 +247,36 @@
         f"Failed to get the number of rows in the structured array-like object of type {type(structured_array_like)}. "
         f"Try using a dictionary, a NumPy structured array, a Pandas DataFrame, or any other standard type."
     )
 
 
 def interact_ids(*columns: Array) -> Array:
     """Create interactions of ID columns."""
-    interacted = columns[0].flatten().astype(np.object)
+    interacted = columns[0].flatten().astype(np.object_)
     if len(columns) > 1:
         interacted[:] = list(zip(*columns))
     return interacted
 
 
+def warn(message: Any) -> None:
+    """Output a warning."""
+    old_formatwarning = warnings.formatwarning
+    warnings.formatwarning = lambda x, *_, **__: f"{x}\n"
+    warnings.warn(message)
+    warnings.formatwarning = old_formatwarning
+
+
 def output(message: Any) -> None:
     """Print a message if verbosity is turned on."""
     if options.verbose:
         if not callable(options.verbose_output):
             raise TypeError("options.verbose_output should be callable.")
         options.verbose_output(str(message))
+        if options.flush_output:
+            sys.stdout.flush()
 
 
 def output_progress(iterable: Iterable, length: int, start_time: float) -> Iterator:
     """Yield results from an iterable while outputting progress updates at most every minute."""
     elapsed = time.time() - start_time
     next_minute = int(elapsed / 60) + 1
     for index, iterated in enumerate(iterable):
@@ -300,20 +376,67 @@
 def get_indices(ids: Array) -> Dict[Hashable, Array]:
     """From a one-dimensional array input, construct a dictionary with keys that are the unique values of the array
     and values that are the indices where the key appears in the array.
     """
     flat = ids.flatten()
     sort_indices = flat.argsort(kind='mergesort')
     sorted_ids = flat[sort_indices]
-    changes = np.ones(flat.shape, np.bool)
+    changes = np.ones(flat.shape, np.bool_)
     changes[1:] = sorted_ids[1:] != sorted_ids[:-1]
     reduce_indices = np.nonzero(changes)[0]
     return dict(zip(sorted_ids[reduce_indices], np.split(sort_indices, reduce_indices)[1:]))
 
 
+def compute_finite_differences(f: Callable[[Array], Array], x: Array, epsilon_scale: float = 1.0) -> Array:
+    """Approximate derivatives with finite differences."""
+    epsilon = epsilon_scale * options.finite_differences_epsilon
+
+    arrays = []
+    for index in range(x.size):
+        x1 = x.copy()
+        x2 = x.copy()
+        x1[index] += epsilon / 2
+        x2[index] -= epsilon / 2
+        arrays.append((f(x1) - f(x2)) / epsilon)
+
+    if len(arrays[0].shape) == 1 or (len(arrays[0].shape) == 2 and arrays[0].shape[1] == 1):
+        return np.column_stack(arrays)
+    return np.dstack(arrays)
+
+
+def compute_second_finite_differences(f: Callable[[Array], Array], x: Array, epsilon_scale: float = 1.0) -> Array:
+    """Approximate second derivatives with finite differences."""
+    epsilon = np.sqrt(epsilon_scale * options.finite_differences_epsilon)
+
+    arrays = []
+    for index1 in range(x.size):
+        arrays1 = []
+        for index2 in range(x.size):
+            x1 = x.copy()
+            x2 = x.copy()
+            x3 = x.copy()
+            x4 = x.copy()
+            x1[index1] += epsilon / 2
+            x2[index1] += epsilon / 2
+            x3[index1] -= epsilon / 2
+            x4[index1] -= epsilon / 2
+            x1[index2] += epsilon / 2
+            x2[index2] -= epsilon / 2
+            x3[index2] += epsilon / 2
+            x4[index2] -= epsilon / 2
+            arrays1.append((f(x1) - f(x2) - f(x3) + f(x4)) / epsilon**2)
+
+        if len(arrays1[0].shape) == 1 or (len(arrays1[0].shape) == 2 and arrays1[0].shape[1] == 1):
+            arrays.append(np.column_stack(arrays1))
+        else:
+            arrays.append(np.dstack(arrays1))
+
+    return np.dstack(arrays)
+
+
 class SolverStats(object):
     """Structured statistics returned by a generic numerical solver."""
 
     converged: bool
     iterations: int
     evaluations: int
 
@@ -347,21 +470,21 @@
 
         # sort the IDs
         flat = ids.flatten()
         self.sort_indices = flat.argsort()
         sorted_ids = flat[self.sort_indices]
 
         # identify groups
-        changes = np.ones(flat.shape, np.bool)
+        changes = np.ones(flat.shape, np.bool_)
         changes[1:] = sorted_ids[1:] != sorted_ids[:-1]
         self.reduce_indices = np.nonzero(changes)[0]
         self.unique = sorted_ids[self.reduce_indices]
 
         # encode the groups
-        sorted_codes = np.cumsum(changes) - 1
+        sorted_codes: Array = np.cumsum(changes) - 1
         self.codes = sorted_codes[self.sort_indices.argsort()]
 
         # compute counts
         self.group_count = self.reduce_indices.size
         self.counts = np.diff(np.append(self.reduce_indices, self.codes.size))
 
     def sum(self, matrix: Array) -> Array:
@@ -487,15 +610,15 @@
         """Compute condition number of the matrix."""
         super().__init__()
         from .algebra import compute_condition_number
         self._condition = compute_condition_number(matrix)
 
     def __str__(self) -> str:
         """Supplement the error with the condition number."""
-        return f"{super().__str__()} Condition number: {format_number(self._condition)}."
+        return f"{super().__str__()} Condition number: {format_number(self._condition).strip()}."
 
 
 class InversionReplacementError(InversionError):
     """Problems with inverting a matrix led to the use of a replacement such as an approximation."""
 
     _replacement: str
```

### Comparing `pyblp-0.9.0/pyblp/utilities/statistics.py` & `pyblp-1.0.0/pyblp/utilities/statistics.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Standard statistical routines."""
 
 from typing import List, Tuple
 
 import numpy as np
 import scipy.linalg
 
-from .algebra import approximately_invert
+from .algebra import approximately_invert, duplication_matrix, elimination_matrix
 from .basics import Array, Error, Groups
 from .. import exceptions
 
 
 class IV(object):
     """Simple model for generalized instrumental variables estimation."""
 
@@ -134,7 +134,23 @@
         N = Z_list[0].shape[0]
         return Z_list[0].T @ jacobian_list[0] / N
 
     # tensors are faster than loops for more than one equation
     Z_transpose_stack = np.dstack(np.split(scipy.linalg.block_diag(*Z_list), len(jacobian_list)))
     jacobian_stack = np.dstack(jacobian_list).swapaxes(1, 2)
     return (Z_transpose_stack @ jacobian_stack).mean(axis=0)
+
+
+def compute_sigma_squared_vector_covariances(sigma: Array, sigma_vector_covariances: Array) -> Array:
+    """Use the delta method to transform the asymptotic covariance matrix of vech(sigma) into the asymptotic covariance
+    matrix of vech(sigma * sigma') where sigma is a lower triangular Cholesky root of parameters. See Section 10.5.4 in
+    the Handbook of Matrices (Lutkepohl and Lutkepohl).
+    """
+    if sigma_vector_covariances.size == 0:
+        return sigma_vector_covariances
+
+    k = sigma.shape[0]
+    L = elimination_matrix(k)
+    D = duplication_matrix(k)
+    I = np.eye(k)
+    jacobian = 2 * scipy.linalg.pinv(D) @ np.kron(sigma, I) @ L.T
+    return jacobian @ sigma_vector_covariances @ jacobian.T
```

### Comparing `pyblp-0.9.0/pyblp/__init__.py` & `pyblp-1.0.0/pyblp/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,28 +3,29 @@
 from . import data, exceptions, options
 from .configurations.formulation import Formulation
 from .configurations.integration import Integration
 from .configurations.iteration import Iteration
 from .configurations.optimization import Optimization
 from .construction import (
     build_blp_instruments, build_differentiation_instruments, build_id_data, build_integration, build_matrix,
-    build_ownership, data_to_dict
+    build_ownership, data_to_dict, save_pickle, read_pickle
 )
 from .economies.problem import ImportanceSamplingProblem, OptimalInstrumentProblem, Problem
 from .economies.simulation import Simulation
-from .moments import FirstChoiceCovarianceMoment
+from .micro import MicroDataset, MicroPart, MicroMoment
 from .primitives import Agents, Products
 from .results.bootstrapped_results import BootstrappedResults
 from .results.importance_sampling_results import ImportanceSamplingResults
 from .results.optimal_instrument_results import OptimalInstrumentResults
 from .results.problem_results import ProblemResults
 from .results.simulation_results import SimulationResults
 from .utilities.basics import parallel
 from .version import __version__
 
 __all__ = [
     'data', 'exceptions', 'options', 'Formulation', 'Integration', 'Iteration', 'Optimization', 'build_blp_instruments',
     'build_differentiation_instruments', 'build_id_data', 'build_integration', 'build_matrix', 'build_ownership',
-    'data_to_dict', 'ImportanceSamplingProblem', 'OptimalInstrumentProblem', 'Problem', 'Simulation',
-    'FirstChoiceCovarianceMoment', 'Agents', 'Products', 'BootstrappedResults', 'ImportanceSamplingResults',
-    'OptimalInstrumentResults', 'ProblemResults', 'SimulationResults', 'parallel', '__version__'
+    'data_to_dict', 'save_pickle', 'read_pickle', 'ImportanceSamplingProblem', 'OptimalInstrumentProblem', 'Problem',
+    'Simulation', 'MicroDataset', 'MicroPart', 'MicroMoment', 'Agents', 'Products', 'BootstrappedResults',
+    'ImportanceSamplingResults', 'OptimalInstrumentResults', 'ProblemResults', 'SimulationResults', 'parallel',
+    '__version__'
 ]
```

### Comparing `pyblp-0.9.0/pyblp.egg-info/SOURCES.txt` & `pyblp-1.0.0/pyblp.egg-info/SOURCES.txt`

 * *Files 6% similar despite different names*

```diff
@@ -36,24 +36,28 @@
 docs/notebooks/api/.ipynb_checkpoints/build_matrix-checkpoint.ipynb
 docs/notebooks/api/.ipynb_checkpoints/build_ownership-checkpoint.ipynb
 docs/notebooks/api/.ipynb_checkpoints/data-checkpoint.ipynb
 docs/notebooks/api/.ipynb_checkpoints/data_to_dict-checkpoint.ipynb
 docs/notebooks/api/.ipynb_checkpoints/formulation-checkpoint.ipynb
 docs/notebooks/api/.ipynb_checkpoints/integration-checkpoint.ipynb
 docs/notebooks/api/.ipynb_checkpoints/iteration-checkpoint.ipynb
+docs/notebooks/api/.ipynb_checkpoints/micro_dataset-checkpoint.ipynb
+docs/notebooks/api/.ipynb_checkpoints/micro_part-checkpoint.ipynb
 docs/notebooks/api/.ipynb_checkpoints/optimization-checkpoint.ipynb
 docs/notebooks/api/.ipynb_checkpoints/parallel-checkpoint.ipynb
 docs/notebooks/tutorial/blp.ipynb
 docs/notebooks/tutorial/logit_nested.ipynb
 docs/notebooks/tutorial/nevo.ipynb
+docs/notebooks/tutorial/petrin.ipynb
 docs/notebooks/tutorial/post_estimation.ipynb
 docs/notebooks/tutorial/simulation.ipynb
 docs/notebooks/tutorial/.ipynb_checkpoints/blp-checkpoint.ipynb
 docs/notebooks/tutorial/.ipynb_checkpoints/logit_nested-checkpoint.ipynb
 docs/notebooks/tutorial/.ipynb_checkpoints/nevo-checkpoint.ipynb
+docs/notebooks/tutorial/.ipynb_checkpoints/petrin-checkpoint.ipynb
 docs/notebooks/tutorial/.ipynb_checkpoints/post_estimation-checkpoint.ipynb
 docs/notebooks/tutorial/.ipynb_checkpoints/simulation-checkpoint.ipynb
 docs/static/override.css
 docs/static/override.js
 docs/static/preamble.tex
 docs/templates/breadcrumbs.html
 docs/templates/class_with_signature.rst
@@ -61,15 +65,15 @@
 docs/templates/class_without_methods_or_signature.rst
 docs/templates/class_without_signature.rst
 docs/templates/nbsphinx_epilog.rst
 docs/templates/nbsphinx_prolog.rst
 pyblp/__init__.py
 pyblp/construction.py
 pyblp/exceptions.py
-pyblp/moments.py
+pyblp/micro.py
 pyblp/options.py
 pyblp/parameters.py
 pyblp/primitives.py
 pyblp/version.py
 pyblp.egg-info/PKG-INFO
 pyblp.egg-info/SOURCES.txt
 pyblp.egg-info/dependency_links.txt
@@ -81,30 +85,33 @@
 pyblp/configurations/iteration.py
 pyblp/configurations/optimization.py
 pyblp/data/__init__.py
 pyblp/data/blp_agents.csv
 pyblp/data/blp_products.csv
 pyblp/data/nevo_agents.csv
 pyblp/data/nevo_products.csv
+pyblp/data/petrin_agents.csv
+pyblp/data/petrin_covariances.csv
+pyblp/data/petrin_products.csv
+pyblp/data/petrin_values.csv
 pyblp/economies/__init__.py
 pyblp/economies/economy.py
 pyblp/economies/problem.py
 pyblp/economies/simulation.py
 pyblp/markets/__init__.py
+pyblp/markets/economy_results_market.py
 pyblp/markets/market.py
 pyblp/markets/problem_market.py
-pyblp/markets/results_market.py
 pyblp/markets/simulation_market.py
-pyblp/markets/simulation_results_market.py
 pyblp/results/__init__.py
 pyblp/results/bootstrapped_results.py
+pyblp/results/economy_results.py
 pyblp/results/importance_sampling_results.py
 pyblp/results/optimal_instrument_results.py
 pyblp/results/problem_results.py
-pyblp/results/results.py
 pyblp/results/simulation_results.py
 pyblp/utilities/__init__.py
 pyblp/utilities/algebra.py
 pyblp/utilities/basics.py
 pyblp/utilities/statistics.py
 tests/__init__.py
 tests/conftest.py
```

### Comparing `pyblp-0.9.0/README.rst` & `pyblp-1.0.0/README.rst`

 * *Files 26% similar despite different names*

```diff
@@ -29,20 +29,45 @@
 Development of the package has been guided by the work of many researchers and practitioners. For a full list of references, including the original work of `Berry, Levinsohn, and Pakes (1995) <https://ideas.repec.org/a/ecm/emetrp/v63y1995i4p841-90.html>`_, refer to the `references <https://pyblp.readthedocs.io/en/stable/references.html>`_ section of the documentation.
 
 
 Citation
 --------
 
 If you use PyBLP in your research, we ask that you also cite `Conlon and Gortmaker (2020) <https://jeffgortmaker.com/files/pyblp.pdf>`_, which describes the advances implemented in the package.
+::
+
+    @article{PyBLP,
+        author = {Conlon, Christopher and Gortmaker, Jeff},
+        title = {Best practices for differentiated products demand estimation with {PyBLP}},
+        journal = {The RAND Journal of Economics},
+        volume = {51},
+        number = {4},
+        pages = {1108-1161},
+        doi = {https://doi.org/10.1111/1756-2171.12352},
+        url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1756-2171.12352},
+        eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1756-2171.12352},
+        year = {2020}
+    }
+
+
+If you use PyBLP's micro moments functionality, we ask that you also cite `Conlon and Gortmaker (2023) <https://jeffgortmaker.com/files/micro.pdf>`_, which describes the standardized framework implemented by PyBLP for incorporating micro data into BLP-style estimation.
+::
+
+    @misc{MicroPyBLP,
+        author = {Conlon, Christopher and Gortmaker, Jeff},
+        title = {Incorporating micro data into differentiated products demand estimation with {PyBLP}},
+        note = {Working paper},
+        year = {2023}
+    }
 
 
 Installation
 ------------
 
-The PyBLP package has been tested on `Python <https://www.python.org/downloads/>`_ versions 3.6 and 3.7. The `SciPy instructions <https://scipy.org/install.html>`_ for installing related packages is a good guide for how to install a scientific Python environment. A good choice is the `Anaconda Distribution <https://www.anaconda.com/distribution/>`_, since it comes packaged with the following PyBLP dependencies: `NumPy <https://numpy.org/>`_, `SciPy <https://www.scipy.org/>`_, `SymPy <https://www.sympy.org/en/index.html>`_, and `Patsy <https://patsy.readthedocs.io/en/latest/>`_. For absorption of high dimension fixed effects, PyBLP also depends on its companion package `PyHDFE <https://github.com/jeffgortmaker/pyhdfe>`_, which will be installed when PyBLP is installed.
+The PyBLP package has been tested on `Python <https://www.python.org/downloads/>`_ versions 3.6 through 3.9. The `SciPy instructions <https://scipy.org/install/>`_ for installing related packages is a good guide for how to install a scientific Python environment. A good choice is the `Anaconda Distribution <https://www.anaconda.com/download>`_, since it comes packaged with the following PyBLP dependencies: `NumPy <https://numpy.org/>`_, `SciPy <https://scipy.org/>`_, `SymPy <https://www.sympy.org/en/index.html>`_, and `Patsy <https://patsy.readthedocs.io/en/latest/>`_. For absorption of high dimension fixed effects, PyBLP also depends on its companion package `PyHDFE <https://github.com/jeffgortmaker/pyhdfe>`_, which will be installed when PyBLP is installed.
 
 However, PyBLP may not work with old versions of its dependencies. You can update PyBLP's Anaconda dependencies with::
 
     conda update numpy scipy sympy patsy
 
 You can update PyHDFE with::
 
@@ -64,18 +89,19 @@
 
 
 Other Languages
 ---------------
 
 Once installed, PyBLP can be incorporated into projects written in many other languages with the help of various tools that enable interoperability with Python.
 
-For example, the `reticulate <https://github.com/rstudio/reticulate>`_ package makes interacting with PyBLP in R straightforward::
+For example, the `reticulate <https://github.com/rstudio/reticulate>`_ package makes interacting with PyBLP in R straightforward (when supported, Python objects can be converted to their R counterparts with the ``py_to_r`` function, which needs to be used manually because we set ``convert=FALSE`` to get rid of errors about trying to automatically convert unsupported objects)::
 
     library(reticulate)
-    pyblp <- import("pyblp")
+    pyblp <- import("pyblp", convert=FALSE)
+    pyblp$options$flush_output <- TRUE
 
 Similarly, `PyCall <https://github.com/JuliaPy/PyCall.jl>`_ can be used to incorporate PyBLP into a Julia workflow::
 
     using PyCall
     pyblp = pyimport("pyblp")
 
 The `py command <https://www.mathworks.com/help/matlab/call-python-libraries.html>`_ serves a similar purpose in MATLAB::
@@ -86,31 +112,38 @@
 Features
 --------
 
 - R-style formula interface
 - Bertrand-Nash supply-side moments
 - Multiple equation GMM
 - Demographic interactions
-- Micro moments that match product and agent characteristic covariances
+- Product-specific demographics
+- Flexible micro moments that can match statistics based on survey data
+- Support for micro moments based on second choice data
+- Support for optimal micro moments that match micro data scores
 - Fixed effect absorption
 - Nonlinear functions of product characteristics
 - Concentrating out linear parameters
-- Normal and lognormal random coefficients
+- Flexible random coefficient distributions
 - Parameter bounds and constraints
 - Random coefficients nested logit (RCNL)
+- Approximation to the pure characteristics model
 - Varying nesting parameters across groups
 - Logit and nested logit benchmarks
 - Classic BLP instruments
 - Differentiation instruments
 - Optimal instruments
+- Adjustments for simulation error
 - Tests of overidentifying and model restrictions
 - Parametric boostrapping post-estimation outputs
 - Elasticities and diversion ratios
 - Marginal costs and markups
+- Passthrough calculations
 - Profits and consumer surplus
+- Newton and fixed point methods for computing pricing equilibria
 - Merger simulation
 - Custom counterfactual simulation
 - Synthetic data construction
 - SciPy or Artleys Knitro optimization
 - Fixed point acceleration
 - Monte Carlo, quasi-random sequences, quadrature, and sparse grids
 - Importance sampling
@@ -121,24 +154,11 @@
 - Analytic gradients
 - Finite difference Hessians
 - Market-by-market parallelization
 - Extended floating point precision
 - Robust error handling
 
 
-Features Slated for Future Versions
------------------------------------
-
-- Other micro moments
-- Fast, "Robust," and Approximately Correct (FRAC) estimation
-- Analytic Hessians
-- Mathematical Program with Equilibrium Constraints (MPEC)
-- Generalized Empirical Likelihood (GEL)
-- Discrete types
-- Pure characteristics model
-- Newton methods for computing equilibrium prices
-
-
 Bugs and Requests
 -----------------
 
 Please use the `GitHub issue tracker <https://github.com/jeffgortmaker/pyblp/issues>`_ to submit bugs or to request features.
```

### Comparing `pyblp-0.9.0/setup.py` & `pyblp-1.0.0/setup.py`

 * *Files 12% similar despite different names*

```diff
@@ -16,35 +16,38 @@
 # set up the package
 setup(
     name='pyblp',
     packages=find_packages(),
     python_requires='>=3.6',
     install_requires=read('requirements.txt').splitlines(),
     extras_require={
-        'tests': ['pytest', 'pytest-xdist', 'linearmodels'],
+        'tests': ['pytest', 'pytest-xdist', 'linearmodels', 'dill', 'pathos'],
         'docs': [
-            'sphinx', 'pandas', 'ipython', 'matplotlib', 'astunparse', 'sphinx-rtd-theme', 'nbsphinx'
+            'sphinx==2.0.0', 'pandas', 'ipython', 'matplotlib', 'astunparse', 'sphinx-rtd-theme==0.4.3',
+            'nbsphinx==0.5.0', 'jinja2~=2.11', 'docutils==0.17',
         ],
     },
     include_package_data=True,
     description="BLP demand estimation with Python 3",
     long_description=read('README.rst').split('description-start')[1].strip(),
     version=version_match.group(1),
-    author="Jeff Gortmaker",
+    author="Jeff Gortmaker, Christopher Conlon",
     author_email="jeff@jeffgortmaker.com",
     license="MIT",
     classifiers=[
         "Development Status :: 4 - Beta",
         "Intended Audience :: Science/Research",
         "License :: OSI Approved :: MIT License",
         "Operating System :: OS Independent",
         "Programming Language :: Python",
         "Programming Language :: Python :: 3",
         "Programming Language :: Python :: 3.6",
         "Programming Language :: Python :: 3.7",
+        "Programming Language :: Python :: 3.8",
+        "Programming Language :: Python :: 3.9",
         "Topic :: Scientific/Engineering"
     ],
     url="https://github.com/jeffgortmaker/pyblp",
     project_urls={
         "Documentation": "http://pyblp.readthedocs.io/en/latest",
         "Tracker": "https://github.com/jeffgortmaker/pyblp/issues"
     }
```

### Comparing `pyblp-0.9.0/tests/test_blp.py` & `pyblp-1.0.0/tests/test_blp.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,53 +1,66 @@
 """Primary tests."""
 
+import copy
+import itertools
+import os
 import pickle
+import shutil
+import subprocess
 import tempfile
 from typing import Any, Callable, Dict, List, Optional, Tuple
 import warnings
 
 import numpy as np
 import pytest
 import scipy.optimize
+import scipy.special
 
+import pyblp.exceptions
 from pyblp import (
-    Formulation, Integration, Iteration, Optimization, Problem, Simulation, build_ownership, data_to_dict, parallel
+    Formulation, Integration, Iteration, MicroDataset, MicroPart, MicroMoment, Optimization, Problem, Simulation,
+    build_ownership, data_to_dict, parallel
+)
+from pyblp.utilities.basics import (
+    Array, Options, RecArray, update_matrices, compute_finite_differences, compute_second_finite_differences
 )
-from pyblp.utilities.basics import Array, Options, update_matrices
 from .conftest import SimulatedProblemFixture
 
 
 @pytest.mark.usefixtures('simulated_problem')
 @pytest.mark.parametrize('solve_options_update', [
     pytest.param({'method': '2s'}, id="two-step"),
     pytest.param({'scale_objective': True}, id="scaled objective"),
     pytest.param({'center_moments': False, 'W_type': 'unadjusted', 'se_type': 'clustered'}, id="complex covariances"),
     pytest.param({'delta_behavior': 'last'}, id="faster starting delta values"),
+    pytest.param({'delta_behavior': 'logit'}, id="logit starting delta values"),
     pytest.param({'fp_type': 'linear'}, id="non-safe linear fixed point"),
     pytest.param({'fp_type': 'safe_nonlinear'}, id="nonlinear fixed point"),
     pytest.param({'fp_type': 'nonlinear'}, id="non-safe nonlinear fixed point"),
     pytest.param(
-        {'iteration': Iteration('hybr', {'xtol': 1e-12}, compute_jacobian=True)},
+        {'iteration': Iteration('hybr', {'xtol': 1e-12}, compute_jacobian=True, universal_display=True)},
         id="linear Newton fixed point"
     ),
     pytest.param(
         {'fp_type': 'safe_nonlinear', 'iteration': Iteration('hybr', {'xtol': 1e-12}, compute_jacobian=True)},
         id="nonlinear Newton fixed point"
     )
 ])
 def test_accuracy(simulated_problem: SimulatedProblemFixture, solve_options_update: Options) -> None:
     """Test that starting parameters that are half their true values give rise to errors of less than 10%."""
     simulation, _, problem, solve_options, _ = simulated_problem
 
     # skip different iteration configurations when they won't matter
     if simulation.K2 == 0 and {'delta_behavior', 'fp_type', 'iteration'} & set(solve_options_update):
         return pytest.skip("A different iteration configuration has no impact when there is no heterogeneity.")
+    if simulation.epsilon_scale != 1 and 'nonlinear' in solve_options_update.get('fp_type', 'safe_linear'):
+        return pytest.skip("Nonlinear fixed point configurations are not supported when epsilon is scaled.")
 
     # update the default options and solve the problem
-    updated_solve_options = solve_options.copy()
+    updated_solve_options = copy.deepcopy(solve_options)
     updated_solve_options.update(solve_options_update)
     updated_solve_options.update({k: 0.5 * solve_options[k] for k in ['sigma', 'pi', 'rho', 'beta']})
     results = problem.solve(**updated_solve_options)
 
     # test the accuracy of the estimated parameters
     keys = ['sigma', 'pi', 'rho', 'beta']
     if problem.K3 > 0:
@@ -65,23 +78,23 @@
 def test_optimal_instruments(simulated_problem: SimulatedProblemFixture, compute_options: Options) -> None:
     """Test that starting parameters that are half their true values also give rise to errors of less than 10% under
     optimal instruments.
     """
     simulation, _, problem, solve_options, problem_results = simulated_problem
 
     # compute optimal instruments and update the problem (only use a few draws to speed up the test)
-    compute_options = compute_options.copy()
+    compute_options = copy.deepcopy(compute_options)
     compute_options.update({
         'draws': 5,
         'seed': 0
     })
     new_problem = problem_results.compute_optimal_instruments(**compute_options).to_problem()
 
     # update the default options and solve the problem
-    updated_solve_options = solve_options.copy()
+    updated_solve_options = copy.deepcopy(solve_options)
     updated_solve_options.update({k: 0.5 * solve_options[k] for k in ['sigma', 'pi', 'rho', 'beta']})
     new_results = new_problem.solve(**updated_solve_options)
 
     # test the accuracy of the estimated parameters
     keys = ['beta', 'sigma', 'pi', 'rho']
     if problem.K3 > 0:
         keys.append('gamma')
@@ -114,95 +127,213 @@
         seed=0,
         delta=delta,
         integration=Integration('mlhs', 50000, {'seed': 0}),
     )
 
     # solve the new problem
     new_problem = sampling_results.to_problem()
-    updated_solve_options = solve_options.copy()
+    updated_solve_options = copy.deepcopy(solve_options)
     updated_solve_options.update({k: 0.5 * solve_options[k] for k in ['sigma', 'pi', 'rho', 'beta']})
     new_results = new_problem.solve(**updated_solve_options)
 
     # test the accuracy of the estimated parameters
     keys = ['beta', 'sigma', 'pi', 'rho']
     if problem.K3 > 0:
         keys.append('gamma')
     for key in keys:
         np.testing.assert_allclose(getattr(simulation, key), getattr(new_results, key), atol=0, rtol=0.2, err_msg=key)
 
 
 @pytest.mark.usefixtures('simulated_problem')
 def test_bootstrap(simulated_problem: SimulatedProblemFixture) -> None:
-    """Test that post-estimation output medians are within 10% parametric bootstrap confidence intervals."""
-    _, _, problem, solve_options, _ = simulated_problem
-
-    # use second-step results to compute bootstrapped output
-    solve_options = solve_options.copy()
-    solve_options['method'] = '2s'
-    results = problem.solve(**solve_options)
+    """Test that post-estimation output medians are within 5% parametric bootstrap confidence intervals."""
+    _, _, problem, solve_options, problem_results = simulated_problem
 
-    # create bootstrapped results (use only a few draws for speed)
-    bootstrapped_results = results.bootstrap(draws=100, seed=0)
+    # create bootstrapped results (use only a few draws and don't iterate for speed)
+    bootstrapped_results = problem_results.bootstrap(draws=100, seed=0, iteration=Iteration('return'))
 
     # test that post-estimation outputs are within 95% confidence intervals
     t = problem.products.market_ids[0]
     merger_ids = np.where(problem.products.firm_ids == 1, 0, problem.products.firm_ids)
     merger_ids_t = merger_ids[problem.products.market_ids == t]
     method_mapping = {
         "aggregate elasticities": lambda r: r.compute_aggregate_elasticities(),
         "consumer surpluses": lambda r: r.compute_consumer_surpluses(),
         "approximate prices": lambda r: r.compute_approximate_prices(merger_ids),
         "own elasticities": lambda r: r.extract_diagonals(r.compute_elasticities()),
+        "own elasticities in t": lambda r: r.extract_diagonals(r.compute_elasticities(market_id=t), market_id=t),
         "aggregate elasticity in t": lambda r: r.compute_aggregate_elasticities(market_id=t),
         "consumer surplus in t": lambda r: r.compute_consumer_surpluses(market_id=t),
         "approximate prices in t": lambda r: r.compute_approximate_prices(merger_ids_t, market_id=t)
     }
     for name, method in method_mapping.items():
-        values = method(results)
+        values = method(problem_results)
         bootstrapped_values = method(bootstrapped_results)
         median = np.median(values)
         bootstrapped_medians = np.nanmedian(bootstrapped_values, axis=range(1, bootstrapped_values.ndim))
-        lb, ub = np.percentile(bootstrapped_medians, [5, 95])
+        lb = np.percentile(bootstrapped_medians, 2.5)
+        ub = np.percentile(bootstrapped_medians, 97.5)
         np.testing.assert_array_less(np.squeeze(lb), np.squeeze(median) + 1e-14, err_msg=name)
         np.testing.assert_array_less(np.squeeze(median), np.squeeze(ub) + 1e-14, err_msg=name)
 
 
 @pytest.mark.usefixtures('simulated_problem')
+def test_bootstrap_se(simulated_problem: SimulatedProblemFixture) -> None:
+    """Test that bootstrapped SEs are close to analytic ones. Or at least the same order of magnitude -- especially for
+    large numbers of RCs they may not necessarily be very close to each other.
+    """
+    _, _, _, _, problem_results = simulated_problem
+
+    # compute bootstrapped results (ignore supply side iteration because we will only use the parameter draws)
+    bootstrapped_results = problem_results.bootstrap(draws=1000, seed=0, iteration=Iteration('return'))
+
+    # compare SEs
+    for key in ['sigma', 'pi', 'rho', 'beta', 'gamma']:
+        analytic_se = np.nan_to_num(getattr(problem_results, f'{key}_se'))
+        bootstrapped_se = getattr(bootstrapped_results, f'bootstrapped_{key}').std(axis=0)
+        np.testing.assert_allclose(analytic_se, bootstrapped_se, atol=0.001, rtol=0.5, err_msg=key)
+
+
+@pytest.mark.usefixtures('simulated_problem')
+def test_agent_resampling(simulated_problem: SimulatedProblemFixture) -> None:
+    """Test that estimating simulation error by resampling agents seems to work."""
+    simulation, _, problem, solve_options, _ = simulated_problem
+
+    # skip configurations when they won't matter
+    if simulation.K2 == 0:
+        return pytest.skip("Agent resampling does nothing when there is no heterogeneity.")
+
+    # resample agent data but trivially to get all zeros
+    updated_solve_options = copy.deepcopy(solve_options)
+    updated_solve_options.update({
+        'optimization': Optimization('return'),
+        'resample_agent_data': lambda i: None if i > 3 else simulation.agent_data,
+    })
+    problem_results1 = problem.solve(**updated_solve_options)
+    assert (problem_results1.simulation_covariances == 0).all()
+
+    def resample_agent_data(index: int) -> Optional[RecArray]:
+        """Non-trivially resample agent data."""
+        if index > 3:
+            return None
+
+        assert simulation.agent_data is not None
+        resampled_agent_data = simulation.agent_data.copy()
+        resampled_weights = simulation.agent_data.weights.copy()
+        np.random.default_rng(index).shuffle(resampled_weights)
+        resampled_agent_data.weights[:] = resampled_weights
+        return resampled_agent_data
+
+    # resample agent data non-trivially
+    updated_solve_options['resample_agent_data'] = resample_agent_data
+    problem_results2 = problem.solve(**updated_solve_options)
+    assert not (problem_results2.simulation_covariances == 0).all()
+
+
+@pytest.mark.usefixtures('simulated_problem')
+def test_micro_chunking(simulated_problem: SimulatedProblemFixture) -> None:
+    """Test that micro chunking doesn't substantially change anything."""
+    simulation, simulation_results, problem, solve_options, _ = simulated_problem
+
+    # skip configurations without micro moments
+    if 'micro_moments' not in solve_options or not solve_options['micro_moments']:
+        return pytest.skip("Micro chunking has no effect when there are no micro moments.")
+
+    # get some baseline results
+    updated_solve_options = copy.deepcopy(solve_options)
+    updated_solve_options['optimization'] = Optimization('return')
+    agent_scores1 = simulation_results.compute_agent_scores(
+        solve_options['micro_moments'][0].parts[0].dataset, integration=simulation.integration
+    )
+    problem_results1 = problem.solve(**updated_solve_options)
+
+    # get some results with chunking
+    assert pyblp.options.micro_computation_chunks == 1
+    for chunks in [2, 3]:
+        pyblp.options.micro_computation_chunks = chunks
+        agent_scores2 = simulation_results.compute_agent_scores(
+            solve_options['micro_moments'][0].parts[0].dataset, integration=simulation.integration
+        )
+        problem_results2 = problem.solve(**updated_solve_options)
+        pyblp.options.micro_computation_chunks = 1
+
+        # verify that all results are very close
+        np.testing.assert_allclose(
+            list(agent_scores1[0].values())[0], list(agent_scores2[0].values())[0], atol=1e-12, rtol=1e-6
+        )
+        for key, result in problem_results1.__dict__.items():
+            if isinstance(result, np.ndarray) and result.dtype != np.object_:
+                np.testing.assert_allclose(result, getattr(problem_results2, key), atol=1e-12, rtol=1e-6, err_msg=key)
+
+
+@pytest.mark.usefixtures('simulated_problem')
+def test_profit_gradients(simulated_problem: SimulatedProblemFixture) -> None:
+    """Test that if equilibrium prices were computed, profit gradients are nearly zero."""
+    _, simulation_results, _, _, _ = simulated_problem
+
+    if simulation_results.profit_gradient_norms is None:
+        return pytest.skip("Equilibrium prices were not computed.")
+
+    for t, profit_gradient_norms_t in simulation_results.profit_gradient_norms.items():
+        for f, profit_gradient_norm_ft in profit_gradient_norms_t.items():
+            assert profit_gradient_norm_ft < 1e-12, (t, f)
+
+
+@pytest.mark.usefixtures('simulated_problem')
 def test_result_serialization(simulated_problem: SimulatedProblemFixture) -> None:
-    """Test that result objects can be serialized after being converted to dictionaries."""
-    _, simulation_results, _, _, problem_results = simulated_problem
-    instrument_results = problem_results.compute_optimal_instruments()
-    bootstrapped_results = problem_results.bootstrap(draws=1, seed=0)
-    with tempfile.TemporaryFile() as handle:
-        pickle.dump(simulation_results.to_dict(), handle)
-        pickle.dump(problem_results.to_dict(), handle)
-        pickle.dump(instrument_results.to_dict(), handle)
-        pickle.dump(bootstrapped_results.to_dict(), handle)
-        pickle.dump(data_to_dict(simulation_results.product_data), handle)
+    """Test that result objects can be serialized and that their string representations are the same when they are
+    unpickled.
+    """
+    simulation, simulation_results, problem, solve_options, problem_results = simulated_problem
+    originals = [
+        Formulation('x + y', absorb='C(z)', absorb_method='lsmr', absorb_options={'tol': 1e-10}),
+        Integration('halton', size=10, specification_options={'seed': 0, 'scramble': True}),
+        Iteration('lm', method_options={'max_evaluations': 100}, compute_jacobian=True, universal_display=True),
+        Optimization('nelder-mead', method_options={'xatol': 1e-5}, compute_gradient=False, universal_display=False),
+        problem,
+        simulation,
+        simulation_results,
+        problem_results,
+        problem_results.compute_optimal_instruments(),
+        problem_results.bootstrap(draws=1, seed=0),
+        data_to_dict(simulation_results.product_data),
+        solve_options['micro_moments'],
+    ]
+    for original in originals:
+        try:
+            unpickled = pickle.loads(pickle.dumps(original))
+        except AttributeError as exception:
+            # use dill to pickle lambda functions
+            if "Can't pickle local object" not in str(exception) or "<lambda>" not in str(exception):
+                raise
+            import dill
+            unpickled = dill.loads(dill.dumps(original))
+
+        assert str(original) == str(unpickled), str(original)
 
 
 @pytest.mark.usefixtures('simulated_problem')
 @pytest.mark.parametrize('solve_options_update', [
     pytest.param({'costs_bounds': (-1e10, 1e10)}, id="non-binding costs bounds"),
     pytest.param({'check_optimality': 'both'}, id="Hessian computation")
 ])
 def test_trivial_changes(simulated_problem: SimulatedProblemFixture, solve_options_update: Dict) -> None:
     """Test that solving a problem with arguments that shouldn't give rise to meaningful differences doesn't give rise
     to any differences.
     """
     simulation, _, problem, solve_options, results = simulated_problem
 
     # solve the problem with the updated options
-    updated_solve_options = solve_options.copy()
+    updated_solve_options = copy.deepcopy(solve_options)
     updated_solve_options.update(solve_options_update)
     updated_results = problem.solve(**updated_solve_options)
 
     # test that all arrays in the results are essentially identical
     for key, result in results.__dict__.items():
-        if isinstance(result, np.ndarray) and result.dtype != np.object:
+        if isinstance(result, np.ndarray) and result.dtype != np.object_:
             if 'hessian' not in key:
                 np.testing.assert_allclose(result, getattr(updated_results, key), atol=1e-14, rtol=0, err_msg=key)
 
 
 @pytest.mark.usefixtures('simulated_problem')
 def test_parallel(simulated_problem: SimulatedProblemFixture) -> None:
     """Test that solving problems and computing results in parallel gives rise to the same results as when using serial
@@ -210,21 +341,29 @@
     """
     _, _, problem, solve_options, results = simulated_problem
 
     # compute marginal costs as a test of results (everything else has already been computed without parallelization)
     costs = results.compute_costs()
 
     # solve the problem and compute costs in parallel
-    with parallel(2):
-        parallel_results = problem.solve(**solve_options)
-        parallel_costs = parallel_results.compute_costs()
+    try:
+        with parallel(2):
+            parallel_results = problem.solve(**solve_options)
+            parallel_costs = parallel_results.compute_costs()
+    except RuntimeError as exception:
+        # use dill via pathos to handle lambda function pickling
+        if "multiprocessing module does not support lambda functions" not in str(exception):
+            raise
+        with parallel(2, use_pathos=True):
+            parallel_results = problem.solve(**solve_options)
+            parallel_costs = parallel_results.compute_costs()
 
     # test that all arrays in the results are essentially identical
     for key, result in results.__dict__.items():
-        if isinstance(result, np.ndarray) and result.dtype != np.object:
+        if isinstance(result, np.ndarray) and result.dtype != np.object_:
             np.testing.assert_allclose(result, getattr(parallel_results, key), atol=1e-14, rtol=0, err_msg=key)
 
     # test that marginal costs are essentially equal
     np.testing.assert_allclose(costs, parallel_costs, atol=1e-14, rtol=0)
 
 
 @pytest.mark.usefixtures('simulated_problem')
@@ -250,15 +389,15 @@
     if problem.K3 == 0:
         ES = 0
     if ED == ES == 0:
         return pytest.skip("There are no fixed effects to test.")
 
     # configure the optimization routine to only do a few iterations to save time and never get to the point where small
     #   numerical differences between methods build up into noticeable differences
-    solve_options = solve_options.copy()
+    solve_options = copy.deepcopy(solve_options)
     solve_options['optimization'] = Optimization('l-bfgs-b', {'maxfun': 3})
 
     # make product data mutable and add instruments
     product_data = {k: simulation_results.product_data[k] for k in simulation_results.product_data.dtype.names}
     product_data.update({
         'demand_instruments': problem.products.ZD[:, :-problem.K1],
         'supply_instruments': problem.products.ZS[:, :-problem.K3]
@@ -294,66 +433,93 @@
         instrument_names.append(name)
 
     # build formulas for the IDs
     demand_id_formula = ' + '.join(demand_id_names)
     supply_id_formula = ' + '.join(supply_id_names)
 
     # solve the first stage of a problem in which the fixed effects are absorbed
-    solve_options1 = solve_options.copy()
+    solve_options1 = copy.deepcopy(solve_options)
     product_formulations1 = product_formulations.copy()
     if ED > 0:
         assert product_formulations[0] is not None
         product_formulations1[0] = Formulation(
             product_formulations[0]._formula, demand_id_formula, absorb_method, absorb_options
         )
     if ES > 0:
         assert product_formulations[2] is not None
         product_formulations1[2] = Formulation(
             product_formulations[2]._formula, supply_id_formula, absorb_method, absorb_options
         )
-    problem1 = Problem(product_formulations1, product_data, problem.agent_formulation, simulation.agent_data)
+    problem1 = Problem(
+        product_formulations1, product_data, problem.agent_formulation, simulation.agent_data,
+        rc_types=simulation.rc_types, epsilon_scale=simulation.epsilon_scale, costs_type=simulation.costs_type
+    )
+    if solve_options1['micro_moments']:
+        solve_options1['W'] = scipy.linalg.pinv(scipy.linalg.block_diag(
+            problem1.products.ZD.T @ problem1.products.ZD,
+            problem1.products.ZS.T @ problem1.products.ZS,
+            np.eye(len(solve_options1['micro_moments'])),
+        ))
     problem_results1 = problem1.solve(**solve_options1)
 
     # solve the first stage of a problem in which fixed effects are included as indicator variables
-    solve_options2 = solve_options.copy()
+    solve_options2 = copy.deepcopy(solve_options)
     product_formulations2 = product_formulations.copy()
     if ED > 0:
         assert product_formulations[0] is not None
         product_formulations2[0] = Formulation(f'{product_formulations[0]._formula} + {demand_id_formula}')
     if ES > 0:
         assert product_formulations[2] is not None
         product_formulations2[2] = Formulation(f'{product_formulations[2]._formula} + {supply_id_formula}')
-    problem2 = Problem(product_formulations2, product_data, problem.agent_formulation, simulation.agent_data)
+    problem2 = Problem(
+        product_formulations2, product_data, problem.agent_formulation, simulation.agent_data,
+        rc_types=simulation.rc_types, epsilon_scale=simulation.epsilon_scale, costs_type=simulation.costs_type
+    )
     solve_options2['beta'] = np.r_[
         solve_options2['beta'],
         np.full((problem2.K1 - solve_options2['beta'].size, 1), np.nan)
     ]
+    if solve_options2['micro_moments']:
+        solve_options2['W'] = scipy.linalg.pinv(scipy.linalg.block_diag(
+            problem2.products.ZD.T @ problem2.products.ZD,
+            problem2.products.ZS.T @ problem2.products.ZS,
+            np.eye(len(solve_options2['micro_moments'])),
+        ))
     problem_results2 = problem2.solve(**solve_options2)
 
     # solve the first stage of a problem in which some fixed effects are absorbed and some are included as indicators
     if ED == ES == 0:
         problem_results3 = problem_results2
     else:
-        solve_options3 = solve_options.copy()
+        solve_options3 = copy.deepcopy(solve_options)
         product_formulations3 = product_formulations.copy()
         if ED > 0:
             assert product_formulations[0] is not None
             product_formulations3[0] = Formulation(
                 f'{product_formulations[0]._formula} + {demand_id_names[0]}', ' + '.join(demand_id_names[1:]) or None
             )
         if ES > 0:
             assert product_formulations[2] is not None
             product_formulations3[2] = Formulation(
                 f'{product_formulations[2]._formula} + {supply_id_names[0]}', ' + '.join(supply_id_names[1:]) or None
             )
-        problem3 = Problem(product_formulations3, product_data, problem.agent_formulation, simulation.agent_data)
+        problem3 = Problem(
+            product_formulations3, product_data, problem.agent_formulation, simulation.agent_data,
+            rc_types=simulation.rc_types, epsilon_scale=simulation.epsilon_scale, costs_type=simulation.costs_type
+        )
         solve_options3['beta'] = np.r_[
             solve_options3['beta'],
             np.full((problem3.K1 - solve_options3['beta'].size, 1), np.nan)
         ]
+        if solve_options3['micro_moments']:
+            solve_options3['W'] = scipy.linalg.pinv(scipy.linalg.block_diag(
+                problem3.products.ZD.T @ problem3.products.ZD,
+                problem3.products.ZS.T @ problem3.products.ZS,
+                np.eye(len(solve_options3['micro_moments'])),
+            ))
         problem_results3 = problem3.solve(**solve_options3)
 
     # compute optimal instruments (use only two draws for speed; accuracy is not a concern here)
     Z_results1 = problem_results1.compute_optimal_instruments(draws=2, seed=0)
     Z_results2 = problem_results2.compute_optimal_instruments(draws=2, seed=0)
     Z_results3 = problem_results3.compute_optimal_instruments(draws=2, seed=0)
 
@@ -380,14 +546,26 @@
         problem_results3.parameters[:2], np.eye(problem_results3.parameters.size)[:2]
     )
 
     # choose tolerances
     atol = 1e-8
     rtol = 1e-5
 
+    # test that fixed effect estimates are essentially identical
+    if ED == 0:
+        assert (problem_results1.xi_fe == 0).all()
+    else:
+        xi_fe2 = problem2.products.X1[:, problem1.K1:] @ problem_results2.beta[problem1.K1:]
+        np.testing.assert_allclose(problem_results1.xi_fe, xi_fe2, atol=atol, rtol=rtol)
+    if ES == 0:
+        assert (problem_results1.omega_fe == 0).all()
+    else:
+        omega_fe2 = problem2.products.X3[:, problem1.K3:] @ problem_results2.gamma[problem1.K3:]
+        np.testing.assert_allclose(problem_results1.omega_fe, omega_fe2, atol=atol, rtol=rtol)
+
     # test that all problem results expected to be identical are essentially identical, except for standard errors under
     #   micro moments, which are expected to be slightly different
     problem_results_keys = [
         'theta', 'sigma', 'pi', 'rho', 'beta', 'gamma', 'sigma_se', 'pi_se', 'rho_se', 'beta_se', 'gamma_se',
         'delta', 'tilde_costs', 'xi', 'omega', 'xi_by_theta_jacobian', 'omega_by_theta_jacobian', 'objective',
         'gradient', 'projected_gradient'
     ]
@@ -464,21 +642,52 @@
     costs2 = results.compute_costs(firm_ids=simulation_results.product_data.firm_ids)
     costs3 = results.compute_costs(ownership=build_ownership(simulation_results.product_data))
     np.testing.assert_equal(costs1, costs2)
     np.testing.assert_equal(costs1, costs3)
 
 
 @pytest.mark.usefixtures('simulated_problem')
+@pytest.mark.parametrize('iteration', [
+    pytest.param(Iteration('simple', {'atol': 1e-12}, universal_display=True), id="simple"),
+    pytest.param(Iteration('hybr', {'xtol': 1e-12, 'ftol': 0}, compute_jacobian=True), id="Powell/LM"),
+])
+def test_prices(simulated_problem: SimulatedProblemFixture, iteration: Iteration) -> None:
+    """Test that equilibrium prices computed with different methods are approximate the same.
+    """
+    simulation, simulation_results, _, _, _ = simulated_problem
+
+    # skip simulations that didn't compute equilibrium prices
+    if simulation_results.profit_gradient_norms is None:
+        return pytest.skip("Equilibrium prices were not computed.")
+
+    try:
+        new_simulation_results = simulation.replace_endogenous(iteration=iteration, constant_costs=False)
+    except (pyblp.exceptions.SyntheticPricesConvergenceError, pyblp.exceptions.MultipleErrors):
+        # sometimes Powell won't converge at these starting values but LM will
+        if 'hybr' not in str(iteration):
+            raise
+        iteration = Iteration('lm', {'xtol': 1e-12, 'ftol': 0}, compute_jacobian=True)
+        new_simulation_results = simulation.replace_endogenous(iteration=iteration, constant_costs=False)
+
+    np.testing.assert_allclose(
+        simulation_results.product_data.prices,
+        new_simulation_results.product_data.prices,
+        atol=1e-10,
+        rtol=0,
+    )
+
+
+@pytest.mark.usefixtures('simulated_problem')
 @pytest.mark.parametrize('ownership', [
     pytest.param(False, id="firm IDs change"),
     pytest.param(True, id="ownership change")
 ])
 @pytest.mark.parametrize('solve_options', [
     pytest.param({}, id="defaults"),
-    pytest.param({'iteration': Iteration('simple')}, id="configured iteration")
+    pytest.param({'iteration': Iteration('simple')}, id="configured iteration"),
 ])
 def test_merger(simulated_problem: SimulatedProblemFixture, ownership: bool, solve_options: Options) -> None:
     """Test that prices and shares simulated under changed firm IDs are reasonably close to prices and shares computed
     from the results of a solved problem. In particular, test that unchanged prices and shares are farther from their
     simulated counterparts than those computed by approximating a merger, which in turn are farther from their simulated
     counterparts than those computed by fully solving a merger. Also test that simple acquisitions increase HHI. These
     inequalities are only guaranteed because of the way in which the simulations are configured.
@@ -488,15 +697,15 @@
     # skip simulations that complicate the test
     if simulation.products.ownership.size > 0:
         return pytest.skip("Merger testing doesn't work with custom ownership.")
     if 'shares' in str(simulation.product_formulations[2]):
         return pytest.skip("Merger testing doesn't work with quantity-dependent costs.")
 
     # create changed ownership or firm IDs associated with a merger
-    merger_product_data = simulation_results.product_data.copy()
+    merger_product_data = copy.deepcopy(simulation_results.product_data)
     if ownership:
         merger_ids = None
         merger_ownership = build_ownership(merger_product_data, lambda f, g: 1 if f == g or (f < 2 and g < 2) else 0)
         merger_product_data = update_matrices(merger_product_data, {
             'ownership': (merger_ownership, merger_ownership.dtype)
         })
     else:
@@ -504,25 +713,25 @@
         merger_product_data.firm_ids[merger_product_data.firm_ids < 2] = 0
         merger_ids = merger_product_data.firm_ids
 
     # get actual prices and shares
     merger_simulation = Simulation(
         simulation.product_formulations, merger_product_data, simulation.beta, simulation.sigma, simulation.pi,
         simulation.gamma, simulation.rho, simulation.agent_formulation,
-        simulation.agent_data, xi=simulation.xi, omega=simulation.omega, distributions=simulation.distributions,
-        costs_type=simulation.costs_type
+        simulation.agent_data, xi=simulation.xi, omega=simulation.omega, rc_types=simulation.rc_types,
+        epsilon_scale=simulation.epsilon_scale, costs_type=simulation.costs_type
     )
     actual = merger_simulation.replace_endogenous(**solve_options)
 
     # compute marginal costs; get estimated prices and shares
     costs = results.compute_costs()
     results_simulation = Simulation(
         simulation.product_formulations[:2], merger_product_data, results.beta, results.sigma, results.pi,
         rho=results.rho, agent_formulation=simulation.agent_formulation, agent_data=simulation.agent_data,
-        xi=results.xi, distributions=simulation.distributions,
+        xi=results.xi, rc_types=simulation.rc_types, epsilon_scale=simulation.epsilon_scale
     )
     estimated = results_simulation.replace_endogenous(costs, problem.products.prices, **solve_options)
     estimated_prices = results.compute_prices(merger_ids, merger_ownership, costs, **solve_options)
     approximated_prices = results.compute_approximate_prices(merger_ids, merger_ownership, costs)
     estimated_shares = results.compute_shares(estimated_prices)
     approximated_shares = results.compute_shares(approximated_prices)
 
@@ -536,20 +745,14 @@
     np.testing.assert_array_less(estimated_prices_error, approximated_prices_error, verbose=True)
 
     # test that estimated shares are closer to changed shares than approximate shares
     approximated_shares_error = np.linalg.norm(actual.product_data.shares - approximated_shares)
     estimated_shares_error = np.linalg.norm(actual.product_data.shares - estimated_shares)
     np.testing.assert_array_less(estimated_shares_error, approximated_shares_error, verbose=True)
 
-    # test that median HHI increases
-    if not ownership:
-        hhi = results.compute_hhi()
-        changed_hhi = results.compute_hhi(merger_ids, estimated_shares)
-        np.testing.assert_array_less(np.median(hhi), np.median(changed_hhi), verbose=True)
-
 
 @pytest.mark.usefixtures('simulated_problem')
 def test_shares(simulated_problem: SimulatedProblemFixture) -> None:
     """Test that shares computed from estimated parameters are essentially equal to actual shares."""
     simulation, simulation_results, _, _, results = simulated_problem
     shares1 = results.compute_shares()
     shares2 = results.compute_shares(agent_data=simulation.agent_data, delta=results.delta)
@@ -563,51 +766,155 @@
     shares.
     """
     _, simulation_results, problem, _, results = simulated_problem
 
     # only do the test for a single market
     t = problem.products.market_ids[0]
     shares = problem.products.shares[problem.products.market_ids.flat == t]
+    prices = problem.products.prices[problem.products.market_ids.flat == t]
+    delta = results.delta[problem.products.market_ids.flat == t]
     weights = problem.agents.weights[problem.agents.market_ids.flat == t]
 
     # compute and compare shares
-    estimated_shares = results.compute_probabilities(market_id=t) @ weights
-    np.testing.assert_allclose(shares, estimated_shares, atol=1e-14, rtol=0, verbose=True)
+    estimated_shares1 = results.compute_probabilities(market_id=t) @ weights
+    estimated_shares2 = results.compute_probabilities(market_id=t, prices=prices, delta=delta) @ weights
+    np.testing.assert_allclose(shares, estimated_shares1, atol=1e-14, rtol=0, verbose=True)
+    np.testing.assert_allclose(shares, estimated_shares2, atol=1e-14, rtol=0, verbose=True)
 
 
 @pytest.mark.usefixtures('simulated_problem')
-def test_shares_by_prices_jacobian(simulated_problem: SimulatedProblemFixture) -> None:
+def test_surplus(simulated_problem: SimulatedProblemFixture) -> None:
+    """Test that integrating over individual-level surpluses gives market-level surpluses."""
+    _, _, problem, _, results = simulated_problem
+
+    # compute surpluses for a single market
+    t = problem.products.market_ids[0]
+    surpluses = results.compute_consumer_surpluses(market_id=t, keep_all=True)
+    surplus = results.compute_consumer_surpluses(market_id=t)
+
+    # test that we get the same result when manually integrating over surpluses
+    weights = problem.agents.weights[problem.agents.market_ids.flat == t]
+    np.testing.assert_allclose(surpluses @ weights, surplus, atol=1e-14, rtol=0, verbose=True)
+
+
+@pytest.mark.usefixtures('simulated_problem')
+def test_demand_jacobian(simulated_problem: SimulatedProblemFixture) -> None:
     """Use central finite differences to test that analytic values in the Jacobian of shares with respect to prices are
-    essentially equal.
+    essentially correct. Also test that scaled elasticities are exactly equal to this Jacobian.
     """
     simulation, simulation_results, _, _, results = simulated_problem
     product_data = simulation_results.product_data
 
     # only do the test for a single market
     t = product_data.market_ids[0]
     shares = product_data.shares[product_data.market_ids.flat == t]
     prices = product_data.prices[product_data.market_ids.flat == t]
 
-    # extract the Jacobian from the analytic expression for elasticities
-    exact = results.compute_elasticities(market_id=t) * shares / prices.T
+    # extract the Jacobian from the analytic expression for elasticities and approximate it with finite differences
+    exact1 = results.compute_demand_jacobians(market_id=t)
+    exact2 = results.compute_elasticities(market_id=t) * shares / prices.T
+    approximate = compute_finite_differences(lambda p: results.compute_shares(p, market_id=t), prices)
+    np.testing.assert_allclose(exact1, exact2, atol=1e-15, rtol=0)
+    np.testing.assert_allclose(exact1, approximate, atol=1e-8, rtol=0)
+
+
+@pytest.mark.usefixtures('simulated_problem')
+def test_demand_hessian(simulated_problem: SimulatedProblemFixture) -> None:
+    """Use central finite differences to test that analytic values in the Hessian of shares with respect to prices are
+    essentially correct.
+    """
+    simulation, simulation_results, problem, _, results = simulated_problem
+    product_data = simulation_results.product_data
+
+    # only do the test for a single market
+    t = product_data.market_ids[0]
+    prices = product_data.prices[product_data.market_ids.flat == t]
+    xi = results.xi[product_data.market_ids.flat == t]
+    if simulation.agent_data is None:
+        agent_data = None
+    else:
+        agent_data = simulation.agent_data[simulation.agent_data.market_ids.flat == t]
+
+    def compute_perturbed_jacobian(perturbed_prices: Array) -> Array:
+        """Compute the true Jacobian at perturbed prices."""
+        perturbed_product_data = copy.deepcopy(product_data[product_data.market_ids.flat == t])
+        perturbed_product_data.prices[:] = perturbed_prices
+        perturbed_simulation = Simulation(
+            problem.product_formulations[:2], perturbed_product_data, results.beta, results.sigma, results.pi,
+            rho=results.rho, agent_formulation=simulation.agent_formulation, agent_data=agent_data,
+            xi=xi, rc_types=problem.rc_types, epsilon_scale=problem.epsilon_scale, costs_type=problem.costs_type
+        )
+        perturbed_simulation_results = perturbed_simulation.replace_endogenous(
+            costs=perturbed_prices, prices=perturbed_prices, iteration=Iteration('return')
+        )
+        return perturbed_simulation_results.compute_demand_jacobians()
+
+    # compute analytic Hessians and approximate them with finite differences
+    exact = results.compute_demand_hessians(market_id=t)
+    approximate1 = compute_finite_differences(compute_perturbed_jacobian, prices)
+    approximate2 = compute_second_finite_differences(lambda p: results.compute_shares(p, market_id=t), prices)
+    np.testing.assert_allclose(exact, approximate1, atol=1e-7, rtol=0)
+    np.testing.assert_allclose(exact, approximate2, atol=1e-5, rtol=0)
+
+
+@pytest.mark.usefixtures('simulated_problem')
+def test_profit_hessian(simulated_problem: SimulatedProblemFixture) -> None:
+    """Use central finite differences to test that analytic values in the Hessian of profits with respect to prices are
+    essentially correct.
+    """
+    simulation, simulation_results, _, _, results = simulated_problem
+    product_data = simulation_results.product_data
+
+    # only do the test for a single market
+    t = product_data.market_ids[0]
+    prices = product_data.prices[product_data.market_ids.flat == t]
+    costs = results.compute_costs(market_id=t)
+
+    # compute the exact Hessian and approximat it with finite differences
+    exact = results.compute_profit_hessians(costs=costs, market_id=t)
+    approximate = compute_second_finite_differences(
+        lambda p: results.compute_profits(p, results.compute_shares(p, market_id=t), costs, market_id=t),
+        prices,
+    )
+    np.testing.assert_allclose(exact, approximate, atol=1e-6, rtol=0)
+
+
+@pytest.mark.usefixtures('simulated_problem')
+def test_passthrough(simulated_problem: SimulatedProblemFixture) -> None:
+    """Use central finite differences to test that analytic values in the passthrough matrix of prices with respect to
+    marginal costs are essentially correct.
+    """
+    simulation, simulation_results, problem, _, _ = simulated_problem
+    product_data = simulation_results.product_data
 
-    # estimate the Jacobian with central finite differences
-    estimated = np.zeros_like(exact)
-    change = np.sqrt(np.finfo(np.float64).eps)
-    for index in range(estimated.shape[1]):
-        prices1 = prices.copy()
-        prices2 = prices.copy()
-        prices1[index] += change / 2
-        prices2[index] -= change / 2
-        shares1 = results.compute_shares(prices1, market_id=t)
-        shares2 = results.compute_shares(prices2, market_id=t)
-        estimated[:, [index]] = (shares1 - shares2) / change
+    # obtain results at the true parameter values so there aren't issues with FOCs not matching
+    true_results = problem.solve(
+        beta=simulation.beta, sigma=simulation.sigma, pi=simulation.pi, rho=simulation.rho,
+        gamma=simulation.gamma if problem.K3 > 0 else None, delta=simulation_results.delta, method='1s',
+        check_optimality='gradient', optimization=Optimization('return'), iteration=Iteration('return')
+    )
 
-    # compare the two Jacobians
-    np.testing.assert_allclose(exact, estimated, atol=1e-8, rtol=0)
+    # only do the test for a single market
+    t = int(product_data.market_ids[0])
+    costs = true_results.compute_costs(market_id=t)
+
+    # use a looser tolerance if prices enter nonlinearly into utility (this is harder to approximate)
+    atol = 1e-6
+    for key in ['X1', 'X2']:
+        for formulation in problem.products.dtype.fields[key][2]:
+            if any(s.name == 'prices' for s in formulation.differentiate('prices').free_symbols):
+                atol = 2e-3
+
+    # compute the exact passthrough matrix and approximate it with finite differences
+    exact = true_results.compute_passthrough(market_id=t)
+    approximate = compute_finite_differences(
+        lambda c: true_results.compute_prices(costs=c, market_id=t, iteration=Iteration('simple', {'atol': 1e-16})),
+        costs,
+    )
+    np.testing.assert_allclose(exact, approximate, atol=atol, rtol=0)
 
 
 @pytest.mark.usefixtures('simulated_problem')
 @pytest.mark.parametrize('factor', [pytest.param(0.01, id="large"), pytest.param(0.0001, id="small")])
 def test_elasticity_aggregates_and_means(simulated_problem: SimulatedProblemFixture, factor: float) -> None:
     """Test that the magnitude of simulated aggregate elasticities is less than the magnitude of mean elasticities, both
     for prices and for other characteristics.
@@ -617,20 +924,21 @@
     # test that demand for an entire product category is less elastic for prices than for individual products
     np.testing.assert_array_less(
         np.abs(results.compute_aggregate_elasticities(factor)),
         np.abs(results.extract_diagonal_means(results.compute_elasticities())),
         verbose=True
     )
 
-    # test the same inequality but for all non-price variables
-    for name in {n for f in simulation._X1_formulations + simulation._X2_formulations for n in f.names} - {'prices'}:
+    # test the same inequality but for all non-price variables (including the mean utility)
+    names = {n for f in simulation._X1_formulations + simulation._X2_formulations for n in f.names}
+    for name in names - {'prices'} | {None}:
         np.testing.assert_array_less(
             np.abs(results.compute_aggregate_elasticities(factor, name)),
             np.abs(results.extract_diagonal_means(results.compute_elasticities(name))),
-            err_msg=name,
+            err_msg=str(name),
             verbose=True
         )
 
 
 @pytest.mark.usefixtures('simulated_problem')
 def test_diversion_ratios(simulated_problem: SimulatedProblemFixture) -> None:
     """Test that simulated diversion ratio rows sum to one."""
@@ -641,18 +949,19 @@
 
     # test price-based ratios
     ratios = results.compute_diversion_ratios(market_id=t)
     long_run_ratios = results.compute_long_run_diversion_ratios(market_id=t)
     np.testing.assert_allclose(ratios.sum(axis=1), 1, atol=1e-14, rtol=0)
     np.testing.assert_allclose(long_run_ratios.sum(axis=1), 1, atol=1e-14, rtol=0)
 
-    # test ratios based on other variables
-    for name in {n for f in simulation._X1_formulations + simulation._X2_formulations for n in f.names} - {'prices'}:
+    # test ratios based on other variables (including mean utilities)
+    names = {n for f in simulation._X1_formulations + simulation._X2_formulations for n in f.names}
+    for name in names - {'prices'} | {None}:
         ratios = results.compute_diversion_ratios(name, market_id=t)
-        np.testing.assert_allclose(ratios.sum(axis=1), 1, atol=1e-14, rtol=0, err_msg=name)
+        np.testing.assert_allclose(ratios.sum(axis=1), 1, atol=1e-14, rtol=0, err_msg=str(name))
 
 
 @pytest.mark.usefixtures('simulated_problem')
 def test_result_positivity(simulated_problem: SimulatedProblemFixture) -> None:
     """Test that simulated markups, profits, consumer surpluses are positive, both before and after a merger."""
     simulation, _, _, _, results = simulated_problem
 
@@ -668,56 +977,69 @@
     test_positive(results.compute_markups(market_id=t))
     test_positive(results.compute_profits(market_id=t))
     test_positive(results.compute_consumer_surpluses(market_id=t))
     test_positive(results.compute_markups(changed_prices, market_id=t))
     test_positive(results.compute_profits(changed_prices, changed_shares, market_id=t))
     test_positive(results.compute_consumer_surpluses(changed_prices, market_id=t))
 
+    # compute willingness to pay when the simulation has product IDs and test its positivity
+    if simulation.products.product_ids.size > 0:
+        unique_product_ids = np.unique(simulation.products.product_ids[simulation.products.market_ids.flat == t, 0])
+        eliminate0 = results.compute_consumer_surpluses(market_id=t)
+        eliminate1 = results.compute_consumer_surpluses(market_id=t, eliminate_product_ids=unique_product_ids[:1])
+        eliminate2 = results.compute_consumer_surpluses(market_id=t, eliminate_product_ids=unique_product_ids[:2])
+        test_positive(eliminate0 - eliminate1)
+        test_positive(eliminate1 - eliminate2)
+
 
 @pytest.mark.usefixtures('simulated_problem')
 def test_second_step(simulated_problem: SimulatedProblemFixture) -> None:
     """Test that results from two-step GMM on simulated data are identical to results from one-step GMM configured with
     results from a first step.
     """
     simulation, _, problem, solve_options, _ = simulated_problem
 
-    # use two steps and remove sigma bounds so that it can't get stuck at zero
-    updated_solve_options = solve_options.copy()
+    # use two steps and remove sigma bounds so that it can't get stuck at zero (use a small number of optimization
+    #   iterations to speed up the test)
+    updated_solve_options = copy.deepcopy(solve_options)
     updated_solve_options.update({
         'method': '2s',
-        'sigma_bounds': (np.full_like(simulation.sigma, -np.inf), np.full_like(simulation.sigma, +np.inf))
+        'optimization': Optimization('l-bfgs-b', {'maxfun': 3}),
+        'sigma_bounds': (np.full_like(simulation.sigma, -np.inf), np.full_like(simulation.sigma, +np.inf)),
     })
 
     # get two-step GMM results
     results12 = problem.solve(**updated_solve_options)
-    assert results12.last_results is not None and results12.last_results.last_results is None
+    assert results12.last_results is not None
+    assert results12.last_results.last_results is None or results12.last_results.last_results.step == 0
     assert results12.step == 2 and results12.last_results.step == 1
 
     # get results from the first step
-    updated_solve_options1 = updated_solve_options.copy()
+    updated_solve_options1 = copy.deepcopy(updated_solve_options)
     updated_solve_options1['method'] = '1s'
     results1 = problem.solve(**updated_solve_options1)
 
     # get results from the second step
-    updated_solve_options2 = updated_solve_options1.copy()
+    updated_solve_options2 = copy.deepcopy(updated_solve_options1)
     updated_solve_options2.update({
         'sigma': results1.sigma,
         'pi': results1.pi,
         'rho': results1.rho,
         'beta': np.where(np.isnan(solve_options['beta']), np.nan, results1.beta),
         'delta': results1.delta,
-        'W': results1.updated_W
+        'W': results1.updated_W,
     })
     results2 = problem.solve(**updated_solve_options2)
-    assert results1.last_results is None and results2.last_results is None
+    assert results1.last_results is None or results1.last_results.step == 0
+    assert results2.last_results is None or results2.last_results.step == 0
     assert results1.step == results2.step == 1
 
     # test that results are essentially identical
     for key, result12 in results12.__dict__.items():
-        if 'cumulative' not in key and isinstance(result12, np.ndarray) and result12.dtype != np.object:
+        if 'cumulative' not in key and isinstance(result12, np.ndarray) and result12.dtype != np.object_:
             np.testing.assert_allclose(result12, getattr(results2, key), atol=1e-14, rtol=0, err_msg=key)
 
 
 @pytest.mark.usefixtures('simulated_problem')
 def test_return(simulated_problem: SimulatedProblemFixture) -> None:
     """Test that using a trivial optimization and fixed point iteration routines that just return initial values yield
     results that are the same as the specified initial values.
@@ -729,122 +1051,111 @@
         'sigma': simulation.sigma,
         'pi': simulation.pi,
         'rho': simulation.rho,
         'beta': simulation.beta,
         'gamma': simulation.gamma if problem.K3 > 0 else None,
         'delta': problem.products.X1 @ simulation.beta + simulation.xi
     }
-    updated_solve_options = solve_options.copy()
+    updated_solve_options = copy.deepcopy(solve_options)
     updated_solve_options.update({
         'optimization': Optimization('return'),
         'iteration': Iteration('return'),
         **initial_values
     })
 
     # obtain problem results and test that initial values are the same
     results = problem.solve(**updated_solve_options)
     for key, initial in initial_values.items():
         if initial is not None:
-            np.testing.assert_allclose(initial, getattr(results, key), atol=1e-14, rtol=0, err_msg=key)
-
-
-@pytest.mark.usefixtures('simulated_problem')
-def test_initial_update(simulated_problem: SimulatedProblemFixture) -> None:
-    """Test that estimation under an initial update gives the same results as when it's done manually."""
-    simulation, _, problem, solve_options, _ = simulated_problem
-
-    # obtain initial results at the starting values
-    initial_solve_options = solve_options.copy()
-    initial_solve_options['optimization'] = Optimization('return')
-    initial_results = problem.solve(**initial_solve_options)
-
-    # manually solve the problem with initial weighting matrix and delta values
-    manual_solve_options = solve_options.copy()
-    manual_solve_options.update({
-        'W': initial_results.updated_W,
-        'delta': initial_results.delta
-    })
-    manual_results = problem.solve(**manual_solve_options)
-
-    # automatically do the same
-    automatic_solve_options = solve_options.copy()
-    automatic_solve_options['initial_update'] = True
-    automatic_results = problem.solve(**automatic_solve_options)
-
-    # test that results are essentially identical
-    for key, manual_result in manual_results.__dict__.items():
-        if 'cumulative' not in key and isinstance(manual_result, np.ndarray) and manual_result.dtype != np.object:
-            np.testing.assert_allclose(manual_result, getattr(automatic_results, key), atol=1e-14, rtol=0, err_msg=key)
+            np.testing.assert_allclose(initial, getattr(results, key), atol=1e-14, rtol=1e-14, err_msg=key)
 
 
 @pytest.mark.usefixtures('simulated_problem')
 @pytest.mark.parametrize('scipy_method', [
     pytest.param('l-bfgs-b', id="L-BFGS-B"),
     pytest.param('trust-constr', id="Trust Region")
 ])
 def test_gradient_optionality(simulated_problem: SimulatedProblemFixture, scipy_method: str) -> None:
     """Test that the option of not computing the gradient for simulated data does not affect estimates when the gradient
     isn't used.
     """
-    simulation, _, problem, solve_options, _ = simulated_problem
+    simulation, _, problem, solve_options, results = simulated_problem
+
+    # this test doesn't work when there's a supply side because we use an inverse instead of solving a linear system
+    #   directly to save on computation when computing gradients
+    if problem.K3 > 0:
+        return pytest.skip("Results are expected to change when there's a supply side.")
+
+    # this test only requires a few optimization iterations (enough for gradient problems to be clear)
+    method_options = {'maxiter': 2}
 
     def custom_method(
             initial: Array, bounds: List[Tuple[float, float]], objective_function: Callable, _: Any) -> (
             Tuple[Array, bool]):
         """Optimize without gradients."""
-        wrapper = lambda x: objective_function(x)[0]
-        optimize_results = scipy.optimize.minimize(wrapper, initial, method=scipy_method, bounds=bounds)
+        optimize_results = scipy.optimize.minimize(
+            lambda x: objective_function(x)[0], initial, method=scipy_method, bounds=bounds, options=method_options
+        )
         return optimize_results.x, optimize_results.success
 
-    # solve the problem when not using gradients and when not computing them
-    updated_solve_options1 = solve_options.copy()
-    updated_solve_options2 = solve_options.copy()
-    updated_solve_options1['optimization'] = Optimization(custom_method)
-    updated_solve_options2['optimization'] = Optimization(scipy_method, compute_gradient=False)
+    # solve the problem when not using gradients and when not computing them (use the identity weighting matrix to make
+    #   tiny gradients with some initial weighting matrices less problematic when comparing values)
+    updated_solve_options1 = copy.deepcopy(solve_options)
+    updated_solve_options2 = copy.deepcopy(solve_options)
+    updated_solve_options1.update({
+        'optimization': Optimization(custom_method),
+    })
+    updated_solve_options2.update({
+        'optimization': Optimization(scipy_method, method_options, compute_gradient=False),
+        'finite_differences': True,
+    })
     results1 = problem.solve(**updated_solve_options1)
     results2 = problem.solve(**updated_solve_options2)
 
-    # test that all arrays are essentially identical
+    # test that all arrays close except for those created with finite differences after the fact
     for key, result1 in results1.__dict__.items():
-        if isinstance(result1, np.ndarray) and result1.dtype != np.object:
-            np.testing.assert_allclose(result1, getattr(results2, key), atol=1e-14, rtol=0, err_msg=key)
+        if isinstance(result1, np.ndarray) and result1.dtype != np.object_:
+            if not any(s in key for s in ['gradient', '_jacobian', '_se', '_covariances']):
+                np.testing.assert_allclose(result1, getattr(results2, key), atol=1e-14, rtol=0, err_msg=key)
 
 
 @pytest.mark.usefixtures('simulated_problem')
-@pytest.mark.parametrize('method', [
-    pytest.param('l-bfgs-b', id="L-BFGS-B"),
-    pytest.param('trust-constr', id="trust-region"),
-    pytest.param('tnc', id="TNC"),
-    pytest.param('slsqp', id="SLSQP"),
-    pytest.param('knitro', id="Knitro"),
-    pytest.param('cg', id="CG"),
-    pytest.param('bfgs', id="BFGS"),
-    pytest.param('newton-cg', id="Newton-CG"),
-    pytest.param('nelder-mead', id="Nelder-Mead"),
-    pytest.param('powell', id="Powell")
+@pytest.mark.parametrize('method_info', [
+    pytest.param(('l-bfgs-b', 'gtol'), id="L-BFGS-B"),
+    pytest.param(('trust-constr', 'gtol'), id="trust-region"),
+    pytest.param(('tnc', 'gtol'), id="TNC"),
+    pytest.param(('slsqp', 'ftol'), id="SLSQP"),
+    pytest.param(('knitro', 'ftol'), id="Knitro"),
+    pytest.param(('cg', 'gtol'), id="CG"),
+    pytest.param(('bfgs', 'gtol'), id="BFGS"),
+    pytest.param(('newton-cg', 'xtol'), id="Newton-CG"),
+    pytest.param(('nelder-mead', 'xatol'), id="Nelder-Mead"),
+    pytest.param(('powell', 'xtol'), id="Powell")
 ])
-def test_bounds(simulated_problem: SimulatedProblemFixture, method: str) -> None:
+def test_bounds(simulated_problem: SimulatedProblemFixture, method_info: Tuple[str, str]) -> None:
     """Test that non-binding bounds on parameters in simulated problems do not affect estimates and that binding bounds
     are respected. Forcing parameters to be far from their optimal values creates instability problems, so this is also
     a test of how well estimation handles unstable problems.
     """
     simulation, _, problem, solve_options, _ = simulated_problem
+    method, tol_option = method_info
 
-    # skip optimization methods that haven't been configured properly
-    updated_solve_options = solve_options.copy()
+    # skip optimization methods that haven't been configured properly, using a loose tolerance to speed up the test
+    updated_solve_options = copy.deepcopy(solve_options)
     try:
         updated_solve_options['optimization'] = Optimization(
             method,
+            method_options={tol_option: 1e-8 if method == 'trust-constr' else 0.1},
             compute_gradient=method not in {'nelder-mead', 'powell'}
         )
     except OSError as exception:
         return pytest.skip(f"Failed to use the {method} method in this environment: {exception}.")
 
     # solve the problem when unbounded
-    unbounded_solve_options = updated_solve_options.copy()
+    unbounded_solve_options = copy.deepcopy(updated_solve_options)
     unbounded_solve_options.update({
         'sigma_bounds': (np.full_like(simulation.sigma, -np.inf), np.full_like(simulation.sigma, +np.inf)),
         'pi_bounds': (np.full_like(simulation.pi, -np.inf), np.full_like(simulation.pi, +np.inf)),
         'rho_bounds': (np.full_like(simulation.rho, -np.inf), np.full_like(simulation.rho, +np.inf)),
         'beta_bounds': (np.full_like(simulation.beta, -np.inf), np.full_like(simulation.beta, +np.inf)),
         'gamma_bounds': (np.full_like(simulation.gamma, -np.inf), np.full_like(simulation.gamma, +np.inf))
     })
@@ -894,15 +1205,15 @@
             binding_beta_bounds[0][beta_index] = beta_value - lb_scale * np.abs(beta_value)
             binding_beta_bounds[1][beta_index] = beta_value + ub_scale * np.abs(beta_value)
         if problem.K3 > 0:
             binding_gamma_bounds[0][gamma_index] = gamma_value - lb_scale * np.abs(gamma_value)
             binding_gamma_bounds[1][gamma_index] = gamma_value + ub_scale * np.abs(gamma_value)
 
         # update options with the binding bounds
-        binding_solve_options = updated_solve_options.copy()
+        binding_solve_options = copy.deepcopy(updated_solve_options)
         binding_solve_options.update({
             'sigma': np.clip(binding_solve_options['sigma'], *binding_sigma_bounds),
             'pi': np.clip(binding_solve_options['pi'], *binding_pi_bounds),
             'rho': np.clip(binding_solve_options['rho'], *binding_rho_bounds),
             'sigma_bounds': binding_sigma_bounds,
             'pi_bounds': binding_pi_bounds,
             'rho_bounds': binding_rho_bounds,
@@ -958,45 +1269,325 @@
     product_data = simulation_results.product_data
     extra_agent_data = {k: simulation.agent_data[k] for k in simulation.agent_data.dtype.names}
     extra_agent_data['nodes'] = np.c_[extra_agent_data['nodes'], extra_agent_data['nodes']]
     new_problem = Problem(problem.product_formulations, product_data, problem.agent_formulation, extra_agent_data)
 
     # test that the agents are essentially identical
     for key in problem.agents.dtype.names:
-        if problem.agents[key].dtype != np.object:
+        if problem.agents[key].dtype != np.object_:
             np.testing.assert_allclose(problem.agents[key], new_problem.agents[key], atol=1e-14, rtol=0, err_msg=key)
 
 
 @pytest.mark.usefixtures('simulated_problem')
 def test_extra_demographics(simulated_problem: SimulatedProblemFixture) -> None:
     """Test that agents in a simulated problem are identical to agents in a problem created with agent data built
     according to the same integration specification and but containing unnecessary rows of demographics.
     """
     simulation, simulation_results, problem, _, _ = simulated_problem
 
     # skip simulations without demographics
     if simulation.D == 0:
         return pytest.skip("There are no demographics.")
 
-    # reconstruct the problem with unnecessary rows of demographics
+    # also skip those with custom agents
     assert simulation.agent_data is not None
-    product_data = simulation_results.product_data
     agent_data = simulation.agent_data
+    if 'weights' in agent_data.dtype.names:
+        return pytest.skip("There are custom agents.")
+
+    # reconstruct the problem with unnecessary rows of demographics
+    product_data = simulation_results.product_data
     extra_agent_data = {k: np.r_[agent_data[k], agent_data[k]] for k in agent_data.dtype.names}
     new_problem = Problem(
         problem.product_formulations, product_data, problem.agent_formulation, extra_agent_data, simulation.integration
     )
 
     # test that the agents are essentially identical
     for key in problem.agents.dtype.names:
-        if problem.agents[key].dtype != np.object:
+        if problem.agents[key].dtype != np.object_:
             np.testing.assert_allclose(problem.agents[key], new_problem.agents[key], atol=1e-14, rtol=0, err_msg=key)
 
 
 @pytest.mark.usefixtures('simulated_problem')
+def test_logit_errors(simulated_problem: SimulatedProblemFixture) -> None:
+    """Test that first and second choice probabilities are correct by comparing frequencies of second choices when
+    simulated according to analytic expression that integrate over logit errors when simulating logit errors directly.
+    For nested logit errors, call the R package evd by Alec Stephenson.
+    """
+    simulation, simulation_results, problem, _, _ = simulated_problem
+
+    # skip problems with supply since this test will be the same
+    if problem.K3 > 0:
+        return pytest.skip("Configurations with supply give the same test.")
+
+    # select only a few products (if there's nesting, select two from each of two nests)
+    J = 4
+    small_product_data = simulation_results.product_data[:J].copy()
+    small_product_data.shares[:] = np.c_[[0.1, 0.2, 0.3, 0.15]]
+    small_product_data.market_ids[:] = simulation.unique_market_ids[0]
+    if small_product_data.ownership.size > 0:
+        small_product_data = update_matrices(small_product_data, {
+            'ownership': (small_product_data.ownership[:, :J], small_product_data.ownership.dtype)
+        })
+    if simulation.H > 0:
+        small_product_data.nesting_ids[:2] = simulation.unique_nesting_ids[0]
+        small_product_data.nesting_ids[2:] = simulation.unique_nesting_ids[1]
+
+    # select just the first consumer type
+    small_agent_data = None
+    if simulation.agent_data is not None:
+        small_agent_data = simulation.agent_data[[0]].copy()
+        small_agent_data.weights[:] = 1
+
+    # create a simulation with only a few products and one consumer type to reduce the number of needed simulation draws
+    small_product_data = update_matrices(small_product_data, {'extra': (np.zeros(J), np.float64)})
+    assert simulation.product_formulations[0] is not None
+    product_formulations = (
+        Formulation(f'{simulation.product_formulations[0]._formula} + extra'),
+        simulation.product_formulations[1],
+    )
+    beta = np.r_[simulation.beta.flatten(), 1]
+    small_simulation = Simulation(
+        product_formulations, small_product_data, beta, simulation.sigma, simulation.pi, rho=simulation.rho[:2],
+        agent_formulation=simulation.agent_formulation, agent_data=small_agent_data, xi=simulation.xi[:J],
+        rc_types=simulation.rc_types, epsilon_scale=simulation.epsilon_scale
+    )
+    small_results = small_simulation.replace_exogenous('extra')
+
+    # simulate according to analytic second choice probabilities, integrating over idiosyncratic preferences
+    observations = 1_000_000
+    dataset = MicroDataset(
+        name="Second Choices",
+        observations=observations,
+        compute_weights=lambda _, p, a: np.ones((a.size, 1 + p.size, 1 + p.size)),
+    )
+    micro_data = small_results.simulate_micro_data(dataset, seed=0)
+
+    # compute random coefficients
+    coefficients = small_simulation.sigma @ small_simulation.agents.nodes.T
+    if small_simulation.D > 0:
+        coefficients = coefficients + small_simulation.pi @ small_simulation.agents.demographics.T
+
+    for k, rc_type in enumerate(small_simulation.rc_types):
+        if rc_type == 'log':
+            if len(coefficients.shape) == 2:
+                coefficients[k] = np.exp(coefficients[k])
+            else:
+                coefficients[:, k] = np.exp(coefficients[:, k])
+        elif rc_type == 'logit':
+            if len(coefficients.shape) == 2:
+                coefficients[k] = scipy.special.expit(coefficients[k])
+            else:
+                coefficients[:, k] = scipy.special.expit(coefficients[:, k])
+
+    # compute mu
+    if len(coefficients.shape) == 2:
+        mu = small_simulation.products.X2 @ coefficients
+    else:
+        mu = (small_simulation.products.X2[..., None] * coefficients).sum(axis=1)
+
+    # simulate logit shocks, either in Python for simple logit or by calling R for nested logit
+    epsilon: Array
+    if simulation.H == 0:
+        epsilon = simulation.epsilon_scale * np.random.default_rng(0).gumbel(size=(observations, 1 + J))
+    else:
+        if shutil.which('Rscript') is None:
+            return pytest.skip("Failed to find an R executable in this environment.")
+
+        # build asymmetry and dependence configurations for simulating nested logit shocks with the evd package
+        asymmetry = []
+        dependence = []
+        for size in range(1, 2 + J):
+            for combination in itertools.combinations(range(1 + J), size):
+                asymmetry.append(len(combination) * [int(combination in {(0,), (1, 2), (3, 4)})])
+                if size > 1:
+                    if combination == (1, 2):
+                        dependence.append(1 - float(simulation.rho if simulation.rho.size == 1 else simulation.rho[0]))
+                    elif combination == (3, 4):
+                        dependence.append(1 - float(simulation.rho if simulation.rho.size == 1 else simulation.rho[1]))
+                    else:
+                        dependence.append(1)
+
+        # simulate the shocks with R, save them to a temporary file, and load them into memory
+        epsilon = None
+        with tempfile.NamedTemporaryFile(delete=False) as handle:
+            asy = 'list(' + ', '.join('c(' + ', '.join(str(v) for v in a) + ')' for a in asymmetry) + ')'
+            dep = 'c(' + ', '.join(str(v) for v in dependence) + ')'
+            path = handle.name.replace('\\', '/')
+            try:
+                command = ';'.join([
+                    "suppressWarnings(library('evd'))",
+                    "set.seed(0)",
+                    f"epsilon = rmvevd(n={observations}, dep={dep}, asy={asy}, model='alog', d={1 + J})",
+                    f"write.table(epsilon, file='{path}', row.names=FALSE, col.names=FALSE)",
+                ])
+                subprocess.run(['Rscript', '-e', command])
+                epsilon = np.loadtxt(handle.name)
+            finally:
+                try:
+                    os.remove(handle.name)
+                except OSError:
+                    pass
+
+    # compute deterministic choices given simulated idiosyncratic preferences
+    utilities = np.r_[0, small_results.delta.flatten() + mu.flatten()] + epsilon
+    choice_indices = np.argmax(utilities, axis=1)
+    utilities[np.arange(observations), choice_indices] = -np.inf
+    second_choice_indices = np.argmax(utilities, axis=1)
+
+    # compute histograms
+    histogram1 = np.zeros(1 + J)
+    histogram2 = np.zeros(1 + J)
+    second_histogram1 = np.zeros((1 + J, 1 + J))
+    second_histogram2 = np.zeros((1 + J, 1 + J))
+    np.add.at(histogram1, micro_data.choice_indices, 1 / observations)
+    np.add.at(histogram2, choice_indices, 1 / observations)
+    np.add.at(second_histogram1, (micro_data.choice_indices, micro_data.second_choice_indices), 1 / observations)
+    np.add.at(second_histogram2, (choice_indices, second_choice_indices), 1 / observations)
+
+    # compare histograms
+    np.testing.assert_allclose(histogram2, histogram1, rtol=0.01, atol=0.001)
+    np.testing.assert_allclose(second_histogram2, second_histogram1, rtol=0.01, atol=0.001)
+
+
+@pytest.mark.usefixtures('simulated_problem')
+def test_micro_values(simulated_problem: SimulatedProblemFixture) -> None:
+    """Test that true micro values are close to those computed from simulated micro data."""
+    simulation, simulation_results, problem, solve_options, problem_results = simulated_problem
+
+    # skip simulations without micro moments
+    if not solve_options['micro_moments']:
+        return pytest.skip("There are no micro moments.")
+
+    # test each micro value separately
+    for moment in solve_options['micro_moments']:
+        part_values = []
+        for part in moment.parts:
+            # simulate micro data
+            dataset = MicroDataset(
+                name=part.dataset.name,
+                observations=1_000_000,
+                compute_weights=part.dataset.compute_weights,
+                market_ids=part.dataset.market_ids,
+                eliminated_product_ids_index=part.dataset.eliminated_product_ids_index,
+            )
+            micro_data = simulation_results.simulate_micro_data(dataset, seed=0)
+
+            # collect market IDs
+            if dataset.market_ids is None:
+                market_ids = problem.unique_market_ids
+            else:
+                market_ids = np.asarray(list(dataset.market_ids))
+
+            # collect values market-by-market
+            values_list = []
+            for t in market_ids:
+                products = problem.products[problem.products.market_ids.flat == t]
+                agents = problem.agents[problem.agents.market_ids.flat == t]
+                values = part.compute_values(t, products, agents)
+
+                data = micro_data[micro_data.market_ids.flat == t]
+                indices = [data.agent_indices.flatten(), data.choice_indices.flatten()]
+                if len(values.shape) == 3:
+                    indices.append(data.second_choice_indices.flatten())
+
+                values_list.append(values[tuple(indices)])
+
+            part_values.append(np.concatenate(values_list).mean())
+
+        # compute the micro values
+        value = moment.compute_value(np.array(part_values))
+        np.testing.assert_allclose(moment.value, value, atol=0, rtol=0.01, err_msg=moment)
+
+
+@pytest.mark.usefixtures('simulated_problem')
+def test_micro_scores(simulated_problem: SimulatedProblemFixture) -> None:
+    """Test that true micro scores without any unobserved heterogeneity are zero, and that micro scores with unobserved
+    heterogeneity based on micro data are of the same order of magnitude as the truth.
+    """
+    simulation, simulation_results, problem, solve_options, problem_results = simulated_problem
+
+    # skip simulations without micro moments
+    if not solve_options['micro_moments']:
+        return pytest.skip("There are no micro moments.")
+
+    # skip simulations with supply because testing their demand-only version is sufficient
+    if problem.K3 > 0:
+        return pytest.skip("There is a supply side.")
+
+    # test each results type and dataset separately
+    results_mapping = {
+        'simulation': simulation_results,
+        'problem': problem_results,
+    }
+    datasets = {p.dataset for m in solve_options['micro_moments'] for p in m.parts}
+    for (name, results), dataset in itertools.product(results_mapping.items(), datasets):
+        # test that true micro score expectations without any unobserved heterogeneity are zero
+        true_observed_scores = results.compute_agent_scores(dataset)
+        true_observed_moments = []
+        for p, true_observed_scores_p in enumerate(true_observed_scores):
+            true_observed_moments.append(MicroMoment(
+                name=f'{dataset.name}, parameter {p}',
+                value=0,
+                parts=MicroPart(
+                    name=f'{dataset.name}, parameter {p}',
+                    dataset=dataset,
+                    compute_values=lambda t, _, __, v=true_observed_scores_p: np.nan_to_num(v[t]),
+                ),
+            ))
+
+        true_observed_means = results.compute_micro_values(true_observed_moments)
+        np.testing.assert_allclose(true_observed_means, 0, atol=1e-13, rtol=0, err_msg=f'{name}: {dataset}')
+
+        # either use the simulation's integration configuration or choose one
+        integration = simulation.integration
+        if integration is None:
+            integration = Integration('product', 2)
+
+        # test that micro scores with unobserved heterogeneity based micro data are of the same order of magnitude as
+        #   the truth
+        true_scores = results.compute_agent_scores(dataset, integration=integration)
+        true_moments = []
+        for p, true_scores_p in enumerate(true_scores):
+            true_moments.append(MicroMoment(
+                name=f'{dataset.name}, parameter {p}',
+                value=0,
+                parts=MicroPart(
+                    name=f'{dataset.name}, parameter {p}',
+                    dataset=dataset,
+                    compute_values=lambda t, _, __, v=true_scores_p: np.nan_to_num(v[t]),
+                ),
+            ))
+
+        true_means = results.compute_micro_values(true_moments)
+
+        dataset = MicroDataset(
+            name=dataset.name,
+            observations=2_000,
+            compute_weights=dataset.compute_weights,
+            market_ids=dataset.market_ids,
+            eliminated_product_ids_index=dataset.eliminated_product_ids_index,
+        )
+        data = data_to_dict(results.simulate_micro_data(dataset, seed=0))
+
+        # merge in demographics
+        assert simulation.agent_data is not None
+        for key in simulation.agent_data.dtype.names:
+            if key not in data:
+                data[key] = np.zeros((data['micro_ids'].size, *simulation.agent_data[key].shape[1:]))
+                for s, agent_indices in simulation._agent_market_indices.items():
+                    indices = data['market_ids'] == s
+                    data[key][indices] = simulation.agent_data[key][agent_indices][data['agent_indices'][indices]]
+
+        scores = results.compute_micro_scores(dataset, data, integration)
+        means = np.array(scores).mean(axis=1)
+        np.testing.assert_allclose(means, true_means, atol=1e-13, rtol=10, err_msg=f'{name}: {dataset}')
+
+
+@pytest.mark.usefixtures('simulated_problem')
 @pytest.mark.parametrize('eliminate', [
     pytest.param(True, id="linear parameters eliminated"),
     pytest.param(False, id="linear parameters not eliminated")
 ])
 @pytest.mark.parametrize('demand', [
     pytest.param(True, id="demand"),
     pytest.param(False, id="no demand")
@@ -1021,27 +1612,31 @@
         return pytest.skip("The problem does not have supply-side moments to test.")
     if micro and not solve_options['micro_moments']:
         return pytest.skip("The problem does not have micro moments to test.")
     if not demand and not supply and not micro:
         return pytest.skip("There are no moments to test.")
 
     # configure the options used to solve the problem
-    updated_solve_options = solve_options.copy()
+    updated_solve_options = copy.deepcopy(solve_options)
     updated_solve_options.update({k: 0.9 * solve_options[k] for k in ['sigma', 'pi', 'rho', 'beta']})
 
     # optionally include linear parameters in theta
     if not eliminate:
         if problem.K1 > 0:
             updated_solve_options['beta'][-1] = 0.9 * simulation.beta[-1]
         if problem.K3 > 0:
             updated_solve_options['gamma'] = np.full_like(simulation.gamma, np.nan)
             updated_solve_options['gamma'][-1] = 0.9 * simulation.gamma[-1]
 
     # zero out weighting matrix blocks to only test individual contributions of the gradient
-    updated_solve_options['W'] = problem_results.W.copy()
+    updated_solve_options['W'] = copy.deepcopy(problem_results.W)
+    if micro:
+        MM = len(updated_solve_options['micro_moments'])
+        updated_solve_options['W'][-MM:, -MM:] = np.eye(MM)
+    updated_solve_options['W'] = np.eye(problem_results.W.shape[0])
     if not demand:
         updated_solve_options['W'][:problem.MD, :problem.MD] = 0
     if not supply and problem.K3 > 0:
         updated_solve_options['W'][problem.MD:problem.MD + problem.MS, problem.MD:problem.MD + problem.MS] = 0
     if not micro and updated_solve_options['micro_moments']:
         MM = len(updated_solve_options['micro_moments'])
         updated_solve_options['W'][-MM:, -MM:] = 0
@@ -1051,31 +1646,42 @@
 
     # compute the analytic gradient
     updated_solve_options['optimization'] = Optimization('return')
     exact = problem.solve(**updated_solve_options).gradient
 
     def test_finite_differences(theta: Array, _: Any, objective_function: Callable, __: Any) -> Tuple[Array, bool]:
         """Test central finite differences around starting parameter values."""
-        estimated = np.zeros_like(exact)
-        change = 10 * np.sqrt(np.finfo(np.float64).eps)
-        for index in range(theta.size):
-            theta1 = theta.copy()
-            theta2 = theta.copy()
-            theta1[index] += change / 2
-            theta2[index] -= change / 2
-            estimated[index] = (objective_function(theta1)[0] - objective_function(theta2)[0]) / change
-        np.testing.assert_allclose(estimated, exact, atol=1e-8, rtol=1e-3)
+        approximated = compute_finite_differences(lambda x: objective_function(x)[0], theta, epsilon_scale=10.0)
+        np.testing.assert_allclose(approximated.flatten(), exact.flatten(), atol=1e-8, rtol=1e-2)
         return theta, True
 
     # test the gradient
     updated_solve_options['optimization'] = Optimization(test_finite_differences, compute_gradient=False)
     problem.solve(**updated_solve_options)
 
 
 @pytest.mark.usefixtures('simulated_problem')
+def test_sigma_squared_se(simulated_problem: SimulatedProblemFixture) -> None:
+    """Test that standard errors for sigma * sigma' computed with the delta method match a simple expression for when
+    sigma is diagonal.
+    """
+    _, _, problem, _, results = simulated_problem
+
+    # skip some unneeded tests
+    if problem.K2 == 0:
+        return pytest.skip("There's nothing to test without random coefficients.")
+    if (np.tril(results.sigma, k=-1) != 0).any():
+        return pytest.skip("There isn't a simple expression for when sigma isn't diagonal.")
+
+    # compute standard errors with the simple expression and compare
+    sigma_squared_se = np.nan_to_num(2 * results.sigma.diagonal() * results.sigma_se.diagonal())
+    np.testing.assert_allclose(sigma_squared_se, results.sigma_squared_se.diagonal(), atol=1e-14, rtol=0)
+
+
+@pytest.mark.usefixtures('simulated_problem')
 @pytest.mark.parametrize('method', [
     pytest.param('1s', id="one-step"),
     pytest.param('2s', id="two-step")
 ])
 @pytest.mark.parametrize('center_moments', [pytest.param(True, id="centered"), pytest.param(False, id="uncentered")])
 @pytest.mark.parametrize('W_type', [
     pytest.param('robust', id="robust W"),
@@ -1086,19 +1692,19 @@
     pytest.param('robust', id="robust SEs"),
     pytest.param('unadjusted', id="unadjusted SEs"),
     pytest.param('clustered', id="clustered SEs")
 ])
 def test_logit(
         simulated_problem: SimulatedProblemFixture, method: str, center_moments: bool, W_type: str, se_type: str) -> (
         None):
-    """Test that Logit estimates are the same as those from the the linearmodels package."""
+    """Test that Logit estimates are the same as those from the linearmodels package."""
     _, simulation_results, problem, _, _ = simulated_problem
 
     # skip more complicated simulations
-    if problem.K2 > 0 or problem.K3 > 0 or problem.H > 0:
+    if problem.K2 > 0 or problem.K3 > 0 or problem.H > 0 or problem.epsilon_scale != 1:
         return pytest.skip("This simulation cannot be tested against linearmodels.")
 
     # solve the problem
     results1 = problem.solve(method=method, center_moments=center_moments, W_type=W_type, se_type=se_type)
 
     # compute the delta from the logit problem
     delta = np.log(simulation_results.product_data.shares)
```

### Comparing `pyblp-0.9.0/tests/test_formulation.py` & `pyblp-1.0.0/tests/test_formulation.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 """Tests of formulation of data matrices."""
 
+import copy
 import itertools
 import traceback
 from typing import Any, Callable, Iterable, Mapping, Sequence, Type
 
 import numpy as np
 import patsy
 import pytest
@@ -136,24 +137,24 @@
 ])
 def test_ids(
         formula_data: Data, formulas: Iterable[str],
         build_columns: Callable[[Mapping[str, Array]], Sequence[Array]]) -> None:
     """Test that equivalent formulas build IDs as expected."""
 
     # create convenience columns of tuples of categorical variables
-    formula_data = formula_data.copy()
+    formula_data = copy.deepcopy(formula_data)
     for (key1, values1), (key2, values2), (key3, values3) in itertools.product(formula_data.items(), repeat=3):
         key12 = f'{key1}{key2}'
         key123 = f'{key1}{key2}{key3}'
         if key12 not in formula_data:
-            values12 = np.empty_like(values1, np.object)
+            values12 = np.empty_like(values1, np.object_)
             values12[:] = list(zip(values1, values2))
             formula_data[key12] = values12
         if key123 not in formula_data:
-            values123 = np.empty_like(values1, np.object)
+            values123 = np.empty_like(values1, np.object_)
             values123[:] = list(zip(values1, values2, values3))
             formula_data[key123] = values123
 
     # build and compare columns for each formula, making sure that it can be formatted
     for absorb in formulas:
         formulation = Formulation('x', absorb)
         assert str(formulation)
```

### Comparing `pyblp-0.9.0/tests/test_integration.py` & `pyblp-1.0.0/tests/test_integration.py`

 * *Files identical despite different names*

### Comparing `pyblp-0.9.0/tests/test_iteration.py` & `pyblp-1.0.0/tests/test_iteration.py`

 * *Files 8% similar despite different names*

```diff
@@ -24,37 +24,44 @@
     pytest.param('lm', {}, id="Levenberg-Marquardt"),
     pytest.param('return', {}, id="Return")
 ])
 @pytest.mark.parametrize('compute_jacobian', [
     pytest.param(True, id="analytic Jacobian"),
     pytest.param(False, id="no analytic Jacobian")
 ])
+@pytest.mark.parametrize('universal_display', [
+    pytest.param(True, id="universal display"),
+    pytest.param(False, id="no universal display")
+])
 @pytest.mark.parametrize('use_weights', [
     pytest.param(True, id="weights"),
     pytest.param(False, id="no weights")
 ])
-def test_scipy(method: str, method_options: Options, compute_jacobian: bool, use_weights: bool) -> None:
+def test_scipy(
+        method: str, method_options: Options, compute_jacobian: bool, universal_display: bool,
+        use_weights: bool) -> None:
     """Test that the solution to the example fixed point problem from scipy.optimize.fixed_point is reasonably close to
     the exact solution. Also verify that the configuration can be formatted.
     """
-    def contraction(x: Array) -> ContractionResults:
+    def contraction(x: Array, iterations: int, evaluations: int) -> ContractionResults:
         """Evaluate the contraction."""
+        assert evaluations >= iterations >= 0
         c1 = np.array([10, 12])
         c2 = np.array([3, 5])
         x0, x = x, np.sqrt(c1 / (x + c2))
         weights = np.ones_like(x) if use_weights else None
         jacobian = -0.5 * np.eye(2) * x / (x0 + c2) if compute_jacobian else None
         return x, weights, jacobian
 
     # simple methods do not accept an analytic Jacobian
     if compute_jacobian and method not in {'hybr', 'lm'}:
         return pytest.skip("This method does not accept an analytic Jacobian.")
 
     # initialize the configuration and test that it can be formatted
-    iteration = Iteration(method, method_options, compute_jacobian)
+    iteration = Iteration(method, method_options, compute_jacobian, universal_display)
     assert str(iteration)
 
     # define the exact solution
     exact_values = np.array([1.4920333, 1.37228132])
 
     # test that the solution is reasonably close (use the exact values if the iteration routine will just return them)
     start_values = exact_values if method == 'return' else np.ones_like(exact_values)
@@ -66,16 +73,17 @@
 @pytest.mark.parametrize('scheme', [pytest.param(1, id="S1"), pytest.param(2, id="S2"), pytest.param(3, id="S3")])
 def test_hasselblad(scheme: int) -> None:
     """Test that the solution to the fixed point problem from Hasselblad (1969) is reasonably close to the exact
     solution and that SQUAREM takes at least an order of magnitude fewer fixed point evaluations than does simple
     iteration. This same problem is used in an original SQUAREM unit test and is the first one discussed in Varadhan and
     Roland (2008).
     """
-    def contraction(x: Array) -> ContractionResults:
+    def contraction(x: Array, iterations: int, evaluations: int) -> ContractionResults:
         """Evaluate the contraction."""
+        assert evaluations >= iterations >= 0
         y = np.array([162, 267, 271, 185, 111, 61, 27, 8, 3, 1])
         i = np.arange(y.size)
         z = np.divide(
             x[0] * np.exp(-x[1]) * x[1]**i,
             x[0] * np.exp(-x[1]) * x[1]**i + (1 - x[0]) * np.exp(-x[2]) * x[2]**i
         )
         x = np.array([
@@ -85,15 +93,15 @@
         ])
         return x, None, None
 
     # solve the problem with SQUAREM and verify that the solution is reasonably close to the true solution
     method_options = {
         'atol': 1e-8,
         'max_evaluations': 100,
-        'scheme': scheme
+        'scheme': scheme,
     }
     initial_values = np.array([0.2, 2.5, 1.5])
     exact_values = np.array([0.6401146029910, 2.6634043566619, 1.2560951012662])
     computed_values = Iteration('squarem', method_options)._iterate(initial_values, contraction)[0]
     np.testing.assert_allclose(exact_values, computed_values, rtol=0, atol=1e-5)
 
     # verify that many more iterations would be needed to solve the problem with simple iteration
```

### Comparing `pyblp-0.9.0/tests/test_optimization.py` & `pyblp-1.0.0/tests/test_optimization.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,14 +21,15 @@
     pytest.param('knitro', {'algorithm': 4}, id="Knitro algorithm 4"),
     pytest.param('knitro', {'hessopt': 2}, id="Knitro hessopt 2"),
     pytest.param('knitro', {'hessopt': 3}, id="Knitro hessopt 3"),
     pytest.param('knitro', {'hessopt': 4}, id="Knitro hessopt 4"),
     pytest.param('slsqp', {}, id="SLSQP"),
     pytest.param('l-bfgs-b', {}, id="L-BFGS-B"),
     pytest.param('trust-constr', {}, id="trust-region"),
+    pytest.param('trust-constr', {'keep_feasible': True}, id="trust-region feasible"),
     pytest.param('tnc', {}, id="TNC"),
     pytest.param('nelder-mead', {}, id="Nelder-Mead"),
     pytest.param('powell', {}, id="Powell"),
     pytest.param('cg', {}, id="CG"),
     pytest.param('bfgs', {}, id="BFGS"),
     pytest.param('newton-cg', {}, id="Newton-CG"),
     pytest.param('return', {}, id="Return")
```

