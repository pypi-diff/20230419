# Comparing `tmp/c65faucet-1.0.46.tar.gz` & `tmp/c65faucet-1.0.47.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "c65faucet-1.0.46.tar", last modified: Wed Feb 22 03:44:59 2023, max compression
+gzip compressed data, was "c65faucet-1.0.47.tar", last modified: Wed Apr 19 00:35:56 2023, max compression
```

## Comparing `c65faucet-1.0.46.tar` & `c65faucet-1.0.47.tar`

### file list

```diff
@@ -1,374 +1,374 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.137842 c65faucet-1.0.46/
--rw-r--r--   0 runner    (1001) docker     (123)      142 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.codecov.yml
--rw-r--r--   0 runner    (1001) docker     (123)       34 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.dockerignore
--rw-r--r--   0 runner    (1001) docker     (123)       31 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.flake8
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.077840 c65faucet-1.0.46/.github/
--rw-r--r--   0 runner    (1001) docker     (123)      346 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/dependabot.yml
--rw-r--r--   0 runner    (1001) docker     (123)      322 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/renovate.json
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.077840 c65faucet-1.0.46/.github/workflows/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.077840 c65faucet-1.0.46/.github/workflows/disabled/
--rw-r--r--   0 runner    (1001) docker     (123)     1230 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/workflows/disabled/periodic.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1642 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/workflows/disabled/release-debian.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4199 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/workflows/release-docker.yml
--rw-r--r--   0 runner    (1001) docker     (123)      741 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/workflows/release-python.yml
--rw-r--r--   0 runner    (1001) docker     (123)     3356 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/workflows/tests-codecheck.yml
--rw-r--r--   0 runner    (1001) docker     (123)     3110 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/workflows/tests-docs.yml
--rw-r--r--   0 runner    (1001) docker     (123)     7537 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/workflows/tests-integration.yml
--rw-r--r--   0 runner    (1001) docker     (123)     3898 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/workflows/tests-unit.yml
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.github/workflows/tests-yaml-lint.yml
--rw-r--r--   0 runner    (1001) docker     (123)      612 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.pylintrc
--rw-r--r--   0 runner    (1001) docker     (123)      197 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.readthedocs.yml
--rw-r--r--   0 runner    (1001) docker     (123)       88 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.renovaterc.json
--rw-r--r--   0 runner    (1001) docker     (123)      125 2023-02-22 03:44:51.000000 c65faucet-1.0.46/.stickler.yml
--rw-r--r--   0 runner    (1001) docker     (123)       48 2023-02-22 03:44:58.000000 c65faucet-1.0.46/AUTHORS
--rw-r--r--   0 runner    (1001) docker     (123)      830 2023-02-22 03:44:51.000000 c65faucet-1.0.46/CONTRIBUTING.rst
--rw-r--r--   0 runner    (1001) docker     (123)       32 2023-02-22 03:44:58.000000 c65faucet-1.0.46/ChangeLog
--rw-r--r--   0 runner    (1001) docker     (123)      221 2023-02-22 03:44:51.000000 c65faucet-1.0.46/Dockerfile.faucet
--rw-r--r--   0 runner    (1001) docker     (123)      485 2023-02-22 03:44:51.000000 c65faucet-1.0.46/Dockerfile.fuzz-config
--rw-r--r--   0 runner    (1001) docker     (123)      464 2023-02-22 03:44:51.000000 c65faucet-1.0.46/Dockerfile.fuzz-packet
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-02-22 03:44:51.000000 c65faucet-1.0.46/Dockerfile.gauge
--rw-r--r--   0 runner    (1001) docker     (123)      129 2023-02-22 03:44:51.000000 c65faucet-1.0.46/Dockerfile.tests
--rw-r--r--   0 runner    (1001) docker     (123)    10912 2023-02-22 03:44:51.000000 c65faucet-1.0.46/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)     2538 2023-02-22 03:44:51.000000 c65faucet-1.0.46/Makefile
--rw-r--r--   0 runner    (1001) docker     (123)      787 2023-02-22 03:44:59.137842 c65faucet-1.0.46/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     3610 2023-02-22 03:44:51.000000 c65faucet-1.0.46/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.077840 c65faucet-1.0.46/adapters/
--rw-r--r--   0 runner    (1001) docker     (123)      141 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.065840 c65faucet-1.0.46/adapters/vendors/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.077840 c65faucet-1.0.46/adapters/vendors/faucetagent/
--rw-r--r--   0 runner    (1001) docker     (123)     1085 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/faucetagent/Dockerfile
--rw-r--r--   0 runner    (1001) docker     (123)      985 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/faucetagent/README.rst
--rw-r--r--   0 runner    (1001) docker     (123)      451 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/faucetagent/docker-compose.yaml
--rwxr-xr-x   0 runner    (1001) docker     (123)      694 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/faucetagent/example_client.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)     1142 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/faucetagent/gencerts.sh
--rw-r--r--   0 runner    (1001) docker     (123)       56 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/faucetagent/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.081840 c65faucet-1.0.46/adapters/vendors/rabbitmq/
--rw-r--r--   0 runner    (1001) docker     (123)      763 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/rabbitmq/Dockerfile
--rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/rabbitmq/README.rst
--rw-r--r--   0 runner    (1001) docker     (123)      152 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/rabbitmq/docker-compose-rabbitmq.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/rabbitmq/docker-compose.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/rabbitmq/example_consumer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.081840 c65faucet-1.0.46/adapters/vendors/rabbitmq/hooks/
--rw-r--r--   0 runner    (1001) docker     (123)      131 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/rabbitmq/hooks/pre_build
--rw-r--r--   0 runner    (1001) docker     (123)     6487 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/rabbitmq/rabbit.py
--rw-r--r--   0 runner    (1001) docker     (123)       12 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/rabbitmq/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     3779 2023-02-22 03:44:51.000000 c65faucet-1.0.46/adapters/vendors/rabbitmq/test_rabbit.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.081840 c65faucet-1.0.46/c65faucet.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)      787 2023-02-22 03:44:58.000000 c65faucet-1.0.46/c65faucet.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     9704 2023-02-22 03:44:59.000000 c65faucet-1.0.46/c65faucet.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-02-22 03:44:58.000000 c65faucet-1.0.46/c65faucet.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      156 2023-02-22 03:44:58.000000 c65faucet-1.0.46/c65faucet.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-02-22 03:44:58.000000 c65faucet-1.0.46/c65faucet.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (123)       47 2023-02-22 03:44:58.000000 c65faucet-1.0.46/c65faucet.egg-info/pbr.json
--rw-r--r--   0 runner    (1001) docker     (123)      165 2023-02-22 03:44:58.000000 c65faucet-1.0.46/c65faucet.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        7 2023-02-22 03:44:58.000000 c65faucet-1.0.46/c65faucet.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.085841 c65faucet-1.0.46/clib/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      540 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/clib_mininet_test.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    32146 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/clib_mininet_test_main.py
--rw-r--r--   0 runner    (1001) docker     (123)     5995 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/clib_mininet_tests.py
--rw-r--r--   0 runner    (1001) docker     (123)    24759 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/config_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)    12531 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/docker_host.py
--rw-r--r--   0 runner    (1001) docker     (123)    49833 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/fakeoftable.py
--rw-r--r--   0 runner    (1001) docker     (123)   135065 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/mininet_test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)    33547 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/mininet_test_base_topo.py
--rw-r--r--   0 runner    (1001) docker     (123)    27383 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/mininet_test_topo.py
--rw-r--r--   0 runner    (1001) docker     (123)     7087 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/mininet_test_util.py
--rw-r--r--   0 runner    (1001) docker     (123)     9471 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/mininet_test_watcher.py
--rw-r--r--   0 runner    (1001) docker     (123)     4992 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/tcpdump_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)   109310 2023-02-22 03:44:51.000000 c65faucet-1.0.46/clib/valve_test_lib.py
--rw-r--r--   0 runner    (1001) docker     (123)       48 2023-02-22 03:44:51.000000 c65faucet-1.0.46/codecheck-requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.085841 c65faucet-1.0.46/debian/
--rw-r--r--   0 runner    (1001) docker     (123)      134 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/changelog
--rw-r--r--   0 runner    (1001) docker     (123)        3 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/compat
--rw-r--r--   0 runner    (1001) docker     (123)     3403 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/control
--rw-r--r--   0 runner    (1001) docker     (123)      905 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/copyright
--rw-r--r--   0 runner    (1001) docker     (123)       54 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/faucet-all-in-one.install
--rw-r--r--   0 runner    (1001) docker     (123)       28 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/faucet-docs.docs
--rw-r--r--   0 runner    (1001) docker     (123)      224 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/faucet.default
--rw-r--r--   0 runner    (1001) docker     (123)      204 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/faucet.install
--rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/faucet.postinst
--rw-r--r--   0 runner    (1001) docker     (123)      421 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/faucet.service
--rw-r--r--   0 runner    (1001) docker     (123)      216 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/gauge.default
--rw-r--r--   0 runner    (1001) docker     (123)       35 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/gauge.install
--rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/gauge.postinst
--rw-r--r--   0 runner    (1001) docker     (123)      385 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/gauge.service
--rwxr-xr-x   0 runner    (1001) docker     (123)      555 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/rules
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.085841 c65faucet-1.0.46/debian/source/
--rw-r--r--   0 runner    (1001) docker     (123)       12 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/source/format
--rw-r--r--   0 runner    (1001) docker     (123)      344 2023-02-22 03:44:51.000000 c65faucet-1.0.46/debian/watch
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.089841 c65faucet-1.0.46/docker/
--rwxr-xr-x   0 runner    (1001) docker     (123)      989 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docker/fuzz_config.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      624 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docker/fuzz_packet.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)     1176 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docker/install-faucet.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)     1232 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docker/localtest.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      797 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docker/pip_deps.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      220 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docker/retrycmd.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)     7159 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docker/runtests.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      697 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docker/shard_tests.sh
--rw-r--r--   0 runner    (1001) docker     (123)     2208 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docker-compose.yaml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.093841 c65faucet-1.0.46/docs/
--rw-r--r--   0 runner    (1001) docker     (123)       22 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)      620 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/Makefile
--rw-r--r--   0 runner    (1001) docker     (123)      308 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.065840 c65faucet-1.0.46/docs/_static/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.093841 c65faucet-1.0.46/docs/_static/css/
--rw-r--r--   0 runner    (1001) docker     (123)      365 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/css/responsive-tables.css
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.093841 c65faucet-1.0.46/docs/_static/deployments/
--rw-r--r--   0 runner    (1001) docker     (123)    89120 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/deployments/ONF_Faucet_deploy1.png
--rw-r--r--   0 runner    (1001) docker     (123)   984943 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/deployments/nznog17-physical-network.jpg
--rw-r--r--   0 runner    (1001) docker     (123)  1106115 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/deployments/nznog17-virtual-network.jpg
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.093841 c65faucet-1.0.46/docs/_static/grafana-dashboards/
--rw-r--r--   0 runner    (1001) docker     (123)    23629 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/grafana-dashboards/faucet_instrumentation.json
--rw-r--r--   0 runner    (1001) docker     (123)    22310 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/grafana-dashboards/faucet_inventory.json
--rw-r--r--   0 runner    (1001) docker     (123)    18566 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/grafana-dashboards/faucet_port_statistics.json
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.101841 c65faucet-1.0.46/docs/_static/images/
--rwxr-xr-x   0 runner    (1001) docker     (123)     8466 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/8021X-conf-diagram.svg
--rw-r--r--   0 runner    (1001) docker     (123)    97737 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/faucet-architecture.svg
--rw-r--r--   0 runner    (1001) docker     (123)    23716 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/faucet-pipeline.png
--rw-r--r--   0 runner    (1001) docker     (123)    58351 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/faucet-pipeline.svg
--rw-r--r--   0 runner    (1001) docker     (123)     2421 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/faucet-pipeline.txt
--rw-r--r--   0 runner    (1001) docker     (123)   136689 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/gauge-nznog17.png
--rw-r--r--   0 runner    (1001) docker     (123)   285918 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/gauge-snapshot1.png
--rw-r--r--   0 runner    (1001) docker     (123)   452164 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/gauge-snapshot2.png
--rw-r--r--   0 runner    (1001) docker     (123)   467326 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/gauge-snapshot3.png
--rw-r--r--   0 runner    (1001) docker     (123)    51732 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-acls.svg
--rw-r--r--   0 runner    (1001) docker     (123)    77255 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-bgp-routing.svg
--rw-r--r--   0 runner    (1001) docker     (123)    47652 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-ivr.svg
--rw-r--r--   0 runner    (1001) docker     (123)   120062 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-multi-root-stack.svg
--rw-r--r--   0 runner    (1001) docker     (123)    87114 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-nfv-services.svg
--rw-r--r--   0 runner    (1001) docker     (123)   105019 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-stack-loop.svg
--rw-r--r--   0 runner    (1001) docker     (123)    78230 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-stack-tunnel.svg
--rw-r--r--   0 runner    (1001) docker     (123)    53594 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-stack.svg
--rw-r--r--   0 runner    (1001) docker     (123)   109550 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-stackwithivr.svg
--rw-r--r--   0 runner    (1001) docker     (123)    59007 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-static-routing.svg
--rw-r--r--   0 runner    (1001) docker     (123)   172474 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/images/tutorial-vlans.svg
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.101841 c65faucet-1.0.46/docs/_static/tutorial/
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/tutorial/add_tagged_interface
--rw-r--r--   0 runner    (1001) docker     (123)      134 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/tutorial/as_ns
--rw-r--r--   0 runner    (1001) docker     (123)     1449 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/tutorial/cleanup
--rw-r--r--   0 runner    (1001) docker     (123)      389 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/tutorial/create_ns
--rw-r--r--   0 runner    (1001) docker     (123)      154 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/tutorial/destroy_ns
--rw-r--r--   0 runner    (1001) docker     (123)      793 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/_static/tutorial/inter_switch_link
--rw-r--r--   0 runner    (1001) docker     (123)     5063 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/architecture.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.101841 c65faucet-1.0.46/docs/autogen/
--rw-r--r--   0 runner    (1001) docker     (123)        6 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/autogen/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)     7237 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/conf.py
--rw-r--r--   0 runner    (1001) docker     (123)    40109 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/configuration.rst
--rw-r--r--   0 runner    (1001) docker     (123)    14496 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/developer_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     1499 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/external_resources.rst
--rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/fuzzing.rst
--rw-r--r--   0 runner    (1001) docker     (123)      492 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)     9939 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/installation.rst
--rw-r--r--   0 runner    (1001) docker     (123)     5125 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/intro.rst
--rw-r--r--   0 runner    (1001) docker     (123)      930 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/monitoring.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.105841 c65faucet-1.0.46/docs/recipe_book/
--rw-r--r--   0 runner    (1001) docker     (123)       22 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/recipe_book/forwarding.rst
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/recipe_book/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)       14 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/recipe_book/policy.rst
--rw-r--r--   0 runner    (1001) docker     (123)       16 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/recipe_book/routing.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.105841 c65faucet-1.0.46/docs/release_notes/
--rw-r--r--   0 runner    (1001) docker     (123)     3303 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/release_notes/1.7.0.rst
--rw-r--r--   0 runner    (1001) docker     (123)     3271 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/release_notes/1.9.0.rst
--rw-r--r--   0 runner    (1001) docker     (123)       99 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.105841 c65faucet-1.0.46/docs/source/
--rw-r--r--   0 runner    (1001) docker     (123)       73 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/source/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)    11599 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/testing.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.105841 c65faucet-1.0.46/docs/tutorials/
--rw-r--r--   0 runner    (1001) docker     (123)    13460 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/tutorials/acls.rst
--rw-r--r--   0 runner    (1001) docker     (123)    13064 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/tutorials/conntrack.rst
--rw-r--r--   0 runner    (1001) docker     (123)    18441 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/tutorials/first_time.rst
--rw-r--r--   0 runner    (1001) docker     (123)      142 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/tutorials/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)    17927 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/tutorials/nfv_services.rst
--rw-r--r--   0 runner    (1001) docker     (123)    15268 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/tutorials/routing.rst
--rw-r--r--   0 runner    (1001) docker     (123)    30053 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/tutorials/stacking.rst
--rw-r--r--   0 runner    (1001) docker     (123)     8477 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/tutorials/vlans.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.105841 c65faucet-1.0.46/docs/vendors/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.105841 c65faucet-1.0.46/docs/vendors/allied-telesis/
--rw-r--r--   0 runner    (1001) docker     (123)    14233 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/allied-telesis/README_Allied_Telesis.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.105841 c65faucet-1.0.46/docs/vendors/cisco/
--rwxr-xr-x   0 runner    (1001) docker     (123)     9404 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/cisco/README_Cisco.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.105841 c65faucet-1.0.46/docs/vendors/hpe/
--rw-r--r--   0 runner    (1001) docker     (123)    15292 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/hpe/README_Aruba.rst
--rw-r--r--   0 runner    (1001) docker     (123)      405 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/index.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.105841 c65faucet-1.0.46/docs/vendors/lagopus/
--rw-r--r--   0 runner    (1001) docker     (123)     3638 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/lagopus/README_Lagopus.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.109841 c65faucet-1.0.46/docs/vendors/northboundnetworks/
--rw-r--r--   0 runner    (1001) docker     (123)     1661 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/northboundnetworks/README_ZodiacFX.rst
--rw-r--r--   0 runner    (1001) docker     (123)      605 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/northboundnetworks/README_ZodiacGX.rst
--rwxr-xr-x   0 runner    (1001) docker     (123)     1292 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/northboundnetworks/conf-zodiac.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.109841 c65faucet-1.0.46/docs/vendors/noviflow/
--rw-r--r--   0 runner    (1001) docker     (123)     3286 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/noviflow/README_noviflow.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.109841 c65faucet-1.0.46/docs/vendors/ovs/
--rw-r--r--   0 runner    (1001) docker     (123)     6413 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/ovs/README_OVS-DPDK.rst
--rw-r--r--   0 runner    (1001) docker     (123)    18223 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/ovs/faucet_ovs_test.png
--rw-r--r--   0 runner    (1001) docker     (123)     9332 2023-02-22 03:44:51.000000 c65faucet-1.0.46/docs/vendors/ovs/faucet_testing_with_OVS_on_hardware.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.069840 c65faucet-1.0.46/etc/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.109841 c65faucet-1.0.46/etc/default/
--rw-r--r--   0 runner    (1001) docker     (123)      223 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/default/faucet
--rw-r--r--   0 runner    (1001) docker     (123)      215 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/default/gauge
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.109841 c65faucet-1.0.46/etc/faucet/
--rw-r--r--   0 runner    (1001) docker     (123)     2826 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/faucet/acls.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     2423 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/faucet/faucet.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      965 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/faucet/gauge.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      403 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/faucet/os_ken.conf
--rw-r--r--   0 runner    (1001) docker     (123)      403 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/faucet/ryu.conf
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.109841 c65faucet-1.0.46/etc/logrotate.d/
--rw-r--r--   0 runner    (1001) docker     (123)      243 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/logrotate.d/faucet
--rw-r--r--   0 runner    (1001) docker     (123)      241 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/logrotate.d/gauge
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.109841 c65faucet-1.0.46/etc/prometheus/
--rw-r--r--   0 runner    (1001) docker     (123)      660 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/prometheus/faucet.rules.yml
--rw-r--r--   0 runner    (1001) docker     (123)      263 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/prometheus/prometheus-docker-compose.yml
--rw-r--r--   0 runner    (1001) docker     (123)      872 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/prometheus/prometheus.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.069840 c65faucet-1.0.46/etc/systemd/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.109841 c65faucet-1.0.46/etc/systemd/system/
--rw-r--r--   0 runner    (1001) docker     (123)      388 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/systemd/system/faucet.service
--rw-r--r--   0 runner    (1001) docker     (123)      352 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/systemd/system/gauge.service
--rw-r--r--   0 runner    (1001) docker     (123)      355 2023-02-22 03:44:51.000000 c65faucet-1.0.46/etc/systemd/system/prometheus.service
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.121841 c65faucet-1.0.46/faucet/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5713 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/__main__.py
--rw-r--r--   0 runner    (1001) docker     (123)    33387 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/acl.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2276 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/check_faucet_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     9540 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/conf.py
--rw-r--r--   0 runner    (1001) docker     (123)    13820 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/config_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     7554 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/config_parser_util.py
--rw-r--r--   0 runner    (1001) docker     (123)    66832 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/dp.py
--rw-r--r--   0 runner    (1001) docker     (123)    14561 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/faucet.py
--rw-r--r--   0 runner    (1001) docker     (123)     9853 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/faucet_bgp.py
--rw-r--r--   0 runner    (1001) docker     (123)    15344 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/faucet_dot1x.py
--rw-r--r--   0 runner    (1001) docker     (123)     4921 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/faucet_event.py
--rw-r--r--   0 runner    (1001) docker     (123)      440 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/faucet_metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)    11238 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/faucet_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     7310 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/faucet_pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     7620 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/fctl.py
--rw-r--r--   0 runner    (1001) docker     (123)     8303 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/gauge.py
--rw-r--r--   0 runner    (1001) docker     (123)     9569 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/gauge_influx.py
--rw-r--r--   0 runner    (1001) docker     (123)     9861 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/gauge_pollers.py
--rw-r--r--   0 runner    (1001) docker     (123)     8661 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/gauge_prom.py
--rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/meter.py
--rw-r--r--   0 runner    (1001) docker     (123)    30246 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/port.py
--rw-r--r--   0 runner    (1001) docker     (123)     3490 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/prom_client.py
--rw-r--r--   0 runner    (1001) docker     (123)     6089 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/router.py
--rw-r--r--   0 runner    (1001) docker     (123)    18991 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/stack.py
--rw-r--r--   0 runner    (1001) docker     (123)     6250 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/tfm_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)    69278 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve.py
--rw-r--r--   0 runner    (1001) docker     (123)    27173 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_acl.py
--rw-r--r--   0 runner    (1001) docker     (123)     2756 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_coprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     7623 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_lldp.py
--rw-r--r--   0 runner    (1001) docker     (123)     1106 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_manager_base.py
--rw-r--r--   0 runner    (1001) docker     (123)    41011 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_of.py
--rw-r--r--   0 runner    (1001) docker     (123)     1072 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_of_old.py
--rw-r--r--   0 runner    (1001) docker     (123)     1671 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_outonly.py
--rw-r--r--   0 runner    (1001) docker     (123)    29202 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_packet.py
--rw-r--r--   0 runner    (1001) docker     (123)     8494 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)    46329 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_route.py
--rw-r--r--   0 runner    (1001) docker     (123)     7609 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_ryuapp.py
--rw-r--r--   0 runner    (1001) docker     (123)    17891 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_stack.py
--rw-r--r--   0 runner    (1001) docker     (123)     3429 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_switch.py
--rw-r--r--   0 runner    (1001) docker     (123)    28337 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_switch_stack.py
--rw-r--r--   0 runner    (1001) docker     (123)    46796 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_switch_standalone.py
--rw-r--r--   0 runner    (1001) docker     (123)    11803 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_table.py
--rw-r--r--   0 runner    (1001) docker     (123)     6388 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valve_util.py
--rw-r--r--   0 runner    (1001) docker     (123)    17938 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/valves_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)    25252 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/vlan.py
--rw-r--r--   0 runner    (1001) docker     (123)     6361 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/watcher.py
--rw-r--r--   0 runner    (1001) docker     (123)     6930 2023-02-22 03:44:51.000000 c65faucet-1.0.46/faucet/watcher_conf.py
--rw-r--r--   0 runner    (1001) docker     (123)       43 2023-02-22 03:44:51.000000 c65faucet-1.0.46/fuzz-requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.121841 c65faucet-1.0.46/git-hook/
--rwxr-xr-x   0 runner    (1001) docker     (123)      456 2023-02-22 03:44:51.000000 c65faucet-1.0.46/git-hook/pre-commit
--rw-r--r--   0 runner    (1001) docker     (123)      358 2023-02-22 03:44:51.000000 c65faucet-1.0.46/helper-funcs
--rw-r--r--   0 runner    (1001) docker     (123)     1061 2023-02-22 03:44:51.000000 c65faucet-1.0.46/hw_switch_config.yaml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.121841 c65faucet-1.0.46/ofctl_rest/
--rw-r--r--   0 runner    (1001) docker     (123)    26377 2023-02-22 03:44:51.000000 c65faucet-1.0.46/ofctl_rest/ofctl_rest.py
--rw-r--r--   0 runner    (1001) docker     (123)     9460 2023-02-22 03:44:51.000000 c65faucet-1.0.46/ofctl_rest/wsgi.py
--rw-r--r--   0 runner    (1001) docker     (123)      310 2023-02-22 03:44:51.000000 c65faucet-1.0.46/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     1064 2023-02-22 03:44:59.137842 c65faucet-1.0.46/setup.cfg
--rwxr-xr-x   0 runner    (1001) docker     (123)     3674 2023-02-22 03:44:51.000000 c65faucet-1.0.46/setup.py
--rw-r--r--   0 runner    (1001) docker     (123)      196 2023-02-22 03:44:51.000000 c65faucet-1.0.46/test-requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.121841 c65faucet-1.0.46/tests/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.121841 c65faucet-1.0.46/tests/codecheck/
--rwxr-xr-x   0 runner    (1001) docker     (123)      278 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/codecheck/flake8.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      500 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/codecheck/min_pylint.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      234 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/codecheck/pylint.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      342 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/codecheck/pytype.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      701 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/codecheck/src_files.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.069840 c65faucet-1.0.46/tests/generative/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.069840 c65faucet-1.0.46/tests/generative/fuzzer/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.121841 c65faucet-1.0.46/tests/generative/fuzzer/config/
--rw-r--r--   0 runner    (1001) docker     (123)      405 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/config/config.dict
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.125841 c65faucet-1.0.46/tests/generative/fuzzer/config/examples/
--rw-r--r--   0 runner    (1001) docker     (123)      432 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/config/examples/ex0
--rw-r--r--   0 runner    (1001) docker     (123)     1004 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/config/fuzz_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4755 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/config/generate_dict.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.125841 c65faucet-1.0.46/tests/generative/fuzzer/packet/
--rw-r--r--   0 runner    (1001) docker     (123)     1548 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/display_packet_crash.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.129842 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/
--rw-r--r--   0 runner    (1001) docker     (123)       65 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/aoe.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      121 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/arp.ex1
--rw-r--r--   0 runner    (1001) docker     (123)       85 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/arp.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      183 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/asap.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      183 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/asap.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/diameter.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      169 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/dns.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      277 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/dns.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      465 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/http.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      925 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/http.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      933 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/icmp.ex1
--rw-r--r--   0 runner    (1001) docker     (123)     2885 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/icmp.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      141 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/icmp.ex3
--rw-r--r--   0 runner    (1001) docker     (123)      141 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/icmp.ex4
--rw-r--r--   0 runner    (1001) docker     (123)      121 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/igmpv2.ex1
--rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/ipv4.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      381 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/ipv6.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      193 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/irc.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      265 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/irc.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      311 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/irc.ex3
--rw-r--r--   0 runner    (1001) docker     (123)      249 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/lacp.ex1
--rw-r--r--   0 runner    (1001) docker     (123)     1001 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/msger.ex1
--rw-r--r--   0 runner    (1001) docker     (123)       63 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/packet.dict
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/tcp.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/tcp.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      109 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/tcp.ex3
--rw-r--r--   0 runner    (1001) docker     (123)      125 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/tcp.ex4
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/tcp.ex5
--rw-r--r--   0 runner    (1001) docker     (123)      121 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/udp.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/udp.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/udp.ex3
--rw-r--r--   0 runner    (1001) docker     (123)      147 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/udp.ex4
--rw-r--r--   0 runner    (1001) docker     (123)      715 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/fake_packet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1406 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/fuzzer/packet/fuzz_packet.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.129842 c65faucet-1.0.46/tests/generative/integration/
--rwxr-xr-x   0 runner    (1001) docker     (123)     2288 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/integration/fault_tolerance_main.py
--rw-r--r--   0 runner    (1001) docker     (123)    17828 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/integration/fault_tolerance_tests.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.129842 c65faucet-1.0.46/tests/generative/unit/
--rwxr-xr-x   0 runner    (1001) docker     (123)    12548 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/generative/unit/test_topology.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.133842 c65faucet-1.0.46/tests/integration/
--rwxr-xr-x   0 runner    (1001) docker     (123)      565 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/integration/mininet_main.py
--rw-r--r--   0 runner    (1001) docker     (123)   143643 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/integration/mininet_multidp_tests.py
--rw-r--r--   0 runner    (1001) docker     (123)   292355 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/integration/mininet_tests.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      574 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/run_unit_tests.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      244 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/sysctls_for_tests.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.069840 c65faucet-1.0.46/tests/unit/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.133842 c65faucet-1.0.46/tests/unit/clib/
--rwxr-xr-x   0 runner    (1001) docker     (123)     8000 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/clib/test_topo.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.133842 c65faucet-1.0.46/tests/unit/faucet/
--rwxr-xr-x   0 runner    (1001) docker     (123)    10418 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_check_config.py
--rwxr-xr-x   0 runner    (1001) docker     (123)   134075 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_config.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     6535 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_fctl.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1497 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_main.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5749 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_port.py
--rw-r--r--   0 runner    (1001) docker     (123)    30606 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_valve.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    37632 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_valve_config.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     4843 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_valve_dot1x.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5572 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_valve_egress.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2780 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_valve_of.py
--rw-r--r--   0 runner    (1001) docker     (123)   146582 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_valve_stack.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2825 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_valveapp_smoke.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     8955 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/faucet/test_vlan.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.137842 c65faucet-1.0.46/tests/unit/gauge/
--rwxr-xr-x   0 runner    (1001) docker     (123)     5559 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/gauge/test_config_gauge.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    37880 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/gauge/test_gauge.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1494 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/gauge/test_main.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-22 03:44:59.137842 c65faucet-1.0.46/tests/unit/packaging/
--rwxr-xr-x   0 runner    (1001) docker     (123)     5354 2023-02-22 03:44:51.000000 c65faucet-1.0.46/tests/unit/packaging/test_packaging.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.245929 c65faucet-1.0.47/
+-rw-r--r--   0 runner    (1001) docker     (123)      142 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.codecov.yml
+-rw-r--r--   0 runner    (1001) docker     (123)       34 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.dockerignore
+-rw-r--r--   0 runner    (1001) docker     (123)       31 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.flake8
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.209929 c65faucet-1.0.47/.github/
+-rw-r--r--   0 runner    (1001) docker     (123)      346 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/dependabot.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      322 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/renovate.json
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.209929 c65faucet-1.0.47/.github/workflows/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.209929 c65faucet-1.0.47/.github/workflows/disabled/
+-rw-r--r--   0 runner    (1001) docker     (123)     1230 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/workflows/disabled/periodic.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1642 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/workflows/disabled/release-debian.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4199 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/workflows/release-docker.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      741 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/workflows/release-python.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3502 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/workflows/tests-codecheck.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3256 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/workflows/tests-docs.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     7898 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/workflows/tests-integration.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4044 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/workflows/tests-unit.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.github/workflows/tests-yaml-lint.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      641 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.pylintrc
+-rw-r--r--   0 runner    (1001) docker     (123)      197 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.readthedocs.yml
+-rw-r--r--   0 runner    (1001) docker     (123)       88 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.renovaterc.json
+-rw-r--r--   0 runner    (1001) docker     (123)      125 2023-04-19 00:35:54.000000 c65faucet-1.0.47/.stickler.yml
+-rw-r--r--   0 runner    (1001) docker     (123)       36 2023-04-19 00:35:56.000000 c65faucet-1.0.47/AUTHORS
+-rw-r--r--   0 runner    (1001) docker     (123)      830 2023-04-19 00:35:54.000000 c65faucet-1.0.47/CONTRIBUTING.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       32 2023-04-19 00:35:56.000000 c65faucet-1.0.47/ChangeLog
+-rw-r--r--   0 runner    (1001) docker     (123)      221 2023-04-19 00:35:54.000000 c65faucet-1.0.47/Dockerfile.faucet
+-rw-r--r--   0 runner    (1001) docker     (123)      485 2023-04-19 00:35:54.000000 c65faucet-1.0.47/Dockerfile.fuzz-config
+-rw-r--r--   0 runner    (1001) docker     (123)      464 2023-04-19 00:35:54.000000 c65faucet-1.0.47/Dockerfile.fuzz-packet
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-04-19 00:35:54.000000 c65faucet-1.0.47/Dockerfile.gauge
+-rw-r--r--   0 runner    (1001) docker     (123)      129 2023-04-19 00:35:54.000000 c65faucet-1.0.47/Dockerfile.tests
+-rw-r--r--   0 runner    (1001) docker     (123)    10912 2023-04-19 00:35:54.000000 c65faucet-1.0.47/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)     2538 2023-04-19 00:35:54.000000 c65faucet-1.0.47/Makefile
+-rw-r--r--   0 runner    (1001) docker     (123)      787 2023-04-19 00:35:56.249929 c65faucet-1.0.47/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     3610 2023-04-19 00:35:54.000000 c65faucet-1.0.47/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.209929 c65faucet-1.0.47/adapters/
+-rw-r--r--   0 runner    (1001) docker     (123)      141 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.205928 c65faucet-1.0.47/adapters/vendors/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.209929 c65faucet-1.0.47/adapters/vendors/faucetagent/
+-rw-r--r--   0 runner    (1001) docker     (123)     1086 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/faucetagent/Dockerfile
+-rw-r--r--   0 runner    (1001) docker     (123)      985 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/faucetagent/README.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      451 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/faucetagent/docker-compose.yaml
+-rwxr-xr-x   0 runner    (1001) docker     (123)      694 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/faucetagent/example_client.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1142 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/faucetagent/gencerts.sh
+-rw-r--r--   0 runner    (1001) docker     (123)       56 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/faucetagent/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.213929 c65faucet-1.0.47/adapters/vendors/rabbitmq/
+-rw-r--r--   0 runner    (1001) docker     (123)      763 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/rabbitmq/Dockerfile
+-rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/rabbitmq/README.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      152 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/rabbitmq/docker-compose-rabbitmq.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      659 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/rabbitmq/docker-compose.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/rabbitmq/example_consumer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.213929 c65faucet-1.0.47/adapters/vendors/rabbitmq/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)      131 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/rabbitmq/hooks/pre_build
+-rw-r--r--   0 runner    (1001) docker     (123)     6487 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/rabbitmq/rabbit.py
+-rw-r--r--   0 runner    (1001) docker     (123)       12 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/rabbitmq/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     3779 2023-04-19 00:35:54.000000 c65faucet-1.0.47/adapters/vendors/rabbitmq/test_rabbit.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.213929 c65faucet-1.0.47/c65faucet.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)      787 2023-04-19 00:35:56.000000 c65faucet-1.0.47/c65faucet.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     9704 2023-04-19 00:35:56.000000 c65faucet-1.0.47/c65faucet.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-04-19 00:35:56.000000 c65faucet-1.0.47/c65faucet.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      156 2023-04-19 00:35:56.000000 c65faucet-1.0.47/c65faucet.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-04-19 00:35:56.000000 c65faucet-1.0.47/c65faucet.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)       47 2023-04-19 00:35:56.000000 c65faucet-1.0.47/c65faucet.egg-info/pbr.json
+-rw-r--r--   0 runner    (1001) docker     (123)      149 2023-04-19 00:35:56.000000 c65faucet-1.0.47/c65faucet.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        7 2023-04-19 00:35:56.000000 c65faucet-1.0.47/c65faucet.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.213929 c65faucet-1.0.47/clib/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      540 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/clib_mininet_test.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    32146 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/clib_mininet_test_main.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5995 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/clib_mininet_tests.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24759 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/config_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12531 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/docker_host.py
+-rw-r--r--   0 runner    (1001) docker     (123)    49833 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/fakeoftable.py
+-rw-r--r--   0 runner    (1001) docker     (123)   135065 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/mininet_test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33547 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/mininet_test_base_topo.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27383 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/mininet_test_topo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7087 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/mininet_test_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9471 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/mininet_test_watcher.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4992 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/tcpdump_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)   109310 2023-04-19 00:35:54.000000 c65faucet-1.0.47/clib/valve_test_lib.py
+-rw-r--r--   0 runner    (1001) docker     (123)       48 2023-04-19 00:35:54.000000 c65faucet-1.0.47/codecheck-requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.217929 c65faucet-1.0.47/debian/
+-rw-r--r--   0 runner    (1001) docker     (123)      134 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/changelog
+-rw-r--r--   0 runner    (1001) docker     (123)        3 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/compat
+-rw-r--r--   0 runner    (1001) docker     (123)     3364 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/control
+-rw-r--r--   0 runner    (1001) docker     (123)      905 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/copyright
+-rw-r--r--   0 runner    (1001) docker     (123)       54 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/faucet-all-in-one.install
+-rw-r--r--   0 runner    (1001) docker     (123)       28 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/faucet-docs.docs
+-rw-r--r--   0 runner    (1001) docker     (123)      224 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/faucet.default
+-rw-r--r--   0 runner    (1001) docker     (123)      204 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/faucet.install
+-rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/faucet.postinst
+-rw-r--r--   0 runner    (1001) docker     (123)      421 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/faucet.service
+-rw-r--r--   0 runner    (1001) docker     (123)      216 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/gauge.default
+-rw-r--r--   0 runner    (1001) docker     (123)       35 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/gauge.install
+-rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/gauge.postinst
+-rw-r--r--   0 runner    (1001) docker     (123)      385 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/gauge.service
+-rwxr-xr-x   0 runner    (1001) docker     (123)      555 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/rules
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.217929 c65faucet-1.0.47/debian/source/
+-rw-r--r--   0 runner    (1001) docker     (123)       12 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/source/format
+-rw-r--r--   0 runner    (1001) docker     (123)      344 2023-04-19 00:35:54.000000 c65faucet-1.0.47/debian/watch
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.217929 c65faucet-1.0.47/docker/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      989 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docker/fuzz_config.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      624 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docker/fuzz_packet.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1176 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docker/install-faucet.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1232 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docker/localtest.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      797 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docker/pip_deps.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      220 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docker/retrycmd.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7205 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docker/runtests.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      697 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docker/shard_tests.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     2208 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docker-compose.yaml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.217929 c65faucet-1.0.47/docs/
+-rw-r--r--   0 runner    (1001) docker     (123)       22 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)      620 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/Makefile
+-rw-r--r--   0 runner    (1001) docker     (123)      308 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.205928 c65faucet-1.0.47/docs/_static/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.217929 c65faucet-1.0.47/docs/_static/css/
+-rw-r--r--   0 runner    (1001) docker     (123)      365 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/css/responsive-tables.css
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.221929 c65faucet-1.0.47/docs/_static/deployments/
+-rw-r--r--   0 runner    (1001) docker     (123)    89120 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/deployments/ONF_Faucet_deploy1.png
+-rw-r--r--   0 runner    (1001) docker     (123)   984943 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/deployments/nznog17-physical-network.jpg
+-rw-r--r--   0 runner    (1001) docker     (123)  1106115 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/deployments/nznog17-virtual-network.jpg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.221929 c65faucet-1.0.47/docs/_static/grafana-dashboards/
+-rw-r--r--   0 runner    (1001) docker     (123)    23629 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/grafana-dashboards/faucet_instrumentation.json
+-rw-r--r--   0 runner    (1001) docker     (123)    22310 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/grafana-dashboards/faucet_inventory.json
+-rw-r--r--   0 runner    (1001) docker     (123)    18566 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/grafana-dashboards/faucet_port_statistics.json
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.225929 c65faucet-1.0.47/docs/_static/images/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8466 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/8021X-conf-diagram.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    97737 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/faucet-architecture.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    23716 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/faucet-pipeline.png
+-rw-r--r--   0 runner    (1001) docker     (123)    58351 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/faucet-pipeline.svg
+-rw-r--r--   0 runner    (1001) docker     (123)     2421 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/faucet-pipeline.txt
+-rw-r--r--   0 runner    (1001) docker     (123)   136689 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/gauge-nznog17.png
+-rw-r--r--   0 runner    (1001) docker     (123)   285918 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/gauge-snapshot1.png
+-rw-r--r--   0 runner    (1001) docker     (123)   452164 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/gauge-snapshot2.png
+-rw-r--r--   0 runner    (1001) docker     (123)   467326 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/gauge-snapshot3.png
+-rw-r--r--   0 runner    (1001) docker     (123)    51732 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-acls.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    77255 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-bgp-routing.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    47652 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-ivr.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   120062 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-multi-root-stack.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    87114 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-nfv-services.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   105019 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-stack-loop.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    78230 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-stack-tunnel.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    53594 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-stack.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   109550 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-stackwithivr.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    59007 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-static-routing.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   172474 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/images/tutorial-vlans.svg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.225929 c65faucet-1.0.47/docs/_static/tutorial/
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/tutorial/add_tagged_interface
+-rw-r--r--   0 runner    (1001) docker     (123)      134 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/tutorial/as_ns
+-rw-r--r--   0 runner    (1001) docker     (123)     1449 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/tutorial/cleanup
+-rw-r--r--   0 runner    (1001) docker     (123)      389 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/tutorial/create_ns
+-rw-r--r--   0 runner    (1001) docker     (123)      154 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/tutorial/destroy_ns
+-rw-r--r--   0 runner    (1001) docker     (123)      793 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/_static/tutorial/inter_switch_link
+-rw-r--r--   0 runner    (1001) docker     (123)     5063 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/architecture.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/autogen/
+-rw-r--r--   0 runner    (1001) docker     (123)        6 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/autogen/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)     7237 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)    40109 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/configuration.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    14496 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/developer_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     1499 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/external_resources.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/fuzzing.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      492 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     9939 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/installation.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     5125 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/intro.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      930 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/monitoring.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/recipe_book/
+-rw-r--r--   0 runner    (1001) docker     (123)       22 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/recipe_book/forwarding.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/recipe_book/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       14 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/recipe_book/policy.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       16 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/recipe_book/routing.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/release_notes/
+-rw-r--r--   0 runner    (1001) docker     (123)     3303 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/release_notes/1.7.0.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     3271 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/release_notes/1.9.0.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       99 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/source/
+-rw-r--r--   0 runner    (1001) docker     (123)       73 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/source/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    11599 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/testing.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/tutorials/
+-rw-r--r--   0 runner    (1001) docker     (123)    13460 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/tutorials/acls.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    13064 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/tutorials/conntrack.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    18441 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/tutorials/first_time.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      142 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/tutorials/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    17927 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/tutorials/nfv_services.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    15268 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/tutorials/routing.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    30053 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/tutorials/stacking.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     8477 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/tutorials/vlans.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/vendors/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/vendors/allied-telesis/
+-rw-r--r--   0 runner    (1001) docker     (123)    14233 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/allied-telesis/README_Allied_Telesis.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/vendors/cisco/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9404 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/cisco/README_Cisco.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/vendors/hpe/
+-rw-r--r--   0 runner    (1001) docker     (123)    15292 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/hpe/README_Aruba.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      405 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/vendors/lagopus/
+-rw-r--r--   0 runner    (1001) docker     (123)     3638 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/lagopus/README_Lagopus.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/vendors/northboundnetworks/
+-rw-r--r--   0 runner    (1001) docker     (123)     1661 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/northboundnetworks/README_ZodiacFX.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      605 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/northboundnetworks/README_ZodiacGX.rst
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1292 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/northboundnetworks/conf-zodiac.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/vendors/noviflow/
+-rw-r--r--   0 runner    (1001) docker     (123)     3286 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/noviflow/README_noviflow.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/docs/vendors/ovs/
+-rw-r--r--   0 runner    (1001) docker     (123)     6413 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/ovs/README_OVS-DPDK.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    18223 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/ovs/faucet_ovs_test.png
+-rw-r--r--   0 runner    (1001) docker     (123)     9332 2023-04-19 00:35:54.000000 c65faucet-1.0.47/docs/vendors/ovs/faucet_testing_with_OVS_on_hardware.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.205928 c65faucet-1.0.47/etc/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/etc/default/
+-rw-r--r--   0 runner    (1001) docker     (123)      223 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/default/faucet
+-rw-r--r--   0 runner    (1001) docker     (123)      215 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/default/gauge
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.229929 c65faucet-1.0.47/etc/faucet/
+-rw-r--r--   0 runner    (1001) docker     (123)     2826 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/faucet/acls.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     2423 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/faucet/faucet.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      965 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/faucet/gauge.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      403 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/faucet/os_ken.conf
+-rw-r--r--   0 runner    (1001) docker     (123)      403 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/faucet/ryu.conf
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.233929 c65faucet-1.0.47/etc/logrotate.d/
+-rw-r--r--   0 runner    (1001) docker     (123)      243 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/logrotate.d/faucet
+-rw-r--r--   0 runner    (1001) docker     (123)      241 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/logrotate.d/gauge
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.233929 c65faucet-1.0.47/etc/prometheus/
+-rw-r--r--   0 runner    (1001) docker     (123)      660 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/prometheus/faucet.rules.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      263 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/prometheus/prometheus-docker-compose.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      872 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/prometheus/prometheus.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.205928 c65faucet-1.0.47/etc/systemd/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.233929 c65faucet-1.0.47/etc/systemd/system/
+-rw-r--r--   0 runner    (1001) docker     (123)      388 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/systemd/system/faucet.service
+-rw-r--r--   0 runner    (1001) docker     (123)      352 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/systemd/system/gauge.service
+-rw-r--r--   0 runner    (1001) docker     (123)      355 2023-04-19 00:35:54.000000 c65faucet-1.0.47/etc/systemd/system/prometheus.service
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.237929 c65faucet-1.0.47/faucet/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5723 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35363 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/acl.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2276 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/check_faucet_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9540 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14246 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/config_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7721 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/config_parser_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    71188 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/dp.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14561 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/faucet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9912 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/faucet_bgp.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15483 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/faucet_dot1x.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4921 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/faucet_event.py
+-rw-r--r--   0 runner    (1001) docker     (123)      440 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/faucet_metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11238 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/faucet_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7322 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/faucet_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7620 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/fctl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8303 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/gauge.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9579 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/gauge_influx.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9861 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/gauge_pollers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8661 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/gauge_prom.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/meter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31122 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/port.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3487 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/prom_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6089 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/router.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19588 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/stack.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6703 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/tfm_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)    73345 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30131 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_acl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3036 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_coprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8518 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_lldp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1106 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_manager_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    42091 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1072 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_of_old.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_outonly.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28984 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_packet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8441 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)    49835 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_route.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7609 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_ryuapp.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18431 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_stack.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3429 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_switch.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30701 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_switch_stack.py
+-rw-r--r--   0 runner    (1001) docker     (123)    52744 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_switch_standalone.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12574 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_table.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6406 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valve_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18634 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/valves_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26393 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/vlan.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6424 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/watcher.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6940 2023-04-19 00:35:54.000000 c65faucet-1.0.47/faucet/watcher_conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)       43 2023-04-19 00:35:54.000000 c65faucet-1.0.47/fuzz-requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.237929 c65faucet-1.0.47/git-hook/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      456 2023-04-19 00:35:54.000000 c65faucet-1.0.47/git-hook/pre-commit
+-rw-r--r--   0 runner    (1001) docker     (123)      358 2023-04-19 00:35:54.000000 c65faucet-1.0.47/helper-funcs
+-rw-r--r--   0 runner    (1001) docker     (123)     1061 2023-04-19 00:35:54.000000 c65faucet-1.0.47/hw_switch_config.yaml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.237929 c65faucet-1.0.47/ofctl_rest/
+-rw-r--r--   0 runner    (1001) docker     (123)    26377 2023-04-19 00:35:54.000000 c65faucet-1.0.47/ofctl_rest/ofctl_rest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9460 2023-04-19 00:35:54.000000 c65faucet-1.0.47/ofctl_rest/wsgi.py
+-rw-r--r--   0 runner    (1001) docker     (123)      149 2023-04-19 00:35:54.000000 c65faucet-1.0.47/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     1064 2023-04-19 00:35:56.249929 c65faucet-1.0.47/setup.cfg
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3674 2023-04-19 00:35:54.000000 c65faucet-1.0.47/setup.py
+-rw-r--r--   0 runner    (1001) docker     (123)      196 2023-04-19 00:35:54.000000 c65faucet-1.0.47/test-requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.237929 c65faucet-1.0.47/tests/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.241929 c65faucet-1.0.47/tests/codecheck/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      278 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/codecheck/flake8.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      500 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/codecheck/min_pylint.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      234 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/codecheck/pylint.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      342 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/codecheck/pytype.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      701 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/codecheck/src_files.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.205928 c65faucet-1.0.47/tests/generative/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.205928 c65faucet-1.0.47/tests/generative/fuzzer/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.241929 c65faucet-1.0.47/tests/generative/fuzzer/config/
+-rw-r--r--   0 runner    (1001) docker     (123)      405 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/config/config.dict
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.241929 c65faucet-1.0.47/tests/generative/fuzzer/config/examples/
+-rw-r--r--   0 runner    (1001) docker     (123)      432 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/config/examples/ex0
+-rw-r--r--   0 runner    (1001) docker     (123)     1004 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/config/fuzz_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4755 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/config/generate_dict.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.241929 c65faucet-1.0.47/tests/generative/fuzzer/packet/
+-rw-r--r--   0 runner    (1001) docker     (123)     1548 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/display_packet_crash.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.245929 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/
+-rw-r--r--   0 runner    (1001) docker     (123)       65 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/aoe.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      121 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/arp.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)       85 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/arp.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      183 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/asap.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      183 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/asap.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/diameter.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      169 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/dns.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      277 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/dns.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      465 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/http.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      925 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/http.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      933 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/icmp.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)     2885 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/icmp.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      141 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/icmp.ex3
+-rw-r--r--   0 runner    (1001) docker     (123)      141 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/icmp.ex4
+-rw-r--r--   0 runner    (1001) docker     (123)      121 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/igmpv2.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/ipv4.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      381 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/ipv6.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      193 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/irc.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      265 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/irc.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      311 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/irc.ex3
+-rw-r--r--   0 runner    (1001) docker     (123)      249 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/lacp.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)     1001 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/msger.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)       63 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/packet.dict
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/tcp.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/tcp.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      109 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/tcp.ex3
+-rw-r--r--   0 runner    (1001) docker     (123)      125 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/tcp.ex4
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/tcp.ex5
+-rw-r--r--   0 runner    (1001) docker     (123)      121 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/udp.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/udp.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/udp.ex3
+-rw-r--r--   0 runner    (1001) docker     (123)      147 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/udp.ex4
+-rw-r--r--   0 runner    (1001) docker     (123)      715 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/fake_packet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1406 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/fuzzer/packet/fuzz_packet.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.245929 c65faucet-1.0.47/tests/generative/integration/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2288 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/integration/fault_tolerance_main.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17828 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/integration/fault_tolerance_tests.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.245929 c65faucet-1.0.47/tests/generative/unit/
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12548 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/generative/unit/test_topology.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.245929 c65faucet-1.0.47/tests/integration/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      565 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/integration/mininet_main.py
+-rw-r--r--   0 runner    (1001) docker     (123)   143643 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/integration/mininet_multidp_tests.py
+-rw-r--r--   0 runner    (1001) docker     (123)   292341 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/integration/mininet_tests.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      574 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/run_unit_tests.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      244 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/sysctls_for_tests.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.205928 c65faucet-1.0.47/tests/unit/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.245929 c65faucet-1.0.47/tests/unit/clib/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8000 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/clib/test_topo.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.245929 c65faucet-1.0.47/tests/unit/faucet/
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10401 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_check_config.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)   134114 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_config.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6666 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_fctl.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1519 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_main.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5773 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_port.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31562 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_valve.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    39653 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_valve_config.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4960 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_valve_dot1x.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5875 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_valve_egress.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2968 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_valve_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)   154810 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_valve_stack.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2770 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_valveapp_smoke.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9217 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/faucet/test_vlan.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.245929 c65faucet-1.0.47/tests/unit/gauge/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5559 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/gauge/test_config_gauge.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    37880 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/gauge/test_gauge.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1494 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/gauge/test_main.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 00:35:56.245929 c65faucet-1.0.47/tests/unit/packaging/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5354 2023-04-19 00:35:54.000000 c65faucet-1.0.47/tests/unit/packaging/test_packaging.py
```

### Comparing `c65faucet-1.0.46/.github/workflows/disabled/periodic.yml` & `c65faucet-1.0.47/.github/workflows/disabled/periodic.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/.github/workflows/disabled/release-debian.yml` & `c65faucet-1.0.47/.github/workflows/disabled/release-debian.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/.github/workflows/release-docker.yml` & `c65faucet-1.0.47/.github/workflows/release-docker.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/.github/workflows/release-python.yml` & `c65faucet-1.0.47/.github/workflows/release-python.yml`

 * *Files 1% similar despite different names*

```diff
@@ -20,11 +20,11 @@
       - name: Set up python-${{ env.RELEASE_PY_VER }}
         uses: actions/setup-python@v4
         with:
           python-version: ${{ env.RELEASE_PY_VER }}
       - name: Build python package
         run: python3 setup.py sdist
       - name: Publish python package to PyPI
-        uses: pypa/gh-action-pypi-publish@v1.6.4
+        uses: pypa/gh-action-pypi-publish@v1.8.5
         with:
           user: __token__
           password: ${{ secrets.PYPI_TOKEN }}
```

### Comparing `c65faucet-1.0.46/.github/workflows/tests-codecheck.yml` & `c65faucet-1.0.47/.github/workflows/tests-docs.yml`

 * *Files 8% similar despite different names*

```diff
@@ -1,41 +1,43 @@
-name: Code health checks
+name: Documentation checks
 
 on: [push, pull_request]
 
 env:
   FILES_CHANGED: "all"
-  CODECHECK_PY_VER: 3.8
+  DOCS_PY_VER: 3.8
 
 jobs:
-  codecheck:
-    name: Code check
+  build-docs:
+    name: Build documentation
     runs-on: ubuntu-latest
     steps:
       - name: Checkout repo
         uses: actions/checkout@v3
       - if: ${{ github.event_name == 'pull_request' }}
-        name: Get file changes
-        id: file_changes
-        uses: lots0logs/gh-action-get-changed-files@2.1.4
-        with:
-          token: ${{ secrets.GITHUB_TOKEN }}
-      - if: ${{ steps.file_changes.outputs.all }}
-        name: Setup dependencies
+        name: Setup dependencies for changed files action
         run: |
           sudo apt-get update -y
           sudo apt-get install -y jq
-      - if: ${{ steps.file_changes.outputs.all }}
+          git config --global --add safe.directory "$GITHUB_WORKSPACE"
+      - if: ${{ github.event_name == 'pull_request' }}
+        name: Get file changes
+        id: file_changes
+        uses: tj-actions/changed-files@v35
+        with:
+          json: true
+          json_raw_format: true
+      - if: ${{ steps.file_changes.outputs.all_changed_files }}
         name: Compare file changes
         run: |
-          FILES_ALL="$(echo '${{ steps.file_changes.outputs.all }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_ADDED="$(echo '${{ steps.file_changes.outputs.added }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_DELETED="$(echo '${{ steps.file_changes.outputs.deleted }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_RENAMED="$(echo '${{ steps.file_changes.outputs.renamed }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_MODIFIED="$(echo '${{ steps.file_changes.outputs.modified }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_ALL="$(echo '${{ steps.file_changes.outputs.all_changed_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_ADDED="$(echo '${{ steps.file_changes.outputs.added_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_DELETED="$(echo '${{ steps.file_changes.outputs.deleted_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_RENAMED="$(echo '${{ steps.file_changes.outputs.renamed_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_MODIFIED="$(echo '${{ steps.file_changes.outputs.modified_files }}' | jq -r '.[]' | tr '\n' ' ')"
           FILES_ADDED_MODIFIED="${FILES_ADDED} ${FILES_MODIFIED}"
           PY_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '\.py$' | tr '\n' ' ')"
           CI_TEST_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '(workflows\/tests-.+\.yml$|Dockerfile\.tests$|^docker\/.+$|tests\/.+\.sh$)' | tr '\n' ' ')"
           RQ_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E 'requirements(.*)txt$' | tr '\n' ' ')"
           DOC_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '^docs/.+$' | tr '\n' ' ')"
           if [ ! -z "${CI_TEST_FILES_CHANGED}" ]; then FILES_CHANGED="all"; else FILES_CHANGED="${FILES_ADDED_MODIFIED}"; fi
           echo "Git added/modified: ${FILES_ADDED_MODIFIED}"
@@ -44,25 +46,21 @@
           echo "Requirement changes: ${RQ_FILES_CHANGED}"
           echo "Documentation changes: ${DOC_FILES_CHANGED}"
           echo "Files changed: ${FILES_CHANGED}"
           echo "FILES_CHANGED=${FILES_CHANGED}" >> ${GITHUB_ENV}
           echo "PY_FILES_CHANGED=${PY_FILES_CHANGED}" >> ${GITHUB_ENV}
           echo "RQ_FILES_CHANGED=${RQ_FILES_CHANGED}" >> ${GITHUB_ENV}
           echo "DOC_FILES_CHANGED=${DOC_FILES_CHANGED}" >> ${GITHUB_ENV}
-      - name: Set up python-${{ env.CODECHECK_PY_VER }}
+      - name: Set up python-${{ env.DOCS_PY_VER }}
         uses: actions/setup-python@v4
         with:
-          python-version: ${{ env.CODECHECK_PY_VER }}
+          python-version: ${{ env.DOCS_PY_VER }}
       - name: Install dependencies
         run: |
-          ./docker/pip_deps.sh --extra-requirements="codecheck-requirements.txt"
-      - if: ${{ env.FILES_CHANGED == 'all' || env.PY_FILES_CHANGED }}
-        name: Run pylint
+          ./docker/pip_deps.sh --extra-requirements="docs/requirements.txt"
+      - if: ${{ env.FILES_CHANGED == 'all' || env.DOC_FILES_CHANGED }}
+        name: Build docs
         run: |
-          cd ./tests/codecheck
-          if [[ "${{ env.FILES_CHANGED }}" == "all" || ! -z "${{ env.RQ_FILES_CHANGED }}" ]]; then
-            echo "Running pylint on everything"
-            ./pylint.sh
-          else
-            echo "Running pylint on ${{ env.PY_FILES_CHANGED }}"
-            ./pylint.sh ${{ env.PY_FILES_CHANGED }}
-          fi
+          cd ./docs
+          sudo apt-get install librsvg2-bin
+          make html
+          rm -rf _build
```

### Comparing `c65faucet-1.0.46/.github/workflows/tests-docs.yml` & `c65faucet-1.0.47/.github/workflows/tests-codecheck.yml`

 * *Files 24% similar despite different names*

```diff
@@ -1,41 +1,43 @@
-name: Documentation checks
+name: Code health checks
 
 on: [push, pull_request]
 
 env:
   FILES_CHANGED: "all"
-  DOCS_PY_VER: 3.8
+  CODECHECK_PY_VER: 3.8
 
 jobs:
-  build-docs:
-    name: Build documentation
+  codecheck:
+    name: Code check
     runs-on: ubuntu-latest
     steps:
       - name: Checkout repo
         uses: actions/checkout@v3
       - if: ${{ github.event_name == 'pull_request' }}
-        name: Get file changes
-        id: file_changes
-        uses: lots0logs/gh-action-get-changed-files@2.1.4
-        with:
-          token: ${{ secrets.GITHUB_TOKEN }}
-      - if: ${{ steps.file_changes.outputs.all }}
-        name: Setup dependencies
+        name: Setup dependencies for changed files action
         run: |
           sudo apt-get update -y
           sudo apt-get install -y jq
-      - if: ${{ steps.file_changes.outputs.all }}
+          git config --global --add safe.directory "$GITHUB_WORKSPACE"
+      - if: ${{ github.event_name == 'pull_request' }}
+        name: Get file changes
+        id: file_changes
+        uses: tj-actions/changed-files@v35
+        with:
+          json: true
+          json_raw_format: true
+      - if: ${{ steps.file_changes.outputs.all_changed_files }}
         name: Compare file changes
         run: |
-          FILES_ALL="$(echo '${{ steps.file_changes.outputs.all }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_ADDED="$(echo '${{ steps.file_changes.outputs.added }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_DELETED="$(echo '${{ steps.file_changes.outputs.deleted }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_RENAMED="$(echo '${{ steps.file_changes.outputs.renamed }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_MODIFIED="$(echo '${{ steps.file_changes.outputs.modified }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_ALL="$(echo '${{ steps.file_changes.outputs.all_changed_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_ADDED="$(echo '${{ steps.file_changes.outputs.added_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_DELETED="$(echo '${{ steps.file_changes.outputs.deleted_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_RENAMED="$(echo '${{ steps.file_changes.outputs.renamed_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_MODIFIED="$(echo '${{ steps.file_changes.outputs.modified_files }}' | jq -r '.[]' | tr '\n' ' ')"
           FILES_ADDED_MODIFIED="${FILES_ADDED} ${FILES_MODIFIED}"
           PY_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '\.py$' | tr '\n' ' ')"
           CI_TEST_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '(workflows\/tests-.+\.yml$|Dockerfile\.tests$|^docker\/.+$|tests\/.+\.sh$)' | tr '\n' ' ')"
           RQ_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E 'requirements(.*)txt$' | tr '\n' ' ')"
           DOC_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '^docs/.+$' | tr '\n' ' ')"
           if [ ! -z "${CI_TEST_FILES_CHANGED}" ]; then FILES_CHANGED="all"; else FILES_CHANGED="${FILES_ADDED_MODIFIED}"; fi
           echo "Git added/modified: ${FILES_ADDED_MODIFIED}"
@@ -44,21 +46,25 @@
           echo "Requirement changes: ${RQ_FILES_CHANGED}"
           echo "Documentation changes: ${DOC_FILES_CHANGED}"
           echo "Files changed: ${FILES_CHANGED}"
           echo "FILES_CHANGED=${FILES_CHANGED}" >> ${GITHUB_ENV}
           echo "PY_FILES_CHANGED=${PY_FILES_CHANGED}" >> ${GITHUB_ENV}
           echo "RQ_FILES_CHANGED=${RQ_FILES_CHANGED}" >> ${GITHUB_ENV}
           echo "DOC_FILES_CHANGED=${DOC_FILES_CHANGED}" >> ${GITHUB_ENV}
-      - name: Set up python-${{ env.DOCS_PY_VER }}
+      - name: Set up python-${{ env.CODECHECK_PY_VER }}
         uses: actions/setup-python@v4
         with:
-          python-version: ${{ env.DOCS_PY_VER }}
+          python-version: ${{ env.CODECHECK_PY_VER }}
       - name: Install dependencies
         run: |
-          ./docker/pip_deps.sh --extra-requirements="docs/requirements.txt"
-      - if: ${{ env.FILES_CHANGED == 'all' || env.DOC_FILES_CHANGED }}
-        name: Build docs
+          ./docker/pip_deps.sh --extra-requirements="codecheck-requirements.txt"
+      - if: ${{ env.FILES_CHANGED == 'all' || env.PY_FILES_CHANGED }}
+        name: Run pylint
         run: |
-          cd ./docs
-          sudo apt-get install librsvg2-bin
-          make html
-          rm -rf _build
+          cd ./tests/codecheck
+          if [[ "${{ env.FILES_CHANGED }}" == "all" || ! -z "${{ env.RQ_FILES_CHANGED }}" ]]; then
+            echo "Running pylint on everything"
+            ./pylint.sh
+          else
+            echo "Running pylint on ${{ env.PY_FILES_CHANGED }}"
+            ./pylint.sh ${{ env.PY_FILES_CHANGED }}
+          fi
```

### Comparing `c65faucet-1.0.46/.github/workflows/tests-integration.yml` & `c65faucet-1.0.47/.github/workflows/tests-integration.yml`

 * *Files 8% similar despite different names*

```diff
@@ -1,44 +1,46 @@
 name: Integration tests
 
 on: [push, pull_request]
 
 env:
   FILES_CHANGED: "all"
-  MATRIX_SHARDS: 15
+  MATRIX_SHARDS: 10
 
 jobs:
   sanity-tests:
     name: Sanity tests
     runs-on: ubuntu-20.04
     container:
       image: c65sdn/test-base:latest
       options: --privileged --cap-add=ALL -v /lib/modules:/lib/modules -v /var/local/lib/docker:/var/lib/docker --sysctl net.ipv6.conf.all.disable_ipv6=0 --ulimit core=-1
     steps:
       - name: Checkout repo
         uses: actions/checkout@v3
       - if: ${{ github.event_name == 'pull_request' }}
-        name: Get file changes
-        id: file_changes
-        uses: lots0logs/gh-action-get-changed-files@2.1.4
-        with:
-          token: ${{ secrets.GITHUB_TOKEN }}
-      - if: ${{ steps.file_changes.outputs.all }}
-        name: Setup dependencies
+        name: Setup dependencies for changed files action
         run: |
           sudo apt-get update -y
           sudo apt-get install -y jq
-      - if: ${{ steps.file_changes.outputs.all }}
+          git config --global --add safe.directory "$GITHUB_WORKSPACE"
+      - if: ${{ github.event_name == 'pull_request' }}
+        name: Get file changes
+        id: file_changes
+        uses: tj-actions/changed-files@v35
+        with:
+          json: true
+          json_raw_format: true
+      - if: ${{ steps.file_changes.outputs.all_changed_files }}
         name: Compare file changes
         run: |
-          FILES_ALL="$(echo '${{ steps.file_changes.outputs.all }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_ADDED="$(echo '${{ steps.file_changes.outputs.added }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_DELETED="$(echo '${{ steps.file_changes.outputs.deleted }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_RENAMED="$(echo '${{ steps.file_changes.outputs.renamed }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_MODIFIED="$(echo '${{ steps.file_changes.outputs.modified }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_ALL="$(echo '${{ steps.file_changes.outputs.all_changed_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_ADDED="$(echo '${{ steps.file_changes.outputs.added_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_DELETED="$(echo '${{ steps.file_changes.outputs.deleted_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_RENAMED="$(echo '${{ steps.file_changes.outputs.renamed_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_MODIFIED="$(echo '${{ steps.file_changes.outputs.modified_files }}' | jq -r '.[]' | tr '\n' ' ')"
           FILES_ADDED_MODIFIED="${FILES_ADDED} ${FILES_MODIFIED}"
           PY_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '\.py$' | tr '\n' ' ')"
           CI_TEST_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '(workflows\/tests-.+\.yml$|Dockerfile\.tests$|^docker\/.+$|tests\/.+\.sh$)' | tr '\n' ' ')"
           RQ_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E 'requirements(.*)txt$' | tr '\n' ' ')"
           DOC_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '^docs/.+$' | tr '\n' ' ')"
           if [ ! -z "${CI_TEST_FILES_CHANGED}" ]; then FILES_CHANGED="all"; else FILES_CHANGED="${FILES_ADDED_MODIFIED}"; fi
           echo "Git added/modified: ${FILES_ADDED_MODIFIED}"
@@ -81,32 +83,34 @@
     strategy:
       matrix:
         MATRIX_SHARD: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
     steps:
       - name: Checkout repo
         uses: actions/checkout@v3
       - if: ${{ github.event_name == 'pull_request' && github.event.before != '0000000000000000000000000000000000000000' }}
-        name: Get file changes
-        id: file_changes
-        uses: lots0logs/gh-action-get-changed-files@2.1.4
-        with:
-          token: ${{ secrets.GITHUB_TOKEN }}
-      - if: ${{ steps.file_changes.outputs.all }}
-        name: Setup dependencies
+        name: Setup dependencies for changed files action
         run: |
           sudo apt-get update -y
           sudo apt-get install -y jq
-      - if: ${{ steps.file_changes.outputs.all }}
+          git config --global --add safe.directory "$GITHUB_WORKSPACE"
+      - if: ${{ github.event_name == 'pull_request' && github.event.before != '0000000000000000000000000000000000000000' }}
+        name: Get file changes
+        id: file_changes
+        uses: tj-actions/changed-files@v35
+        with:
+          json: true
+          json_raw_format: true
+      - if: ${{ steps.file_changes.outputs.all_changed_files }}
         name: Compare file changes
         run: |
-          FILES_ALL="$(echo '${{ steps.file_changes.outputs.all }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_ADDED="$(echo '${{ steps.file_changes.outputs.added }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_DELETED="$(echo '${{ steps.file_changes.outputs.deleted }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_RENAMED="$(echo '${{ steps.file_changes.outputs.renamed }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_MODIFIED="$(echo '${{ steps.file_changes.outputs.modified }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_ALL="$(echo '${{ steps.file_changes.outputs.all_changed_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_ADDED="$(echo '${{ steps.file_changes.outputs.added_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_DELETED="$(echo '${{ steps.file_changes.outputs.deleted_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_RENAMED="$(echo '${{ steps.file_changes.outputs.renamed_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_MODIFIED="$(echo '${{ steps.file_changes.outputs.modified_files }}' | jq -r '.[]' | tr '\n' ' ')"
           FILES_ADDED_MODIFIED="${FILES_ADDED} ${FILES_MODIFIED}"
           PY_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '\.py$' | tr '\n' ' ')"
           CI_TEST_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '(workflows\/tests-.+\.yml$|Dockerfile\.tests$|^docker\/.+$|tests\/.+\.sh$)' | tr '\n' ' ')"
           RQ_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E 'requirements(.*)txt$' | tr '\n' ' ')"
           DOC_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '^docs/.+$' | tr '\n' ' ')"
           if [ ! -z "${CI_TEST_FILES_CHANGED}" ]; then FILES_CHANGED="all"; else FILES_CHANGED="${FILES_ADDED_MODIFIED}"; fi
           echo "Git added/modified: ${FILES_ADDED_MODIFIED}"
```

### Comparing `c65faucet-1.0.46/.github/workflows/tests-unit.yml` & `c65faucet-1.0.47/.github/workflows/tests-unit.yml`

 * *Files 9% similar despite different names*

```diff
@@ -14,32 +14,34 @@
     strategy:
       matrix:
         python-version: [3.8, 3.9, '3.10', 3.11]
     steps:
       - name: Checkout repo
         uses: actions/checkout@v3
       - if: ${{ github.event_name == 'pull_request' }}
-        name: Get file changes
-        id: file_changes
-        uses: lots0logs/gh-action-get-changed-files@2.1.4
-        with:
-          token: ${{ secrets.GITHUB_TOKEN }}
-      - if: ${{ steps.file_changes.outputs.all }}
-        name: Setup dependencies
+        name: Setup dependencies for changed files action
         run: |
           sudo apt-get update -y
           sudo apt-get install -y jq
-      - if: ${{ steps.file_changes.outputs.all }}
+          git config --global --add safe.directory "$GITHUB_WORKSPACE"
+      - if: ${{ github.event_name == 'pull_request' }}
+        name: Get file changes
+        id: file_changes
+        uses: tj-actions/changed-files@v35
+        with:
+          json: true
+          json_raw_format: true
+      - if: ${{ steps.file_changes.outputs.all_changed_files }}
         name: Compare file changes
         run: |
-          FILES_ALL="$(echo '${{ steps.file_changes.outputs.all }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_ADDED="$(echo '${{ steps.file_changes.outputs.added }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_DELETED="$(echo '${{ steps.file_changes.outputs.deleted }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_RENAMED="$(echo '${{ steps.file_changes.outputs.renamed }}' | jq -r '.[]' | tr '\n' ' ')"
-          FILES_MODIFIED="$(echo '${{ steps.file_changes.outputs.modified }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_ALL="$(echo '${{ steps.file_changes.outputs.all_changed_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_ADDED="$(echo '${{ steps.file_changes.outputs.added_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_DELETED="$(echo '${{ steps.file_changes.outputs.deleted_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_RENAMED="$(echo '${{ steps.file_changes.outputs.renamed_files }}' | jq -r '.[]' | tr '\n' ' ')"
+          FILES_MODIFIED="$(echo '${{ steps.file_changes.outputs.modified_files }}' | jq -r '.[]' | tr '\n' ' ')"
           FILES_ADDED_MODIFIED="${FILES_ADDED} ${FILES_MODIFIED}"
           PY_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '\.py$' | tr '\n' ' ')"
           CI_TEST_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '(workflows\/tests-.+\.yml$|Dockerfile\.tests$|^docker\/.+$|tests\/.+\.sh$)' | tr '\n' ' ')"
           RQ_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E 'requirements(.*)txt$' | tr '\n' ' ')"
           DOC_FILES_CHANGED="$(echo ${FILES_ADDED_MODIFIED} | tr ' ' '\n' | grep -E '^docs/.+$' | tr '\n' ' ')"
           if [ ! -z "${CI_TEST_FILES_CHANGED}" ]; then FILES_CHANGED="all"; else FILES_CHANGED="${FILES_ADDED_MODIFIED}"; fi
           echo "Git added/modified: ${FILES_ADDED_MODIFIED}"
```

### Comparing `c65faucet-1.0.46/CONTRIBUTING.rst` & `c65faucet-1.0.47/CONTRIBUTING.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/LICENSE` & `c65faucet-1.0.47/LICENSE`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/Makefile` & `c65faucet-1.0.47/Makefile`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/PKG-INFO` & `c65faucet-1.0.47/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 1.2
 Name: c65faucet
-Version: 1.0.46
+Version: 1.0.47
 Summary: Faucet is an OpenFlow controller that implements a layer 2 and layer 3 switch.
 Home-page: https://faucet.nz
 Author: Faucet development team
 Author-email: faucetsdn@googlegroups.com
 License: Apache-2
 Description: UNKNOWN
 Keywords: openflow,openvswitch,ryu
```

### Comparing `c65faucet-1.0.46/README.rst` & `c65faucet-1.0.47/README.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/adapters/vendors/faucetagent/Dockerfile` & `c65faucet-1.0.47/adapters/vendors/faucetagent/Dockerfile`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 ## Image name: faucet/adapter-faucetagent
 
-FROM faucet/python3:9.0.0
+FROM faucet/python3:10.0.0
 LABEL maintainer="Charlie Lewis <clewis@iqt.org>"
 
 ENV PYTHONUNBUFFERED=0
 
 RUN apk add --update \
     gcc \
     git \
```

### Comparing `c65faucet-1.0.46/adapters/vendors/faucetagent/README.rst` & `c65faucet-1.0.47/adapters/vendors/faucetagent/README.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/adapters/vendors/faucetagent/example_client.sh` & `c65faucet-1.0.47/adapters/vendors/faucetagent/example_client.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/adapters/vendors/faucetagent/gencerts.sh` & `c65faucet-1.0.47/adapters/vendors/faucetagent/gencerts.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/adapters/vendors/rabbitmq/Dockerfile` & `c65faucet-1.0.47/adapters/vendors/rabbitmq/Dockerfile`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/adapters/vendors/rabbitmq/README.rst` & `c65faucet-1.0.47/adapters/vendors/rabbitmq/README.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/adapters/vendors/rabbitmq/docker-compose.yaml` & `c65faucet-1.0.47/adapters/vendors/rabbitmq/docker-compose.yaml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/adapters/vendors/rabbitmq/example_consumer.py` & `c65faucet-1.0.47/adapters/vendors/rabbitmq/example_consumer.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/adapters/vendors/rabbitmq/rabbit.py` & `c65faucet-1.0.47/adapters/vendors/rabbitmq/rabbit.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/adapters/vendors/rabbitmq/test_rabbit.py` & `c65faucet-1.0.47/adapters/vendors/rabbitmq/test_rabbit.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/c65faucet.egg-info/PKG-INFO` & `c65faucet-1.0.47/c65faucet.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 1.2
 Name: c65faucet
-Version: 1.0.46
+Version: 1.0.47
 Summary: Faucet is an OpenFlow controller that implements a layer 2 and layer 3 switch.
 Home-page: https://faucet.nz
 Author: Faucet development team
 Author-email: faucetsdn@googlegroups.com
 License: Apache-2
 Description: UNKNOWN
 Keywords: openflow,openvswitch,ryu
```

### Comparing `c65faucet-1.0.46/c65faucet.egg-info/SOURCES.txt` & `c65faucet-1.0.47/c65faucet.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/clib_mininet_test.py` & `c65faucet-1.0.47/clib/clib_mininet_test.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/clib_mininet_test_main.py` & `c65faucet-1.0.47/clib/clib_mininet_test_main.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/clib_mininet_tests.py` & `c65faucet-1.0.47/clib/clib_mininet_tests.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/config_generator.py` & `c65faucet-1.0.47/clib/config_generator.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/docker_host.py` & `c65faucet-1.0.47/clib/docker_host.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/fakeoftable.py` & `c65faucet-1.0.47/clib/fakeoftable.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/mininet_test_base.py` & `c65faucet-1.0.47/clib/mininet_test_base.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/mininet_test_base_topo.py` & `c65faucet-1.0.47/clib/mininet_test_base_topo.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/mininet_test_topo.py` & `c65faucet-1.0.47/clib/mininet_test_topo.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/mininet_test_util.py` & `c65faucet-1.0.47/clib/mininet_test_util.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/mininet_test_watcher.py` & `c65faucet-1.0.47/clib/mininet_test_watcher.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/tcpdump_helper.py` & `c65faucet-1.0.47/clib/tcpdump_helper.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/clib/valve_test_lib.py` & `c65faucet-1.0.47/clib/valve_test_lib.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/debian/control` & `c65faucet-1.0.47/debian/control`

 * *Files 2% similar despite different names*

```diff
@@ -27,18 +27,17 @@
 Package: python3-faucet
 Architecture: all
 Depends: python3-influxdb (>= 2.12.0),
          python3-networkx (>= 1.9),
          python3-pbr (>= 1.9),
          python3-prometheus-client (>= 0.16.0), python3-prometheus-client (<< 0.16.1),
          python3-ruamel.yaml (>= 0.17.21), python3-ruamel.yaml (<< 0.17.22),
-         python3-dnspython (<< 2.3.0),
          python3-os-ken (>= 2.6.0), python3-os-ken (<< 2.6.1),
-         python3-beka (>= 0.4.1), python3-beka (<< 0.4.2),
-         python3-chewie (>= 0.0.24), python3-chewie (<< 0.0.25),
+         python3-beka (>= 0.4.2), python3-beka (<< 0.4.3),
+         python3-chewie (>= 0.0.25), python3-chewie (<< 0.0.26),
          python3-pytricia (>= 1.0.0),
          python3:any (>= 3.8~),
          ${misc:Depends},
 Suggests: python-faucet-doc, faucet, gauge
 Description: source code for faucet and gauge (Python3)
  Python3 library that contains the source code for the Faucet open source
  OpenFlow controller, see faucet and gauge packages for further information.
```

### Comparing `c65faucet-1.0.46/debian/copyright` & `c65faucet-1.0.47/debian/copyright`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/debian/faucet.postinst` & `c65faucet-1.0.47/debian/faucet.postinst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/debian/gauge.postinst` & `c65faucet-1.0.47/debian/gauge.postinst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/debian/rules` & `c65faucet-1.0.47/debian/rules`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docker/fuzz_config.sh` & `c65faucet-1.0.47/docker/fuzz_config.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docker/fuzz_packet.sh` & `c65faucet-1.0.47/docker/fuzz_packet.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docker/install-faucet.sh` & `c65faucet-1.0.47/docker/install-faucet.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docker/localtest.sh` & `c65faucet-1.0.47/docker/localtest.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docker/pip_deps.sh` & `c65faucet-1.0.47/docker/pip_deps.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docker/runtests.sh` & `c65faucet-1.0.47/docker/runtests.sh`

 * *Files 2% similar despite different names*

```diff
@@ -113,14 +113,16 @@
       ;;
   esac
 done
 
 # Remove leading space
 FAUCET_TESTS="${PARAMS#"${PARAMS%%[![:space:]]*}"}"
 
+git config --global --add safe.directory '*'
+
 cd /faucet-src
 
 if [ "$SKIP_PIP" == 0 ] ; then
     pip_deps_args=()
   if [ -d /var/tmp/pip-cache ] ; then
     echo "Using pip cache"
     pip_deps_args+=("--pip-args=--cache-dir=/var/tmp/pip-cache")
```

### Comparing `c65faucet-1.0.46/docker/shard_tests.sh` & `c65faucet-1.0.47/docker/shard_tests.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docker-compose.yaml` & `c65faucet-1.0.47/docker-compose.yaml`

 * *Files 2% similar despite different names*

```diff
@@ -1,41 +1,41 @@
 ---
 version: '2'
 services:
     influxdb:
         restart: always
-        image: 'influxdb:2.6'
+        image: 'influxdb:2.7'
         ports:
             - '8086'
             - '8083'
         volumes:
             - '${FAUCET_PREFIX}/opt/influxdb/shared/data/db:/var/lib/influxdb'
         environment:
             INFLUXDB_DB: 'faucet'
             INFLUXDB_HTTP_AUTH_ENABLED: 'true'
             INFLUXDB_ADMIN_USER: 'faucet'
             INFLUXDB_ADMIN_PASSWORD: 'faucet'
 
     prometheus:
         restart: always
-        image: 'prom/prometheus:v2.42.0'
+        image: 'prom/prometheus:v2.43.0'
         user: 'root'
         ports:
             - '9090:9090'
         volumes:
             - '${FAUCET_PREFIX}/opt/prometheus/:/prometheus'
             - './etc/prometheus/prometheus-docker-compose.yml:/etc/prometheus/prometheus.yml'
             - './etc/prometheus/faucet.rules.yml:/etc/prometheus/faucet.rules.yml'
         links:
             - faucet
             - gauge
 
     grafana:
         restart: always
-        image: 'grafana/grafana:9.3.6'
+        image: 'grafana/grafana:9.4.7'
         user: 'root'
         ports:
             - '3000:3000'
         volumes:
             - '${FAUCET_PREFIX}/opt/grafana:/var/lib/grafana'
         links:
             - influxdb
```

### Comparing `c65faucet-1.0.46/docs/Makefile` & `c65faucet-1.0.47/docs/Makefile`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/deployments/ONF_Faucet_deploy1.png` & `c65faucet-1.0.47/docs/_static/deployments/ONF_Faucet_deploy1.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/deployments/nznog17-physical-network.jpg` & `c65faucet-1.0.47/docs/_static/deployments/nznog17-physical-network.jpg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/deployments/nznog17-virtual-network.jpg` & `c65faucet-1.0.47/docs/_static/deployments/nznog17-virtual-network.jpg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/grafana-dashboards/faucet_instrumentation.json` & `c65faucet-1.0.47/docs/_static/grafana-dashboards/faucet_instrumentation.json`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/grafana-dashboards/faucet_inventory.json` & `c65faucet-1.0.47/docs/_static/grafana-dashboards/faucet_inventory.json`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/grafana-dashboards/faucet_port_statistics.json` & `c65faucet-1.0.47/docs/_static/grafana-dashboards/faucet_port_statistics.json`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/8021X-conf-diagram.svg` & `c65faucet-1.0.47/docs/_static/images/8021X-conf-diagram.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/faucet-architecture.svg` & `c65faucet-1.0.47/docs/_static/images/faucet-architecture.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/faucet-pipeline.png` & `c65faucet-1.0.47/docs/_static/images/faucet-pipeline.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/faucet-pipeline.svg` & `c65faucet-1.0.47/docs/_static/images/faucet-pipeline.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/faucet-pipeline.txt` & `c65faucet-1.0.47/docs/_static/images/faucet-pipeline.txt`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/gauge-nznog17.png` & `c65faucet-1.0.47/docs/_static/images/gauge-nznog17.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/gauge-snapshot1.png` & `c65faucet-1.0.47/docs/_static/images/gauge-snapshot1.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/gauge-snapshot2.png` & `c65faucet-1.0.47/docs/_static/images/gauge-snapshot2.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/gauge-snapshot3.png` & `c65faucet-1.0.47/docs/_static/images/gauge-snapshot3.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-acls.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-acls.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-bgp-routing.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-bgp-routing.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-ivr.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-ivr.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-multi-root-stack.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-multi-root-stack.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-nfv-services.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-nfv-services.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-stack-loop.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-stack-loop.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-stack-tunnel.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-stack-tunnel.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-stack.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-stack.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-stackwithivr.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-stackwithivr.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-static-routing.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-static-routing.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/images/tutorial-vlans.svg` & `c65faucet-1.0.47/docs/_static/images/tutorial-vlans.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/tutorial/cleanup` & `c65faucet-1.0.47/docs/_static/tutorial/cleanup`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/_static/tutorial/inter_switch_link` & `c65faucet-1.0.47/docs/_static/tutorial/inter_switch_link`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/architecture.rst` & `c65faucet-1.0.47/docs/architecture.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/conf.py` & `c65faucet-1.0.47/docs/conf.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/configuration.rst` & `c65faucet-1.0.47/docs/configuration.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/developer_guide.rst` & `c65faucet-1.0.47/docs/developer_guide.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/external_resources.rst` & `c65faucet-1.0.47/docs/external_resources.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/fuzzing.rst` & `c65faucet-1.0.47/docs/fuzzing.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/installation.rst` & `c65faucet-1.0.47/docs/installation.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/intro.rst` & `c65faucet-1.0.47/docs/intro.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/monitoring.rst` & `c65faucet-1.0.47/docs/monitoring.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/release_notes/1.7.0.rst` & `c65faucet-1.0.47/docs/release_notes/1.7.0.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/release_notes/1.9.0.rst` & `c65faucet-1.0.47/docs/release_notes/1.9.0.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/testing.rst` & `c65faucet-1.0.47/docs/testing.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/tutorials/acls.rst` & `c65faucet-1.0.47/docs/tutorials/acls.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/tutorials/conntrack.rst` & `c65faucet-1.0.47/docs/tutorials/conntrack.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/tutorials/first_time.rst` & `c65faucet-1.0.47/docs/tutorials/first_time.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/tutorials/nfv_services.rst` & `c65faucet-1.0.47/docs/tutorials/nfv_services.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/tutorials/routing.rst` & `c65faucet-1.0.47/docs/tutorials/routing.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/tutorials/stacking.rst` & `c65faucet-1.0.47/docs/tutorials/stacking.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/tutorials/vlans.rst` & `c65faucet-1.0.47/docs/tutorials/vlans.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/allied-telesis/README_Allied_Telesis.rst` & `c65faucet-1.0.47/docs/vendors/allied-telesis/README_Allied_Telesis.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/cisco/README_Cisco.rst` & `c65faucet-1.0.47/docs/vendors/cisco/README_Cisco.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/hpe/README_Aruba.rst` & `c65faucet-1.0.47/docs/vendors/hpe/README_Aruba.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/lagopus/README_Lagopus.rst` & `c65faucet-1.0.47/docs/vendors/lagopus/README_Lagopus.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/northboundnetworks/README_ZodiacFX.rst` & `c65faucet-1.0.47/docs/vendors/northboundnetworks/README_ZodiacFX.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/northboundnetworks/README_ZodiacGX.rst` & `c65faucet-1.0.47/docs/vendors/northboundnetworks/README_ZodiacGX.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/northboundnetworks/conf-zodiac.sh` & `c65faucet-1.0.47/docs/vendors/northboundnetworks/conf-zodiac.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/noviflow/README_noviflow.rst` & `c65faucet-1.0.47/docs/vendors/noviflow/README_noviflow.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/ovs/README_OVS-DPDK.rst` & `c65faucet-1.0.47/docs/vendors/ovs/README_OVS-DPDK.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/ovs/faucet_ovs_test.png` & `c65faucet-1.0.47/docs/vendors/ovs/faucet_ovs_test.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/docs/vendors/ovs/faucet_testing_with_OVS_on_hardware.rst` & `c65faucet-1.0.47/docs/vendors/ovs/faucet_testing_with_OVS_on_hardware.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/etc/faucet/acls.yaml` & `c65faucet-1.0.47/etc/faucet/acls.yaml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/etc/faucet/faucet.yaml` & `c65faucet-1.0.47/etc/faucet/faucet.yaml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/etc/faucet/gauge.yaml` & `c65faucet-1.0.47/etc/faucet/gauge.yaml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/etc/prometheus/faucet.rules.yml` & `c65faucet-1.0.47/etc/prometheus/faucet.rules.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/etc/prometheus/prometheus.yml` & `c65faucet-1.0.47/etc/prometheus/prometheus.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/__main__.py` & `c65faucet-1.0.47/faucet/__main__.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,18 +20,18 @@
 
 import argparse
 import os
 import sys
 
 from pbr.version import VersionInfo
 
-if sys.version_info < (3,) or sys.version_info < (3, 6):
+if sys.version_info < (3,) or sys.version_info < (3, 5):
     raise ImportError("""You are trying to run faucet on python {py}
 
-Faucet is not compatible with python {py}, please upgrade to python 3.6 or newer."""
+Faucet is not compatible with python {py}, please upgrade to python 3.5 or newer."""
                       .format(py='.'.join([str(v) for v in sys.version_info[:3]])))
 
 RYU_OPTIONAL_ARGS = [
     ('ca-certs', 'CA certificates'),
     ('config-dir', """Path to a config directory to pull `*.conf` files
                       from. This file set is sorted, so as to provide a
                       predictable parse order if individual options are
@@ -83,20 +83,20 @@
         action='append',
         help='add Ryu app (can be specified multiple times)',
         metavar='APP')
 
     for ryu_arg in RYU_OPTIONAL_ARGS:
         if len(ryu_arg) >= 3:
             args.add_argument(
-                f'--ryu-{ryu_arg[0]}',
+                '--ryu-%s' % ryu_arg[0],
                 help=ryu_arg[1],
                 default=ryu_arg[2])
         else:
             args.add_argument(
-                f'--ryu-{ryu_arg[0]}',
+                '--ryu-%s' % ryu_arg[0],
                 help=ryu_arg[1])
 
     return args.parse_args(sys_args)
 
 
 def print_version():
     """Print version number and exit."""
@@ -130,15 +130,15 @@
         if not val or not arg.startswith('ryu'):
             continue
         if arg == 'ryu_app_lists':
             continue
         if arg == 'ryu_config_file' and not os.path.isfile(val):
             continue
         arg_name = arg.replace('ryu_', '').replace('_', '-')
-        ryu_args.append(f'--{arg_name}={val}')
+        ryu_args.append('--%s=%s' % (arg_name, val))
 
     # Running Faucet or Gauge?
     if args.gauge or os.path.basename(prog) == 'gauge':
         ryu_args.append('faucet.gauge')
     else:
         ryu_args.append('faucet.faucet')
```

### Comparing `c65faucet-1.0.46/faucet/acl.py` & `c65faucet-1.0.47/faucet/acl.py`

 * *Files 16% similar despite different names*

```diff
@@ -63,77 +63,77 @@
  * vlan_vid: (int): push the vlan vid on the packet when outputting
  * vlan_vids: (list): push the list of vlans on the packet when outputting, with option eth_type
  * swap_vid (int): rewrite the vlan vid of the packet when outputting
  * failover (dict): Output with a failover port (experimental)
 """
 
     defaults = {
-        'rules': None,
-        'exact_match': False,
-        'dot1x_assigned': False,
+        "rules": None,
+        "exact_match": False,
+        "dot1x_assigned": False,
     }
     defaults_types = {
-        'rules': list,
-        'exact_match': bool,
-        'dot1x_assigned': bool,
+        "rules": list,
+        "exact_match": bool,
+        "dot1x_assigned": bool,
     }
     rule_types = {
-        'cookie': int,
-        'actions': dict,
-        'description': str,
+        "cookie": int,
+        "actions": dict,
+        "description": str,
     }
     actions_types = {
-        'meter': str,
-        'mirror': (str, int),
-        'output': (dict, list),
-        'allow': int,
-        'force_port_vlan': int,
-        'ct': dict,
+        "meter": str,
+        "mirror": (str, int),
+        "output": (dict, list),
+        "allow": int,
+        "force_port_vlan": int,
+        "ct": dict,
     }
     output_actions_types = {
-        'tunnel': dict,
-        'port': (str, int),
-        'ports': list,
-        'failover': dict,
-        'set_fields': list,
-        'pop_vlans': int,
-        'swap_vid': int,
-        'vlan_vid': int,
-        'vlan_vids': list,
+        "tunnel": dict,
+        "port": (str, int),
+        "ports": list,
+        "failover": dict,
+        "set_fields": list,
+        "pop_vlans": int,
+        "swap_vid": int,
+        "vlan_vid": int,
+        "vlan_vids": list,
     }
     ct_action_types = {
-        'flags': int,
-        'alg': int,
-        'table': int,
-        'zone': int,
-        'zone_src': int,
-        'clear': bool,
-        'nat': dict,
+        "flags": int,
+        "alg": int,
+        "table": int,
+        "zone": int,
+        "zone_src": int,
+        "clear": bool,
+        "nat": dict,
     }
     ct_action_nat_types = {
-        'flags': int,
-        'range_ipv4_min': str,
-        'range_ipv4_max': str,
-        'range_ipv6_min': str,
-        'range_ipv6_max': str,
-        'range_proto_min': int,
-        'range_proto_max': int
+        "flags": int,
+        "range_ipv4_min": str,
+        "range_ipv4_max": str,
+        "range_ipv6_min": str,
+        "range_ipv6_max": str,
+        "range_proto_min": int,
+        "range_proto_max": int,
     }
     tunnel_types = {
-        'type': (str, None),
-        'tunnel_id': (str, int, None),
-        'dp': str,
-        'port': (str, int, None),
-        'exit_instructions': (list, None),
-        'maintain_encapsulation': bool,
-        'bi_directional': bool,
-        'reverse': bool,
+        "type": (str, None),
+        "tunnel_id": (str, int, None),
+        "dp": str,
+        "port": (str, int, None),
+        "exit_instructions": (list, None),
+        "maintain_encapsulation": bool,
+        "bi_directional": bool,
+        "reverse": bool,
     }
 
-    mutable_attrs = frozenset(['tunnel_sources'])
+    mutable_attrs = frozenset(["tunnel_sources"])
 
     def __init__(self, _id, dp_id, conf):
         self.rules = []
         self.exact_match = None
         self.dot1x_assigned = None
         self.meter = False
         self.matches = {}
@@ -148,102 +148,117 @@
         self.dyn_tunnel_rules = {}
         self.dyn_reverse_tunnel_rules = {}
 
         for match_fields in (MATCH_FIELDS, OLD_MATCH_FIELDS):
             self.rule_types.update({match: (str, int) for match in match_fields})
         conf = copy.deepcopy(conf)
         if isinstance(conf, dict):
-            rules = conf.get('rules', [])
+            rules = conf.get("rules", [])
         elif isinstance(conf, list):
             rules = conf
             conf = {}
         else:
-            raise InvalidConfigError(
-                f'ACL conf is an invalid type {_id}')
-        conf['rules'] = []
+            raise InvalidConfigError("ACL conf is an invalid type %s" % _id)
+        conf["rules"] = []
         for rule in rules:
             normalized_rule = rule
             if isinstance(rule, dict):
-                normalized_rule = rule.get('rule', rule)
+                normalized_rule = rule.get("rule", rule)
                 if normalized_rule is None:
                     normalized_rule = {k: v for k, v in rule.items() if v is not None}
-            test_config_condition(not isinstance(normalized_rule, dict), (
-                f'ACL rule is {type(normalized_rule)} not {dict} ({rules})'))
-            conf['rules'].append(normalized_rule)
+            test_config_condition(
+                not isinstance(normalized_rule, dict),
+                ("ACL rule is %s not %s (%s)" % (type(normalized_rule), dict, rules)),
+            )
+            conf["rules"].append(normalized_rule)
         super().__init__(_id, dp_id, conf)
 
     def finalize(self):
         self._ports_resolved = True
         super().finalize()
 
     def check_config(self):
-        test_config_condition(
-            not self.rules, f'no rules found for ACL {self._id}')
+        test_config_condition(not self.rules, "no rules found for ACL %s" % self._id)
         for rule in self.rules:
             self._check_conf_types(rule, self.rule_types)
             for rule_field, rule_conf in rule.items():
-                if rule_field == 'cookie':
+                if rule_field == "cookie":
                     test_config_condition(
                         rule_conf < 0 or rule_conf > 2**16,
-                        'rule cookie value must be 0-2**16')
-                elif rule_field == 'actions':
+                        "rule cookie value must be 0-2**16",
+                    )
+                elif rule_field == "actions":
                     test_config_condition(
-                        not rule_conf,
-                        f'Missing rule actions in ACL {self._id}')
+                        not rule_conf, "Missing rule actions in ACL %s" % self._id
+                    )
                     self._check_conf_types(rule_conf, self.actions_types)
                     for action_name, action_conf in rule_conf.items():
-                        if action_name == 'output':
+                        if action_name == "output":
                             if isinstance(action_conf, (list, tuple)):
                                 # New ordered format
                                 for subconf in action_conf:
                                     # Make sure only one specified action per list element
                                     test_config_condition(
                                         len(subconf) > 1,
-                                        'ACL ordered output must have only one action per element')
+                                        "ACL ordered output must have only one action per element",
+                                    )
                                     # Ensure correct action format
-                                    self._check_conf_types(subconf, self.output_actions_types)
+                                    self._check_conf_types(
+                                        subconf, self.output_actions_types
+                                    )
                             else:
                                 # Old format
                                 self._check_conf_types(
-                                    action_conf, self.output_actions_types)
-                        elif action_name == 'ct':
+                                    action_conf, self.output_actions_types
+                                )
+                        elif action_name == "ct":
                             self._check_conf_types(action_conf, self.ct_action_types)
                             # if clear set, make sure nothing else is
-                            if 'clear' in action_conf and action_conf['clear']:
+                            if "clear" in action_conf and action_conf["clear"]:
                                 test_config_condition(
                                     len(action_conf) != 1,
                                     "no other parameters can be set when 'clear' set on "
-                                    "conntrack ACL")
+                                    "conntrack ACL",
+                                )
                             else:
                                 test_config_condition(
-                                    'table' not in action_conf,
-                                    "required parameter 'table' not set for conntrack ACL")
+                                    "table" not in action_conf,
+                                    "required parameter 'table' not set for conntrack ACL",
+                                )
                                 test_config_condition(
-                                    'zone' not in action_conf,
-                                    "required parameter 'zone' not set for conntrack ACL")
-                            if 'nat' in action_conf:
-                                self._check_conf_types(action_conf['nat'], self.ct_action_nat_types)
+                                    "zone" not in action_conf,
+                                    "required parameter 'zone' not set for conntrack ACL",
+                                )
+                            if "nat" in action_conf:
+                                self._check_conf_types(
+                                    action_conf["nat"], self.ct_action_nat_types
+                                )
 
     def build(self, meters, vid, port_num):
         """Check that ACL can be built from config."""
 
         self.matches = {}
         self.set_fields = set()
         self.meter = False
         if self.rules:
             try:
                 ofmsgs = valve_acl.build_acl_ofmsgs(
-                    [self], wildcard_table,
+                    [self],
+                    wildcard_table,
                     [valve_of.goto_table(wildcard_table)],
                     [valve_of.goto_table(wildcard_table)],
-                    2**16 - 1, meters, self.exact_match,
-                    vlan_vid=vid, port_num=port_num)
+                    2**16 - 1,
+                    meters,
+                    self.exact_match,
+                    vlan_vid=vid,
+                    port_num=port_num,
+                )
             except (netaddr.core.AddrFormatError, KeyError, ValueError) as err:
                 raise InvalidConfigError from err
-            test_config_condition(not ofmsgs, 'OF messages is empty')
+            test_config_condition(not ofmsgs, "OF messages is empty")
             for ofmsg in ofmsgs:
                 try:
                     valve_of.verify_flowmod(ofmsg)
                 except (KeyError, ValueError) as err:
                     raise InvalidConfigError from err
                 except Exception as err:
                     raise err
@@ -258,22 +273,21 @@
                         if valve_of.is_set_field(action):
                             self.set_fields.add(action.key)
                     for match, value in ofmsg.match.items():
                         has_mask = isinstance(value, (tuple, list))
                         if has_mask or match not in self.matches:
                             self.matches[match] = has_mask
         for tunnel_rules in self.tunnel_dests.values():
-            if 'exit_instructions' in tunnel_rules:
-                exit_inst = tunnel_rules['exit_instructions']
+            if "exit_instructions" in tunnel_rules:
+                exit_inst = tunnel_rules["exit_instructions"]
                 try:
-                    ofmsgs = valve_acl.build_tunnel_ofmsgs(
-                        exit_inst, wildcard_table, 1)
+                    ofmsgs = valve_acl.build_tunnel_ofmsgs(exit_inst, wildcard_table, 1)
                 except (netaddr.core.AddrFormatError, KeyError, ValueError) as err:
                     raise InvalidConfigError from err
-                test_config_condition(not ofmsgs, 'OF messages is empty')
+                test_config_condition(not ofmsgs, "OF messages is empty")
                 for ofmsg in ofmsgs:
                     try:
                         valve_of.verify_flowmod(ofmsg)
                     except (KeyError, ValueError) as err:
                         raise InvalidConfigError from err
                     except Exception as err:
                         raise err
@@ -292,259 +306,294 @@
                             if has_mask or match not in self.matches:
                                 self.matches[match] = has_mask
         return (self.matches, self.set_fields, self.meter)
 
     def get_meters(self):
         """Yield meters for each rule in ACL"""
         for rule in self.rules:
-            if 'actions' not in rule or 'meter' not in rule['actions']:
+            if "actions" not in rule or "meter" not in rule["actions"]:
                 continue
-            yield rule['actions']['meter']
+            yield rule["actions"]["meter"]
 
     def get_mirror_destinations(self):
         """Yield mirror destinations for each rule in ACL"""
         for rule in self.rules:
-            if 'actions' not in rule or 'mirror' not in rule['actions']:
+            if "actions" not in rule or "mirror" not in rule["actions"]:
                 continue
-            yield rule['actions']['mirror']
+            yield rule["actions"]["mirror"]
 
-    def _resolve_ordered_output_ports(self, output_list, resolve_port_cb, resolve_tunnel_objects):
+    def _resolve_ordered_output_ports(
+        self, output_list, resolve_port_cb, resolve_tunnel_objects
+    ):
         """Resolve output actions in the ordered list format"""
         result = []
         for action in output_list:
             for key, value in action.items():
-                if key == 'tunnel':
+                if key == "tunnel":
                     tunnel = value
                     # Fetch tunnel items from the tunnel output dict
                     test_config_condition(
-                        'dp' not in tunnel,
-                        f'ACL ({self._id}) tunnel DP not defined')
-                    tunnel_dp = tunnel['dp']
-                    tunnel_port = tunnel.get('port', None)
-                    tunnel_id = tunnel.get('tunnel_id', None)
-                    tunnel_type = tunnel.get('type', 'vlan')
-                    tunnel_exit_instructions = tunnel.get('exit_instructions', [])
-                    tunnel_direction = tunnel.get('bi_directional', False)
-                    tunnel_maintain = tunnel.get('maintain_encapsulation', False)
-                    tunnel_reverse = tunnel.get('reverse', False)
+                        "dp" not in tunnel, "ACL (%s) tunnel DP not defined" % self._id
+                    )
+                    tunnel_dp = tunnel["dp"]
+                    tunnel_port = tunnel.get("port", None)
+                    tunnel_id = tunnel.get("tunnel_id", None)
+                    tunnel_type = tunnel.get("type", "vlan")
+                    tunnel_exit_instructions = tunnel.get("exit_instructions", [])
+                    tunnel_direction = tunnel.get("bi_directional", False)
+                    tunnel_maintain = tunnel.get("maintain_encapsulation", False)
+                    tunnel_reverse = tunnel.get("reverse", False)
                     test_config_condition(
                         tunnel_reverse and tunnel_direction,
-                        (f'Tunnel ACL {self._id} cannot contain values for the fields'
-                         '`bi_directional` and `reverse` at the same time'))
+                        (
+                            "Tunnel ACL %s cannot contain values for the fields"
+                            "`bi_directional` and `reverse` at the same time" % self._id
+                        ),
+                    )
                     # Resolve the tunnel items
                     dst_dp, dst_port, tunnel_id = resolve_tunnel_objects(
-                        tunnel_dp, tunnel_port, tunnel_id)
+                        tunnel_dp, tunnel_port, tunnel_id
+                    )
                     # Compile the tunnel into an easy-access dictionary
                     tunnel_dict = {
-                        'dst_dp': dst_dp,
-                        'dst_port': dst_port,
-                        'tunnel_id': tunnel_id,
-                        'type': tunnel_type,
-                        'exit_instructions': tunnel_exit_instructions,
-                        'bi_directional': tunnel_direction,
-                        'maintain_encapsulation': tunnel_maintain,
-                        'reverse': tunnel_reverse,
+                        "dst_dp": dst_dp,
+                        "dst_port": dst_port,
+                        "tunnel_id": tunnel_id,
+                        "type": tunnel_type,
+                        "exit_instructions": tunnel_exit_instructions,
+                        "bi_directional": tunnel_direction,
+                        "maintain_encapsulation": tunnel_maintain,
+                        "reverse": tunnel_reverse,
                     }
                     self.tunnel_dests[tunnel_id] = tunnel_dict
                     result.append({key: tunnel_id})
-                elif key == 'port':
+                elif key == "port":
                     port_name = value
                     port = resolve_port_cb(port_name)
                     test_config_condition(
                         not port,
-                        f'ACL ({self._id}) output port undefined in DP: {self.dp_id}')
+                        "ACL (%s) output port undefined in DP: %s"
+                        % (self._id, self.dp_id),
+                    )
                     result.append({key: port})
-                elif key == 'ports':
-                    resolved_ports = [
-                        resolve_port_cb(p) for p in value]
+                elif key == "ports":
+                    resolved_ports = [resolve_port_cb(p) for p in value]
                     test_config_condition(
                         None in resolved_ports,
-                        f'ACL ({self._id}) output port(s) not defined in DP: {self.dp_id}')
+                        "ACL (%s) output port(s) not defined in DP: %s"
+                        % (self._id, self.dp_id),
+                    )
                     result.append({key: resolved_ports})
-                elif key == 'failover':
+                elif key == "failover":
                     failover = value
-                    test_config_condition(not isinstance(failover, dict), (
-                        'failover is not a dictionary'))
+                    test_config_condition(
+                        not isinstance(failover, dict), ("failover is not a dictionary")
+                    )
                     failover_dict = {}
                     for failover_name, failover_values in failover.items():
-                        if failover_name == 'ports':
+                        if failover_name == "ports":
                             resolved_ports = [
-                                resolve_port_cb(p) for p in failover_values]
+                                resolve_port_cb(p) for p in failover_values
+                            ]
                             test_config_condition(
                                 None in resolved_ports,
-                                f'ACL ({self._id}) failover port(s) not defined in DP: {self.dp_id}')
+                                "ACL (%s) failover port(s) not defined in DP: %s"
+                                % (self._id, self.dp_id),
+                            )
                             failover_dict[failover_name] = resolved_ports
                         else:
                             failover_dict[failover_name] = failover_values
                     result.append({key: failover_dict})
                 else:
                     result.append(action)
         return result
 
-    def _resolve_output_ports(self, action_conf, resolve_port_cb, resolve_tunnel_objects):
+    def _resolve_output_ports(
+        self, action_conf, resolve_port_cb, resolve_tunnel_objects
+    ):
         """Resolve the values for output actions in the ACL"""
         if isinstance(action_conf, (list, tuple)):
             return self._resolve_ordered_output_ports(
-                action_conf, resolve_port_cb, resolve_tunnel_objects)
+                action_conf, resolve_port_cb, resolve_tunnel_objects
+            )
         result = {}
         test_config_condition(
-            'vlan_vid' in action_conf and 'vlan_vids' in action_conf,
-            f'ACL {self._id} has both vlan_vid and vlan_vids defined')
+            "vlan_vid" in action_conf and "vlan_vids" in action_conf,
+            "ACL %s has both vlan_vid and vlan_vids defined" % self._id,
+        )
         test_config_condition(
-            'port' in action_conf and 'ports' in action_conf,
-            f'ACL {self._id} has both port and ports defined')
+            "port" in action_conf and "ports" in action_conf,
+            "ACL %s has both port and ports defined" % self._id,
+        )
         for output_action, output_action_values in action_conf.items():
-            if output_action == 'tunnel':
+            if output_action == "tunnel":
                 tunnel = output_action_values
                 # Fetch tunnel items from the tunnel output dict
                 test_config_condition(
-                    'dp' not in tunnel,
-                    f'ACL ({self._id}) tunnel DP not defined')
-                tunnel_dp = tunnel['dp']
-                tunnel_port = tunnel.get('port', None)
-                tunnel_id = tunnel.get('tunnel_id', None)
-                tunnel_type = tunnel.get('type', 'vlan')
-                tunnel_exit_instructions = tunnel.get('exit_instructions', [])
-                tunnel_direction = tunnel.get('bi_directional', False)
-                tunnel_maintain = tunnel.get('maintain_encapsulation', False)
-                tunnel_reverse = tunnel.get('reverse', False)
+                    "dp" not in tunnel, "ACL (%s) tunnel DP not defined" % self._id
+                )
+                tunnel_dp = tunnel["dp"]
+                tunnel_port = tunnel.get("port", None)
+                tunnel_id = tunnel.get("tunnel_id", None)
+                tunnel_type = tunnel.get("type", "vlan")
+                tunnel_exit_instructions = tunnel.get("exit_instructions", [])
+                tunnel_direction = tunnel.get("bi_directional", False)
+                tunnel_maintain = tunnel.get("maintain_encapsulation", False)
+                tunnel_reverse = tunnel.get("reverse", False)
                 test_config_condition(
                     tunnel_reverse and tunnel_direction,
-                    (f'Tunnel ACL {self._id} cannot contain values for the fields'
-                     '`bi_directional` and `reverse` at the same time')
+                    (
+                        "Tunnel ACL %s cannot contain values for the fields"
+                        "`bi_directional` and `reverse` at the same time" % self._id
+                    ),
                 )
                 # Resolve the tunnel items
                 dst_dp, dst_port, tunnel_id = resolve_tunnel_objects(
-                    tunnel_dp, tunnel_port, tunnel_id)
+                    tunnel_dp, tunnel_port, tunnel_id
+                )
                 # Compile the tunnel into an easy-access dictionary
                 tunnel_dict = {
-                    'dst_dp': dst_dp,
-                    'dst_port': dst_port,
-                    'tunnel_id': tunnel_id,
-                    'type': tunnel_type,
-                    'exit_instructions': tunnel_exit_instructions,
-                    'bi_directional': tunnel_direction,
-                    'maintain_encapsulation': tunnel_maintain,
-                    'reverse': tunnel_reverse,
+                    "dst_dp": dst_dp,
+                    "dst_port": dst_port,
+                    "tunnel_id": tunnel_id,
+                    "type": tunnel_type,
+                    "exit_instructions": tunnel_exit_instructions,
+                    "bi_directional": tunnel_direction,
+                    "maintain_encapsulation": tunnel_maintain,
+                    "reverse": tunnel_reverse,
                 }
                 self.tunnel_dests[tunnel_id] = tunnel_dict
                 result[output_action] = tunnel_id
-            elif output_action == 'port':
+            elif output_action == "port":
                 port_name = output_action_values
                 port = resolve_port_cb(port_name)
                 test_config_condition(
                     not port,
-                    (f'ACL ({self._id}) output port undefined in DP: {self.dp_id}')
+                    (
+                        "ACL (%s) output port undefined in DP: %s"
+                        % (self._id, self.dp_id)
+                    ),
                 )
                 result[output_action] = port
-            elif output_action == 'ports':
-                resolved_ports = [
-                    resolve_port_cb(p) for p in output_action_values]
+            elif output_action == "ports":
+                resolved_ports = [resolve_port_cb(p) for p in output_action_values]
                 test_config_condition(
                     None in resolved_ports,
-                    (f'ACL ({self._id}) output port(s) not defined in DP: {self.dp_id}')
+                    (
+                        "ACL (%s) output port(s) not defined in DP: %s"
+                        % (self._id, self.dp_id)
+                    ),
                 )
                 result[output_action] = resolved_ports
-            elif output_action == 'failover':
+            elif output_action == "failover":
                 failover = output_action_values
-                test_config_condition(not isinstance(failover, dict), (
-                    'failover is not a dictionary'))
+                test_config_condition(
+                    not isinstance(failover, dict), ("failover is not a dictionary")
+                )
                 result[output_action] = {}
                 for failover_name, failover_values in failover.items():
-                    if failover_name == 'ports':
-                        resolved_ports = [
-                            resolve_port_cb(p) for p in failover_values]
+                    if failover_name == "ports":
+                        resolved_ports = [resolve_port_cb(p) for p in failover_values]
                         test_config_condition(
                             None in resolved_ports,
-                            (f'ACL ({self._id}) failover port(s) not defined in DP: {self.dp_id}')
+                            (
+                                "ACL (%s) failover port(s) not defined in DP: %s"
+                                % (self._id, self.dp_id)
+                            ),
                         )
                         result[output_action][failover_name] = resolved_ports
                     else:
                         result[output_action][failover_name] = failover_values
             else:
                 result[output_action] = output_action_values
         return result
 
     def resolve_ports(self, resolve_port_cb, resolve_tunnel_objects):
         """Resolve the values for the actions of an ACL"""
         if self._ports_resolved:
             return
         for rule_conf in self.rules:
-            if 'actions' in rule_conf:
-                actions_conf = rule_conf['actions']
+            if "actions" in rule_conf:
+                actions_conf = rule_conf["actions"]
                 resolved_actions = {}
-                test_config_condition(not isinstance(actions_conf, dict), (
-                    'actions value is not a dictionary'))
+                test_config_condition(
+                    not isinstance(actions_conf, dict),
+                    ("actions value is not a dictionary"),
+                )
                 for action_name, action_conf in actions_conf.items():
-                    if action_name == 'mirror':
+                    if action_name == "mirror":
                         resolved_port = resolve_port_cb(action_conf)
                         test_config_condition(
                             resolved_port is None,
-                            (f'ACL ({self._id}) mirror port is not defined in DP: {self.dp_id}')
+                            (
+                                "ACL (%s) mirror port is not defined in DP: %s"
+                                % (self._id, self.dp_id)
+                            ),
                         )
                         resolved_actions[action_name] = resolved_port
-                    elif action_name == 'output':
+                    elif action_name == "output":
                         resolved_action = self._resolve_output_ports(
-                            action_conf, resolve_port_cb, resolve_tunnel_objects)
+                            action_conf, resolve_port_cb, resolve_tunnel_objects
+                        )
                         resolved_actions[action_name] = resolved_action
                     else:
                         resolved_actions[action_name] = action_conf
-                rule_conf['actions'] = resolved_actions
+                rule_conf["actions"] = resolved_actions
         self._ports_resolved = True
 
     def requires_reverse_tunnel(self, tunnel_id):
         """Returns true if the tunnel requires a reverse pathway"""
-        return self.tunnel_dests[tunnel_id]['bi_directional']
+        return self.tunnel_dests[tunnel_id]["bi_directional"]
 
     def get_num_tunnels(self):
         """Returns the number of tunnels specified in the ACL"""
         num_tunnels = 0
         for rule_conf in self.rules:
             if self.does_rule_contain_tunnel(rule_conf):
-                output_conf = rule_conf['actions']['output']
+                output_conf = rule_conf["actions"]["output"]
                 if isinstance(output_conf, list):
                     for action in output_conf:
                         for key in action:
-                            if key == 'tunnel':
+                            if key == "tunnel":
                                 num_tunnels += 1
                 else:
-                    if 'tunnel' in output_conf:
+                    if "tunnel" in output_conf:
                         num_tunnels += 1
         return num_tunnels
 
     def get_tunnel_rules(self, tunnel_id):
         """Return the list of rules that apply a specific tunnel ID"""
         rules = []
         for rule_conf in self.rules:
             if self.does_rule_contain_tunnel(rule_conf):
-                output_conf = rule_conf['actions']['output']
+                output_conf = rule_conf["actions"]["output"]
                 if isinstance(output_conf, (list, tuple)):
                     for action in output_conf:
                         for key, value in action.items():
-                            if key == 'tunnel' and value == tunnel_id:
+                            if key == "tunnel" and value == tunnel_id:
                                 rules.append(rule_conf)
                                 continue
                 else:
-                    if output_conf['tunnel'] == tunnel_id:
+                    if output_conf["tunnel"] == tunnel_id:
                         rules.append(rule_conf)
         return rules
 
     @staticmethod
     def does_rule_contain_tunnel(rule_conf):
         """Return true if the ACL rule contains a tunnel"""
-        if 'actions' in rule_conf:
-            if 'output' in rule_conf['actions']:
-                output_conf = rule_conf['actions']['output']
+        if "actions" in rule_conf:
+            if "output" in rule_conf["actions"]:
+                output_conf = rule_conf["actions"]["output"]
                 if isinstance(output_conf, (list, tuple)):
                     for action in output_conf:
                         for key in action:
-                            if key == 'tunnel':
+                            if key == "tunnel":
                                 return True
                 else:
-                    if 'tunnel' in output_conf:
+                    if "tunnel" in output_conf:
                         return True
         return False
 
     def is_tunnel_acl(self):
         """Return true if the ACL contains a tunnel"""
         if self.tunnel_dests:
             return True
@@ -556,152 +605,198 @@
     @staticmethod
     def _tunnel_source_id(source):
         """Return ID for a tunnel source."""
         return tuple(sorted(source.items()))
 
     def add_tunnel_source(self, dp_name, port, reverse=False, bi_directional=False):
         """Add a source dp/port pair for the tunnel ACL"""
-        source = {'dp': dp_name, 'port': port, 'reverse': reverse, 'bi_directional': bi_directional}
+        source = {
+            "dp": dp_name,
+            "port": port,
+            "reverse": reverse,
+            "bi_directional": bi_directional,
+        }
         source_id = self._tunnel_source_id(source)
         self.tunnel_sources[source_id] = source
         for _id in self.tunnel_dests:
             self.dyn_tunnel_rules.setdefault(_id, {})
             self.dyn_reverse_tunnel_rules.setdefault(_id, {})
 
     def verify_tunnel_rules(self):
         """Make sure that matches & set fields are configured correctly to handle tunnels"""
-        if 'eth_type' not in self.matches:
-            self.matches['eth_type'] = False
-        if 'in_port' not in self.matches:
-            self.matches['in_port'] = False
-        if 'vlan_vid' not in self.matches:
-            self.matches['vlan_vid'] = False
-        if 'vlan_vid' not in self.set_fields:
-            self.set_fields.add('vlan_vid')
-        if 'vlan_pcp' not in self.matches:
-            self.matches['vlan_pcp'] = False
-        if 'vlan_pcp' not in self.set_fields:
-            self.set_fields.add('vlan_pcp')
-
-    def update_reverse_tunnel_rules(self, curr_dp, source_id, tunnel_id, out_port, output_table):
+        if "eth_type" not in self.matches:
+            self.matches["eth_type"] = False
+        if "in_port" not in self.matches:
+            self.matches["in_port"] = False
+        if "vlan_vid" not in self.matches:
+            self.matches["vlan_vid"] = False
+        if "vlan_vid" not in self.set_fields:
+            self.set_fields.add("vlan_vid")
+        if "vlan_pcp" not in self.matches:
+            self.matches["vlan_pcp"] = False
+        if "vlan_pcp" not in self.set_fields:
+            self.set_fields.add("vlan_pcp")
+
+    def update_reverse_tunnel_rules(
+        self, curr_dp, source_id, tunnel_id, out_port, output_table
+    ):
         """Update the tunnel rulelist for when the output port has changed (reverse direction)"""
         if not self.requires_reverse_tunnel(tunnel_id):
             return False
-        dst_dp = self.tunnel_sources[source_id]['dp']
-        src_dp = self.tunnel_dests[tunnel_id]['dst_dp']
+        dst_dp = self.tunnel_sources[source_id]["dp"]
+        src_dp = self.tunnel_dests[tunnel_id]["dst_dp"]
         prev_list = self.dyn_reverse_tunnel_rules[tunnel_id].get(source_id, [])
         new_list = []
         if curr_dp == src_dp and curr_dp != dst_dp:
             # SRC DP: vlan_vid, vlan_pcp, actions=[out_port]
             # NOTE: For the bi_directional reverse tunnel, we assume that
             #       the packet already has the required encapsulation
-            new_list = [{'port': out_port}]
+            new_list = [{"port": out_port}]
         elif curr_dp == dst_dp and curr_dp != src_dp:
             # DST DP: vlan_vid, vlan_pcp, actions=[pop_vlans, output]
-            new_list = [{'pop_vlans': 1}]
+            new_list = [{"pop_vlans": 1}]
             if out_port is None:
                 # DP dest tunnel, so we fall through into the eth_dst output table
-                new_list.append({'goto': output_table.table_id})
+                new_list.append({"goto": output_table.table_id})
             else:
                 # Tunnel has port specified, so output to destination
-                new_list.append({'port': out_port})
+                new_list.append({"port": out_port})
         elif curr_dp == src_dp and curr_dp == dst_dp:
             # SINGLE DP: actions=[pop_vlans, out_port]
-            new_list = [{'pop_vlans': 1}]
+            new_list = [{"pop_vlans": 1}]
             if out_port is None:
                 # DP dest tunnel, so we fall through into the eth_dst output table
-                new_list.extend([{'goto': output_table.table_id}])
+                new_list.extend([{"goto": output_table.table_id}])
             else:
                 # Tunnel has port specified, so output to destination
-                new_list.extend([{'port': out_port}])
+                new_list.extend([{"port": out_port}])
         else:
             # TRANSIT DP: vlan_vid, vlan_pcp, actions=[output]
-            new_list = [{'port': out_port}]
+            new_list = [{"port": out_port}]
         if new_list != prev_list:
             self.dyn_reverse_tunnel_rules[tunnel_id][source_id] = new_list
             return True
         return True
 
-    def update_source_tunnel_rules(self, curr_dp, source_id, tunnel_id, out_port, output_table):
+    def update_source_tunnel_rules(
+        self, curr_dp, source_id, tunnel_id, out_port, output_table
+    ):
         """Update the tunnel rulelist for when the output port has changed"""
-        src_dp = self.tunnel_sources[source_id]['dp']
-        dst_dp = self.tunnel_dests[tunnel_id]['dst_dp']
+        src_dp = self.tunnel_sources[source_id]["dp"]
+        dst_dp = self.tunnel_dests[tunnel_id]["dst_dp"]
         prev_list = self.dyn_tunnel_rules[tunnel_id].get(source_id, [])
         new_list = []
         pcp_flag = valve_of.PCP_TUNNEL_FLAG
-        if self.tunnel_dests[tunnel_id]['reverse']:
+        if self.tunnel_dests[tunnel_id]["reverse"]:
             pcp_flag = valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG
         if curr_dp == src_dp and curr_dp != dst_dp:
             # SRC DP: in_port, actions=[push_vlan, output, pop_vlans]
             # Ideally, we would be able to detect if the tunnel has an `allow` action clause.
             #   However, this is difficult as a single ACL can have multiple rules using the same
             #   tunnel, but with one instance requiring the `allow` clause and another, not.
             # This means it is easier to always append the `pop_vlans` in assumption that the
             #   `allow` action does exist, and then optimize/reduce the redundant rules before
             #   outputting the flowrule.
             # We also set the tunnel VLAN header with a PCP value indicating that we are in
             #   the tunnel, which will save the VLANs from being reserved.
             new_list = [
-                {'vlan_vids': [{'vid': tunnel_id, 'eth_type': ether.ETH_TYPE_8021Q}]},
-                {'set_fields': [{'vlan_pcp': pcp_flag}]},
-                {'port': out_port},
-                {'pop_vlans': 1}]
+                {"vlan_vids": [{"vid": tunnel_id, "eth_type": ether.ETH_TYPE_8021Q}]},
+                {"set_fields": [{"vlan_pcp": pcp_flag}]},
+                {"port": out_port},
+                {"pop_vlans": 1},
+            ]
         elif curr_dp == dst_dp and curr_dp != src_dp:
             # DST DP: in_port, vlan_vid, actions=[pop_vlan, additional_instructions, output]
             # If exit_instructions are applied, then we want to pop off the tunnel
             #   VLAN header, then apply the additional instructions, then output
-            if self.tunnel_dests[tunnel_id]['maintain_encapsulation']:
+            if self.tunnel_dests[tunnel_id]["maintain_encapsulation"]:
                 # We wish to maintain tunnel encapsulation before outputting
                 #   So do not add the pop_vlans rule
                 new_list = []
             else:
-                new_list = [{'pop_vlans': 1}]
-            exit_instructions = self.tunnel_dests[tunnel_id].get('exit_instructions', [])
+                new_list = [{"pop_vlans": 1}]
+            exit_instructions = self.tunnel_dests[tunnel_id].get(
+                "exit_instructions", []
+            )
             new_list.extend(copy.copy(list(exit_instructions)))
             if out_port is None:
                 # DP dest tunnel, so we fall through into the eth_dst output table
-                new_list.append({'goto': output_table.table_id})
+                new_list.append({"goto": output_table.table_id})
             else:
                 # Tunnel has port specified, so output to destination
-                new_list.append({'port': out_port})
+                new_list.append({"port": out_port})
         elif curr_dp == src_dp and curr_dp == dst_dp:
             # SINGLE DP: in_port, actions=[additional_instructions, out_port]
-            exit_instructions = self.tunnel_dests[tunnel_id].get('exit_instructions', [])
+            exit_instructions = self.tunnel_dests[tunnel_id].get(
+                "exit_instructions", []
+            )
             new_list.extend(copy.copy(list(exit_instructions)))
-            if self.tunnel_dests[tunnel_id].get('maintain_encapsulation', False):
+            if self.tunnel_dests[tunnel_id].get("maintain_encapsulation", False):
                 # Maintain encapsulation implies we want the tunnel VID on the packet,
                 #   so ensure it is purposefully put onto the packet, even when
                 #   there would originally be no need to push on a tunnel VID
-                new_list.extend([
-                    {'vlan_vids': [{'vid': tunnel_id, 'eth_type': ether.ETH_TYPE_8021Q}]},
-                    {'set_fields': [{'vlan_pcp': pcp_flag}]}])
+                new_list.extend(
+                    [
+                        {
+                            "vlan_vids": [
+                                {"vid": tunnel_id, "eth_type": ether.ETH_TYPE_8021Q}
+                            ]
+                        },
+                        {"set_fields": [{"vlan_pcp": pcp_flag}]},
+                    ]
+                )
             if out_port is None:
                 # DP dest tunnel, so we fall through into the eth_dst output table
-                new_list.extend([{'goto': output_table.table_id}])
+                new_list.extend([{"goto": output_table.table_id}])
             else:
                 # Tunnel has port specified, so output to destination
-                new_list.extend([{'port': out_port}])
+                new_list.extend([{"port": out_port}])
         else:
             # TRANSIT DP: in_port, vlan_vid, actions=[output]
-            new_list = [{'port': out_port}]
+            new_list = [{"port": out_port}]
         if new_list != prev_list:
             self.dyn_tunnel_rules[tunnel_id][source_id] = new_list
             return True
         return True
 
 
 # NOTE: 802.1x steals the port ACL table.
 PORT_ACL_8021X = ACL(
-    'port_acl_8021x', 0,
-    {'rules': [
-        {'eth_type': 1, 'eth_src': '01:02:03:04:05:06', 'actions': {'output': {
-            'port': valve_of.ofp.OFPP_LOCAL, 'set_fields': [
-                {'eth_src': '01:02:03:04:05:06'}, {'eth_dst': '01:02:03:04:05:06'}]}}}]})
+    "port_acl_8021x",
+    0,
+    {
+        "rules": [
+            {
+                "eth_type": 1,
+                "eth_src": "01:02:03:04:05:06",
+                "actions": {
+                    "output": {
+                        "port": valve_of.ofp.OFPP_LOCAL,
+                        "set_fields": [
+                            {"eth_src": "01:02:03:04:05:06"},
+                            {"eth_dst": "01:02:03:04:05:06"},
+                        ],
+                    }
+                },
+            }
+        ]
+    },
+)
 PORT_ACL_8021X.build({}, None, 1)
 
 MAB_ACL_8021X = ACL(
-    'mab_acl_8021x', 0,
-    {'rules': [{
-        'eth_type': valve_of.ether.ETH_TYPE_IP, 'eth_src': '01:02:03:04:05:06',
-        'ip_proto': valve_of.inet.IPPROTO_UDP, 'udp_src': 68, 'udp_dst': 67,
-        'actions': {'output': {'port': valve_of.ofp.OFPP_LOCAL}}}]})
+    "mab_acl_8021x",
+    0,
+    {
+        "rules": [
+            {
+                "eth_type": valve_of.ether.ETH_TYPE_IP,
+                "eth_src": "01:02:03:04:05:06",
+                "ip_proto": valve_of.inet.IPPROTO_UDP,
+                "udp_src": 68,
+                "udp_dst": 67,
+                "actions": {"output": {"port": valve_of.ofp.OFPP_LOCAL}},
+            }
+        ]
+    },
+)
 MAB_ACL_8021X.build({}, None, 1)
```

### Comparing `c65faucet-1.0.46/faucet/check_faucet_config.py` & `c65faucet-1.0.47/faucet/check_faucet_config.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/conf.py` & `c65faucet-1.0.47/faucet/conf.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/config_parser.py` & `c65faucet-1.0.47/faucet/config_parser.py`

 * *Files 9% similar despite different names*

```diff
@@ -28,118 +28,136 @@
 from faucet.meter import Meter
 from faucet.port import Port
 from faucet.router import Router
 from faucet.vlan import VLAN
 from faucet.watcher_conf import WatcherConf
 
 
-V2_TOP_CONFS = (
-    'acls',
-    'dps',
-    'meters',
-    'routers',
-    'vlans')
+V2_TOP_CONFS = ("acls", "dps", "meters", "routers", "vlans")
 
 
 def dp_parser(config_file, logname, meta_dp_state=None):
     """Parse a config file into DP configuration objects with hashes of config include/files."""
     conf, _ = config_parser_util.read_config(config_file, logname)
     config_hashes = None
     dps = None
 
-    test_config_condition(conf is None, 'Config file is empty')
+    test_config_condition(conf is None, "Config file is empty")
     test_config_condition(
-        not isinstance(conf, dict),
-        'Config file does not have valid syntax')
-    version = conf.pop('version', 2)
-    test_config_condition(version != 2, 'Only config version 2 is supported')
+        not isinstance(conf, dict), "Config file does not have valid syntax"
+    )
+    version = conf.pop("version", 2)
+    test_config_condition(version != 2, "Only config version 2 is supported")
     config_hashes, config_contents, dps, top_conf = _config_parser_v2(
-        config_file, logname, meta_dp_state)
-    test_config_condition(dps is None, 'no DPs are not defined')
+        config_file, logname, meta_dp_state
+    )
+    test_config_condition(dps is None, "no DPs are not defined")
 
     return config_hashes, config_contents, dps, top_conf
 
 
 def _get_vlan_by_key(dp_id, vlan_key, vlans):
     try:
         if vlan_key in vlans:
             return vlans[vlan_key]
     except TypeError as err:
         raise InvalidConfigError(err) from err
     for vlan in vlans.values():
         if vlan_key == vlan.vid:
             return vlan
-    test_config_condition(not isinstance(vlan_key, int), (
-        f'Implicitly created VLAN {vlan_key} must be an int (not {type(vlan_key)})'))
+    test_config_condition(
+        not isinstance(vlan_key, int),
+        (
+            "Implicitly created VLAN %s must be an int (not %s)"
+            % (vlan_key, type(vlan_key))
+        ),
+    )
     # Create VLAN with VID, if not defined.
     return vlans.setdefault(vlan_key, VLAN(vlan_key, dp_id))
 
 
 def _dp_parse_port(dp_id, port_key, port_conf, vlans):
-
     def _dp_parse_native_port_vlan():
         if port.native_vlan is not None:
             vlan = _get_vlan_by_key(dp_id, port.native_vlan, vlans)
             port.native_vlan = vlan
 
     def _dp_parse_tagged_port_vlans():
         if port.tagged_vlans:
             port_tagged_vlans = [
-                _get_vlan_by_key(dp_id, vlan_key, vlans) for vlan_key in port.tagged_vlans]
+                _get_vlan_by_key(dp_id, vlan_key, vlans)
+                for vlan_key in port.tagged_vlans
+            ]
             port.tagged_vlans = port_tagged_vlans
 
     port = Port(port_key, dp_id, port_conf)
-    test_config_condition(str(port_key) not in (str(port.number), port.name), (
-        f'Port key {port_key} match port name or port number'))
+    test_config_condition(
+        str(port_key) not in (str(port.number), port.name),
+        ("Port key %s match port name or port number" % port_key),
+    )
     _dp_parse_native_port_vlan()
     _dp_parse_tagged_port_vlans()
     return port
 
 
 def _dp_add_ports(dp, dp_conf, dp_id, vlans):
-    ports_conf = dp_conf.get('interfaces', {})
-    port_ranges_conf = dp_conf.get('interface_ranges', {})
+    ports_conf = dp_conf.get("interfaces", {})
+    port_ranges_conf = dp_conf.get("interface_ranges", {})
     # as users can config port VLAN by using VLAN name, we store vid in
     # Port instance instead of VLAN name for data consistency
-    test_config_condition(not isinstance(ports_conf, dict), (
-        'Invalid syntax in interface config'))
-    test_config_condition(not isinstance(port_ranges_conf, dict), (
-        'Invalid syntax in interface ranges config'))
+    test_config_condition(
+        not isinstance(ports_conf, dict), ("Invalid syntax in interface config")
+    )
+    test_config_condition(
+        not isinstance(port_ranges_conf, dict),
+        ("Invalid syntax in interface ranges config"),
+    )
 
     def _map_port_num_to_port(ports_conf):
         port_num_to_port_conf = {}
         for port_key, port_conf in ports_conf.items():
-            test_config_condition(not isinstance(port_conf, dict), 'Invalid syntax in port config')
-            port_num = port_conf.get('number', port_key)
+            test_config_condition(
+                not isinstance(port_conf, dict), "Invalid syntax in port config"
+            )
+            port_num = port_conf.get("number", port_key)
             try:
                 port_num_to_port_conf[port_num] = (port_key, port_conf)
             except TypeError as type_error:
-                raise InvalidConfigError('Invalid syntax in port config') from type_error
+                raise InvalidConfigError(
+                    "Invalid syntax in port config"
+                ) from type_error
         return port_num_to_port_conf
 
     def _parse_port_ranges(port_ranges_conf, port_num_to_port_conf):
         all_port_nums = set()
         for port_range, port_conf in port_ranges_conf.items():
             # port range format: 1-6 OR 1-6,8-9 OR 1-3,5,7-9
-            test_config_condition(not isinstance(port_conf, dict), 'Invalid syntax in port config')
+            test_config_condition(
+                not isinstance(port_conf, dict), "Invalid syntax in port config"
+            )
             port_nums = set()
-            if 'number' in port_conf:
-                del port_conf['number']
-            for range_ in re.findall(r'(\d+-\d+)', str(port_range)):
-                start_num, end_num = [int(num) for num in range_.split('-')]
-                test_config_condition(start_num >= end_num, (
-                    f'Incorrect port range ({start_num} - {end_num})'))
+            if "number" in port_conf:
+                del port_conf["number"]
+            for range_ in re.findall(r"(\d+-\d+)", str(port_range)):
+                start_num, end_num = [int(num) for num in range_.split("-")]
+                test_config_condition(
+                    start_num >= end_num,
+                    ("Incorrect port range (%d - %d)" % (start_num, end_num)),
+                )
                 port_nums.update(range(start_num, end_num + 1))
-                port_range = re.sub(range_, '', port_range)
-            other_nums = [int(p) for p in re.findall(r'\d+', str(port_range))]
+                port_range = re.sub(range_, "", port_range)
+            other_nums = [int(p) for p in re.findall(r"\d+", str(port_range))]
             port_nums.update(other_nums)
             test_config_condition(
-                not port_nums, 'interface-ranges contain invalid config')
+                not port_nums, "interface-ranges contain invalid config"
+            )
             test_config_condition(
-                port_nums.intersection(all_port_nums), 'interfaces-ranges cannot overlap')
+                port_nums.intersection(all_port_nums),
+                "interfaces-ranges cannot overlap",
+            )
             all_port_nums.update(port_nums)
             for port_num in port_nums:
                 if port_num in port_num_to_port_conf:
                     # port range config has lower priority than individual port config
                     for attr, value in port_conf.items():
                         port_num_to_port_conf[port_num][1].setdefault(attr, value)
                 else:
@@ -168,48 +186,56 @@
 def _parse_meters(dp, meters_conf):
     for meter_key, meter_conf in meters_conf.items():
         meter = Meter(meter_key, dp.dp_id, meter_conf)
         dp.meters[meter_key] = meter
 
 
 def _parse_dp(dp_key, dp_conf, acls_conf, meters_conf, routers_conf, vlans_conf):
-    test_config_condition(not isinstance(dp_conf, dict), 'DP config must be dict')
-    dp = DP(dp_key, dp_conf.get('dp_id', None), dp_conf)
-    test_config_condition(dp.name != dp_key, (
-        f'DP key {dp_key} and DP name must match'))
+    test_config_condition(not isinstance(dp_conf, dict), "DP config must be dict")
+    dp = DP(dp_key, dp_conf.get("dp_id", None), dp_conf)
+    test_config_condition(
+        dp.name != dp_key, ("DP key %s and DP name must match" % dp_key)
+    )
     vlans = {}
     vids = set()
     for vlan_key, vlan_conf in vlans_conf.items():
         vlan = VLAN(vlan_key, dp.dp_id, vlan_conf)
-        test_config_condition(str(vlan_key) not in (str(vlan.vid), vlan.name), (
-            f'VLAN {vlan_key} key must match VLAN name or VLAN VID'))
-        test_config_condition(not isinstance(vlan_key, (str, int)), (
-            f'VLAN {vlan_key} key must not be type {type(vlan_key)}'))
-        test_config_condition(vlan.vid in vids, (
-            f'VLAN VID {vlan.vid} multiply configured'))
+        test_config_condition(
+            str(vlan_key) not in (str(vlan.vid), vlan.name),
+            ("VLAN %s key must match VLAN name or VLAN VID" % vlan_key),
+        )
+        test_config_condition(
+            not isinstance(vlan_key, (str, int)),
+            ("VLAN %s key must not be type %s" % (vlan_key, type(vlan_key))),
+        )
+        test_config_condition(
+            vlan.vid in vids, ("VLAN VID %u multiply configured" % vlan.vid)
+        )
         vlans[vlan_key] = vlan
         vids.add(vlan.vid)
     _parse_acls(dp, acls_conf)
     _parse_routers(dp, routers_conf)
     _parse_meters(dp, meters_conf)
     _dp_add_ports(dp, dp_conf, dp.dp_id, vlans)
     return (dp, vlans)
 
 
-def _dp_parser_v2(dps_conf, acls_conf, meters_conf,
-                  routers_conf, vlans_conf, meta_dp_state):
+def _dp_parser_v2(
+    dps_conf, acls_conf, meters_conf, routers_conf, vlans_conf, meta_dp_state
+):
     # pylint: disable=invalid-name
     dp_vlans = []
     for dp_key, dp_conf in dps_conf.items():
         try:
             dp, vlans = _parse_dp(
-                dp_key, dp_conf, acls_conf, meters_conf, routers_conf, vlans_conf)
+                dp_key, dp_conf, acls_conf, meters_conf, routers_conf, vlans_conf
+            )
             dp_vlans.append((dp, vlans))
         except InvalidConfigError as err:
-            raise InvalidConfigError(f'DP {dp_key}: {err}') from err
+            raise InvalidConfigError("DP %s: %s" % (dp_key, err)) from err
 
     # Some VLANs are created implicitly just by referencing them in tagged/native,
     # so we must make them available to all DPs.
     implicit_vids = set()
     for dp, vlans in dp_vlans:
         implicit_vids.update(set(vlans.keys()) - set(vlans_conf.keys()))
     dps = []
@@ -225,121 +251,129 @@
     for dp in dps:
         dp.resolve_stack_topology(dps, meta_dp_state)
     for dp in dps:
         dp.finalize()
 
     dpid_refs = set()
     for dp in dps:
-        test_config_condition(dp.dp_id in dpid_refs, (
-            f'DPID {dp.dp_id} is duplicated'))
+        test_config_condition(
+            dp.dp_id in dpid_refs, ("DPID %u is duplicated" % dp.dp_id)
+        )
         dpid_refs.add(dp.dp_id)
 
     routers_referenced = set()
     for dp in dps:
         routers_referenced.update(dp.routers.keys())
     for router in routers_conf:
-        test_config_condition(router not in routers_referenced, (
-            f'router {router} configured but not used by any DP'))
+        test_config_condition(
+            router not in routers_referenced,
+            ("router %s configured but not used by any DP" % router),
+        )
 
     return dps
 
 
 def dp_preparsed_parser(top_confs, meta_dp_state):
     """Parse a preparsed (after include files have been applied) FAUCET config."""
     local_top_confs = copy.deepcopy(top_confs)
     return _dp_parser_v2(
-        local_top_confs.get('dps', {}),
-        local_top_confs.get('acls', {}),
-        local_top_confs.get('meters', {}),
-        local_top_confs.get('routers', {}),
-        local_top_confs.get('vlans', {}),
-        meta_dp_state)
+        local_top_confs.get("dps", {}),
+        local_top_confs.get("acls", {}),
+        local_top_confs.get("meters", {}),
+        local_top_confs.get("routers", {}),
+        local_top_confs.get("vlans", {}),
+        meta_dp_state,
+    )
 
 
 def _config_parser_v2(config_file, logname, meta_dp_state):
     config_path = config_parser_util.dp_config_path(config_file)
     top_confs = {top_conf: {} for top_conf in V2_TOP_CONFS}
     config_hashes = {}
     config_contents = {}
     dps = None
 
     if not config_parser_util.dp_include(
-            config_hashes, config_contents, config_path, logname, top_confs):
-        raise InvalidConfigError(f'Error found while loading config file: {config_path}')
+        config_hashes, config_contents, config_path, logname, top_confs
+    ):
+        raise InvalidConfigError(
+            "Error found while loading config file: %s" % config_path
+        )
 
-    if not top_confs['dps']:
-        raise InvalidConfigError(f'DPs not configured in file: {config_path}')
+    if not top_confs["dps"]:
+        raise InvalidConfigError("DPs not configured in file: %s" % config_path)
 
     dps = dp_preparsed_parser(top_confs, meta_dp_state)
     return (config_hashes, config_contents, dps, top_confs)
 
 
 def watcher_parser(config_file, logname, prom_client):
     """Return Watcher instances from config."""
     conf, _ = config_parser_util.read_config(config_file, logname)
     conf_hash = config_parser_util.config_file_hash(config_file)
     faucet_config_files, faucet_conf_hashes, result = _watcher_parser_v2(
-        conf, logname, prom_client)
+        conf, logname, prom_client
+    )
     return conf_hash, faucet_config_files, faucet_conf_hashes, result
 
 
 def _parse_dps_for_watchers(conf, logname, meta_dp_state=None):
     all_dps_list = []
     faucet_conf_hashes = {}
 
     if not isinstance(conf, dict):
-        raise InvalidConfigError('Gauge config not valid')
+        raise InvalidConfigError("Gauge config not valid")
 
-    faucet_config_files = conf.get('faucet_configs', [])
+    faucet_config_files = conf.get("faucet_configs", [])
     for faucet_config_file in faucet_config_files:
         conf_hashes, _, dp_list, _ = dp_parser(faucet_config_file, logname)
         if dp_list:
             faucet_conf_hashes[faucet_config_file] = conf_hashes
             all_dps_list.extend(dp_list)
 
-    faucet_config = conf.get('faucet', None)
+    faucet_config = conf.get("faucet", None)
     if faucet_config:
         all_dps_list.extend(dp_preparsed_parser(faucet_config, meta_dp_state))
 
     dps = {dp.name: dp for dp in all_dps_list}
     if not dps:
-        raise InvalidConfigError(
-            'Gauge configured without any FAUCET configuration')
+        raise InvalidConfigError("Gauge configured without any FAUCET configuration")
     return faucet_config_files, faucet_conf_hashes, dps
 
 
 def _watcher_parser_v2(conf, logname, prom_client):
     logger = config_parser_util.get_logger(logname)
 
     if conf is None:
         conf = {}
     faucet_config_files, faucet_conf_hashes, dps = _parse_dps_for_watchers(
-        conf, logname)
-    dbs = conf.pop('dbs')
+        conf, logname
+    )
+    dbs = conf.pop("dbs")
 
     result = []
-    for watcher_name, watcher_conf in conf['watchers'].items():
-        if watcher_conf.get('all_dps', False):
+    for watcher_name, watcher_conf in conf["watchers"].items():
+        if watcher_conf.get("all_dps", False):
             watcher_dps = dps.keys()
         else:
-            watcher_dps = watcher_conf['dps']
+            watcher_dps = watcher_conf["dps"]
         # Watcher config has a list of DPs, but actually a WatcherConf is
         # created for each DP.
         # TODO: refactor watcher_conf as a container.
         for dp_name in watcher_dps:
             if dp_name not in dps:
-                logger.error(f'DP {dp_name} in Gauge but not configured in FAUCET')
+                logger.error("DP %s in Gauge but not configured in FAUCET", dp_name)
                 continue
             dp = dps[dp_name]
-            if 'dbs' in watcher_conf:
-                watcher_dbs = watcher_conf['dbs']
-            elif 'db' in watcher_conf:
-                watcher_dbs = [watcher_conf['db']]
+            if "dbs" in watcher_conf:
+                watcher_dbs = watcher_conf["dbs"]
+            elif "db" in watcher_conf:
+                watcher_dbs = [watcher_conf["db"]]
             else:
-                raise InvalidConfigError('Watcher configured without DB')
+                raise InvalidConfigError("Watcher configured without DB")
             for db in watcher_dbs:
                 watcher = WatcherConf(watcher_name, dp.dp_id, watcher_conf, prom_client)
                 watcher.add_db(dbs[db])
                 watcher.add_dp(dp)
                 result.append(watcher)
 
     return faucet_config_files, faucet_conf_hashes, result
```

### Comparing `c65faucet-1.0.46/faucet/config_parser_util.py` & `c65faucet-1.0.47/faucet/config_parser_util.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,90 +23,105 @@
 from ruamel.yaml import YAML
 from ruamel.yaml.constructor import DuplicateKeyError
 from ruamel.yaml.scanner import ScannerError
 from ruamel.yaml.composer import ComposerError
 from ruamel.yaml.constructor import ConstructorError
 from ruamel.yaml.parser import ParserError
 
-CONFIG_HASH_FUNC = 'sha256'
+CONFIG_HASH_FUNC = "sha256"
 
 
 def yaml_load(yaml_str):
     """Wrap YAML load library."""
-    yml = YAML(typ='safe')
+    yml = YAML(typ="safe")
     return yml.load(yaml_str)
 
 
 def yaml_dump(yaml_dict):
     """Wrap YAML dump library."""
     with StringIO() as stream:
-        yml = YAML(typ='safe')
+        yml = YAML(typ="safe")
         yml.dump(yaml_dict, stream=stream)
         return stream.getvalue()
 
 
 def get_logger(logname):
     """Return logger instance for config parsing."""
-    return logging.getLogger(logname + '.config')
+    return logging.getLogger(logname + ".config")
 
 
 def read_config(config_file, logname):
     """Return a parsed YAML config file or None."""
     logger = get_logger(logname)
     conf_txt = None
     conf = None
 
     try:
-        with open(config_file, 'r', encoding='utf-8') as stream:
+        with open(config_file, "r", encoding="utf-8") as stream:
             conf_txt = stream.read()
         conf = yaml_load(conf_txt)
-    except (TypeError, UnicodeDecodeError, PermissionError, ValueError,
-            ScannerError, DuplicateKeyError, ComposerError,
-            ConstructorError, ParserError) as err:  # pytype: disable=name-error
-        logger.error('Error in file %s (%s)', config_file, str(err))
+    except (
+        TypeError,
+        UnicodeDecodeError,
+        PermissionError,
+        ValueError,
+        ScannerError,
+        DuplicateKeyError,
+        ComposerError,
+        ConstructorError,
+        ParserError,
+    ) as err:  # pytype: disable=name-error
+        logger.error("Error in file %s (%s)", config_file, str(err))
     except FileNotFoundError as err:  # pytype: disable=name-error
-        logger.error('Could not find requested file: %s (%s)', config_file, str(err))
+        logger.error("Could not find requested file: %s (%s)", config_file, str(err))
     return conf, conf_txt
 
 
 def config_hash_content(content):
     """Return hash of config file content."""
     config_hash = getattr(hashlib, CONFIG_HASH_FUNC)
-    return config_hash(content.encode('utf-8')).hexdigest()
+    return config_hash(content.encode("utf-8")).hexdigest()
 
 
 def config_file_hash(config_file_name):
     """Return hash of YAML config file contents."""
-    with open(config_file_name, encoding='utf-8') as config_file:
+    with open(config_file_name, encoding="utf-8") as config_file:
         return config_hash_content(config_file.read())
 
 
 def dp_config_path(config_file, parent_file=None):
     """Return full path to config file."""
     if parent_file and not os.path.isabs(config_file):
         return os.path.realpath(os.path.join(os.path.dirname(parent_file), config_file))
     return os.path.realpath(config_file)
 
 
-def dp_include(config_hashes, config_contents, config_file, logname,  # pylint: disable=too-many-locals
-               top_confs):
+def dp_include(
+    config_hashes,
+    config_contents,
+    config_file,
+    logname,  # pylint: disable=too-many-locals
+    top_confs,
+):
     """Handles including additional config files"""
     logger = get_logger(logname)
     if not os.path.isfile(config_file):
-        logger.warning('not a regular file or does not exist: %s', config_file)
+        logger.warning("not a regular file or does not exist: %s", config_file)
         return False
     conf, config_content = read_config(config_file, logname)
     if not conf:
-        logger.warning('error loading config from file: %s', config_file)
+        logger.warning("error loading config from file: %s", config_file)
         return False
 
-    valid_conf_keys = set(top_confs.keys()).union({'include', 'include-optional', 'version'})
+    valid_conf_keys = set(top_confs.keys()).union(
+        {"include", "include-optional", "version"}
+    )
     unknown_top_confs = set(conf.keys()) - valid_conf_keys
     if unknown_top_confs:
-        logger.error('unknown top level config items: %s', unknown_top_confs)
+        logger.error("unknown top level config items: %s", unknown_top_confs)
         return False
 
     # Add the SHA256 hash for this configuration file, so FAUCET can determine
     # whether or not this configuration file should be reloaded upon receiving
     # a HUP signal.
     new_config_hashes = config_hashes.copy()
     new_config_hashes[config_file] = config_hash_content(config_content)
@@ -121,38 +136,44 @@
         try:
             new_top_confs[conf_name].update(conf.pop(conf_name, {}))
         except (TypeError, ValueError):
             logger.error('Invalid config for "%s"', conf_name)
             return False
 
     for include_directive, file_required in (
-            ('include', True),
-            ('include-optional', False)):
+        ("include", True),
+        ("include-optional", False),
+    ):
         include_values = conf.pop(include_directive, [])
         if not isinstance(include_values, list):
-            logger.error('Include directive is not in a valid format')
+            logger.error("Include directive is not in a valid format")
             return False
         for include_file in include_values:
             if not isinstance(include_file, str):
                 include_file = str(include_file)
 
             include_path = dp_config_path(include_file, parent_file=config_file)
-            logger.info('including file: %s', include_path)
+            logger.info("including file: %s", include_path)
             if include_path in config_hashes:
                 logger.error(
-                    'include file %s already loaded, include loop found in file: %s',
-                    include_path, config_file,)
+                    "include file %s already loaded, include loop found in file: %s",
+                    include_path,
+                    config_file,
+                )
                 return False
             if not dp_include(
-                    new_config_hashes, config_contents, include_path, logname, new_top_confs):
+                new_config_hashes, config_contents, include_path, logname, new_top_confs
+            ):
                 if file_required:
-                    logger.error('unable to load required include file: %s', include_path)
+                    logger.error(
+                        "unable to load required include file: %s", include_path
+                    )
                     return False
                 new_config_hashes[include_path] = None
-                logger.warning('skipping optional include file: %s', include_path)
+                logger.warning("skipping optional include file: %s", include_path)
 
     # Actually update the configuration data structures,
     # now that this file has been successfully loaded.
     config_hashes.update(new_config_hashes)
     config_contents.update(new_config_contents)
     for conf_name, new_conf in new_top_confs.items():
         top_confs[conf_name].update(new_conf)
```

### Comparing `c65faucet-1.0.46/faucet/dp.py` & `c65faucet-1.0.47/faucet/dp.py`

 * *Files 12% similar despite different names*

```diff
@@ -40,207 +40,207 @@
 
 # Documentation generated using documentation_generator.py
 # For attributues to be included in documentation they must
 # have a default value, and their descriptor must come
 # immediately after being set. See below for example.
 class DP(Conf):
     """Stores state related to a datapath controlled by Faucet, including
-configuration.
-"""
+    configuration."""
+
     DEFAULT_LLDP_SEND_INTERVAL = 5
     DEFAULT_LLDP_MAX_PER_INTERVAL = 5
-    mutable_attrs = frozenset(['vlans'])
+    mutable_attrs = frozenset(["vlans"])
 
     # Values that are set to None will be set using set_defaults
     # they are included here for testing and informational purposes
     defaults = {
-        'dp_id': None,
+        "dp_id": None,
         # Name for this dp, used for stats reporting and configuration
-        'name': None,
-        'interfaces': {},
-        'interface_ranges': {},
+        "name": None,
+        "interfaces": {},
+        "interface_ranges": {},
         # How much to offset default priority by
-        'priority_offset': 0,
+        "priority_offset": 0,
         # Some priority values
-        'lowest_priority': None,
-        'low_priority': None,
-        'high_priority': None,
-        'highest_priority': None,
-        'cookie': 1524372928,
+        "lowest_priority": None,
+        "low_priority": None,
+        "high_priority": None,
+        "highest_priority": None,
+        "cookie": 1524372928,
         # Identification cookie value to allow for multiple controllers to control the same datapath
-        'timeout': 300,
+        "timeout": 300,
         # inactive MAC timeout
-        'description': None,
+        "description": None,
         # description, strictly informational
-        'hardware': 'Open vSwitch',
+        "hardware": "Open vSwitch",
         # The hardware maker (for chosing an openflow driver)
-        'arp_neighbor_timeout': 30,
+        "arp_neighbor_timeout": 30,
         # ARP neighbor timeout (seconds)
-        'nd_neighbor_timeout': 30,
+        "nd_neighbor_timeout": 30,
         # IPv6 ND neighbor timeout (seconds)
-        'ofchannel_log': None,
+        "ofchannel_log": None,
         # OF channel log
-        'stack': None,
+        "stack": None,
         # stacking config, when cross connecting multiple DPs
-        'ignore_learn_ins': 10,
+        "ignore_learn_ins": 10,
         # Ignore every approx nth packet for learning.
         # 2 will ignore 1 out of 2 packets; 3 will ignore 1 out of 3 packets.
         # This limits control plane activity when learning new hosts rapidly.
         # Flooding will still be done by the dataplane even with a packet
         # is ignored for learning purposes.
-        'drop_broadcast_source_address': True,
+        "drop_broadcast_source_address": True,
         # By default drop packets with a broadcast source address
-        'drop_spoofed_faucet_mac': True,
+        "drop_spoofed_faucet_mac": True,
         # By default drop packets on datapath spoofing the FAUCET_MAC
-        'group_table': False,
+        "group_table": False,
         # Use GROUP tables for VLAN flooding
-        'max_hosts_per_resolve_cycle': 5,
+        "max_hosts_per_resolve_cycle": 5,
         # Max hosts to try to resolve per gateway resolution cycle.
-        'max_host_fib_retry_count': 10,
+        "max_host_fib_retry_count": 10,
         # Max number of times to retry resolution of a host FIB route.
-        'max_resolve_backoff_time': 64,
+        "max_resolve_backoff_time": 64,
         # Max number of seconds to back off to when resolving nexthops.
-        'packetin_pps': None,
+        "packetin_pps": None,
         # Ask switch to rate limit packetin pps. TODO: Not supported by OVS in 2.7.0
-        'slowpath_pps': None,
+        "slowpath_pps": None,
         # Ask switch to rate limit slowpath pps. TODO: Not supported by OVS in 2.7.0
-        'learn_jitter': 0,
+        "learn_jitter": 0,
         # Jitter learn timeouts by up to this many seconds
-        'learn_ban_timeout': 0,
+        "learn_ban_timeout": 0,
         # When banning/limiting learning, wait this many seconds before learning can be retried
-        'advertise_interval': 30,
+        "advertise_interval": 30,
         # How often to slow advertise (eg. IPv6 RAs)
-        'fast_advertise_interval': 5,
+        "fast_advertise_interval": 5,
         # How often to fast advertise (eg. LACP)
-        'proactive_learn_v4': True,
+        "proactive_learn_v4": True,
         # whether proactive learning is enabled for IPv4 nexthops
-        'proactive_learn_v6': True,
+        "proactive_learn_v6": True,
         # whether proactive learning is enabled for IPv6 nexthops
-        'use_idle_timeout': False,
+        "use_idle_timeout": False,
         # Turn on/off the use of idle timeout for src_table, default OFF.
-        'lldp_beacon': {},
+        "lldp_beacon": {},
         # Config for LLDP beacon service.
-        'metrics_rate_limit_sec': 0,
+        "metrics_rate_limit_sec": 0,
         # Rate limit metric updates if last update was less than this many seconds ago.
-        'faucet_dp_mac': valve_packet.FAUCET_MAC,
+        "faucet_dp_mac": valve_packet.FAUCET_MAC,
         # MAC address of packets sent by FAUCET, not associated with any VLAN.
-        'combinatorial_port_flood': False,
+        "combinatorial_port_flood": False,
         # if True, use a seperate output flow for each input port on this VLAN.
-        'lacp_timeout': 30,
+        "lacp_timeout": 30,
         # Number of seconds without a LACP message when we consider a LACP group down.
-        'dp_acls': None,
+        "dp_acls": None,
         # List of dataplane ACLs (overriding per port ACLs).
-        'dot1x': {},
+        "dot1x": {},
         # Experimental dot1x configuration.
-        'table_sizes': {},
+        "table_sizes": {},
         # Table sizes for TFM switches.
-        'min_wildcard_table_size': 32,
+        "min_wildcard_table_size": 32,
         # Minimum table size for wildcard tables.
-        'max_wildcard_table_size': 1024 + 256,
+        "max_wildcard_table_size": 1024 + 256,
         # Maximum table size for wildcard tables.
-        'port_table_scale_factor': 1.0,
+        "port_table_scale_factor": 1.0,
         # Amount to scale port scaled table sizes by.
-        'global_vlan': 0,
+        "global_vlan": 0,
         # Reserved VID for internal global router VLAN.
-        'cache_update_guard_time': 0,
+        "cache_update_guard_time": 0,
         # Don't update L2 cache if port didn't change within this many seconds (default timeout/2).
-        'use_classification': False,
+        "use_classification": False,
         # Don't update L2 cache if port didn't change within this many seconds.
-        'egress_pipeline': False,
+        "egress_pipeline": False,
         # Experimental inclusion of an egress pipeline
-        'strict_packet_in_cookie': True,
+        "strict_packet_in_cookie": True,
         # Apply strict packet in checking to all packet ins.
-        'multi_out': True,
+        "multi_out": True,
         # Have OFA copy packet outs to multiple ports.
-        'idle_dst': True,
+        "idle_dst": True,
         # If False, workaround for flow idle timer not reset on flow refresh.
     }
 
     defaults_types = {
-        'dp_id': int,
-        'name': str,
-        'interfaces': dict,
-        'interface_ranges': dict,
-        'priority_offset': int,
-        'lowest_priority': int,
-        'low_priority': int,
-        'high_priority': int,
-        'highest_priority': int,
-        'cookie': int,
-        'timeout': int,
-        'description': str,
-        'hardware': str,
-        'arp_neighbor_timeout': int,
-        'nd_neighbor_timeout': int,
-        'ofchannel_log': str,
-        'stack': dict,
-        'ignore_learn_ins': int,
-        'drop_broadcast_source_address': bool,
-        'drop_spoofed_faucet_mac': bool,
-        'group_table': bool,
-        'max_hosts_per_resolve_cycle': int,
-        'max_host_fib_retry_count': int,
-        'max_resolve_backoff_time': int,
-        'packetin_pps': int,
-        'slowpath_pps': int,
-        'learn_jitter': int,
-        'learn_ban_timeout': int,
-        'advertise_interval': int,
-        'fast_advertise_interval': int,
-        'proactive_learn_v4': bool,
-        'proactive_learn_v6': bool,
-        'use_idle_timeout': bool,
-        'lldp_beacon': dict,
-        'metrics_rate_limit_sec': int,
-        'faucet_dp_mac': str,
-        'combinatorial_port_flood': bool,
-        'dp_acls': list,
-        'dot1x': dict,
-        'table_sizes': dict,
-        'min_wildcard_table_size': int,
-        'max_wildcard_table_size': int,
-        'port_table_scale_factor': float,
-        'global_vlan': int,
-        'cache_update_guard_time': int,
-        'use_classification': bool,
-        'egress_pipeline': bool,
-        'strict_packet_in_cookie': bool,
-        'multi_out': bool,
-        'lacp_timeout': int,
-        'idle_dst': bool,
+        "dp_id": int,
+        "name": str,
+        "interfaces": dict,
+        "interface_ranges": dict,
+        "priority_offset": int,
+        "lowest_priority": int,
+        "low_priority": int,
+        "high_priority": int,
+        "highest_priority": int,
+        "cookie": int,
+        "timeout": int,
+        "description": str,
+        "hardware": str,
+        "arp_neighbor_timeout": int,
+        "nd_neighbor_timeout": int,
+        "ofchannel_log": str,
+        "stack": dict,
+        "ignore_learn_ins": int,
+        "drop_broadcast_source_address": bool,
+        "drop_spoofed_faucet_mac": bool,
+        "group_table": bool,
+        "max_hosts_per_resolve_cycle": int,
+        "max_host_fib_retry_count": int,
+        "max_resolve_backoff_time": int,
+        "packetin_pps": int,
+        "slowpath_pps": int,
+        "learn_jitter": int,
+        "learn_ban_timeout": int,
+        "advertise_interval": int,
+        "fast_advertise_interval": int,
+        "proactive_learn_v4": bool,
+        "proactive_learn_v6": bool,
+        "use_idle_timeout": bool,
+        "lldp_beacon": dict,
+        "metrics_rate_limit_sec": int,
+        "faucet_dp_mac": str,
+        "combinatorial_port_flood": bool,
+        "dp_acls": list,
+        "dot1x": dict,
+        "table_sizes": dict,
+        "min_wildcard_table_size": int,
+        "max_wildcard_table_size": int,
+        "port_table_scale_factor": float,
+        "global_vlan": int,
+        "cache_update_guard_time": int,
+        "use_classification": bool,
+        "egress_pipeline": bool,
+        "strict_packet_in_cookie": bool,
+        "multi_out": bool,
+        "lacp_timeout": int,
+        "idle_dst": bool,
     }
 
     default_table_sizes_types = {
-        'port_acl': int,
-        'vlan': int,
-        'vlan_acl': int,
-        'classification': int,
-        'eth_src': int,
-        'ipv4_fib': int,
-        'ipv6_fib': int,
-        'vip': int,
-        'eth_dst_hairpin': int,
-        'eth_dst': int,
-        'flood': int,
+        "port_acl": int,
+        "vlan": int,
+        "vlan_acl": int,
+        "classification": int,
+        "eth_src": int,
+        "ipv4_fib": int,
+        "ipv6_fib": int,
+        "vip": int,
+        "eth_dst_hairpin": int,
+        "eth_dst": int,
+        "flood": int,
     }
 
     lldp_beacon_defaults_types = {
-        'send_interval': int,
-        'max_per_interval': int,
-        'system_name': str,
+        "send_interval": int,
+        "max_per_interval": int,
+        "system_name": str,
     }
 
     dot1x_defaults_types = {
-        'nfv_intf': str,
-        'nfv_sw_port': int,
-        'radius_ip': str,
-        'radius_port': int,
-        'radius_secret': str,
-        'auth_acl': str,
-        'noauth_acl': str,
+        "nfv_intf": str,
+        "nfv_sw_port": int,
+        "radius_ip": str,
+        "radius_port": int,
+        "radius_secret": str,
+        "auth_acl": str,
+        "noauth_acl": str,
     }
 
     def __init__(self, _id, dp_id, conf):
         """Constructs a new DP object"""
         self.acls = None
         self.acls_in = None
         self.advertise_interval = None
@@ -347,101 +347,135 @@
         self.dyn_running = True
         for vlan in self.vlans.values():
             vlan.reset_caches()
 
     def check_config(self):
         """Check configuration of this dp"""
         super().check_config()
-        test_config_condition(not isinstance(self.dp_id, int), (
-            f'dp_id must be {int} not {type(self.dp_id)}'))
-        test_config_condition(self.dp_id < 0 or self.dp_id > 2**64 - 1, (
-            f'DP ID {self.dp_id} not in valid range'))
-        test_config_condition(not netaddr.valid_mac(self.faucet_dp_mac), (
-            f'invalid MAC address {self.faucet_dp_mac}'))
-        test_config_condition(not (self.interfaces or self.interface_ranges), (
-            f'DP {self} must have at least one interface'))
-        test_config_condition(self.timeout < 15, 'timeout must be > 15')
-        test_config_condition(self.timeout > 65535, 'timeout cannot be > than 65335')
+        test_config_condition(
+            not isinstance(self.dp_id, int),
+            ("dp_id must be %s not %s" % (int, type(self.dp_id))),
+        )
+        test_config_condition(
+            self.dp_id < 0 or self.dp_id > 2**64 - 1,
+            ("DP ID %s not in valid range" % self.dp_id),
+        )
+        test_config_condition(
+            not netaddr.valid_mac(self.faucet_dp_mac),
+            ("invalid MAC address %s" % self.faucet_dp_mac),
+        )
+        test_config_condition(
+            not (self.interfaces or self.interface_ranges),
+            ("DP %s must have at least one interface" % self),
+        )
+        test_config_condition(self.timeout < 15, "timeout must be > 15")
+        test_config_condition(self.timeout > 65535, "timeout cannot be > than 65335")
         # To prevent L2 learning from timing out before L3 can refresh
-        test_config_condition(not (self.arp_neighbor_timeout < (self.timeout / 2)), (
-            'L2 timeout must be > ARP timeout * 2'))
         test_config_condition(
-            self.arp_neighbor_timeout > 65535, 'arp_neighbor_timeout cannot be > 65535')
-        test_config_condition(not (self.nd_neighbor_timeout < (self.timeout / 2)), (
-            'L2 timeout must be > ND timeout * 2'))
+            not (self.arp_neighbor_timeout < (self.timeout / 2)),
+            ("L2 timeout must be > ARP timeout * 2"),
+        )
+        test_config_condition(
+            self.arp_neighbor_timeout > 65535, "arp_neighbor_timeout cannot be > 65535"
+        )
         test_config_condition(
-            self.nd_neighbor_timeout > 65535, 'nd_neighbor_timeout cannot be > 65535')
-        test_config_condition(self.combinatorial_port_flood and self.group_table, (
-            'combinatorial_port_flood and group_table mutually exclusive'))
+            not (self.nd_neighbor_timeout < (self.timeout / 2)),
+            ("L2 timeout must be > ND timeout * 2"),
+        )
+        test_config_condition(
+            self.nd_neighbor_timeout > 65535, "nd_neighbor_timeout cannot be > 65535"
+        )
+        test_config_condition(
+            self.combinatorial_port_flood and self.group_table,
+            ("combinatorial_port_flood and group_table mutually exclusive"),
+        )
         if self.cache_update_guard_time == 0:
             self.cache_update_guard_time = int(self.timeout / 2)
         if self.learn_jitter == 0:
             self.learn_jitter = int(max(math.sqrt(self.timeout) * 3, 1))
         if self.learn_ban_timeout == 0:
             self.learn_ban_timeout = self.learn_jitter
         if self.lldp_beacon:
             self._lldp_defaults()
         if self.dot1x:
             self._check_conf_types(self.dot1x, self.dot1x_defaults_types)
         self._check_conf_types(self.table_sizes, self.default_table_sizes_types)
-        self.stack = Stack('stack', self.dp_id, self.name,
-                           self.canonical_port_order, self.lacp_down_ports, self.lacp_ports,
-                           self.stack)
+        self.stack = Stack(
+            "stack",
+            self.dp_id,
+            self.name,
+            self.canonical_port_order,
+            self.lacp_down_ports,
+            self.lacp_ports,
+            self.stack,
+        )
 
     def _lldp_defaults(self):
         self._check_conf_types(self.lldp_beacon, self.lldp_beacon_defaults_types)
-        if 'send_interval' not in self.lldp_beacon:
-            self.lldp_beacon['send_interval'] = self.DEFAULT_LLDP_SEND_INTERVAL
-        test_config_condition(self.lldp_beacon['send_interval'] < 1, (
-            f'DP ID {self.dp_id} LLDP beacon send_interval not in valid range'))
-        if 'max_per_interval' not in self.lldp_beacon:
-            self.lldp_beacon['max_per_interval'] = self.DEFAULT_LLDP_MAX_PER_INTERVAL
+        if "send_interval" not in self.lldp_beacon:
+            self.lldp_beacon["send_interval"] = self.DEFAULT_LLDP_SEND_INTERVAL
+        test_config_condition(
+            self.lldp_beacon["send_interval"] < 1,
+            ("DP ID %s LLDP beacon send_interval not in valid range" % self.dp_id),
+        )
+        if "max_per_interval" not in self.lldp_beacon:
+            self.lldp_beacon["max_per_interval"] = self.DEFAULT_LLDP_MAX_PER_INTERVAL
         self.lldp_beacon = self._set_unknown_conf(
-            self.lldp_beacon, self.lldp_beacon_defaults_types)
-        if self.lldp_beacon.get('system_name', None) is None:
-            self.lldp_beacon['system_name'] = self.name
+            self.lldp_beacon, self.lldp_beacon_defaults_types
+        )
+        if self.lldp_beacon.get("system_name", None) is None:
+            self.lldp_beacon["system_name"] = self.name
 
     def _generate_acl_tables(self):
         all_acls = {}
         if self.dot1x:
             # NOTE: All acl's are added to the acl list and then referred to later by ports
-            acls = [PORT_ACL_8021X, MAB_ACL_8021X,
-                    self.acls.get(self.dot1x.get('auth_acl'), None),
-                    self.acls.get(self.dot1x.get('noauth_acl'), None)]
+            acls = [
+                PORT_ACL_8021X,
+                MAB_ACL_8021X,
+                self.acls.get(self.dot1x.get("auth_acl"), None),
+                self.acls.get(self.dot1x.get("noauth_acl"), None),
+            ]
 
-            acls.extend([acl for acl_name, acl in self.acls.items() if acl.dot1x_assigned])
-            all_acls['port_acl'] = [acl for acl in acls if acl is not None]
+            acls.extend(
+                [acl for acl_name, acl in self.acls.items() if acl.dot1x_assigned]
+            )
+            all_acls["port_acl"] = [acl for acl in acls if acl is not None]
 
         for vlan in self.vlans.values():
             if vlan.acls_in:
-                all_acls.setdefault('vlan_acl', [])
-                all_acls['vlan_acl'].extend(vlan.acls_in)
+                all_acls.setdefault("vlan_acl", [])
+                all_acls["vlan_acl"].extend(vlan.acls_in)
             if vlan.acls_out:
-                all_acls.setdefault('egress_acl', [])
-                all_acls['egress_acl'].extend(vlan.acls_out)
+                all_acls.setdefault("egress_acl", [])
+                all_acls["egress_acl"].extend(vlan.acls_out)
                 self.egress_pipeline = True
         if self.dp_acls:
-            test_config_condition(self.dot1x, (
-                'DP ACLs and 802.1x cannot be configured together'))
-            all_acls.setdefault('port_acl', [])
-            all_acls['port_acl'].extend(self.dp_acls)
+            test_config_condition(
+                self.dot1x, ("DP ACLs and 802.1x cannot be configured together")
+            )
+            all_acls.setdefault("port_acl", [])
+            all_acls["port_acl"].extend(self.dp_acls)
         else:
             for port in self.ports.values():
                 if port.acls_in:
-                    test_config_condition(port.dot1x, (
-                        'port ACLs and 802.1x cannot be configured together'))
-                    all_acls.setdefault('port_acl', [])
-                    all_acls['port_acl'].extend(port.acls_in)
-                if self.dot1x and port.number == self.dot1x['nfv_sw_port']:
-                    test_config_condition(not port.output_only, (
-                        'NFV Ports must have output_only set to True.'
-                    ))
+                    test_config_condition(
+                        port.dot1x,
+                        ("port ACLs and 802.1x cannot be configured together"),
+                    )
+                    all_acls.setdefault("port_acl", [])
+                    all_acls["port_acl"].extend(port.acls_in)
+                if self.dot1x and port.number == self.dot1x["nfv_sw_port"]:
+                    test_config_condition(
+                        not port.output_only,
+                        ("NFV Ports must have output_only set to True."),
+                    )
         if self.tunnel_acls:
-            all_acls.setdefault('port_acl', [])
-            all_acls['port_acl'].extend(self.tunnel_acls)
+            all_acls.setdefault("port_acl", [])
+            all_acls["port_acl"].extend(self.tunnel_acls)
         table_config = {}
         for table_name, acls in all_acls.items():
             matches = {}
             set_fields = set()
             meter = False
             exact_match = False
             default = faucet_pipeline.DEFAULT_CONFIGS[table_name]
@@ -456,123 +490,140 @@
                 table_name,
                 default.table_id,
                 exact_match=exact_match,
                 meter=meter,
                 output=True,
                 match_types=tuple(sorted(matches.items())),
                 set_fields=tuple(sorted(set_fields)),
-                next_tables=default.next_tables)
+                next_tables=default.next_tables,
+            )
         # TODO: dynamically configure output attribute
         return table_config
 
     def pipeline_str(self):
         """Text description of pipeline."""
-        table_configs = sorted([
-            (table.table_id, str(table.table_config))
-            for table in self.tables.values()])
-        return '\n'.join([
-            f'table ID {table_id} {table_config}'
-            for table_id, table_config in table_configs])
+        table_configs = sorted(
+            [
+                (table.table_id, str(table.table_config))
+                for table in self.tables.values()
+            ]
+        )
+        return "\n".join(
+            [
+                "table ID %u %s" % (table_id, table_config)
+                for table_id, table_config in table_configs
+            ]
+        )
 
     def pipeline_tableids(self):
         """Return pipeline table IDs."""
         return {table.table_id for table in self.tables.values()}
 
     def _configure_tables(self):
         """Configure FAUCET pipeline with tables."""
         valve_cl = SUPPORTED_HARDWARE.get(self.hardware, None)
         test_config_condition(
-            not valve_cl, f'hardware {self.hardware} must be in {list(SUPPORTED_HARDWARE)}')
+            not valve_cl,
+            "hardware %s must be in %s" % (self.hardware, list(SUPPORTED_HARDWARE)),
+        )
         if valve_cl is None:
             return
 
         tables = {}
         self.groups = ValveGroupTable()
         relative_table_id = 0
         included_tables = copy.deepcopy(faucet_pipeline.MINIMUM_FAUCET_PIPELINE_TABLES)
         acl_tables = self._generate_acl_tables()
         if acl_tables:
             included_tables.update(set(acl_tables.keys()))
             self.has_acls = True
         # Only configure IP routing tables if enabled.
         for vlan in self.vlans.values():
             for ipv in vlan.ipvs():
-                included_tables.add(f'ipv{ipv}_fib')
-                included_tables.add('vip')
+                included_tables.add("ipv%u_fib" % ipv)
+                included_tables.add("vip")
         if valve_cl.STATIC_TABLE_IDS:
-            included_tables.add('port_acl')
+            included_tables.add("port_acl")
             self.has_acls = True
         if self.hairpin_ports:
-            included_tables.add('eth_dst_hairpin')
+            included_tables.add("eth_dst_hairpin")
         if self.use_classification:
-            included_tables.add('classification')
+            included_tables.add("classification")
         if self.egress_pipeline:
-            included_tables.add('egress')
+            included_tables.add("egress")
         if self.coprocessor_ports():
-            included_tables.add('copro')
+            included_tables.add("copro")
         canonical_configs = [
-            config for config in faucet_pipeline.FAUCET_PIPELINE
-            if config.name in included_tables]
+            config
+            for config in faucet_pipeline.FAUCET_PIPELINE
+            if config.name in included_tables
+        ]
         table_configs = {}
-        for relative_table_id, canonical_table_config in enumerate(canonical_configs, start=0):
+        for relative_table_id, canonical_table_config in enumerate(
+            canonical_configs, start=0
+        ):
             name = canonical_table_config.name
-            table_config = acl_tables.get(
-                name, copy.deepcopy(canonical_table_config))
+            table_config = acl_tables.get(name, copy.deepcopy(canonical_table_config))
             if not self.egress_pipeline:
                 table_config.metadata_write = 0
                 table_config.metadata_match = 0
             if not valve_cl.STATIC_TABLE_IDS:
                 table_config.table_id = relative_table_id
             table_configs[name] = table_config
 
         # Stacking with external ports, so need external forwarding request field.
         if self.has_externals:
-            for table_name in ('vlan', 'eth_dst', 'flood'):
+            for table_name in ("vlan", "eth_dst", "flood"):
                 table = table_configs[table_name]
                 table.match_types += ((valve_of.EXTERNAL_FORWARDING_FIELD, False),)
                 if table.set_fields is not None:
                     table.set_fields += (valve_of.EXTERNAL_FORWARDING_FIELD,)
                 else:
                     table.set_fields = (valve_of.EXTERNAL_FORWARDING_FIELD,)
 
         if self.restricted_bcast_arpnd_ports():
-            table_configs['flood'].match_types += (('eth_type', False),)
+            table_configs["flood"].match_types += (("eth_type", False),)
 
-        if 'egress_acl' in included_tables:
-            table_configs['eth_dst'].miss_goto = 'egress_acl'
+        if "egress_acl" in included_tables:
+            table_configs["eth_dst"].miss_goto = "egress_acl"
 
         oxm_fields = set(valve_of.MATCH_FIELDS.keys())
 
         for table_name, table_config in table_configs.items():
             if table_config.set_fields:
                 set_fields = set(table_config.set_fields)
                 test_config_condition(
                     not set_fields.issubset(oxm_fields),
-                    f'set_fields not all OpenFlow OXM fields {set_fields - oxm_fields}')
+                    "set_fields not all OpenFlow OXM fields %s"
+                    % (set_fields - oxm_fields),
+                )
             if table_config.match_types:
                 matches = set(match for match, _ in table_config.match_types)
                 test_config_condition(
                     not matches.issubset(oxm_fields),
-                    f'matches not all OpenFlow OXM fields {matches - oxm_fields}')
+                    "matches not all OpenFlow OXM fields %s" % (matches - oxm_fields),
+                )
 
             scale_factor = 1.0
             # Need flows for internal/external.
             if self.has_externals:
                 scale_factor *= 2
 
             # Table scales with number of VLANs only.
             if table_config.vlan_scale:
-                scale_factor *= (len(self.vlans) * table_config.vlan_scale)
+                scale_factor *= len(self.vlans) * table_config.vlan_scale
 
             # Table scales with number of ports and VLANs.
             elif table_config.vlan_port_scale:
-                scale_factor *= (len(self.vlans) * len(self.ports) * table_config.vlan_port_scale)
+                scale_factor *= (
+                    len(self.vlans) * len(self.ports) * table_config.vlan_port_scale
+                )
                 scale_factor *= self.port_table_scale_factor
 
-                if table_config.name == 'flood':
+                if table_config.name == "flood":
                     # We need flows for all ports when using combinatorial port flood.
                     if self.combinatorial_port_flood:
                         scale_factor *= len(self.ports)
                     # We need more flows for more broadcast rules.
                     if self.restricted_bcast_arpnd_ports():
                         scale_factor *= 2
 
@@ -585,34 +636,40 @@
                 size = min(size, self.max_wildcard_table_size)
 
             # Hard override for size if present.
             size = self.table_sizes.get(table_name, size)
 
             table_config.size = size
             table_config.next_tables = [
-                tbl_name for tbl_name in table_config.next_tables
-                if tbl_name in table_configs]
+                tbl_name
+                for tbl_name in table_config.next_tables
+                if tbl_name in table_configs
+            ]
             next_table_ids = [
-                table_configs[tbl_name].table_id for tbl_name in table_config.next_tables]
+                table_configs[tbl_name].table_id
+                for tbl_name in table_config.next_tables
+            ]
             tables[table_name] = ValveTable(
-                table_name, table_config, self.cookie,
+                table_name,
+                table_config,
+                self.cookie,
                 notify_flow_removed=self.use_idle_timeout,
-                next_tables=next_table_ids
+                next_tables=next_table_ids,
             )
         self.tables = tables
 
     def set_defaults(self):
         super().set_defaults()
-        self._set_default('dp_id', self._id)
-        self._set_default('name', str(self._id))
-        self._set_default('lowest_priority', self.priority_offset)
-        self._set_default('low_priority', self.priority_offset + 9000)
-        self._set_default('high_priority', self.low_priority + 1)
-        self._set_default('highest_priority', self.high_priority + 98)
-        self._set_default('description', self.name)
+        self._set_default("dp_id", self._id)
+        self._set_default("name", str(self._id))
+        self._set_default("lowest_priority", self.priority_offset)
+        self._set_default("low_priority", self.priority_offset + 9000)
+        self._set_default("high_priority", self.low_priority + 1)
+        self._set_default("highest_priority", self.high_priority + 98)
+        self._set_default("description", self.name)
 
     def table_by_id(self, table_id):
         """Gets first table with table id"""
         tables = [table for table in self.tables.values() if table_id == table.table_id]
         if tables:
             return tables[0]
         return None
@@ -630,47 +687,55 @@
         port_name = str(port_no)
         port_description = None
         if port_no in self.ports:
             port = self.ports[port_no]
             port_name = port.name
             port_description = port.description
         elif port_no == valve_of.ofp.OFPP_CONTROLLER:
-            port_name = 'CONTROLLER'
+            port_name = "CONTROLLER"
         elif port_no == valve_of.ofp.OFPP_LOCAL:
-            port_name = 'LOCAL'
+            port_name = "LOCAL"
         if port_description is None:
             port_description = port_name
-        return dict(self.base_prom_labels(), port=port_name, port_description=port_description)
+        return dict(
+            self.base_prom_labels(), port=port_name, port_description=port_description
+        )
 
     def classification_table(self):
         """Returns classification table"""
         if self.use_classification:
-            return self.tables['classification']
-        return self.tables['eth_src']
+            return self.tables["classification"]
+        return self.tables["eth_src"]
 
     def output_tables(self):
         """Return tables that cause a packet to be forwarded."""
         if self.hairpin_ports:
-            return (self.tables['eth_dst_hairpin'], self.tables['eth_dst'])
-        return (self.tables['eth_dst'],)
+            return (self.tables["eth_dst_hairpin"], self.tables["eth_dst"])
+        return (self.tables["eth_dst"],)
 
     def output_table(self):
         """Returns first output table"""
         return self.output_tables()[0]
 
     def match_tables(self, match_type):
         """Return list of tables with matches of a specific match type."""
         return [
-            table for table in self.tables.values()
-            if table.match_types is None or match_type in table.match_types]
+            table
+            for table in self.tables.values()
+            if table.match_types is None or match_type in table.match_types
+        ]
 
     def non_vlan_ports(self):
         """Ports that don't have VLANs on them."""
         ports = set()
-        for non_vlan in (self.output_only_ports, self.stack_ports(), self.coprocessor_ports()):
+        for non_vlan in (
+            self.output_only_ports,
+            self.stack_ports(),
+            self.coprocessor_ports(),
+        ):
             ports.update(set(non_vlan))
         return ports
 
     def stack_ports(self):
         """Return list of stack ports"""
         if self.stack:
             return tuple(self.stack.ports)
@@ -678,15 +743,17 @@
 
     def coprocessor_ports(self):
         """Return list of coprocessor ports."""
         return tuple(port for port in self.ports.values() if port.coprocessor)
 
     def restricted_bcast_arpnd_ports(self):
         """Return ports that have restricted broadcast set."""
-        return tuple(port for port in self.ports.values() if port.restricted_bcast_arpnd)
+        return tuple(
+            port for port in self.ports.values() if port.restricted_bcast_arpnd
+        )
 
     def lacp_ports(self):
         """Return ports that have LACP."""
         return tuple(port for port in self.ports.values() if port.lacp)
 
     def lacp_up_ports(self):
         """Return ports that have LACP up."""
@@ -749,27 +816,35 @@
             self.lacp_active_ports.append(port)
 
     def lldp_beacon_send_ports(self, now):
         """Return list of ports to send LLDP packets; stacked ports always send LLDP."""
         send_ports = []
         if self.lldp_beacon:
             priority_ports = {
-                port for port in self.stack_ports()
-                if port.running() and port.lldp_beacon_enabled()}
-            cutoff_beacon_time = now - self.lldp_beacon['send_interval']
+                port
+                for port in self.stack_ports()
+                if port.running() and port.lldp_beacon_enabled()
+            }
+            cutoff_beacon_time = now - self.lldp_beacon["send_interval"]
             nonpriority_ports = {
-                port for port in self.lldp_beacon_ports
-                if port.running() and (
+                port
+                for port in self.lldp_beacon_ports
+                if port.running()
+                and (
                     port.dyn_last_lldp_beacon_time is None
-                    or port.dyn_last_lldp_beacon_time < cutoff_beacon_time)}
+                    or port.dyn_last_lldp_beacon_time < cutoff_beacon_time
+                )
+            }
             nonpriority_ports -= priority_ports
             send_ports.extend(list(priority_ports))
             nonpriority_ports = list(nonpriority_ports)
             random.shuffle(nonpriority_ports)
-            nonpriority_ports = nonpriority_ports[:self.lldp_beacon['max_per_interval']]
+            nonpriority_ports = nonpriority_ports[
+                : self.lldp_beacon["max_per_interval"]
+            ]
             send_ports.extend(nonpriority_ports)
         return send_ports
 
     def resolve_stack_topology(self, dps, meta_dp_state):
         """Resolve inter-DP config for stacking"""
         if self.stack:
             self.stack.resolve_topology(dps, meta_dp_state)
@@ -814,44 +889,54 @@
         return sorted(ports, key=lambda x: x.number)
 
     def reset_refs(self, vlans=None):
         """Resets VLAN references."""
 
         if vlans is None:
             vlans = self.vlans
-            router_vlans = {vlan._id for router in self.routers.values() for vlan in router.vlans}
+            router_vlans = {
+                vlan._id for router in self.routers.values() for vlan in router.vlans
+            }
         else:
-            router_vlans = {vlan for router in self.routers.values() for vlan in router.vlans}
+            router_vlans = {
+                vlan for router in self.routers.values() for vlan in router.vlans
+            }
 
         vlan_ports = defaultdict(set)
         for port in self.ports.values():
             for vlan in port.vlans():
                 vlan_ports[vlan].add(port)
 
         if self.stack_ports or self.stack.is_root():
             new_vlans = list(vlans.values())
         else:
             new_vlans = []
             for vlan in vlans.values():
-                if (vlan_ports[vlan] or vlan.reserved_internal_vlan
-                        or vlan.dot1x_assigned or vlan._id in router_vlans):
+                if (
+                    vlan_ports[vlan]
+                    or vlan.reserved_internal_vlan
+                    or vlan.dot1x_assigned
+                    or vlan._id in router_vlans
+                ):
                     new_vlans.append(vlan)
 
         self.vlans = {}
         for vlan in new_vlans:
             vlan.reset_ports(vlan_ports[vlan])
             self.vlans[vlan.vid] = vlan
 
     def resolve_port(self, port_name):
         """Resolve a port by number or name."""
         if isinstance(port_name, int):
             if port_name in self.ports:
                 return self.ports[port_name]
         elif isinstance(port_name, str):
-            resolved_ports = [port for port in self.ports.values() if port_name == port.name]
+            resolved_ports = [
+                port for port in self.ports.values() if port_name == port.name
+            ]
             if resolved_ports:
                 return resolved_ports[0]
         return None
 
     def finalize_config(self, dps):
         """Perform consistency checks after initial config parsing."""
 
@@ -864,16 +949,18 @@
             used_vids = sorted([vlan.vid for vlan in self.vlans.values()])
             while vid in used_vids:
                 vid += 1
             return vid
 
         def create_vlan(vid):
             """Creates a VLAN object with the VID"""
-            test_config_condition(vid in self.vlans, (
-                'Attempting to dynamically create a VLAN with ID that already exists'))
+            test_config_condition(
+                vid in self.vlans,
+                ("Attempting to dynamically create a VLAN with ID that already exists"),
+            )
             vlan = VLAN(vid, self.dp_id, None)
             self.vlans[vlan.vid] = vlan
             return vlan
 
         def resolve_ports(port_names):
             """Resolve list of ports, by port by name or number."""
             resolved_ports = []
@@ -881,16 +968,18 @@
                 port = self.resolve_port(port_name)
                 if port is not None:
                     resolved_ports.append(port)
             return resolved_ports
 
         def resolve_vlan(vlan_name):
             """Resolve VLAN by name or VID."""
-            test_config_condition(not isinstance(vlan_name, (str, int)), (
-                f'VLAN must be type {str} or {int} not {type(vlan_name)}'))
+            test_config_condition(
+                not isinstance(vlan_name, (str, int)),
+                ("VLAN must be type %s or %s not %s" % (str, int, type(vlan_name))),
+            )
             if vlan_name in vlan_by_name:
                 return vlan_by_name[vlan_name]
             if vlan_name in self.vlans:
                 return self.vlans[vlan_name]
             return None
 
         def resolve_vlans(vlan_names):
@@ -903,33 +992,42 @@
             return vlans
 
         def resolve_stack_dps():
             """Resolve DP references in stacking config."""
             if self.stack_ports():
                 port_stack_dp = {}
                 for port in self.stack_ports():
-                    stack_dp = port.stack['dp']
-                    test_config_condition(stack_dp not in dp_by_name, (
-                        f'stack DP {stack_dp} not defined'))
+                    stack_dp = port.stack["dp"]
+                    test_config_condition(
+                        stack_dp not in dp_by_name,
+                        ("stack DP %s not defined" % stack_dp),
+                    )
                     port_stack_dp[port] = dp_by_name[stack_dp]
                 for port, dp in port_stack_dp.items():
-                    port.stack['dp'] = dp
-                    stack_port = dp.resolve_port(port.stack['port'])
-                    test_config_condition(stack_port is None, (
-                        f'stack port {port.stack["port"]} not defined in DP{dp.name}'))
-                    port.stack['port'] = stack_port
+                    port.stack["dp"] = dp
+                    stack_port = dp.resolve_port(port.stack["port"])
+                    test_config_condition(
+                        stack_port is None,
+                        (
+                            "stack port %s not defined in DP %s"
+                            % (port.stack["port"], dp.name)
+                        ),
+                    )
+                    port.stack["port"] = stack_port
 
         def resolve_mirror_destinations():
             """Resolve mirror port references and destinations."""
             mirror_from_port = defaultdict(list)
             for mirror_port in self.ports.values():
                 if mirror_port.mirror is not None:
                     mirrored_ports = resolve_ports(mirror_port.mirror)
-                    test_config_condition(len(mirrored_ports) != len(mirror_port.mirror), (
-                        f'port mirror not defined in DP {self.name}'))
+                    test_config_condition(
+                        len(mirrored_ports) != len(mirror_port.mirror),
+                        ("port mirror not defined in DP %s" % self.name),
+                    )
                     for mirrored_port in mirrored_ports:
                         mirror_from_port[mirrored_port].append(mirror_port)
 
             # TODO: confusingly, mirror at config time means what ports to mirror from.
             #   But internally we use as a list of ports to mirror to.
             for mirrored_port, mirror_ports in mirror_from_port.items():
                 mirrored_port.mirror = []
@@ -944,16 +1042,18 @@
             Args:
                 acl_in (str): ACL name to find reference in the acl list
                 vid (int): VID of the VLAN the ACL is being applied to
                 port_num (int): The number of the port the ACl is being applied to
             Returns:
                 matches, set_fields, meter (3-Tuple): ACL matches, set fields and meter values
             """
-            test_config_condition(acl_in not in self.acls, (
-                f'missing ACL {acl_in} in DP:{self.name}'))
+            test_config_condition(
+                acl_in not in self.acls,
+                ("missing ACL %s in DP: %s" % (acl_in, self.name)),
+            )
             acl = self.acls[acl_in]
             tunnel_dsts_to_vlan = {}
 
             def resolve_port_cb(port_name):
                 """Resolve port"""
                 port = self.resolve_port(port_name)
                 if port:
@@ -974,98 +1074,132 @@
                 """
                 if not tunnel_id_name:
                     if resolved_dst in tunnel_dsts_to_vlan:
                         tunnel_vlan = tunnel_dsts_to_vlan[resolved_dst]
                     else:
                         # Create a VLAN using the first unused VLAN ID
                         # Get highest non-reserved VLAN
-                        vlan_offset = max([
-                            vlan.vid for vlan in self.vlans.values()
-                            if not vlan.reserved_internal_vlan])
+                        vlan_offset = max(
+                            [
+                                vlan.vid
+                                for vlan in self.vlans.values()
+                                if not vlan.reserved_internal_vlan
+                            ]
+                        )
                         # Also need to account for the potential number of tunnels
                         ordered_acls = sorted(self.acls)
                         index = ordered_acls.index(acl_in) + 1
-                        acl_tunnels = [self.acls[name].get_num_tunnels() for name in ordered_acls]
+                        acl_tunnels = [
+                            self.acls[name].get_num_tunnels() for name in ordered_acls
+                        ]
                         tunnel_offset = sum(acl_tunnels[:index])
                         start_pos = vlan_offset + tunnel_offset
                         tunnel_vid = first_unused_vlan_id(start_pos)
                         tunnel_vlan = create_vlan(tunnel_vid)
                         tunnel_vlan.reserved_internal_vlan = True
                 else:
                     # Tunnel ID has been specified, so search for the VLAN
                     tunnel_vlan = resolve_vlan(tunnel_id_name)
                     if tunnel_vlan:
                         # VLAN exists, i.e: user specified the VLAN so check if it is reserved
-                        test_config_condition(not tunnel_vlan.reserved_internal_vlan, (
-                            f'VLAN {tunnel_vlan.name} is required for use by'
-                            f' tunnel {tunnel_id_name} but is not reserved'))
+                        test_config_condition(
+                            not tunnel_vlan.reserved_internal_vlan,
+                            (
+                                "VLAN %s is required for use by tunnel %s but is not reserved"
+                                % (tunnel_vlan.name, tunnel_id_name)
+                            ),
+                        )
                     else:
                         # VLAN does not exist, so the ID should be the VID the user wants
-                        test_config_condition(isinstance(tunnel_id_name, str), (
-                            f'Tunnel VLAN ({tunnel_id_name}) does not exist'))
+                        test_config_condition(
+                            isinstance(tunnel_id_name, str),
+                            ("Tunnel VLAN (%s) does not exist" % tunnel_id_name),
+                        )
                         # Create the tunnel VLAN object
                         tunnel_vlan = create_vlan(tunnel_id_name)
                         tunnel_vlan.reserved_internal_vlan = True
                     existing_tunnel_vlan = tunnel_dsts_to_vlan.get(resolved_dst, None)
                     if existing_tunnel_vlan is not None:
                         test_config_condition(
                             existing_tunnel_vlan == tunnel_vlan.vid,
-                            f'Cannot have multiple tunnel IDs ({existing_tunnel_vlan.vid},'
-                            f' {tunnel_vlan.vid}) to same destination {resolved_dst}')
+                            "Cannot have multiple tunnel IDs (%u, %u) to same destination %s"
+                            % (existing_tunnel_vlan.vid, tunnel_vlan.vid, resolved_dst),
+                        )
                 return tunnel_vlan
 
             def resolve_tunnel_objects(dst_dp_name, dst_port_name, tunnel_id_name):
                 """
                 Resolves the names of the tunnel src and dst (DP & port) pairs into the correct \
                     objects
                 Args:
                     dst_dp (str): DP of the tunnel's destination port
                     dst_port (int/None): Destination port of the tunnel
                     tunnel_id_name (int/str/None): Tunnel identification number or VLAN reference
                 Returns:
                     dst_dp name, dst_port name and tunnel id
                 """
                 # VLAN tunnel ACL
-                test_config_condition(vid is not None, 'Tunnels do not support VLAN-ACLs')
+                test_config_condition(
+                    vid is not None, "Tunnels do not support VLAN-ACLs"
+                )
                 # Port & DP tunnel ACL
-                test_config_condition(dst_dp_name not in dp_by_name, (
-                    f'Could not find referenced destination DP ({dst_dp_name}) for tunnel ACL {acl_in}'))
+                test_config_condition(
+                    dst_dp_name not in dp_by_name,
+                    (
+                        "Could not find referenced destination DP (%s) for tunnel ACL %s"
+                        % (dst_dp_name, acl_in)
+                    ),
+                )
                 dst_dp = dp_by_name[dst_dp_name]
                 dst_port = None
                 if dst_port_name:
                     dst_port = dst_dp.resolve_port(dst_port_name)
-                    test_config_condition(dst_port is None, (
-                        f'Could not find referenced destination port ({dst_port_name}) for tunnel ACL {acl_in}'))
-                    test_config_condition(dst_port.stack is None, (
-                        f'destination port {dst_port_name} for tunnel ACL {acl_in} cannot be a stack port'))
+                    test_config_condition(
+                        dst_port is None,
+                        (
+                            "Could not find referenced destination port (%s) for tunnel ACL %s"
+                            % (dst_port_name, acl_in)
+                        ),
+                    )
+                    test_config_condition(
+                        dst_port.stack is None,
+                        (
+                            "destination port %s for tunnel ACL %s cannot be a stack port"
+                            % (dst_port_name, acl_in)
+                        ),
+                    )
                     dst_port = dst_port.number
                 dst_dp = dst_dp.name
                 resolved_dst = (dst_dp, dst_port)
                 tunnel_vlan = get_tunnel_vlan(tunnel_id_name, resolved_dst)
                 # Sources will be resolved later on
                 self.tunnel_acls.append(self.acls[acl_in])
                 tunnel_dsts_to_vlan[resolved_dst] = tunnel_vlan
                 tunnel_id = tunnel_vlan.vid
                 return (dst_dp, dst_port, tunnel_id)
 
             acl.resolve_ports(resolve_port_cb, resolve_tunnel_objects)
             for meter_name in acl.get_meters():
-                test_config_condition(meter_name not in self.meters, (
-                    f'meter {meter_name} is not configured'))
+                test_config_condition(
+                    meter_name not in self.meters,
+                    ("meter %s is not configured" % meter_name),
+                )
                 acl_meters.add(meter_name)
             for port_no in acl.get_mirror_destinations():
                 port = self.ports[port_no]
                 port.output_only = True
             return acl.build(self.meters, vid, port_num)
 
         def verify_acl_exact_match(acls):
             """Verify ACLs have equal exact matches"""
             for acl in acls:
-                test_config_condition(acl.exact_match != acls[0].exact_match, (
-                    'ACLs when used together must have consistent exact_match'))
+                test_config_condition(
+                    acl.exact_match != acls[0].exact_match,
+                    ("ACLs when used together must have consistent exact_match"),
+                )
 
         def resolve_acls():
             """Resolve config references in ACLs."""
             for vlan in self.vlans.values():
                 if vlan.acls_in:
                     acls = []
                     for acl in vlan.acls_in:
@@ -1079,32 +1213,38 @@
                         resolve_acl(acl, vid=vlan.vid)
                         acls.append(self.acls[acl])
                     vlan.acls_out = acls
                     verify_acl_exact_match(acls)
             for port in self.ports.values():
                 if port.acls_in:
                     acls = []
-                    test_config_condition(self.dp_acls, (
-                        'dataplane ACLs cannot be used with port ACLs.'))
+                    test_config_condition(
+                        self.dp_acls, ("dataplane ACLs cannot be used with port ACLs.")
+                    )
                     for acl in port.acls_in:
                         resolve_acl(acl, port_num=port.number)
                         acls.append(self.acls[acl])
                     port.acls_in = acls
                     verify_acl_exact_match(acls)
 
                 if port.dot1x_dyn_acl:
-                    acl_names = [acl_name for acl_name, acl in self.acls.items()
-                                 if acl.dot1x_assigned]
+                    acl_names = [
+                        acl_name
+                        for acl_name, acl in self.acls.items()
+                        if acl.dot1x_assigned
+                    ]
 
                     for acl_name in acl_names:
                         resolve_acl(acl_name, port_num=port.number)
 
                 if port.dot1x_acl:
-                    acl_names = [self.dot1x.get('auth_acl'),
-                                 self.dot1x.get('noauth_acl')]
+                    acl_names = [
+                        self.dot1x.get("auth_acl"),
+                        self.dot1x.get("noauth_acl"),
+                    ]
 
                     for acl_name in acl_names:
                         if self.acls.get(acl_name, None):
                             resolve_acl(acl_name, port_num=port.number)
 
             if self.dp_acls:
                 acls = []
@@ -1136,64 +1276,84 @@
                     dp_routers[router_name] = dp_router
             self.routers = dp_routers
 
             if self.global_vlan:
                 vids = {vlan.vid for vlan in self.vlans.values()}
                 test_config_condition(
                     self.global_vlan in vids,
-                    f'global_vlan VID {self.global_vlan} conflicts with existing VLAN')
+                    "global_vlan VID %s conflicts with existing VLAN"
+                    % self.global_vlan,
+                )
 
             # Check for overlapping VIP subnets or VLANs.
             all_router_vlans = set()
             for router_name, router in self.routers.items():
                 vips = set()
                 if router.vlans and len(router.vlans) == 1:
                     lone_vlan = router.vlans[0]
                     test_config_condition(
                         lone_vlan in all_router_vlans,
-                        f'single VLAN {lone_vlan} in more than one router')
+                        "single VLAN %s in more than one router" % lone_vlan,
+                    )
                 for vlan in router.vlans:
-                    vips.update({vip for vip in vlan.faucet_vips if not vip.ip.is_link_local})
+                    vips.update(
+                        {vip for vip in vlan.faucet_vips if not vip.ip.is_link_local}
+                    )
                 all_router_vlans.update(router.vlans)
                 for vip in vips:
                     for other_vip in vips - set([vip]):
                         test_config_condition(
                             vip.ip in other_vip.network,
-                            f'VIPs {vip} and {other_vip} overlap in router {router_name}')
+                            "VIPs %s and %s overlap in router %s"
+                            % (vip, other_vip, router_name),
+                        )
             bgp_routers = self.bgp_routers()
             if bgp_routers:
                 for bgp_router in bgp_routers:
                     bgp_vlan = bgp_router.bgp_vlan()
-                    vlan_dp_ids = [str(dp.dp_id) for dp in dps if bgp_vlan.vid in dp.vlans]
-                    test_config_condition(len(vlan_dp_ids) != 1, (
-                        f'DPs ({", ".join(vlan_dp_ids)}) sharing a BGP speaker VLAN ({bgp_vlan.vid}) is unsupported'))
-                    test_config_condition(bgp_router.bgp_server_addresses() != (
-                        bgp_routers[0].bgp_server_addresses()), (
-                            'BGP server addresses must all be the same'))
+                    vlan_dp_ids = [
+                        str(dp.dp_id) for dp in dps if bgp_vlan.vid in dp.vlans
+                    ]
+                    test_config_condition(
+                        len(vlan_dp_ids) != 1,
+                        ("DPs (%s) sharing a BGP speaker VLAN (%s) is unsupported")
+                        % (", ".join(vlan_dp_ids), bgp_vlan.vid),
+                    )
+                    test_config_condition(
+                        bgp_router.bgp_server_addresses()
+                        != (bgp_routers[0].bgp_server_addresses()),
+                        ("BGP server addresses must all be the same"),
+                    )
                 router_ids = {bgp_router.bgp_routerid() for bgp_router in bgp_routers}
                 test_config_condition(
-                    len(router_ids) != 1, f'BGP router IDs must all be the same: {router_ids}')
+                    len(router_ids) != 1,
+                    "BGP router IDs must all be the same: %s" % router_ids,
+                )
                 bgp_ports = {bgp_router.bgp_port() for bgp_router in bgp_routers}
                 test_config_condition(
-                    len(bgp_ports) != 1, f'BGP ports must all be the same: {bgp_ports}')
+                    len(bgp_ports) != 1,
+                    "BGP ports must all be the same: %s" % bgp_ports,
+                )
 
         if not self.stack_ports():
             # Revert back to None if there are no stack ports
             self.stack = None
         if self.stack:
             # Set LLDP defaults for when stacking is configured
             self._lldp_defaults()
 
         test_config_condition(
             not self.vlans and not self.non_vlan_ports(),
-            f'no VLANs referenced by interfaces in {self.name}')
+            "no VLANs referenced by interfaces in %s" % self.name,
+        )
         dp_by_name = {dp.name: dp for dp in dps}
         vlan_by_name = {vlan.name: vlan for vlan in self.vlans.values()}
         loop_protect_external_ports = {
-            port for port in self.ports.values() if port.loop_protect_external}
+            port for port in self.ports.values() if port.loop_protect_external
+        }
         self.has_externals = bool(loop_protect_external_ports)
 
         # Populate port.lacp_port_id if it wasn't set in config
         for port in self.ports.values():
             if port.lacp and port.lacp_port_id == -1:
                 dp_index = dps.index(self)
                 port.lacp_port_id = dp_index * 100 + port.number
@@ -1223,22 +1383,27 @@
             return self.ports[port_num].native_vlan
         except KeyError:
             return None
 
     def bgp_routers(self):
         """Return list of routers with BGP enabled."""
         return tuple(
-            router for router in self.routers.values() if router.bgp_as() and router.bgp_vlan())
+            router
+            for router in self.routers.values()
+            if router.bgp_as() and router.bgp_vlan()
+        )
 
     def dot1x_ports(self):
         """Return list of ports with 802.1x enabled."""
         return tuple(port for port in self.ports.values() if port.dot1x)
 
     @staticmethod
-    def _get_conf_changes(logger, conf_name, subconf, new_subconf, diff=False, ignore_keys=None):
+    def _get_conf_changes(
+        logger, conf_name, subconf, new_subconf, diff=False, ignore_keys=None
+    ):
         """Generic detection of config changes between DPs, with merge of unchanged instances."""
         if not ignore_keys:
             ignore_keys = []
         ignore_keys = frozenset(ignore_keys)
         curr_confs = frozenset(subconf.keys())
         new_confs = frozenset(new_subconf.keys())
         deleted_confs = set(curr_confs - new_confs)
@@ -1246,86 +1411,108 @@
         changed_confs = set()
         same_confs = set()
         description_only_confs = set()
 
         for conf_id, new_conf in new_subconf.items():
             old_conf = subconf.get(conf_id, None)
             if old_conf:
-                if old_conf.ignore_subconf(
-                        new_conf, ignore_keys=ignore_keys):
+                if old_conf.ignore_subconf(new_conf, ignore_keys=ignore_keys):
                     same_confs.add(conf_id)
                 elif old_conf.ignore_subconf(
-                        new_conf, ignore_keys=(ignore_keys.union(['description']))):
+                    new_conf, ignore_keys=(ignore_keys.union(["description"]))
+                ):
                     same_confs.add(conf_id)
                     description_only_confs.add(conf_id)
-                    logger.info(f'{conf_name} {conf_id} description only changed')
+                    logger.info("%s %s description only changed" % (conf_name, conf_id))
                 else:
                     changed_confs.add(conf_id)
                     if diff:
-                        logger.info(f'{conf_name} {conf_id} changed: {old_conf.conf_diff(new_conf)}')
+                        logger.info(
+                            "%s %s changed: %s"
+                            % (conf_name, conf_id, old_conf.conf_diff(new_conf))
+                        )
                     else:
-                        logger.info(f'{conf_name} {conf_id} changed')
+                        logger.info("%s %s changed" % (conf_name, conf_id))
             else:
                 added_confs.add(conf_id)
-                logger.info(f'{conf_name} {conf_id} added')
+                logger.info("%s %s added" % (conf_name, conf_id))
 
         for conf_id in same_confs:
             old_conf = subconf[conf_id]
             new_subconf[conf_id].merge_dyn(old_conf)
 
         changes = deleted_confs or added_confs or changed_confs
         if changes:
             if deleted_confs:
-                logger.info(f'{conf_name}s deleted: {deleted_confs}')
+                logger.info("%ss deleted: %s" % (conf_name, deleted_confs))
             if added_confs:
-                logger.info(f'{conf_name}s added: {added_confs}')
+                logger.info("%ss added: %s" % (conf_name, added_confs))
             if changed_confs:
-                logger.info(f'{conf_name}s changed: {changed_confs}')
+                logger.info("%ss changed: %s" % (conf_name, changed_confs))
         else:
-            logger.info(f'no {conf_name} changes')
-
+            logger.info("no %s changes" % conf_name)
 
         return (
-            changes, deleted_confs, added_confs, changed_confs, same_confs, description_only_confs)
+            changes,
+            deleted_confs,
+            added_confs,
+            changed_confs,
+            same_confs,
+            description_only_confs,
+        )
 
     def _get_acl_config_changes(self, logger, new_dp):
         """Detect any config changes to ACLs.
 
         Args:
             logger (ValveLogger): logger instance.
             new_dp (DP): new dataplane configuration.
         Returns:
             changed_acls (set): changed/added ACLs.
         """
         _, _, added_acls, changed_acls, _, _ = self._get_conf_changes(
-            logger, 'ACL', self.acls, new_dp.acls, diff=True)
+            logger, "ACL", self.acls, new_dp.acls, diff=True
+        )
         return added_acls.union(changed_acls)
 
     def _get_vlan_config_changes(self, logger, new_dp, changed_acls):
         """Detect any config changes to VLANs.
 
         Args:
             logger (ValveLogger): logger instance.
             new_dp (DP): new dataplane configuration.
             changed_acls (set): changed/added ACL IDs.
         Returns:
             changes (tuple) of:
                 deleted_vlans (set): deleted VLAN IDs.
                 changed_vlans (set): changed/added VLAN IDs.
         """
-        _, deleted_vlans, added_vlans, changed_vlans, same_vlans, _ = self._get_conf_changes(
-            logger, 'VLAN', self.vlans, new_dp.vlans,
-            diff=True, ignore_keys=frozenset(['acls_in']))
+        (
+            _,
+            deleted_vlans,
+            added_vlans,
+            changed_vlans,
+            same_vlans,
+            _,
+        ) = self._get_conf_changes(
+            logger,
+            "VLAN",
+            self.vlans,
+            new_dp.vlans,
+            diff=True,
+            ignore_keys=frozenset(["acls_in"]),
+        )
         changed_vlans = added_vlans.union(changed_vlans)
         # TODO: optimize for warm start.
         for vlan_id in same_vlans:
             old_vlan = self.vlans[vlan_id]
             new_vlan = new_dp.vlans[vlan_id]
             if self._acl_ref_changes(
-                    'VLAN %u' % vlan_id, old_vlan, new_vlan, changed_acls, logger):
+                "VLAN %u" % vlan_id, old_vlan, new_vlan, changed_acls, logger
+            ):
                 changed_vlans.add(vlan_id)
         return (deleted_vlans, changed_vlans)
 
     def _acl_ref_changes(self, conf_desc, old_conf, new_conf, changed_acls, logger):
         changed = False
         new_acl_ids = new_conf.acls_in
         conf_acls_changed = set()
@@ -1333,23 +1520,28 @@
             new_acl_ids = [acl._id for acl in new_acl_ids]
             conf_acls_changed = set(new_acl_ids).intersection(changed_acls)
         old_acl_ids = old_conf.acls_in
         if old_acl_ids:
             old_acl_ids = [acl._id for acl in old_acl_ids]
         if conf_acls_changed:
             changed = True
-            logger.info('%s ACL changed (ACL %s content changed)' % (
-                conf_desc, conf_acls_changed))
+            logger.info(
+                "%s ACL changed (ACL %s content changed)"
+                % (conf_desc, conf_acls_changed)
+            )
         elif (old_acl_ids or new_acl_ids) and old_acl_ids != new_acl_ids:
             changed = True
-            logger.info('%s ACL changed (ACL %s to %s)' % (
-                conf_desc, old_acl_ids, new_acl_ids))
+            logger.info(
+                "%s ACL changed (ACL %s to %s)" % (conf_desc, old_acl_ids, new_acl_ids)
+            )
         return changed
 
-    def _get_port_config_changes(self, logger, new_dp, changed_vlans, deleted_vlans, changed_acls):
+    def _get_port_config_changes(
+        self, logger, new_dp, changed_vlans, deleted_vlans, changed_acls
+    ):
         """Detect any config changes to ports.
 
         Args:
             logger (ValveLogger): logger instance.
             new_dp (DP): new dataplane configuration.
             changed_vlans (set): changed/added VLAN IDs.
             deleted_vlans (set): deleted VLAN IDs.
@@ -1359,48 +1551,63 @@
                 all_ports_changed (bool): True if all ports changed.
                 deleted_ports (set): deleted port numbers.
                 changed_ports (set): changed port numbers.
                 added_ports (set): added port numbers.
                 changed_acl_ports (set): changed ACL only port numbers.
                 changed_vlans (set): changed/added VLAN IDs.
         """
-        _, deleted_ports, added_ports, changed_ports, same_ports, _ = self._get_conf_changes(
-            logger, 'port', self.ports, new_dp.ports,
-            diff=True, ignore_keys=frozenset(['acls_in']))
+        (
+            _,
+            deleted_ports,
+            added_ports,
+            changed_ports,
+            same_ports,
+            _,
+        ) = self._get_conf_changes(
+            logger,
+            "port",
+            self.ports,
+            new_dp.ports,
+            diff=True,
+            ignore_keys=frozenset(["acls_in"]),
+        )
 
         changed_acl_ports = set()
         all_ports_changed = False
 
         topology_changed = False
         if self.stack:
             topology_changed = bool(self.stack.hash() != new_dp.stack.hash())
         if topology_changed:
             # Topology changed so restart stack ports just to be safe
             stack_ports = [
-                port.number for port in new_dp.stack_ports()
-                if port.number not in deleted_ports
-                and port.number not in added_ports]
+                port.number
+                for port in new_dp.stack_ports()
+                if port.number not in deleted_ports and port.number not in added_ports
+            ]
             changed_ports.update(set(stack_ports))
-            logger.info('Stack topology change detected, restarting stack ports')
+            logger.info("Stack topology change detected, restarting stack ports")
             same_ports -= changed_ports
 
         if not same_ports:
             all_ports_changed = True
         # TODO: optimize case where only VLAN ACL changed.
         elif changed_vlans:
             all_ports = frozenset(new_dp.ports.keys())
             new_changed_vlans = {
-                vlan for vlan in new_dp.vlans.values() if vlan.vid in changed_vlans}
+                vlan for vlan in new_dp.vlans.values() if vlan.vid in changed_vlans
+            }
             for vlan in new_changed_vlans:
                 changed_port_nums = {port.number for port in vlan.get_ports()}
                 changed_ports.update(changed_port_nums)
             all_ports_changed = changed_ports == all_ports
 
         # Detect changes to VLANs and ACLs based on port changes.
         if not all_ports_changed:
+
             def get_vids(vlans):
                 if not vlans:
                     return set()
                 if isinstance(vlans, Iterable):
                     return {vlan.vid for vlan in vlans}
                 return {vlans.vid}
 
@@ -1430,53 +1637,69 @@
             for port_no in added_ports:
                 port = new_dp.ports[port_no]
                 _add_changed_vlan_port(port, new_dp)
             for port_no in same_ports:
                 old_port = self.ports[port_no]
                 new_port = new_dp.ports[port_no]
                 if old_port.mirror != new_port.mirror:
-                    logger.info(f'port {port_no} mirror options changed: {new_port.mirror}')
+                    logger.info(
+                        "port %s mirror options changed: %s"
+                        % (port_no, new_port.mirror)
+                    )
                     changed_ports.add(port_no)
                 if self._acl_ref_changes(
-                        'port %u' % port_no, old_port, new_port, changed_acls, logger):
+                    "port %u" % port_no, old_port, new_port, changed_acls, logger
+                ):
                     changed_acl_ports.add(port_no)
 
             if changed_acl_ports:
                 same_ports -= changed_acl_ports
-                logger.info(f'ports where ACL only changed: {changed_acl_ports}')
+                logger.info("ports where ACL only changed: %s" % changed_acl_ports)
 
         same_ports -= changed_ports
         changed_vlans -= deleted_vlans
         # TODO: limit scope to only routers that have affected VLANs.
         changed_vlans_with_vips = []
         for vid in changed_vlans:
             vlan = new_dp.vlans[vid]
             if vlan.faucet_vips:
                 changed_vlans_with_vips.append(vlan)
         if changed_vlans_with_vips:
-            logger.info(f'forcing cold start because {changed_vlans_with_vips} has routing')
+            logger.info(
+                "forcing cold start because %s has routing" % changed_vlans_with_vips
+            )
             all_ports_changed = True
 
-        return (all_ports_changed, deleted_ports,
-                changed_ports, added_ports, changed_acl_ports,
-                changed_vlans)
+        return (
+            all_ports_changed,
+            deleted_ports,
+            changed_ports,
+            added_ports,
+            changed_acl_ports,
+            changed_vlans,
+        )
 
     def _get_meter_config_changes(self, logger, new_dp):
         """Detect any config changes to meters.
         Args:
             logger (ValveLogger): logger instance.
             new_dp (DP): new dataplane configuration.
         Returns:
             changes (tuple) of:
                 deleted_meters (set): deleted Meter IDs.
                 changed_meters (set): changed/added Meter IDs.
         """
-        (all_meters_changed, deleted_meters,
-         added_meters, changed_meters, _, _) = self._get_conf_changes(
-             logger, 'METERS', self.meters, new_dp.meters)
+        (
+            all_meters_changed,
+            deleted_meters,
+            added_meters,
+            changed_meters,
+            _,
+            _,
+        ) = self._get_conf_changes(logger, "METERS", self.meters, new_dp.meters)
 
         return (all_meters_changed, deleted_meters, added_meters, changed_meters)
 
     def get_config_changes(self, logger, new_dp):
         """Detect any config changes.
 
         Args:
@@ -1493,40 +1716,81 @@
                 changed_vlans (set): changed/added VLAN IDs.
                 all_ports_changed (bool): True if all ports changed.
                 all_meters_changed (bool): True if all meters changed
                 deleted_meters (set): deleted meter numbers
                 added_meters (set): Added meter numbers
                 changed_meters (set): changed/added meter numbers
         """
-        if new_dp.stack and self.stack and new_dp.stack.root_name != self.stack.root_name:
-            logger.info('Stack root change - requires cold start')
+        if (
+            new_dp.stack
+            and self.stack
+            and new_dp.stack.root_name != self.stack.root_name
+        ):
+            logger.info("Stack root change - requires cold start")
         elif new_dp.routers != self.routers:
-            logger.info('DP routers config changed - requires cold start')
+            logger.info("DP routers config changed - requires cold start")
         elif not self.ignore_subconf(
-                new_dp, ignore_keys=['interfaces', 'interface_ranges', 'routers']):
-            logger.info(f'DP config changed - requires cold start: {self.conf_diff(new_dp)}')
+            new_dp, ignore_keys=["interfaces", "interface_ranges", "routers"]
+        ):
+            logger.info(
+                "DP config changed - requires cold start: %s" % self.conf_diff(new_dp)
+            )
         else:
             changed_acls = self._get_acl_config_changes(logger, new_dp)
-            deleted_vlans, changed_vlans = self._get_vlan_config_changes(logger, new_dp, changed_acls)
-            (all_meters_changed, deleted_meters,
-             added_meters, changed_meters) = self._get_meter_config_changes(logger, new_dp)
-            (all_ports_changed, deleted_ports, changed_ports, added_ports,
-             changed_acl_ports, changed_vlans) = self._get_port_config_changes(
-                 logger, new_dp, changed_vlans, deleted_vlans, changed_acls)
-            return (deleted_ports, changed_ports, added_ports, changed_acl_ports,
-                    deleted_vlans, changed_vlans, all_ports_changed,
-                    all_meters_changed, deleted_meters,
-                    added_meters, changed_meters)
+            deleted_vlans, changed_vlans = self._get_vlan_config_changes(
+                logger, new_dp, changed_acls
+            )
+            (
+                all_meters_changed,
+                deleted_meters,
+                added_meters,
+                changed_meters,
+            ) = self._get_meter_config_changes(logger, new_dp)
+            (
+                all_ports_changed,
+                deleted_ports,
+                changed_ports,
+                added_ports,
+                changed_acl_ports,
+                changed_vlans,
+            ) = self._get_port_config_changes(
+                logger, new_dp, changed_vlans, deleted_vlans, changed_acls
+            )
+            return (
+                deleted_ports,
+                changed_ports,
+                added_ports,
+                changed_acl_ports,
+                deleted_vlans,
+                changed_vlans,
+                all_ports_changed,
+                all_meters_changed,
+                deleted_meters,
+                added_meters,
+                changed_meters,
+            )
         # default cold start
-        return (set(), set(), set(), set(), set(), set(), True, True, set(), set(), set())
+        return (
+            set(),
+            set(),
+            set(),
+            set(),
+            set(),
+            set(),
+            True,
+            True,
+            set(),
+            set(),
+            set(),
+        )
 
     def get_tables(self):
         """Return tables as dict for API call."""
-        return {
-            table_name: table.table_id for table_name, table in self.tables.items()}
+        return {table_name: table.table_id for table_name, table in self.tables.items()}
 
     def get_config_dict(self):
         """Return DP config as a dict for API call."""
         return {
-            'dps': {self.name: self.to_conf()},
-            'vlans': {vlan.name: vlan.to_conf() for vlan in self.vlans.values()},
-            'acls': {acl_id: acl.to_conf() for acl_id, acl in self.acls.items()}}
+            "dps": {self.name: self.to_conf()},
+            "vlans": {vlan.name: vlan.to_conf() for vlan in self.vlans.values()},
+            "acls": {acl_id: acl.to_conf() for acl_id, acl in self.acls.items()},
+        }
```

### Comparing `c65faucet-1.0.46/faucet/faucet.py` & `c65faucet-1.0.47/faucet/faucet.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/faucet_bgp.py` & `c65faucet-1.0.47/faucet/faucet_bgp.py`

 * *Files 4% similar despite different names*

```diff
@@ -30,15 +30,16 @@
 
     def __init__(self, dp_id, vlan_vid, ipv):
         self.dp_id = dp_id
         self.vlan_vid = vlan_vid
         self.ipv = ipv
 
     def __str__(self):
-        return f'BGP speaker key DP ID: {self.dp_id}, VLAN VID: {self.vlan_vid}, IP version: {self.ipv}'
+        return 'BGP speaker key DP ID: %u, VLAN VID: %u, IP version: %u' % (
+            self.dp_id, self.vlan_vid, self.ipv)
 
     def __repr__(self):
         return self.__str__()
 
     def __hash__(self):
         return hash(self.__str__())
 
@@ -76,19 +77,19 @@
         neighbor_states = []
         if bgp_speaker is not None:
             neighbor_states = bgp_speaker.neighbor_states()
         return neighbor_states
 
     @kill_on_exception(exc_logname)
     def _bgp_up_handler(self, remote_ip, remote_as):
-        self.logger.info(f'BGP peer router ID {remote_ip} AS {remote_as} up')
+        self.logger.info('BGP peer router ID %s AS %s up' % (remote_ip, remote_as))
 
     @kill_on_exception(exc_logname)
     def _bgp_down_handler(self, remote_ip, remote_as):
-        self.logger.info(f'BGP peer router ID {remote_ip} AS {remote_as} down')
+        self.logger.info('BGP peer router ID %s AS %s down' % (remote_ip, remote_as))
         # TODO: delete RIB routes for down peer.
 
     @kill_on_exception(exc_logname)
     def _bgp_route_handler(self, path_change, bgp_speaker_key):
         """Handle a BGP change event.
 
         Args:
@@ -96,28 +97,28 @@
         """
         dp_id = bgp_speaker_key.dp_id
         vlan_vid = bgp_speaker_key.vlan_vid
         valve, vlan = self._valve_vlan(dp_id, vlan_vid)
         if vlan is None:
             return
         prefix = ipaddress.ip_network(str(path_change.prefix))
-        route_str = f'BGP route {prefix}'
+        route_str = 'BGP route %s' % prefix
 
         if path_change.next_hop:
             nexthop = ipaddress.ip_address(str(path_change.next_hop))
-            route_str = f'BGP route {prefix} nexthop {nexthop}'
+            route_str = 'BGP route %s nexthop %s' % (prefix, nexthop)
 
             if vlan.is_faucet_vip(nexthop):
                 self.logger.error(
-                    f'Skipping {route_str} because nexthop cannot be us')
+                    'Skipping %s because nexthop cannot be us' % route_str)
                 return
 
             if valve.router_vlan_for_ip_gw(vlan, nexthop) is None:
                 self.logger.info(
-                    f'Skipping {route_str} because nexthop not in {vlan}')
+                    'Skipping %s because nexthop not in %s' % (route_str, vlan))
                 return
 
         if bgp_speaker_key not in self._dp_bgp_rib:
             self._dp_bgp_rib[bgp_speaker_key] = {}
 
         flowmods = []
         if path_change.is_withdraw:
@@ -174,26 +175,26 @@
         """Shutdown any active BGP speakers."""
         for bgp_speaker in self._dp_bgp_speakers.values():
             bgp_speaker.shutdown()
         self._dp_bgp_speakers = {}
 
     def _add_bgp_speaker(self, valve, bgp_speaker_key, bgp_router):
         if bgp_speaker_key in self._dp_bgp_speakers:
-            self.logger.info(f'Skipping re/configuration of existing {bgp_speaker_key}')
+            self.logger.info('Skipping re/configuration of existing %s' % bgp_speaker_key)
             bgp_speaker = self._dp_bgp_speakers[bgp_speaker_key]
             if bgp_speaker_key in self._dp_bgp_rib:
                 # Re-add routes (to avoid flapping BGP even when VLAN cold starts).
                 for prefix, nexthop in self._dp_bgp_rib[bgp_speaker_key].items():
-                    self.logger.info(f'Re-adding {prefix} via {nexthop}')
+                    self.logger.info('Re-adding %s via %s' % (prefix, nexthop))
                     bgp_vlan = bgp_router.bgp_vlan()
                     flowmods = valve.add_route(bgp_vlan, nexthop, prefix)
                     if flowmods:
                         self._send_flow_msgs(valve, flowmods)
         else:
-            self.logger.info(f'Adding {bgp_speaker_key}')
+            self.logger.info('Adding %s' % bgp_speaker_key)
             bgp_speaker = self._create_bgp_speaker_for_vlan(bgp_speaker_key, bgp_router)
         return {bgp_speaker_key: bgp_speaker}
 
     def _add_valve_bgp_speakers(self, valve):
         bgp_speakers = {}
         bgp_routers = valve.dp.bgp_routers()
         if bgp_routers:
```

### Comparing `c65faucet-1.0.46/faucet/faucet_dot1x.py` & `c65faucet-1.0.47/faucet/faucet_dot1x.py`

 * *Files 2% similar despite different names*

```diff
@@ -88,18 +88,18 @@
         auth_acl = datapath.acls.get(self._auth_acl_name)
         noauth_acl = datapath.acls.get(self._noauth_acl_name)
         return (auth_acl, noauth_acl)
 
     # Loggin Methods
     def log_auth_event(self, valve, port_num, mac_str, status):
         """Log an authentication attempt event"""
-        self.metrics.inc_var(f'dp_dot1x_{status}', valve.dp.base_prom_labels())
-        self.metrics.inc_var(f'port_dot1x_{status}', valve.dp.port_labels(port_num))
+        self.metrics.inc_var('dp_dot1x_{}'.format(status), valve.dp.base_prom_labels())
+        self.metrics.inc_var('port_dot1x_{}'.format(status), valve.dp.port_labels(port_num))
         self.logger.info(
-            f'{status.capitalize()} from MAC {mac_str} on {port_num}')
+            '{} from MAC {} on {}'.format(status.capitalize(), mac_str, port_num))
         valve.dot1x_event({'AUTHENTICATION': {'dp_id': valve.dp.dp_id,
                                               'port': port_num,
                                               'eth_src': mac_str,
                                               'status': status}})
 
     def log_port_event(self, event_type, port_type, valve, port_num):
         """Log a dot1x port event"""
@@ -310,15 +310,16 @@
             radius_ip, radius_port, radius_secret)
 
         for valve_index, valve in enumerate(dot1x_valves, start=0):
             self.dp_id_to_valve_index[valve.dp.dp_id] = valve_index
             for dot1x_port in valve.dp.dot1x_ports():
                 self.set_mac_str(valve, valve_index, dot1x_port.number)
                 self.logger.info(
-                    f'dot1x enabled on {valve.dp} ({valve_index}) port {dot1x_port}, NFV interface {dot1x_intf}')
+                    'dot1x enabled on %s (%s) port %s, NFV interface %s' % (
+                        valve.dp, valve_index, dot1x_port, dot1x_intf))
 
             valve.dot1x_event({'ENABLED': {'dp_id': valve.dp.dp_id}})
 
     def _get_logoff_flowmod(self, dot1x_port, valve, mac_str):
         """Return flowmods required to logoff port"""
         flowmods = []
         flowmods.extend(
@@ -342,21 +343,23 @@
         """Return flowmods for successful authentication on port"""
         port_num = dot1x_port.number
         flowmods = []
         acl_manager = valve.acl_manager
 
         acl = valve.dp.acls.get(acl_name, None)
         if dot1x_port.dot1x_dyn_acl and acl:
-            self.logger.info(f"DOT1X_DYN_ACL: Adding ACL '{acl_name}' for port '{port_num}'")
-            self.logger.debug(f"DOT1X_DYN_ACL: ACL contents: '{str(acl.__dict__)}'")
+            self.logger.info("DOT1X_DYN_ACL: Adding ACL '{0}' for port '{1}'".format(
+                acl_name, port_num))
+            self.logger.debug("DOT1X_DYN_ACL: ACL contents: '{0}'".format(str(acl.__dict__)))
             flowmods.extend(acl_manager.add_port_acl(acl, port_num, mac_str))
         elif dot1x_port.dot1x_acl:
             auth_acl, _ = self._get_acls(valve.dp)
-            self.logger.info(f"DOT1X_PRE_ACL: Adding ACL '{acl_name}' for port '{port_num}'")
-            self.logger.debug(f"DOT1X_PRE_ACL: ACL contents: '{str(auth_acl.__dict__)}'")
+            self.logger.info("DOT1X_PRE_ACL: Adding ACL '{0}' for port '{1}'".format(
+                acl_name, port_num))
+            self.logger.debug("DOT1X_PRE_ACL: ACL contents: '{0}'".format(str(auth_acl.__dict__)))
             flowmods.extend(acl_manager.add_port_acl(auth_acl, port_num, mac_str))
         else:
             flowmods.extend(acl_manager.add_authed_mac(port_num, mac_str))
 
         if vlan_name:
             flowmods.extend(valve.add_dot1x_native_vlan(port_num, vlan_name))
         return flowmods
```

### Comparing `c65faucet-1.0.46/faucet/faucet_event.py` & `c65faucet-1.0.47/faucet/faucet_event.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/faucet_metrics.py` & `c65faucet-1.0.47/faucet/faucet_metrics.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/faucet_pipeline.py` & `c65faucet-1.0.47/faucet/faucet_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -45,18 +45,18 @@
             assert isinstance(next_tables, (list, tuple))
             self.next_tables = next_tables
         else:
             self.next_tables = ()
 
     def __str__(self):
         field_strs = ' '.join([
-            f'{key}: {val}'
+            '%s: %s' % (key, val)
             for key, val in sorted(self.__dict__.items())
             if val])
-        return f'table config {field_strs}'
+        return 'table config %s' % field_strs
 
     def __repr__(self):
         return self.__str__()
 
     def __hash__(self):
         return hash(self.__str__())
 
@@ -69,17 +69,17 @@
 
 _NEXT_ETH = ('eth_dst_hairpin', 'eth_dst', 'flood')
 _NEXT_VIP = ('vip',) + _NEXT_ETH
 
 
 def _fib_table(ipv, table_id):
     return ValveTableConfig(
-        f'ipv{ipv}_fib',
+        'ipv%u_fib' % ipv,
         table_id,
-        match_types=(('eth_type', False), (f'ipv{ipv}_dst', True), ('vlan_vid', False)),
+        match_types=(('eth_type', False), ('ipv%u_dst' % ipv, True), ('vlan_vid', False)),
         set_fields=('eth_dst', 'eth_src', 'vlan_vid'),
         dec_ttl=True,
         vlan_port_scale=3.1,
         next_tables=_NEXT_VIP
     )
```

### Comparing `c65faucet-1.0.46/faucet/fctl.py` & `c65faucet-1.0.47/faucet/fctl.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/gauge.py` & `c65faucet-1.0.47/faucet/gauge.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/gauge_influx.py` & `c65faucet-1.0.47/faucet/gauge_influx.py`

 * *Files 2% similar despite different names*

```diff
@@ -43,21 +43,21 @@
                     password=self.conf.influx_pwd,
                     database=self.conf.influx_db,
                     timeout=self.conf.influx_timeout)
                 if client:
                     if client.write_points(points=points, time_precision='s'):
                         return True
                     self.logger.warning(
-                        f'{self.ship_error_prefix} failed to update InfluxDB')
+                        '%s failed to update InfluxDB' % self.ship_error_prefix)
                 else:
                     self.logger.warning(
-                        f'{self.ship_error_prefix} error connecting to InfluxDB')
+                        '%s error connecting to InfluxDB' % self.ship_error_prefix)
             except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout,
                     InfluxDBClientError, InfluxDBServerError) as err:
-                self.logger.warning(f'{self.ship_error_prefix} {err}')
+                self.logger.warning('%s %s' % (self.ship_error_prefix, err))
         return False
 
     @staticmethod
     def make_point(tags, rcv_time, stat_name, stat_val):
         """Make an InfluxDB point."""
         # InfluxDB has only one integer type, int64. We are logging OF
         # stats that are uint64. Use float64 to prevent an overflow.
```

### Comparing `c65faucet-1.0.46/faucet/gauge_pollers.py` & `c65faucet-1.0.47/faucet/gauge_pollers.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/gauge_prom.py` & `c65faucet-1.0.47/faucet/gauge_prom.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/meter.py` & `c65faucet-1.0.47/faucet/meter.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/port.py` & `c65faucet-1.0.47/faucet/port.py`

 * *Files 5% similar despite different names*

```diff
@@ -30,201 +30,201 @@
 # Stack port up and running, no timeouts, no incorrect cabling
 STACK_STATE_UP = 3
 # Stack timed out, too many packets lost
 STACK_STATE_GONE = 4
 # Not stack port configured
 STACK_STATE_NONE = -1
 STACK_DISPLAY_DICT = {
-    STACK_STATE_ADMIN_DOWN: 'ADMIN_DOWN',
-    STACK_STATE_INIT: 'INITIALIZING',
-    STACK_STATE_BAD: 'BAD',
-    STACK_STATE_UP: 'UP',
-    STACK_STATE_GONE: 'GONE',
-    STACK_STATE_NONE: 'NONE'
+    STACK_STATE_ADMIN_DOWN: "ADMIN_DOWN",
+    STACK_STATE_INIT: "INITIALIZING",
+    STACK_STATE_BAD: "BAD",
+    STACK_STATE_UP: "UP",
+    STACK_STATE_GONE: "GONE",
+    STACK_STATE_NONE: "NONE",
 }
 
 # LACP not configured
 LACP_ACTOR_NOTCONFIGURED = -1
 # Not receiving packets from the actor & port is down
 LACP_ACTOR_NONE = 0
 # Not receiving packets from the actor & port is up
 LACP_ACTOR_INIT = 1
 # Receiving LACP packets with sync bit set
 LACP_ACTOR_UP = 3
 # LACP actor is not sending LACP with sync bit set
 LACP_ACTOR_NOSYNC = 5
 LACP_ACTOR_DISPLAY_DICT = {
-    LACP_ACTOR_NOTCONFIGURED: 'NOT_CONFIGURED',
-    LACP_ACTOR_NONE: 'NONE',
-    LACP_ACTOR_INIT: 'INITIALIZING',
-    LACP_ACTOR_UP: 'UP',
-    LACP_ACTOR_NOSYNC: 'NO_SYNC'
+    LACP_ACTOR_NOTCONFIGURED: "NOT_CONFIGURED",
+    LACP_ACTOR_NONE: "NONE",
+    LACP_ACTOR_INIT: "INITIALIZING",
+    LACP_ACTOR_UP: "UP",
+    LACP_ACTOR_NOSYNC: "NO_SYNC",
 }
 
 # LACP is not configured
 LACP_PORT_NOTCONFIGURED = -1
 # Port is not a LACP port on the nominated DP
 LACP_PORT_UNSELECTED = 1
 # Port is a LACP port on the nominated DP, will send/receive
 LACP_PORT_SELECTED = 2
 # Port is a LACP port that is in standby
 LACP_PORT_STANDBY = 3
 LACP_PORT_DISPLAY_DICT = {
-    LACP_PORT_NOTCONFIGURED: 'NOT_CONFIGURED',
-    LACP_PORT_UNSELECTED: 'UNSELECTED',
-    LACP_PORT_SELECTED: 'SELECTED',
-    LACP_PORT_STANDBY: 'STANDBY'
+    LACP_PORT_NOTCONFIGURED: "NOT_CONFIGURED",
+    LACP_PORT_UNSELECTED: "UNSELECTED",
+    LACP_PORT_SELECTED: "SELECTED",
+    LACP_PORT_STANDBY: "STANDBY",
 }
 
 
 class Port(Conf):
     """Stores state for ports, including the configuration."""
 
     defaults = {
-        'number': None,
-        'name': None,
-        'description': None,
-        'enabled': True,
-        'permanent_learn': False,
+        "number": None,
+        "name": None,
+        "description": None,
+        "enabled": True,
+        "permanent_learn": False,
         # if True, a host once learned on this port cannot be learned on another port.
-        'unicast_flood': True,
+        "unicast_flood": True,
         # if True, do classical unicast flooding on this port (False floods ND/ARP/bcast only).
-        'mirror': None,
+        "mirror": None,
         # If set, mirror packets from that port to this one.
-        'native_vlan': None,
+        "native_vlan": None,
         # Set untagged VLAN on this port.
-        'tagged_vlans': None,
+        "tagged_vlans": None,
         # Set tagged VLANs on this port.
-        'acl_in': None,
-        'acls_in': None,
+        "acl_in": None,
+        "acls_in": None,
         # ACL for input on this port.
-        'stack': None,
+        "stack": None,
         # Configure a stack peer on this port.
-        'max_hosts': 255,
+        "max_hosts": 255,
         # maximum number of hosts
-        'hairpin': False,
+        "hairpin": False,
         # if True, then switch unicast and flood between hosts on this port (eg WiFi radio).
-        'hairpin_unicast': False,
+        "hairpin_unicast": False,
         # if True, then switch unicast between hosts on this port (eg WiFi radio).
-        'lacp': 0,
+        "lacp": 0,
         # if non 0 (LAG ID), experimental LACP support enabled on this port.
-        'lacp_active': False,
+        "lacp_active": False,
         # experimental active LACP
-        'lacp_collect_and_distribute': False,
+        "lacp_collect_and_distribute": False,
         # if true, forces LACP port to collect and distribute when syncing with the peer.
-        'lacp_unselected': False,
+        "lacp_unselected": False,
         # if true, forces LACP port to be in unselected state
-        'lacp_selected': False,
+        "lacp_selected": False,
         # if true, forces LACP port to be in the selected state
-        'lacp_standby': False,
+        "lacp_standby": False,
         # if true, forces LACP port to be in the standby state
-        'lacp_passthrough': None,
+        "lacp_passthrough": None,
         # If set, fail the lacp on this port if any of the peer ports are down.
-        'lacp_resp_interval': 1,
+        "lacp_resp_interval": 1,
         # Min time since last LACP response. Used to control rate of response for LACP
-        'lacp_port_priority': 255,
+        "lacp_port_priority": 255,
         # Sets port priority value sent out in lacp packet
-        'lacp_port_id': -1,
+        "lacp_port_id": -1,
         # Sets port id value sent out in lacp packet
-        'loop_protect': False,
+        "loop_protect": False,
         # if True, do simple (host/access port) loop protection on this port.
-        'loop_protect_external': False,
+        "loop_protect_external": False,
         # if True, do external (other switch) loop protection on this port.
-        'output_only': False,
+        "output_only": False,
         # if True, all packets input from this port are dropped.
-        'lldp_beacon': {},
+        "lldp_beacon": {},
         # LLDP beacon configuration for this port.
-        'opstatus_reconf': True,
+        "opstatus_reconf": True,
         # If True, configure pipeline if operational status of port changes.
-        'receive_lldp': False,
+        "receive_lldp": False,
         # If True, receive LLDP on this port.
-        'lldp_peer_mac': None,
+        "lldp_peer_mac": None,
         # If set, validates src MAC address of incoming LLDP packets
-        'max_lldp_lost': 3,
+        "max_lldp_lost": 3,
         # threshold before marking a stack port as down
-        'dot1x': False,
+        "dot1x": False,
         # If true, block this port until a successful 802.1x auth
-        'dot1x_acl': False,
+        "dot1x_acl": False,
         # If true, expects authentication and default ACLs for 802.1x auth
-        'dot1x_mab': False,
+        "dot1x_mab": False,
         # If true, allows Mac Auth Bypass on port (NOTE: this is less secure as MACs can be spoofed)
-        'dot1x_dyn_acl': False,
+        "dot1x_dyn_acl": False,
         # If true, expects authentication and ACLs with dot1x_assigned flag set
-        'restricted_bcast_arpnd': False,
+        "restricted_bcast_arpnd": False,
         # If true, this port cannot send non-ARP/IPv6 ND broadcasts to other restricted_bcast_arpnd ports.
-        'coprocessor': {},
+        "coprocessor": {},
         # If defined, this port is attached to a packet coprocessor.
-        'count_untag_vlan_miss': False,
+        "count_untag_vlan_miss": False,
         # If defined, this port will explicitly count unconfigured native VLAN packets.
     }
 
     defaults_types = {
-        'number': int,
-        'name': str,
-        'description': str,
-        'enabled': bool,
-        'permanent_learn': bool,
-        'unicast_flood': bool,
-        'mirror': (list, str, int),
-        'native_vlan': (str, int),
-        'tagged_vlans': list,
-        'acl_in': (str, int),
-        'acls_in': list,
-        'stack': dict,
-        'max_hosts': int,
-        'hairpin': bool,
-        'hairpin_unicast': bool,
-        'lacp': int,
-        'lacp_active': bool,
-        'lacp_collect_and_distribute': bool,
-        'lacp_unselected': bool,
-        'lacp_selected': bool,
-        'lacp_standby': bool,
-        'lacp_passthrough': list,
-        'lacp_resp_interval': int,
-        'lacp_port_priority': int,
-        'lacp_port_id': int,
-        'loop_protect': bool,
-        'loop_protect_external': bool,
-        'output_only': bool,
-        'lldp_beacon': dict,
-        'opstatus_reconf': bool,
-        'receive_lldp': bool,
-        'lldp_peer_mac': str,
-        'dot1x': bool,
-        'dot1x_acl': bool,
-        'dot1x_mab': bool,
-        'dot1x_dyn_acl': bool,
-        'max_lldp_lost': int,
-        'restricted_bcast_arpnd': bool,
-        'coprocessor': dict,
-        'count_untag_vlan_miss': bool,
+        "number": int,
+        "name": str,
+        "description": str,
+        "enabled": bool,
+        "permanent_learn": bool,
+        "unicast_flood": bool,
+        "mirror": (list, str, int),
+        "native_vlan": (str, int),
+        "tagged_vlans": list,
+        "acl_in": (str, int),
+        "acls_in": list,
+        "stack": dict,
+        "max_hosts": int,
+        "hairpin": bool,
+        "hairpin_unicast": bool,
+        "lacp": int,
+        "lacp_active": bool,
+        "lacp_collect_and_distribute": bool,
+        "lacp_unselected": bool,
+        "lacp_selected": bool,
+        "lacp_standby": bool,
+        "lacp_passthrough": list,
+        "lacp_resp_interval": int,
+        "lacp_port_priority": int,
+        "lacp_port_id": int,
+        "loop_protect": bool,
+        "loop_protect_external": bool,
+        "output_only": bool,
+        "lldp_beacon": dict,
+        "opstatus_reconf": bool,
+        "receive_lldp": bool,
+        "lldp_peer_mac": str,
+        "dot1x": bool,
+        "dot1x_acl": bool,
+        "dot1x_mab": bool,
+        "dot1x_dyn_acl": bool,
+        "max_lldp_lost": int,
+        "restricted_bcast_arpnd": bool,
+        "coprocessor": dict,
+        "count_untag_vlan_miss": bool,
     }
 
     stack_defaults_types = {
-        'dp': str,
-        'port': (str, int),
+        "dp": str,
+        "port": (str, int),
     }
 
     lldp_beacon_defaults_types = {
-        'enable': bool,
-        'org_tlvs': list,
-        'system_name': str,
-        'port_descr': str,
+        "enable": bool,
+        "org_tlvs": list,
+        "system_name": str,
+        "port_descr": str,
     }
 
     lldp_org_tlv_defaults_types = {
-        'oui': (int, bytearray),
-        'subtype': (int, bytearray),
-        'info': (str, bytearray)
+        "oui": (int, bytearray),
+        "subtype": (int, bytearray),
+        "info": (str, bytearray),
     }
 
     coprocessor_defaults_types = {
-        'strategy': str,
-        'vlan_vid_base': int,
+        "strategy": str,
+        "vlan_vid_base": int,
     }
 
     def __init__(self, _id, dp_id, conf=None):
         self.acl_in = None
         self.acls_in = None
         self.description = None
         self.dot1x = None
@@ -286,15 +286,15 @@
         super().__init__(_id, dp_id, conf)
 
         # If the port is mirrored convert single attributes to a array
         if self.mirror and not isinstance(self.mirror, list):
             self.mirror = [self.mirror]
 
     def __str__(self):
-        return f'Port {self.number}'
+        return "Port %u" % self.number
 
     def __repr__(self):
         return self.__str__()
 
     def clone_dyn_state(self, prev_port):
         if prev_port:
             self.dyn_lldp_beacon_recv_time = prev_port.dyn_lldp_beacon_recv_time
@@ -302,157 +302,210 @@
             self.dyn_stack_current_state = prev_port.dyn_stack_current_state
             self.dyn_last_lldp_beacon_time = prev_port.dyn_last_lldp_beacon_time
             self.dyn_phys_up = prev_port.dyn_phys_up
             self.dyn_stack_probe_info = prev_port.dyn_stack_probe_info
             self.dyn_update_time = prev_port.dyn_update_time
 
     def stack_descr(self):
-        """"Return stacking annotation if this is a stacking port."""
+        """ "Return stacking annotation if this is a stacking port."""
         if self.stack:
-            return f'remote DP {self.stack["dp"].name} {self.stack["port"]}'
-        return ''
+            return "remote DP %s %s" % (self.stack["dp"].name, self.stack["port"])
+        return ""
 
     def set_defaults(self):
         super().set_defaults()
-        self._set_default('number', self._id)
-        self._set_default('name', str(self._id))
-        self._set_default('description', self.name)
-        self._set_default('tagged_vlans', [])
+        self._set_default("number", self._id)
+        self._set_default("name", str(self._id))
+        self._set_default("description", self.name)
+        self._set_default("tagged_vlans", [])
 
     def check_config(self):
         super().check_config()
-        test_config_condition(not (isinstance(self.number, int) and self.number > 0 and (
-            not valve_of.ignore_port(self.number))), (f'Port number invalid: {self.number}'))
-        non_vlan_options = {'stack', 'mirror', 'coprocessor', 'output_only'}
-        vlan_agnostic_options = {'enabled', 'number', 'name', 'description', 'max_lldp_lost'}
+        test_config_condition(
+            not (
+                isinstance(self.number, int)
+                and self.number > 0
+                and (not valve_of.ignore_port(self.number))
+            ),
+            ("Port number invalid: %s" % self.number),
+        )
+        non_vlan_options = {"stack", "mirror", "coprocessor", "output_only"}
+        vlan_agnostic_options = {
+            "enabled",
+            "number",
+            "name",
+            "description",
+            "max_lldp_lost",
+        }
         vlan_port = self.tagged_vlans or self.native_vlan
-        non_vlan_port_options = {option for option in non_vlan_options if getattr(self, option)}
+        non_vlan_port_options = {
+            option for option in non_vlan_options if getattr(self, option)
+        }
         test_config_condition(
             vlan_port and non_vlan_port_options,
-            f'cannot have VLANs configured on non-VLAN ports: {self}')
+            "cannot have VLANs configured on non-VLAN ports: %s" % self,
+        )
         if self.output_only:
             test_config_condition(
-                not non_vlan_port_options.issubset({'mirror', 'output_only'}),
-                f'output_only can only coexist with mirror option on same port {self}')
+                not non_vlan_port_options.issubset({"mirror", "output_only"}),
+                "output_only can only coexist with mirror option on same port %s"
+                % self,
+            )
         elif self.mirror:
             test_config_condition(
-                not non_vlan_port_options.issubset({'mirror', 'coprocessor'}),
-                f'coprocessor can only coexist with mirror option on same port {self}')
+                not non_vlan_port_options.issubset({"mirror", "coprocessor"}),
+                "coprocessor can only coexist with mirror option on same port %s"
+                % self,
+            )
         else:
             test_config_condition(
                 len(non_vlan_port_options) > 1,
-                f'cannot have multiple non-VLAN port options {non_vlan_port_options} on same port: {self}')
+                "cannot have multiple non-VLAN port options %s on same port: %s"
+                % (non_vlan_port_options, self),
+            )
         if non_vlan_port_options:
             for key, default_val in self.defaults.items():
                 if key in vlan_agnostic_options or key in non_vlan_port_options:
                     continue
-                if key.startswith('acl') and (self.stack or self.coprocessor):
+                if key.startswith("acl") and (self.stack or self.coprocessor):
                     continue
                 val = getattr(self, key)
                 test_config_condition(
                     val != default_val and val,
-                    f'Cannot have VLAN option {key}: {val} on non-VLAN port {self}')
+                    "Cannot have VLAN option %s: %s on non-VLAN port %s"
+                    % (key, val, self),
+                )
         test_config_condition(
             self.hairpin and self.hairpin_unicast,
-            'Cannot have both hairpin and hairpin_unicast enabled')
+            "Cannot have both hairpin and hairpin_unicast enabled",
+        )
         dot1x_features = {
-            dot1x_feature for dot1x_feature, dot1x_config in self.defaults.items()
-            if dot1x_feature.startswith('dot1x_') and dot1x_config}
+            dot1x_feature
+            for dot1x_feature, dot1x_config in self.defaults.items()
+            if dot1x_feature.startswith("dot1x_") and dot1x_config
+        }
         test_config_condition(
             dot1x_features and not self.dot1x,
-            f'802.1x features {dot1x_features} require port to have dot1x enabled')
+            "802.1x features %s require port to have dot1x enabled" % dot1x_features,
+        )
         if self.dot1x:
-            test_config_condition(self.number > 65535, (
-                '802.1x not supported on ports > 65535'))
+            test_config_condition(
+                self.number > 65535, ("802.1x not supported on ports > 65535")
+            )
             if self.dot1x_mab:
-                test_config_condition(self.dot1x_dyn_acl, (
-                    '802.1x_MAB cannot be used with 802.1x_DYN_ACL'))
+                test_config_condition(
+                    self.dot1x_dyn_acl,
+                    ("802.1x_MAB cannot be used with 802.1x_DYN_ACL"),
+                )
             if self.dot1x_dyn_acl:
-                test_config_condition(self.dot1x_acl, (
-                    '802.1x_DYN_ACL cannot be used with 802.1x_ACL'))
+                test_config_condition(
+                    self.dot1x_acl, ("802.1x_DYN_ACL cannot be used with 802.1x_ACL")
+                )
         if self.coprocessor:
             self._check_conf_types(self.coprocessor, self.coprocessor_defaults_types)
             test_config_condition(
-                self.coprocessor.get('strategy', None) != 'vlan_vid',
-                'coprocessor only supports vlan_vid strategy')
-            self.coprocessor['vlan_vid_base'] = self.coprocessor.get('vlan_vid_base', 1000)
+                self.coprocessor.get("strategy", None) != "vlan_vid",
+                "coprocessor only supports vlan_vid strategy",
+            )
+            self.coprocessor["vlan_vid_base"] = self.coprocessor.get(
+                "vlan_vid_base", 1000
+            )
         if self.stack:
             self._check_conf_types(self.stack, self.stack_defaults_types)
             for stack_config in list(self.stack_defaults_types.keys()):
-                test_config_condition(stack_config not in self.stack, (
-                    f'stack {stack_config} must be defined'))
+                test_config_condition(
+                    stack_config not in self.stack,
+                    ("stack %s must be defined" % stack_config),
+                )
             # LLDP always enabled for stack ports.
             self.receive_lldp = True
             if not self.lldp_beacon_enabled():
-                self.lldp_beacon.update({'enable': True})
+                self.lldp_beacon.update({"enable": True})
         if self.lacp_resp_interval is not None:
             test_config_condition(
                 self.lacp_resp_interval > 65535 or self.lacp_resp_interval < 0.3,
-                ('interval must be at least 0.3 and less than 65536'))
+                ("interval must be at least 0.3 and less than 65536"),
+            )
         if self.lacp_port_priority is not None:
             test_config_condition(
                 self.lacp_port_priority > 255 or self.lacp_port_priority < 0,
-                ('lacp port priority must be at least 0 and less than 256'))
+                ("lacp port priority must be at least 0 and less than 256"),
+            )
         if self.lldp_peer_mac:
-            test_config_condition(not netaddr.valid_mac(self.lldp_peer_mac), (
-                f'invalid MAC address {self.lldp_peer_mac}'))
+            test_config_condition(
+                not netaddr.valid_mac(self.lldp_peer_mac),
+                ("invalid MAC address %s" % self.lldp_peer_mac),
+            )
         if self.lldp_beacon:
-            self._check_conf_types(
-                self.lldp_beacon, self.lldp_beacon_defaults_types)
+            self._check_conf_types(self.lldp_beacon, self.lldp_beacon_defaults_types)
             self.lldp_beacon = self._set_unknown_conf(
-                self.lldp_beacon, self.lldp_beacon_defaults_types)
+                self.lldp_beacon, self.lldp_beacon_defaults_types
+            )
             if self.lldp_beacon_enabled():
-                if self.lldp_beacon['port_descr'] is None:
-                    self.lldp_beacon['port_descr'] = self.description
+                if self.lldp_beacon["port_descr"] is None:
+                    self.lldp_beacon["port_descr"] = self.description
                 org_tlvs = []
-                for org_tlv in self.lldp_beacon['org_tlvs']:
+                for org_tlv in self.lldp_beacon["org_tlvs"]:
                     self._check_conf_types(org_tlv, self.lldp_org_tlv_defaults_types)
-                    test_config_condition(len(org_tlv) != len(self.lldp_org_tlv_defaults_types), (
-                        'missing org_tlv config'))
-                    if not isinstance(org_tlv['info'], bytearray):
+                    test_config_condition(
+                        len(org_tlv) != len(self.lldp_org_tlv_defaults_types),
+                        ("missing org_tlv config"),
+                    )
+                    if not isinstance(org_tlv["info"], bytearray):
                         try:
-                            org_tlv['info'] = bytearray.fromhex(
-                                org_tlv['info'])  # pytype: disable=missing-parameter
+                            org_tlv["info"] = bytearray.fromhex(
+                                org_tlv["info"]
+                            )  # pytype: disable=missing-parameter
                         except ValueError:
-                            org_tlv['info'] = org_tlv['info'].encode('utf-8')
-                    if not isinstance(org_tlv['oui'], bytearray):
-                        org_tlv['oui'] = bytearray.fromhex(
-                            '%6.6x' % org_tlv['oui'])  # pytype: disable=missing-parameter
+                            org_tlv["info"] = org_tlv["info"].encode("utf-8")
+                    if not isinstance(org_tlv["oui"], bytearray):
+                        org_tlv["oui"] = bytearray.fromhex(
+                            "%6.6x" % org_tlv["oui"]
+                        )  # pytype: disable=missing-parameter
                     org_tlvs.append(org_tlv)
-                self.lldp_beacon['org_tlvs'] = org_tlvs
+                self.lldp_beacon["org_tlvs"] = org_tlvs
         test_config_condition(
             self.acl_in and self.acls_in,
-            'Found both acl_in and acls_in, use only acls_in')
+            "Found both acl_in and acls_in, use only acls_in",
+        )
         if self.acl_in and not isinstance(self.acl_in, list):
             self.acls_in = [self.acl_in]
             self.acl_in = None
         if self.acls_in:
             for acl in self.acls_in:
-                test_config_condition(not isinstance(acl, (int, str)),
-                                      'ACL names must be int or str')
+                test_config_condition(
+                    not isinstance(acl, (int, str)), "ACL names must be int or str"
+                )
         lacp_options = [self.lacp_selected, self.lacp_unselected, self.lacp_standby]
         test_config_condition(
             lacp_options.count(True) > 1,
-            'Cannot force multiple LACP port selection states')
+            "Cannot force multiple LACP port selection states",
+        )
 
     def finalize(self):
         if self.native_vlan:
-            test_config_condition(self.native_vlan in self.tagged_vlans, (
-                'cannot have same native and tagged VLAN on same port'))
+            test_config_condition(
+                self.native_vlan in self.tagged_vlans,
+                ("cannot have same native and tagged VLAN on same port"),
+            )
         self.tagged_vlans = tuple(self.tagged_vlans)
         super().finalize()
 
     def running(self):
         """Return True if port enabled and up."""
         return self.enabled and self.dyn_phys_up
 
     def vlans(self):
         """Return all VLANs this port is in."""
         if self.native_vlan is not None and self.dyn_dot1x_native_vlan is not None:
-            return (self.native_vlan,) + (self.dyn_dot1x_native_vlan,) + tuple(self.tagged_vlans)
+            return (
+                (self.native_vlan,)
+                + (self.dyn_dot1x_native_vlan,)
+                + tuple(self.tagged_vlans)
+            )
         if self.dyn_dot1x_native_vlan is not None:
             return (self.dyn_dot1x_native_vlan,) + tuple(self.tagged_vlans)
         if self.native_vlan is not None:
             return (self.native_vlan,) + tuple(self.tagged_vlans)
         return tuple(self.tagged_vlans)
 
     def hosts(self, vlans=None):
@@ -471,15 +524,15 @@
         hosts_count = 0
         for vlan in vlans:
             hosts_count += vlan.cached_hosts_count_on_port(self)
         return hosts_count
 
     def lldp_beacon_enabled(self):
         """Return True if LLDP beacon enabled on this port."""
-        return self.lldp_beacon and self.lldp_beacon.get('enable', False)
+        return self.lldp_beacon and self.lldp_beacon.get("enable", False)
 
     def mirror_actions(self):
         """Return OF actions to mirror this port."""
         if self.mirror is not None:
             return [valve_of.output_port(mirror_port) for mirror_port in self.mirror]
         return []
 
@@ -678,53 +731,56 @@
 
         Args:
             now (float): Current time
         Returns:
             int: Current (new) stack port state
             string: reason for the state change and additional information
         """
-        reason = ''
+        reason = ""
         if self.is_stack_admin_down():
             # Stack port ADMIN_DOWN, so no next state
             return self.stack_state()
 
-        last_seen_lldp_time = self.dyn_stack_probe_info.get('last_seen_lldp_time', None)
+        last_seen_lldp_time = self.dyn_stack_probe_info.get("last_seen_lldp_time", None)
         if last_seen_lldp_time is None:
             if self.is_stack_none():
                 # New stack, changing to state INIT
                 self.stack_init()
-                reason = 'new'
+                reason = "new"
         else:
             # Not a new stack port, so progess through state machine
-            peer_dp = self.stack['dp']  # pytype: disable=attribute-error
-            stack_correct = self.dyn_stack_probe_info.get(
-                'stack_correct', None)
+            peer_dp = self.stack["dp"]  # pytype: disable=attribute-error
+            stack_correct = self.dyn_stack_probe_info.get("stack_correct", None)
             send_interval = peer_dp.lldp_beacon.get(
-                'send_interval', peer_dp.DEFAULT_LLDP_SEND_INTERVAL)
+                "send_interval", peer_dp.DEFAULT_LLDP_SEND_INTERVAL
+            )
             time_since_lldp_seen = None
             num_lost_lldp = None
             stack_timed_out = True
 
             time_since_lldp_seen = now - last_seen_lldp_time
             num_lost_lldp = time_since_lldp_seen / send_interval
             if num_lost_lldp < self.max_lldp_lost:
                 stack_timed_out = False
 
             if stack_timed_out:
                 # Stack timed out, too many packets lost
                 self.stack_gone()
-                reason = f'too many ({num_lost_lldp}) packets lost, last received {time_since_lldp_seen}s ago'
+                reason = "too many (%u) packets lost, last received %us ago" % (
+                    num_lost_lldp,
+                    time_since_lldp_seen,
+                )
             elif not stack_correct:
                 # Stack bad due to incorrect cabling
                 self.stack_bad()
-                reason = 'incorrect cabling'
+                reason = "incorrect cabling"
             elif not self.is_stack_up():
                 # Nothing gone wrong, so Stack UP
                 self.stack_up()
-                reason = 'up'
+                reason = "up"
         return self.stack_state(), reason
 
     def is_stack_admin_down(self):
         """Return True if port is in ADMIN_DOWN state."""
         return self.dyn_stack_current_state == STACK_STATE_ADMIN_DOWN
 
     def is_stack_none(self):
```

### Comparing `c65faucet-1.0.46/faucet/prom_client.py` & `c65faucet-1.0.47/faucet/prom_client.py`

 * *Files 1% similar despite different names*

```diff
@@ -49,15 +49,15 @@
 
     REQUIRED_LABELS = ['dp_id', 'dp_name']
     _reg = REGISTRY
 
     def __init__(self, reg=None):
         if reg is not None:
             self._reg = reg
-        self.version = VersionInfo('c65faucet').semantic_version().release_string()
+        self.version = VersionInfo('faucet').semantic_version().release_string()
         self.faucet_version = PromGauge(
             'faucet_pbr_version',
             'Faucet PBR version',
             ['version'],
             registry=self._reg)
         self.faucet_version.labels(version=self.version).set(1)  # pylint: disable=no-member
         self.server = None
```

### Comparing `c65faucet-1.0.46/faucet/router.py` & `c65faucet-1.0.47/faucet/router.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/stack.py` & `c65faucet-1.0.47/faucet/stack.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-
 """Configuration for a stack."""
 
 # Copyright (C) 2015 Brad Cowie, Christopher Lorier and Joe Stringer.
 # Copyright (C) 2015 Research and Education Advanced Network New Zealand Ltd.
 # Copyright (C) 2015--2019 The Contributors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -21,40 +20,42 @@
 import networkx
 
 from faucet.conf import Conf, test_config_condition
 
 
 class Stack(Conf):
     """Stores state related to DP stack information, this includes the current elected root as that
-is technically a fixed allocation for this DP Stack instance."""
+    is technically a fixed allocation for this DP Stack instance."""
 
     defaults = {
         # Sets the root priority value of the current DP with stacking
-        'priority': None,
+        "priority": None,
         # Use the stack route algorithms, will be forced true if routing is enabled
-        'route_learning': False,
+        "route_learning": False,
         # Number of update time intervals for a down stack node to still be considered healthy
-        'down_time_multiple': 3,
+        "down_time_multiple": 3,
         # Minimum percentage value of required UP stack ports for this stack
         # node to be considered healthy
-        'min_stack_health': 1.0,
+        "min_stack_health": 1.0,
         # Minimum percentage value of required UP LACP ports for this stack
         # node to be considered healthy
-        'min_lacp_health': 1.0,
+        "min_lacp_health": 1.0,
     }
 
     defaults_types = {
-        'priority': int,
-        'route_learning': bool,
-        'down_time_multiple': int,
-        'min_stack_health': float,
-        'min_lacp_health': float,
+        "priority": int,
+        "route_learning": bool,
+        "down_time_multiple": int,
+        "min_stack_health": float,
+        "min_lacp_health": float,
     }
 
-    def __init__(self, _id, dp_id, name, canonical_port_order, lacp_down_ports, lacp_ports, conf):
+    def __init__(
+        self, _id, dp_id, name, canonical_port_order, lacp_down_ports, lacp_ports, conf
+    ):
         """
         Constructs a new stack object
 
         Args:
             _id (str): Name of the configuration key
             dp_id (int): DP ID of the DP that holds this stack instance
             name (str): Name of the DP that holds this stack instance
@@ -103,15 +104,15 @@
             if dps:
                 stack_port_dps = [dp for dp in dps if dp.stack_ports()]
                 for dp in stack_port_dps:
                     for port in dp.stack_ports():
                         port_up = False
                         if port.is_stack_up():
                             port_up = True
-                        elif port.is_stack_init() and port.stack['port'].is_stack_up():
+                        elif port.is_stack_init() and port.stack["port"].is_stack_up():
                             port_up = True
                         self.modify_link(dp, port, add=port_up)
 
     def live_timeout_healthy(self, last_live_time, now, update_time):
         """
         Determines the timeout of the current stack node, and whether
         the current stack node can be considered healthy according to
@@ -177,65 +178,77 @@
             now (float): Current time
             last_live_times (dict): Last live time value for each DP
             update_time (int): Stack root update interval time
         Return:
             tuple: Current stack node health state,
             str: Reason for the current state
         """
-        reason = ''
+        reason = ""
         last_live_time = dp_last_live_time.get(self.name, 0)
         timeout_healthy, health_timeout = self.live_timeout_healthy(
-            last_live_time, now, update_time)
+            last_live_time, now, update_time
+        )
         if not timeout_healthy:
             # Too long since DP last running, if DP not running then
             #   number of UP stack or LACP ports should be 0
-            reason += f'last running {now - last_live_time}s ago (timeout {health_timeout}s)'
+            reason += "last running %us ago (timeout %us)" % (
+                now - last_live_time,
+                health_timeout,
+            )
             self.dyn_healthy_info = (False, 0.0, 0.0)
             self.dyn_healthy = False
             return self.dyn_healthy, reason
-        reason += f'running {now - last_live_time}s ago'
+        reason += "running %us ago" % (now - last_live_time)
         if reason:
-            reason += ', '
+            reason += ", "
         stack_ports_healthy, stack_percentage = self.stack_port_healthy()
         if not stack_ports_healthy:
             # The number of DOWN stack ports surpasses the threshold for DOWN stack port tolerance
-            reason += 'stack ports %s (%.0f%%) not up' % (
-                list(self.down_ports()), (1.0 - stack_percentage) * 100.0)
+            reason += "stack ports %s (%.0f%%) not up" % (
+                list(self.down_ports()),
+                (1.0 - stack_percentage) * 100.0,
+            )
         else:
-            reason += '%.0f%% stack ports running' % (stack_percentage * 100.0)
+            reason += "%.0f%% stack ports running" % (stack_percentage * 100.0)
         if self.lacp_ports():
             if reason:
-                reason += ', '
+                reason += ", "
             lacp_ports_healthy, lacp_percentage = self.lacp_port_healthy()
             if not lacp_ports_healthy:
                 # The number of DOWN LACP ports surpasses the threshold for DOWN LACP port tolerance
-                reason += 'lacp ports %s (%.0f%%) not up' % (
-                    list(self.lacp_down_ports()), (1.0 - lacp_percentage) * 100.0)
+                reason += "lacp ports %s (%.0f%%) not up" % (
+                    list(self.lacp_down_ports()),
+                    (1.0 - lacp_percentage) * 100.0,
+                )
             else:
-                reason += '%.0f%% lacp ports running' % (lacp_percentage * 100.0)
+                reason += "%.0f%% lacp ports running" % (lacp_percentage * 100.0)
         else:
             # No LACP ports in node, so default to 100% UP & don't report information
             lacp_ports_healthy = True
             lacp_percentage = 0.0
         self.dyn_healthy_info = (timeout_healthy, stack_percentage, lacp_percentage)
         if timeout_healthy and stack_ports_healthy and lacp_ports_healthy:
             self.dyn_healthy = True
         else:
             self.dyn_healthy = False
         return self.dyn_healthy, reason
 
     @staticmethod
     def nominate_stack_root(stacks):
         """Return stack names in priority order and the chosen root"""
+
         def health_priority(stack):
             # Invert the health priority info so it is sorted correctly
             #   in relation to priority and the binary health
-            invert_info = (1.0 - stack.dyn_healthy_info[1],
-                           1.0 - stack.dyn_healthy_info[2])
+            invert_info = (
+                1.0 - stack.dyn_healthy_info[1],
+                1.0 - stack.dyn_healthy_info[2],
+            )
             return (not stack.dyn_healthy, *invert_info, stack.priority, stack.dp_id)
+
         stack_priorities = sorted(stacks, key=health_priority)
         priority_names = tuple(stack.name for stack in stack_priorities)
         nominated_name = priority_names[0]
         return priority_names, nominated_name
 
     def resolve_topology(self, dps, meta_dp_state):
         """
@@ -246,28 +259,35 @@
             meta_dp_state (MetaDPState): Provided if reloading when choosing a new root DP
         """
         stack_dps = [dp for dp in dps if dp.stack is not None]
         stack_priority_dps = [dp for dp in stack_dps if dp.stack.priority]
         stack_port_dps = [dp for dp in dps if dp.stack_ports()]
 
         if not stack_priority_dps:
-            test_config_condition(stack_dps, 'stacking enabled but no root DP')
+            test_config_condition(stack_dps, "stacking enabled but no root DP")
             return
 
         if not self.ports:
             return
 
         for dp in stack_priority_dps:
-            test_config_condition(not isinstance(dp.stack.priority, int), (
-                f'stack priority must be type {int} not {type(dp.stack.priority)}'))
-            test_config_condition(dp.stack.priority <= 0, (
-                'stack priority must be > 0'))
+            test_config_condition(
+                not isinstance(dp.stack.priority, int),
+                (
+                    "stack priority must be type %s not %s"
+                    % (int, type(dp.stack.priority))
+                ),
+            )
+            test_config_condition(
+                dp.stack.priority <= 0, ("stack priority must be > 0")
+            )
 
         self.roots_names, self.root_name = self.nominate_stack_root(
-            [dp.stack for dp in stack_priority_dps])
+            [dp.stack for dp in stack_priority_dps]
+        )
 
         if meta_dp_state:
             # If meta_dp_state exists, then we are reloading a new instance of the stack
             #   for a new 'dynamically' chosen root
             if meta_dp_state.stack_root_name in self.roots_names:
                 self.root_name = meta_dp_state.stack_root_name
 
@@ -280,62 +300,71 @@
         graph = networkx.MultiGraph()
         for dp in stack_port_dps:
             graph.add_node(dp.name)
             for port in dp.stack_ports():
                 edge_name = Stack.modify_topology(graph, dp, port)
                 edge_count[edge_name] += 1
         for edge_name, count in edge_count.items():
-            test_config_condition(count != 2, f'{edge_name} defined only in one direction')
+            test_config_condition(
+                count != 2, "%s defined only in one direction" % edge_name
+            )
         if graph.size() and self.name in graph:
             self.graph = graph
             for dp in graph.nodes():
                 path_to_root_len = len(self.shortest_path(self.root_name, src_dp=dp))
                 test_config_condition(
-                    path_to_root_len == 0, f'{dp} not connected to stack')
+                    path_to_root_len == 0, "%s not connected to stack" % dp
+                )
             root_len = self.longest_path_to_root_len()
             if root_len is not None and root_len > 2:
                 self.root_flood_reflection = True
 
     @staticmethod
     def modify_topology(graph, dp, port, add=True):
         """Add/remove an edge to the stack graph which originates from this dp and port."""
 
         def canonical_edge(dp, port):
-            peer_dp = port.stack['dp']
-            peer_port = port.stack['port']
-            sort_edge_a = (
-                dp.name, port.name, dp, port)
-            sort_edge_z = (
-                peer_dp.name, peer_port.name, peer_dp, peer_port)
+            peer_dp = port.stack["dp"]
+            peer_port = port.stack["port"]
+            sort_edge_a = (dp.name, port.name, dp, port)
+            sort_edge_z = (peer_dp.name, peer_port.name, peer_dp, peer_port)
             sorted_edge = sorted((sort_edge_a, sort_edge_z))
             edge_a, edge_b = sorted_edge[0][2:], sorted_edge[1][2:]
             return edge_a, edge_b
 
         def make_edge_name(edge_a, edge_z):
             edge_a_dp, edge_a_port = edge_a
             edge_z_dp, edge_z_port = edge_z
-            return f'{edge_a_dp.name}:{edge_a_port.name}-{edge_z_dp.name}:{edge_z_port.name}'
+            return "%s:%s-%s:%s" % (
+                edge_a_dp.name,
+                edge_a_port.name,
+                edge_z_dp.name,
+                edge_z_port.name,
+            )
 
         def make_edge_attr(edge_a, edge_z):
             edge_a_dp, edge_a_port = edge_a
             edge_z_dp, edge_z_port = edge_z
             return {
-                'dp_a': edge_a_dp, 'port_a': edge_a_port,
-                'dp_z': edge_z_dp, 'port_z': edge_z_port}
+                "dp_a": edge_a_dp,
+                "port_a": edge_a_port,
+                "dp_z": edge_z_dp,
+                "port_z": edge_z_port,
+            }
 
         edge = canonical_edge(dp, port)
         edge_a, edge_z = edge
         edge_name = make_edge_name(edge_a, edge_z)
         edge_attr = make_edge_attr(edge_a, edge_z)
         edge_a_dp, _ = edge_a
         edge_z_dp, _ = edge_z
         if add:
             graph.add_edge(
-                edge_a_dp.name, edge_z_dp.name,
-                key=edge_name, port_map=edge_attr)
+                edge_a_dp.name, edge_z_dp.name, key=edge_name, port_map=edge_attr
+            )
         elif (edge_a_dp.name, edge_z_dp.name, edge_name) in graph.edges:
             graph.remove_edge(edge_a_dp.name, edge_z_dp.name, edge_name)
 
         return edge_name
 
     def modify_link(self, dp, port, add=True):
         """Update the stack topology according to the event"""
@@ -372,15 +401,17 @@
 
     def shortest_path(self, dest_dp, src_dp=None):
         """Return shortest path to a DP, as a list of DPs."""
         if src_dp is None:
             src_dp = self.name
         if self.graph:
             try:
-                return sorted(networkx.all_shortest_paths(self.graph, src_dp, dest_dp))[0]
+                return sorted(networkx.all_shortest_paths(self.graph, src_dp, dest_dp))[
+                    0
+                ]
             except (networkx.exception.NetworkXNoPath, networkx.exception.NodeNotFound):
                 pass
         return []
 
     def shortest_path_to_root(self, src_dp=None):
         """Return shortest path to root DP, as list of DPs."""
         return self.shortest_path(self.root_name, src_dp=src_dp)
@@ -391,40 +422,46 @@
 
     def is_root_candidate(self):
         """Return True if this DP could be a root of the stack."""
         return self.name in self.roots_names
 
     def is_edge(self):
         """Return True if this DP is a stack edge."""
-        return (not self.is_root()
-                and self.longest_path_to_root_len() == len(self.shortest_path_to_root()))
+        return not self.is_root() and self.longest_path_to_root_len() == len(
+            self.shortest_path_to_root()
+        )
 
     def shortest_path_port(self, dest_dp):
         """Return first port on our DP, that is the shortest path towards dest DP."""
         shortest_path = self.shortest_path(dest_dp)
         if len(shortest_path) > 1:
             peer_dp = shortest_path[1]
             peer_dp_ports = self.peer_up_ports(peer_dp)
             if peer_dp_ports:
                 return peer_dp_ports[0]
         return None
 
     def peer_up_ports(self, peer_dp):
         """Return list of stack ports that are up towards a peer."""
-        return self.canonical_port_order([
-            port for port in self.ports if port.running() and (
-                port.stack['dp'].name == peer_dp)])
+        return self.canonical_port_order(
+            [
+                port
+                for port in self.ports
+                if port.running() and (port.stack["dp"].name == peer_dp)
+            ]
+        )
 
     def longest_path_to_root_len(self):
         """Return length of the longest path to root in the stack."""
         if not self.graph or not self.root_name:
             return None
         len_paths_to_root = [
             len(self.shortest_path(self.root_name, src_dp=dp))
-            for dp in self.graph.nodes()]
+            for dp in self.graph.nodes()
+        ]
         if len_paths_to_root:
             return max(len_paths_to_root)
         return None
 
     def is_in_path(self, src_dp, dst_dp):
         """Return True if the current DP is in the path from src_dp to dst_dp
 
@@ -436,19 +473,23 @@
         """
         path = self.shortest_path(dst_dp, src_dp=src_dp)
         return self.name in path
 
     def peer_symmetric_up_ports(self, peer_dp):
         """Return list of stack ports that are up towards us from a peer"""
         # Sort adjacent ports by canonical port order
-        return self.canonical_port_order([
-            port.stack['port'] for port in self.ports if port.running() and (
-                port.stack['dp'].name == peer_dp)])
+        return self.canonical_port_order(
+            [
+                port.stack["port"]
+                for port in self.ports
+                if port.running() and (port.stack["dp"].name == peer_dp)
+            ]
+        )
 
     def shortest_symmetric_path_port(self, peer_dp):
         """Return port on our DP that is the first port of the adjacent DP towards us"""
         shortest_path = self.shortest_path(self.name, src_dp=peer_dp)
         if len(shortest_path) == 2:
             adjacent_up_ports = self.peer_symmetric_up_ports(peer_dp)
             if adjacent_up_ports:
-                return adjacent_up_ports[0].stack['port']
+                return adjacent_up_ports[0].stack["port"]
         return None
```

### Comparing `c65faucet-1.0.46/faucet/tfm_pipeline.py` & `c65faucet-1.0.47/faucet/tfm_pipeline.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,140 +1,169 @@
 """Configure switch tables with TFM messages."""
 
 from faucet import valve_of
 
-REQUIRED_PROPERTIES = set([
-    valve_of.ofp.OFPTFPT_WRITE_ACTIONS,
-    valve_of.ofp.OFPTFPT_WRITE_ACTIONS_MISS,
-    valve_of.ofp.OFPTFPT_APPLY_ACTIONS,
-    valve_of.ofp.OFPTFPT_APPLY_ACTIONS_MISS,
-    valve_of.ofp.OFPTFPT_WRITE_SETFIELD,
-    valve_of.ofp.OFPTFPT_WRITE_SETFIELD_MISS,
-    valve_of.ofp.OFPTFPT_MATCH,
-    valve_of.ofp.OFPTFPT_WILDCARDS,
-    valve_of.ofp.OFPTFPT_APPLY_SETFIELD_MISS,
-    valve_of.ofp.OFPTFPT_APPLY_SETFIELD,
-    valve_of.ofp.OFPTFPT_NEXT_TABLES,
-    valve_of.ofp.OFPTFPT_NEXT_TABLES_MISS,
-    valve_of.ofp.OFPTFPT_APPLY_SETFIELD,
-    valve_of.ofp.OFPTFPT_INSTRUCTIONS,
-    valve_of.ofp.OFPTFPT_INSTRUCTIONS_MISS])
+REQUIRED_PROPERTIES = set(
+    [
+        valve_of.ofp.OFPTFPT_WRITE_ACTIONS,
+        valve_of.ofp.OFPTFPT_WRITE_ACTIONS_MISS,
+        valve_of.ofp.OFPTFPT_APPLY_ACTIONS,
+        valve_of.ofp.OFPTFPT_APPLY_ACTIONS_MISS,
+        valve_of.ofp.OFPTFPT_WRITE_SETFIELD,
+        valve_of.ofp.OFPTFPT_WRITE_SETFIELD_MISS,
+        valve_of.ofp.OFPTFPT_MATCH,
+        valve_of.ofp.OFPTFPT_WILDCARDS,
+        valve_of.ofp.OFPTFPT_APPLY_SETFIELD_MISS,
+        valve_of.ofp.OFPTFPT_APPLY_SETFIELD,
+        valve_of.ofp.OFPTFPT_NEXT_TABLES,
+        valve_of.ofp.OFPTFPT_NEXT_TABLES_MISS,
+        valve_of.ofp.OFPTFPT_APPLY_SETFIELD,
+        valve_of.ofp.OFPTFPT_INSTRUCTIONS,
+        valve_of.ofp.OFPTFPT_INSTRUCTIONS_MISS,
+    ]
+)
 
 
 def fill_required_properties(new_table):
     """Ensure TFM has all required properties."""
     configured_props = {prop.type for prop in new_table.properties}
     missing_props = REQUIRED_PROPERTIES - configured_props
     for prop in missing_props:
-        new_table.properties.append(
-            valve_of.parser.OFPTableFeaturePropOxm(type_=prop))
+        new_table.properties.append(valve_of.parser.OFPTableFeaturePropOxm(type_=prop))
 
 
 def init_table(table_id, name, max_entries, metadata_match, metadata_write):
     """Initialize a TFM."""
     if not metadata_match:
         metadata_match = 0
     if not metadata_write:
         metadata_write = 0
     table_attr = {
-        'config': 3,
-        'max_entries': max_entries,
-        'metadata_match': metadata_match,
-        'metadata_write': metadata_write,
-        'name': name.encode('utf-8'),
-        'properties': [],
-        'table_id': table_id,
+        "config": 3,
+        "max_entries": max_entries,
+        "metadata_match": metadata_match,
+        "metadata_write": metadata_write,
+        "name": name.encode("utf-8"),
+        "properties": [],
+        "table_id": table_id,
     }
     return valve_of.parser.OFPTableFeaturesStats(**table_attr)
 
 
 # pylint: disable=too-many-arguments
 # pylint: disable=too-many-locals
 def load_tables(dp, valve_cl, max_table_id, min_max_flows, use_oxm_ids, fill_req):
     """Configure switch tables with TFM messages."""
     table_array = []
-    active_table_ids = sorted([valve_table.table_id for valve_table in dp.tables.values()])
+    active_table_ids = sorted(
+        [valve_table.table_id for valve_table in dp.tables.values()]
+    )
     for table_id in active_table_ids:
         valve_table = dp.table_by_id(table_id)
         max_entries = max(min_max_flows, valve_table.table_config.size)
         new_table = init_table(
-            table_id, valve_table.name, max_entries,
-            valve_table.metadata_match, valve_table.metadata_write)
+            table_id,
+            valve_table.name,
+            max_entries,
+            valve_table.metadata_match,
+            valve_table.metadata_write,
+        )
         # Match types
         if valve_table.match_types:
             oxm_ids = []
             if use_oxm_ids:
                 oxm_ids = [
                     valve_of.parser.OFPOxmId(type_=match_type, hasmask=hasmask)
-                    for match_type, hasmask in valve_table.match_types.items()]
+                    for match_type, hasmask in valve_table.match_types.items()
+                ]
             new_table.properties.append(
                 valve_of.parser.OFPTableFeaturePropOxm(
-                    oxm_ids=oxm_ids, type_=valve_of.ofp.OFPTFPT_MATCH))
+                    oxm_ids=oxm_ids, type_=valve_of.ofp.OFPTFPT_MATCH
+                )
+            )
             # Not an exact match table, assume all fields wildcarded.
             if not valve_table.exact_match:
                 new_table.properties.append(
                     valve_of.parser.OFPTableFeaturePropOxm(
-                        oxm_ids=oxm_ids, type_=valve_of.ofp.OFPTFPT_WILDCARDS))
+                        oxm_ids=oxm_ids, type_=valve_of.ofp.OFPTFPT_WILDCARDS
+                    )
+                )
         insts = set([valve_of.ofp.OFPIT_APPLY_ACTIONS])
         # Next tables
         if valve_table.next_tables:
-            new_table.properties.append(valve_of.parser.OFPTableFeaturePropNextTables(
-                table_ids=valve_table.next_tables,
-                type_=valve_of.ofp.OFPTFPT_NEXT_TABLES))
+            new_table.properties.append(
+                valve_of.parser.OFPTableFeaturePropNextTables(
+                    table_ids=valve_table.next_tables,
+                    type_=valve_of.ofp.OFPTFPT_NEXT_TABLES,
+                )
+            )
             insts.add(valve_of.ofp.OFPIT_GOTO_TABLE)
         # Instructions
         if valve_table.table_config.meter:
             insts.add(valve_of.ofp.OFPIT_METER)
         inst_ids = [valve_of.parser.OFPInstructionId(type_) for type_ in insts]
         new_table.properties.append(
             valve_of.parser.OFPTableFeaturePropInstructions(
-                type_=valve_of.ofp.OFPTFPT_INSTRUCTIONS, instruction_ids=inst_ids))
+                type_=valve_of.ofp.OFPTFPT_INSTRUCTIONS, instruction_ids=inst_ids
+            )
+        )
         apply_actions = set()
         if valve_table.table_config.dec_ttl and valve_cl.DEC_TTL:
             apply_actions.add(valve_of.ofp.OFPAT_DEC_NW_TTL)
         # Set fields and apply actions
         if valve_table.set_fields:
             apply_actions.add(valve_of.ofp.OFPAT_SET_FIELD)
-            if 'vlan_vid' in valve_table.set_fields:
+            if "vlan_vid" in valve_table.set_fields:
                 apply_actions.add(valve_of.ofp.OFPAT_PUSH_VLAN)
             oxm_ids = []
             if use_oxm_ids:
                 oxm_ids = [
                     valve_of.parser.OFPOxmId(type_=field, hasmask=False)
-                    for field in valve_table.set_fields]
+                    for field in valve_table.set_fields
+                ]
             new_table.properties.append(
                 valve_of.parser.OFPTableFeaturePropOxm(
-                    oxm_ids=oxm_ids, type_=valve_of.ofp.OFPTFPT_APPLY_SETFIELD))
+                    oxm_ids=oxm_ids, type_=valve_of.ofp.OFPTFPT_APPLY_SETFIELD
+                )
+            )
         if valve_table.table_config.output:
             apply_actions.add(valve_of.ofp.OFPAT_OUTPUT)
             apply_actions.add(valve_of.ofp.OFPAT_POP_VLAN)
             if valve_cl.GROUPS:
                 apply_actions.add(valve_of.ofp.OFPAT_GROUP)
         if apply_actions:
-            action_ids = [
-                valve_of.parser.OFPActionId(type_) for type_ in apply_actions]
+            action_ids = [valve_of.parser.OFPActionId(type_) for type_ in apply_actions]
             new_table.properties.append(
                 valve_of.parser.OFPTableFeaturePropActions(
-                    type_=valve_of.ofp.OFPTFPT_APPLY_ACTIONS, action_ids=action_ids))
+                    type_=valve_of.ofp.OFPTFPT_APPLY_ACTIONS, action_ids=action_ids
+                )
+            )
         # Miss goto table option.
         if valve_table.table_config.miss_goto:
             miss_table_id = dp.tables[valve_table.table_config.miss_goto].table_id
             new_table.properties.append(
                 valve_of.parser.OFPTableFeaturePropNextTables(
-                    table_ids=[miss_table_id], type_=valve_of.ofp.OFPTFPT_NEXT_TABLES_MISS))
+                    table_ids=[miss_table_id],
+                    type_=valve_of.ofp.OFPTFPT_NEXT_TABLES_MISS,
+                )
+            )
             inst_ids = [valve_of.parser.OFPInstructionId(valve_of.ofp.OFPIT_GOTO_TABLE)]
             new_table.properties.append(
                 valve_of.parser.OFPTableFeaturePropInstructions(
-                    type_=valve_of.ofp.OFPTFPT_INSTRUCTIONS_MISS, instruction_ids=inst_ids))
+                    type_=valve_of.ofp.OFPTFPT_INSTRUCTIONS_MISS,
+                    instruction_ids=inst_ids,
+                )
+            )
         if fill_req:
             fill_required_properties(new_table)
         table_array.append(new_table)
 
     tfm_table_ids = {table.table_id for table in table_array}
     for missing_table_id in set(range(max_table_id + 1)) - tfm_table_ids:
         new_table = init_table(
-            missing_table_id, str(missing_table_id), min_max_flows, 0, 0)
+            missing_table_id, str(missing_table_id), min_max_flows, 0, 0
+        )
         if fill_req:
             fill_required_properties(new_table)
         table_array.append(new_table)
 
     return table_array
```

### Comparing `c65faucet-1.0.46/faucet/valve.py` & `c65faucet-1.0.47/faucet/valve.py`

 * *Files 7% similar despite different names*

```diff
@@ -56,33 +56,35 @@
         if port.dot1x:
             ofmsgs.extend(self.dot1x.port_down(self.dp_id, port, self.nfv_sw_port))
         return ofmsgs
 
     def add_port(self, port):
         ofmsgs = []
         if port == self.nfv_sw_port:
-            ofmsgs.extend(self.dot1x.nfv_sw_port_up(
-                self.dp_id, self.dot1x_ports(), self.nfv_sw_port))
+            ofmsgs.extend(
+                self.dot1x.nfv_sw_port_up(
+                    self.dp_id, self.dot1x_ports(), self.nfv_sw_port
+                )
+            )
         elif port.dot1x:
-            ofmsgs.extend(self.dot1x.port_up(
-                self.dp_id, port, self.nfv_sw_port))
+            ofmsgs.extend(self.dot1x.port_up(self.dp_id, port, self.nfv_sw_port))
         return ofmsgs
 
 
 class ValveLogger:
     """Logger for a Valve that adds DP ID."""
 
     def __init__(self, logger, dp_id, dp_name):
         self.logger = logger
         self.dp_id = dp_id
         self.dp_name = dp_name
 
     def _dpid_prefix(self, log_msg):
         """Add DP ID prefix to log message."""
-        return ' '.join((valve_util.dpid_log(self.dp_id), self.dp_name, log_msg))
+        return " ".join((valve_util.dpid_log(self.dp_id), self.dp_name, log_msg))
 
     def debug(self, log_msg):
         """Log debug level message."""
         self.logger.debug(self._dpid_prefix(log_msg))
 
     def info(self, log_msg):
         """Log info level message."""
@@ -101,40 +103,40 @@
     """Generates the messages to configure a datapath as a l2 learning switch.
 
     Vendor specific implementations may require sending configuration flows.
     This can be achieved by inheriting from this class and overwriting the
     function switch_features."""
 
     __slots__ = [
-        '_coprocessor_manager',
-        '_dot1x_manager',
-        '_last_advertise_sec',
-        '_last_fast_advertise_sec',
-        '_last_lldp_advertise_sec',
-        '_last_packet_in_sec',
-        '_packet_in_count_sec',
-        '_port_highwater',
-        '_route_manager_by_eth_type',
-        '_route_manager_by_ipv',
-        '_lldp_manager',
-        '_managers',
-        '_output_only_manager',
-        'switch_manager',
-        'stack_manager',
-        'acl_manager',
-        'dot1x',
-        'dp',
-        'logger',
-        'logname',
-        'metrics',
-        'notifier',
-        'stale_root',
-        'ofchannel_logger',
-        'pipeline',
-        'recent_ofmsgs',
+        "_coprocessor_manager",
+        "_dot1x_manager",
+        "_last_advertise_sec",
+        "_last_fast_advertise_sec",
+        "_last_lldp_advertise_sec",
+        "_last_packet_in_sec",
+        "_packet_in_count_sec",
+        "_port_highwater",
+        "_route_manager_by_eth_type",
+        "_route_manager_by_ipv",
+        "_lldp_manager",
+        "_managers",
+        "_output_only_manager",
+        "switch_manager",
+        "stack_manager",
+        "acl_manager",
+        "dot1x",
+        "dp",
+        "logger",
+        "logname",
+        "metrics",
+        "notifier",
+        "stale_root",
+        "ofchannel_logger",
+        "pipeline",
+        "recent_ofmsgs",
     ]
 
     DEC_TTL = True
     USE_BARRIERS = True
     STATIC_TABLE_IDS = False
     GROUPS = True
 
@@ -175,15 +177,17 @@
     def _inc_port_var(self, var, port, val=1):
         self._inc_var(var, labels=self.dp.port_labels(port.number), val=val)
 
     def _remove_var(self, var, labels=None):
         if labels is None:
             labels = self.dp.base_prom_labels()
         metrics_var = getattr(self.metrics, var)
-        label_values = [labels[key] for key in metrics_var._labelnames]  # pylint: disable=protected-access
+        label_values = [
+            labels[key] for key in metrics_var._labelnames
+        ]  # pylint: disable=protected-access
         metrics_var.remove(*label_values)
 
     def close_logs(self):
         """Explicitly close any active loggers."""
         if self.logger is not None:
             valve_util.close_logger(self.logger.logger)
         valve_util.close_logger(self.ofchannel_logger)
@@ -195,15 +199,16 @@
             if valves:
                 dps = [valve.dp for valve in valves]
             new_dp.clone_dyn_state(self.dp, dps)
             self.dp = new_dp
 
         self.close_logs()
         self.logger = ValveLogger(
-            logging.getLogger(self.logname + '.valve'), self.dp.dp_id, self.dp.name)
+            logging.getLogger(self.logname + ".valve"), self.dp.dp_id, self.dp.name
+        )
         self.ofchannel_logger = None
         self._packet_in_count_sec = 0
         self._last_packet_in_sec = 0
         self._last_advertise_sec = 0
         self._last_fast_advertise_sec = 0
         self._last_lldp_advertise_sec = 0
         self._route_manager_by_ipv = {}
@@ -213,76 +218,127 @@
         self.dp.reset_refs()
         for vlan_vid in self.dp.vlans.keys():
             self._port_highwater[vlan_vid] = {}
             for port_number in self.dp.ports.keys():
                 self._port_highwater[vlan_vid][port_number] = 0
 
         self._output_only_manager = OutputOnlyManager(
-            self.dp.tables['vlan'], self.dp.highest_priority)
+            self.dp.tables["vlan"], self.dp.highest_priority
+        )
         self._dot1x_manager = None
         if self.dp.dot1x and self.dp.dot1x_ports():
-            nfv_sw_port = self.dp.ports[self.dp.dot1x['nfv_sw_port']]
+            nfv_sw_port = self.dp.ports[self.dp.dot1x["nfv_sw_port"]]
             self._dot1x_manager = Dot1xManager(
-                self.dot1x, self.dp.dp_id, self.dp.dot1x_ports, nfv_sw_port)
+                self.dot1x, self.dp.dp_id, self.dp.dot1x_ports, nfv_sw_port
+            )
 
         self.pipeline = valve_pipeline.ValvePipeline(self.dp)
         self.acl_manager = None
         if self.dp.has_acls:
             self.acl_manager = valve_acl.ValveAclManager(
-                self.dp.tables.get('port_acl'), self.dp.tables.get('vlan_acl'),
-                self.dp.tables.get('egress_acl'), self.pipeline,
-                self.dp.meters, self.dp.dp_acls)
+                self.dp.tables.get("port_acl"),
+                self.dp.tables.get("vlan_acl"),
+                self.dp.tables.get("egress_acl"),
+                self.pipeline,
+                self.dp.meters,
+                self.dp.dp_acls,
+            )
 
         self.stack_manager = None
         if self.dp.stack:
             self.stack_manager = ValveStackManager(
-                self.logger, self.dp, self.dp.stack, self.dp.tunnel_acls, self.acl_manager,
-                self.dp.tables['eth_dst'])
+                self.logger,
+                self.dp,
+                self.dp.stack,
+                self.dp.tunnel_acls,
+                self.acl_manager,
+                self.dp.tables["eth_dst"],
+            )
 
         self._lldp_manager = ValveLLDPManager(
-            self.dp.tables['vlan'], self.dp.highest_priority, self.logger,
-            self.notify, self._inc_var, self._set_var, self._set_port_var, self.stack_manager)
+            self.dp.tables["vlan"],
+            self.dp.highest_priority,
+            self.logger,
+            self.notify,
+            self._inc_var,
+            self._set_var,
+            self._set_port_var,
+            self.stack_manager,
+        )
 
         self.switch_manager = valve_switch.valve_switch_factory(
-            self.logger, self.dp, self.pipeline, self.stack_manager)
+            self.logger, self.dp, self.pipeline, self.stack_manager
+        )
         self._coprocessor_manager = None
-        copro_table = self.dp.tables.get('copro', None)
+        copro_table = self.dp.tables.get("copro", None)
         if copro_table:
             self._coprocessor_manager = CoprocessorManager(
-                self.dp.ports, copro_table, self.dp.tables['vlan'], self.dp.tables['eth_src'],
-                self.switch_manager.output_table, self.dp.low_priority, self.dp.high_priority)
+                self.dp.ports,
+                copro_table,
+                self.dp.tables["vlan"],
+                self.dp.tables["eth_src"],
+                self.switch_manager.output_table,
+                self.dp.low_priority,
+                self.dp.high_priority,
+            )
         for ipv, route_manager_class, neighbor_timeout in (
-                (4, valve_route.ValveIPv4RouteManager, self.dp.arp_neighbor_timeout),
-                (6, valve_route.ValveIPv6RouteManager, self.dp.nd_neighbor_timeout)):
-            fib_table_name = f'ipv{ipv}_fib'
+            (4, valve_route.ValveIPv4RouteManager, self.dp.arp_neighbor_timeout),
+            (6, valve_route.ValveIPv6RouteManager, self.dp.nd_neighbor_timeout),
+        ):
+            fib_table_name = "ipv%u_fib" % ipv
             if fib_table_name not in self.dp.tables:
                 continue
             fib_table = self.dp.tables[fib_table_name]
-            proactive_learn = getattr(self.dp, f'proactive_learn_v{ipv}')
+            proactive_learn = getattr(self.dp, "proactive_learn_v%u" % ipv)
             route_manager = route_manager_class(
-                self.logger, self.notify, self.dp.global_vlan, neighbor_timeout,
+                self.logger,
+                self.notify,
+                self.dp.global_vlan,
+                neighbor_timeout,
                 self.dp.max_hosts_per_resolve_cycle,
                 self.dp.max_host_fib_retry_count,
-                self.dp.max_resolve_backoff_time, proactive_learn,
-                self.DEC_TTL, self.dp.multi_out, fib_table,
-                self.dp.tables['vip'], self.pipeline, self.dp.routers, self.stack_manager)
+                self.dp.max_resolve_backoff_time,
+                proactive_learn,
+                self.DEC_TTL,
+                self.dp.multi_out,
+                fib_table,
+                self.dp.tables["vip"],
+                self.pipeline,
+                self.dp.routers,
+                self.stack_manager,
+            )
             self._route_manager_by_ipv[route_manager.IPV] = route_manager
             for vlan in self.dp.vlans.values():
                 if vlan.faucet_vips_by_ipv(route_manager.IPV):
                     route_manager.active = True
-                    vips_str = list(str(vip) for vip in vlan.faucet_vips_by_ipv(route_manager.IPV))
-                    self.logger.info(f'IPv{route_manager.IPV} routing is active on {vlan} with VIPs {vips_str}')
+                    vips_str = list(
+                        str(vip) for vip in vlan.faucet_vips_by_ipv(route_manager.IPV)
+                    )
+                    self.logger.info(
+                        "IPv%u routing is active on %s with VIPs %s"
+                        % (route_manager.IPV, vlan, vips_str)
+                    )
             for eth_type in route_manager.CONTROL_ETH_TYPES:
                 self._route_manager_by_eth_type[eth_type] = route_manager
         self._managers = tuple(
-            manager for manager in (
-                self.pipeline, self.switch_manager, self.acl_manager, self.stack_manager,
-                self._lldp_manager, self._route_manager_by_ipv.get(4),
-                self._route_manager_by_ipv.get(6), self._coprocessor_manager,
-                self._output_only_manager, self._dot1x_manager) if manager is not None)
+            manager
+            for manager in (
+                self.pipeline,
+                self.switch_manager,
+                self.acl_manager,
+                self.stack_manager,
+                self._lldp_manager,
+                self._route_manager_by_ipv.get(4),
+                self._route_manager_by_ipv.get(6),
+                self._coprocessor_manager,
+                self._output_only_manager,
+                self._dot1x_manager,
+            )
+            if manager is not None
+        )
 
     def notify(self, event_dict):
         """Send an event notification."""
         self.notifier.notify(self.dp.dp_id, self.dp.name, event_dict)
 
     def switch_features(self, _msg):
         """Send configuration flows necessary for the switch implementation.
@@ -291,38 +347,37 @@
             msg (OFPSwitchFeatures): msg sent from switch.
 
         Vendor specific configuration should be implemented here.
         """
         ofmsgs = [
             valve_of.faucet_config(),
             valve_of.faucet_async(
-                packet_in=False, notify_flow_removed=False, port_status=False),
-            valve_of.desc_stats_request()]
+                packet_in=False, notify_flow_removed=False, port_status=False
+            ),
+            valve_of.desc_stats_request(),
+        ]
         ofmsgs.extend(self._delete_all_valve_flows())
         return ofmsgs
 
     def ofchannel_log(self, ofmsgs):
         """Log OpenFlow messages in text format to debugging log."""
         if self.dp is None:
             return
         if self.dp.ofchannel_log is None:
             return
         if self.ofchannel_logger is None:
             self.ofchannel_logger = valve_util.get_logger(
-                self.dp.ofchannel_log,
-                self.dp.ofchannel_log,
-                logging.DEBUG,
-                0)
-        log_prefix = f'{len(ofmsgs)} {valve_util.dpid_log(self.dp.dp_id)}'
+                self.dp.ofchannel_log, self.dp.ofchannel_log, logging.DEBUG, 0
+            )
+        log_prefix = "%u %s" % (len(ofmsgs), valve_util.dpid_log(self.dp.dp_id))
         for i, ofmsg in enumerate(ofmsgs, start=1):
-            self.ofchannel_logger.debug(
-                '%u/%s %s', i, log_prefix, ofmsg)
+            self.ofchannel_logger.debug("%u/%s %s", i, log_prefix, ofmsg)
 
     def dot1x_event(self, event_dict):
-        self.notify({'DOT1X': event_dict})
+        self.notify({"DOT1X": event_dict})
 
     def floods_to_root(self):
         """Return True if our dp floods (only) to root switch"""
         return self.switch_manager.floods_to_root(self.dp)
 
     def _delete_all_valve_flows(self):
         """Delete all flows from all FAUCET tables."""
@@ -340,34 +395,34 @@
     def _add_default_drop_flows(self):
         """Add default drop rules on all FAUCET tables."""
         ofmsgs = []
         for table in self.dp.tables.values():
             miss_table_name = table.table_config.miss_goto
             if miss_table_name:
                 miss_table = self.dp.tables[miss_table_name]
-                ofmsgs.append(table.flowmod(
-                    priority=self.dp.lowest_priority,
-                    inst=(table.goto_miss(miss_table),)))
+                ofmsgs.append(
+                    table.flowmod(
+                        priority=self.dp.lowest_priority,
+                        inst=(table.goto_miss(miss_table),),
+                    )
+                )
             else:
-                ofmsgs.append(table.flowdrop(
-                    priority=self.dp.lowest_priority))
+                ofmsgs.append(table.flowdrop(priority=self.dp.lowest_priority))
         return ofmsgs
 
     def _add_packetin_meter(self):
         """Add rate limiting of packetin in pps (not supported by many DPs)."""
         if self.dp.packetin_pps:
-            return [
-                valve_of.controller_pps_meteradd(pps=self.dp.packetin_pps)]
+            return [valve_of.controller_pps_meteradd(pps=self.dp.packetin_pps)]
         return []
 
     def _add_slowpath_meter(self):
         """Add rate limiting of slowpath in pps (not supported by many DPs)."""
         if self.dp.slowpath_pps:
-            return [
-                valve_of.slowpath_pps_meteradd(pps=self.dp.slowpath_pps)]
+            return [valve_of.slowpath_pps_meteradd(pps=self.dp.slowpath_pps)]
         return []
 
     def _add_default_flows(self):
         """Configure datapath with necessary default tables and rules."""
         ofmsgs = []
         ofmsgs.extend(self._delete_all_valve_flows())
         ofmsgs.extend(self._add_packetin_meter())
@@ -376,15 +431,15 @@
             for meter in self.dp.meters.values():
                 ofmsgs.append(meter.entry_msg)
         ofmsgs.extend(self._add_default_drop_flows())
         return ofmsgs
 
     def add_vlan(self, vlan, cold_start=False):
         """Configure a VLAN."""
-        self.logger.info(f'Configuring {vlan}')
+        self.logger.info("Configuring %s" % vlan)
         ofmsgs = []
         if vlan.reserved_internal_vlan:
             return ofmsgs
         for manager in self._managers:
             ofmsgs.extend(manager.add_vlan(vlan, cold_start))
         return ofmsgs
 
@@ -392,15 +447,15 @@
         ofmsgs = []
         for vlan in vlans:
             ofmsgs.extend(self.add_vlan(vlan, cold_start=cold_start))
         return ofmsgs
 
     def del_vlan(self, vlan):
         """Delete a configured VLAN."""
-        self.logger.info(f'Delete VLAN {vlan}')
+        self.logger.info("Delete VLAN %s" % vlan)
         ofmsgs = []
         for manager in self._managers:
             ofmsgs.extend(manager.del_vlan(vlan))
         expired_hosts = list(vlan.dyn_host_cache.values())
         for entry in expired_hosts:
             self._update_expired_host(entry, vlan)
         vlan.reset_caches()
@@ -418,85 +473,93 @@
             ports.update(set(vlan.get_ports()))
         ports = {port.number for port in ports}
         return ports
 
     @staticmethod
     def _get_ports_status(discovered_up_port_nos, all_configured_port_nos):
         port_status = {
-            port_no: (port_no in discovered_up_port_nos) for port_no in all_configured_port_nos}
+            port_no: (port_no in discovered_up_port_nos)
+            for port_no in all_configured_port_nos
+        }
         all_up_port_nos = {port_no for port_no, status in port_status.items() if status}
         return (port_status, all_up_port_nos)
 
     def _cold_start_ports_and_vlans(self, now, discovered_up_port_nos):
         """Add all configured and discovered ports and VLANs at cold start time."""
         always_up_port_nos = {
-            port.number for port in self.dp.ports.values() if not port.opstatus_reconf}
+            port.number for port in self.dp.ports.values() if not port.opstatus_reconf
+        }
         discovered_up_port_nos = discovered_up_port_nos.union(always_up_port_nos)
 
         all_configured_port_nos = self._get_all_configured_port_nos()
         port_status, all_up_port_nos = self._get_ports_status(
-            discovered_up_port_nos, all_configured_port_nos)
+            discovered_up_port_nos, all_configured_port_nos
+        )
 
         for port_no, status in port_status.items():
             self._set_port_status(port_no, status, now)
-        self.notify({'PORTS_STATUS': port_status})
+        self.notify({"PORTS_STATUS": port_status})
 
         ofmsgs = []
         ofmsgs.extend(self._add_default_flows())
         for manager in self._managers:
             ofmsgs.extend(manager.initialise_tables())
         ofmsgs.append(
             valve_of.faucet_async(
                 packet_in=True,
                 port_status=True,
-                notify_flow_removed=self.dp.use_idle_timeout))
-        ofmsgs.extend(self.ports_add(
-            all_up_port_nos, cold_start=True, log_msg='configured'))
+                notify_flow_removed=self.dp.use_idle_timeout,
+            )
+        )
+        ofmsgs.extend(
+            self.ports_add(all_up_port_nos, cold_start=True, log_msg="configured")
+        )
         ofmsgs.extend(self.add_vlans(self.dp.vlans.values(), cold_start=True))
         return ofmsgs
 
     def ofdescstats_handler(self, body):
         """Handle OF DP description."""
         labels = dict(
             self.dp.base_prom_labels(),
             mfr_desc=valve_util.utf8_decode(body.mfr_desc),
             hw_desc=valve_util.utf8_decode(body.hw_desc),
             sw_desc=valve_util.utf8_decode(body.sw_desc),
             serial_num=valve_util.utf8_decode(body.serial_num),
-            dp_desc=valve_util.utf8_decode(body.dp_desc))
-        self._set_var('of_dp_desc_stats', self.dp.dp_id, labels=labels)
+            dp_desc=valve_util.utf8_decode(body.dp_desc),
+        )
+        self._set_var("of_dp_desc_stats", self.dp.dp_id, labels=labels)
 
     def _set_port_status(self, port_no, port_status, now):
         """Set port operational status."""
         if port_status:
             self.dp.dyn_up_port_nos.add(port_no)
         else:
             self.dp.dyn_up_port_nos -= set([port_no])
         port = self.dp.ports.get(port_no, None)
         if port is None:
             return
-        self._set_port_var('port_status', port_status, port)
+        self._set_port_var("port_status", port_status, port)
         port.dyn_update_time = now
 
     _port_status_codes = {
-        valve_of.ofp.OFPPR_ADD: 'ADD',
-        valve_of.ofp.OFPPR_DELETE: 'DELETE',
-        valve_of.ofp.OFPPR_MODIFY: 'MODIFY'
+        valve_of.ofp.OFPPR_ADD: "ADD",
+        valve_of.ofp.OFPPR_DELETE: "DELETE",
+        valve_of.ofp.OFPPR_MODIFY: "MODIFY",
     }
 
     @classmethod
     def _decode_port_status(cls, reason):
         """Humanize the port status reason code."""
 
-        return cls._port_status_codes.get(reason, 'UNKNOWN')
+        return cls._port_status_codes.get(reason, "UNKNOWN")
 
     def port_desc_stats_reply_handler(self, port_desc_stats, _other_valves, now):
         ofmsgs = []
 
-        self.logger.info('port desc stats')
+        self.logger.info("port desc stats")
 
         # There are 4 cases to handle
         #
         # For the phys ports we have no config for
         #  if the phys state is different, fabricate MODIFY port
         # For the ports that we have config for
         #  if the state has not changed, skip
@@ -505,41 +568,52 @@
         #    else fabricate MODIFY port to phys state
         #  else the phys port is not present
         #    if the port was phys up, fabricate DELETE port
         #
 
         def _fabricate(port_no, reason, status):
             self.logger.info(
-                f'Port {port_no} fabricating {Valve._decode_port_status(reason)} status {status}')
+                "Port %s fabricating %s status %s"
+                % (port_no, Valve._decode_port_status(reason), status)
+            )
 
             _ofmsgs_by_valve = self.port_status_handler(
-                port_no, reason, 0 if status else valve_of.ofp.OFPPS_LINK_DOWN,
-                _other_valves, now)
+                port_no,
+                reason,
+                0 if status else valve_of.ofp.OFPPS_LINK_DOWN,
+                _other_valves,
+                now,
+            )
             if self in _ofmsgs_by_valve:
                 ofmsgs.extend(_ofmsgs_by_valve[self])
 
-        curr_dyn_port_nos = set(
-            desc.port_no for desc in port_desc_stats)
+        curr_dyn_port_nos = set(desc.port_no for desc in port_desc_stats)
         curr_dyn_port_nos -= set([valve_of.ofp.OFPP_LOCAL])
 
         prev_dyn_up_port_nos = set(self.dp.dyn_up_port_nos)
         curr_dyn_up_port_nos = set(
-            desc.port_no for desc in port_desc_stats
-            if valve_of.port_status_from_state(desc.state))
+            desc.port_no
+            for desc in port_desc_stats
+            if valve_of.port_status_from_state(desc.state)
+        )
 
         conf_port_nos = set(self.dp.ports.keys())
 
         no_conf_port_nos = curr_dyn_port_nos - conf_port_nos
 
         if conf_port_nos != curr_dyn_port_nos:
             self.logger.info(
-                f'delta in known ports: conf {conf_port_nos} dyn {curr_dyn_port_nos}')
+                "delta in known ports: conf %s dyn %s"
+                % (conf_port_nos, curr_dyn_port_nos)
+            )
         if prev_dyn_up_port_nos != curr_dyn_up_port_nos:
             self.logger.info(
-                f'delta in up state: {prev_dyn_up_port_nos} => {curr_dyn_up_port_nos}')
+                "delta in up state: %s => %s"
+                % (prev_dyn_up_port_nos, curr_dyn_up_port_nos)
+            )
 
         # Ports we have no config for
         for port_no in no_conf_port_nos:
             prev_up = port_no in prev_dyn_up_port_nos
             curr_up = port_no in curr_dyn_up_port_nos
             if prev_up != curr_up:
                 _fabricate(port_no, valve_of.ofp.OFPPR_MODIFY, curr_up)
@@ -565,166 +639,196 @@
         return ofmsgs_by_valve
 
     def port_status_handler(self, port_no, reason, state, _other_valves, now):
         """Return OpenFlow messages responding to port operational status change."""
 
         port_status = valve_of.port_status_from_state(state)
         self.notify(
-            {'PORT_CHANGE': {
-                'port_no': port_no,
-                'reason': Valve._decode_port_status(reason),
-                'state': state,
-                'status': port_status}})
+            {
+                "PORT_CHANGE": {
+                    "port_no": port_no,
+                    "reason": Valve._decode_port_status(reason),
+                    "state": state,
+                    "status": port_status,
+                }
+            }
+        )
         self._set_port_status(port_no, port_status, now)
 
         if not self.dp.port_no_valid(port_no):
             return {}
         port = self.dp.ports[port_no]
         if not port.opstatus_reconf:
             return {}
         if reason not in Valve._port_status_codes:
-            self.logger.warning(f'Unhandled port status {reason}/state {state} for {port}')
+            self.logger.warning(
+                "Unhandled port status %s/state %s for %s" % (reason, state, port)
+            )
             return {}
 
         ofmsgs_by_valve = {self: []}
-        new_port_status = (
-            reason == valve_of.ofp.OFPPR_ADD
-            or (reason == valve_of.ofp.OFPPR_MODIFY and port_status))
-        blocked_down_state = (
-            (state & valve_of.ofp.OFPPS_BLOCKED) or (state & valve_of.ofp.OFPPS_LINK_DOWN))
+        new_port_status = reason == valve_of.ofp.OFPPR_ADD or (
+            reason == valve_of.ofp.OFPPR_MODIFY and port_status
+        )
+        blocked_down_state = (state & valve_of.ofp.OFPPS_BLOCKED) or (
+            state & valve_of.ofp.OFPPS_LINK_DOWN
+        )
         live_state = state & valve_of.ofp.OFPPS_LIVE
         decoded_reason = Valve._decode_port_status(reason)
-        state_description = f'{port} up status {port_status} reason {decoded_reason} state {state}'
+        state_description = "%s up status %s reason %s state %s" % (
+            port,
+            port_status,
+            decoded_reason,
+            state,
+        )
         ofmsgs = []
         if new_port_status != port.dyn_phys_up:
-            self.logger.info(f'status change: {state_description}')
+            self.logger.info("status change: %s" % state_description)
             if new_port_status:
                 ofmsgs = self.port_add(port_no)
             else:
-                ofmsgs = self.port_delete(port_no, keep_cache=True, other_valves=_other_valves)
+                ofmsgs = self.port_delete(
+                    port_no, keep_cache=True, other_valves=_other_valves
+                )
         else:
-            self.logger.info(f'status did not change: {state_description}')
+            self.logger.info("status did not change: %s" % state_description)
             if new_port_status:
                 if blocked_down_state:
                     self.logger.info(
-                        f'{port} state down or blocked despite status up, setting to status down')
-                    ofmsgs = self.port_delete(port_no, keep_cache=True, other_valves=_other_valves)
+                        "%s state down or blocked despite status up, setting to status down"
+                        % port
+                    )
+                    ofmsgs = self.port_delete(
+                        port_no, keep_cache=True, other_valves=_other_valves
+                    )
                 if not live_state:
                     self.logger.info(
-                        f'{port} state OFPPS_LIVE reset, ignoring in expectation of port down')
+                        "%s state OFPPS_LIVE reset, ignoring in expectation of port down"
+                        % port
+                    )
         ofmsgs_by_valve[self].extend(ofmsgs)
         return ofmsgs_by_valve
 
     def advertise(self, now, _other_values):
         """Called periodically to advertise services (eg. IPv6 RAs)."""
-        if (not self.dp.advertise_interval
-                or now - self._last_advertise_sec < self.dp.advertise_interval):
+        if (
+            not self.dp.advertise_interval
+            or now - self._last_advertise_sec < self.dp.advertise_interval
+        ):
             return {}
         self._last_advertise_sec = now
 
         ofmsgs = []
         for route_manager in self._route_manager_by_ipv.values():
             for vlan in self.dp.vlans.values():
                 ofmsgs.extend(route_manager.advertise(vlan))
         if ofmsgs:
             return {self: ofmsgs}
         return {}
 
     def _send_lldp_beacon_on_port(self, port, now):
         chassis_id = str(self.dp.faucet_dp_mac)
         ttl = min(
-            self.dp.lldp_beacon.get('send_interval', self.dp.DEFAULT_LLDP_SEND_INTERVAL) * 3,
-            2**16 - 1)
+            self.dp.lldp_beacon.get("send_interval", self.dp.DEFAULT_LLDP_SEND_INTERVAL)
+            * 3,
+            2**16 - 1,
+        )
         org_tlvs = [
-            (tlv['oui'], tlv['subtype'], tlv['info'])
-            for tlv in port.lldp_beacon['org_tlvs']]
+            (tlv["oui"], tlv["subtype"], tlv["info"])
+            for tlv in port.lldp_beacon["org_tlvs"]
+        ]
         org_tlvs.extend(valve_packet.faucet_lldp_tlvs(self.dp))
         org_tlvs.extend(valve_packet.faucet_lldp_stack_state_tlvs(self.dp, port))
-        system_name = port.lldp_beacon['system_name']
+        system_name = port.lldp_beacon["system_name"]
         if not system_name:
-            system_name = self.dp.lldp_beacon.get('system_name', self.dp.name)
+            system_name = self.dp.lldp_beacon.get("system_name", self.dp.name)
         lldp_beacon_pkt = valve_packet.lldp_beacon(
             self.dp.faucet_dp_mac,
-            chassis_id, port.number, ttl,
+            chassis_id,
+            port.number,
+            ttl,
             org_tlvs=org_tlvs,
             system_name=system_name,
-            port_descr=port.lldp_beacon['port_descr'])
+            port_descr=port.lldp_beacon["port_descr"],
+        )
         port.dyn_last_lldp_beacon_time = now
         return valve_of.packetout(port.number, bytes(lldp_beacon_pkt.data))
 
     def fast_advertise(self, now, _other_valves):
         """Called periodically to send LLDP/LACP packets."""
         # NOTE: The beacon service is specifically NOT to support conventional R/STP.
         #   It is intended to facilitate physical troubleshooting (e.g.
         #       a standard cable tester can display OF port information).
         #   It is used also by stacking to verify stacking links.
         # TODO: In the stacking case, provide an authentication scheme for the probes
         #   so they cannot be forged.
-        if (not self.dp.fast_advertise_interval
-                or now - self._last_fast_advertise_sec < self.dp.fast_advertise_interval):
+        if (
+            not self.dp.fast_advertise_interval
+            or now - self._last_fast_advertise_sec < self.dp.fast_advertise_interval
+        ):
             return {}
         self._last_fast_advertise_sec = now
 
         ofmsgs = []
         for port in self.dp.lacp_active_ports:
             ofmsgs.extend(self.switch_manager.lacp_advertise(port))
 
-        lldp_send_interval = self.dp.lldp_beacon.get('send_interval')
-        if (not lldp_send_interval or now - self._last_lldp_advertise_sec >= lldp_send_interval):
+        lldp_send_interval = self.dp.lldp_beacon.get("send_interval")
+        if (
+            not lldp_send_interval
+            or now - self._last_lldp_advertise_sec >= lldp_send_interval
+        ):
             ports = self.dp.lldp_beacon_send_ports(now)
             ofmsgs.extend([self._send_lldp_beacon_on_port(port, now) for port in ports])
             self._last_lldp_advertise_sec = now
 
         if ofmsgs:
             return {self: ofmsgs}
         return {}
 
     def fast_state_expire(self, now, other_valves):
         """Called periodically to verify the state of stack ports."""
         if self.dp.lldp_beacon:
             for port in self.dp.ports.values():
                 if port.dyn_lldp_beacon_recv_state:
                     age = now - port.dyn_lldp_beacon_recv_time
-                    if age > self.dp.lldp_beacon['send_interval'] * port.max_lldp_lost:
-                        self.logger.info(f'LLDP for {port} inactive after {age}s')
+                    if age > self.dp.lldp_beacon["send_interval"] * port.max_lldp_lost:
+                        self.logger.info("LLDP for %s inactive after %us" % (port, age))
                         port.dyn_lldp_beacon_recv_state = None
         return self._lldp_manager.update_stack_link_state(
-            self.dp.stack_ports(), now, self, other_valves)
+            self.dp.stack_ports(), now, self, other_valves
+        )
 
     def _reset_dp_status(self):
-        self._set_var('dp_status', int(self.dp.dyn_running))
+        self._set_var("dp_status", int(self.dp.dyn_running))
 
     def datapath_connect(self, now, discovered_up_ports):
         """Handle Ryu datapath connection event and provision pipeline.
 
         Args:
             now (float): current epoch time.
             discovered_up_ports (set): datapath port numbers that are up.
         Returns:
             list: OpenFlow messages to send to datapath.
         """
-        self.logger.info('Cold start configuring DP')
-        self.notify(
-            {'DP_CHANGE': {
-                'reason': 'cold_start'}})
+        self.logger.info("Cold start configuring DP")
+        self.notify({"DP_CHANGE": {"reason": "cold_start"}})
         ofmsgs = self._cold_start_ports_and_vlans(now, discovered_up_ports)
         self.dp.cold_start(now)
-        self._inc_var('of_dp_connections')
+        self._inc_var("of_dp_connections")
         self._reset_dp_status()
         self.logger.info(self.dp.pipeline_str())
         return ofmsgs
 
     def datapath_disconnect(self, now):
         """Handle Ryu datapath disconnection event."""
-        self.logger.warning('datapath down')
-        self.notify(
-            {'DP_CHANGE': {
-                'reason': 'disconnect'}})
+        self.logger.warning("datapath down")
+        self.notify({"DP_CHANGE": {"reason": "disconnect"}})
         self.dp.dyn_running = False
-        self._inc_var('of_dp_disconnections')
+        self._inc_var("of_dp_disconnections")
         self._reset_dp_status()
         self.ports_delete(self.dp.ports.keys(), now=now)
 
     def _port_delete_flows_state(self, port, keep_cache=False):
         """Delete flows/state for a port."""
         ofmsgs = []
         for route_manager in self._route_manager_by_ipv.values():
@@ -734,34 +838,35 @@
         if not keep_cache:
             for vlan in port.vlans():
                 for entry in port.hosts([vlan]):
                     self._update_expired_host(entry, vlan)
                 vlan.clear_cache_hosts_on_port(port)
         return ofmsgs
 
-    def ports_add(self, port_nums, cold_start=False, log_msg='up'):
+    def ports_add(self, port_nums, cold_start=False, log_msg="up"):
         """Handle the addition of ports.
 
         Args:
             port_num (list): list of port numbers.
             cold_start (bool): True if configuring datapath from scratch.
         Returns:
             list: OpenFlow messages, if any.
         """
         ofmsgs = []
         vlans_with_ports_added = set()
 
         for port_num in port_nums:
             if not self.dp.port_no_valid(port_num):
                 self.logger.info(
-                    'Ignoring port:%u not present in configuration file' % port_num)
+                    "Ignoring port:%u not present in configuration file" % port_num
+                )
                 continue
             port = self.dp.ports[port_num]
             port.dyn_phys_up = True
-            self.logger.info(f'{port} ({port.description}) {log_msg}')
+            self.logger.info("%s (%s) %s" % (port, port.description, log_msg))
 
             if not port.running():
                 continue
 
             for manager in self._managers:
                 ofmsgs.extend(manager.add_port(port))
 
@@ -785,16 +890,17 @@
         Args:
             port_num (list): list of port numbers.
         Returns:
             list: OpenFlow messages, if any.
         """
         return self.ports_add([port_num])
 
-    def ports_delete(self, port_nums, log_msg='down', keep_cache=False,
-                     other_valves=None, now=None):
+    def ports_delete(
+        self, port_nums, log_msg="down", keep_cache=False, other_valves=None, now=None
+    ):
         """Handle the deletion of ports.
 
         Args:
             port_nums (list): list of port numbers.
         Returns:
             list: OpenFlow messages, if any.
         """
@@ -802,48 +908,66 @@
         vlans_with_deleted_ports = set()
 
         for port_num in port_nums:
             if not self.dp.port_no_valid(port_num):
                 continue
             port = self.dp.ports[port_num]
             port.dyn_phys_up = False
-            self.logger.info(f'{port} ({port.description}) {log_msg}')
+            self.logger.info("%s (%s) %s" % (port, port.description, log_msg))
 
             # now is set to a time value only when ports_delete is called to flush
             if now:
                 self._set_port_status(port_num, False, now)
 
             vlans_with_deleted_ports.update(set(port.vlans()))
 
             if port.lacp:
                 ofmsgs.extend(self.lacp_update(port, False, other_valves=other_valves))
             else:
-                ofmsgs.extend(self._port_delete_flows_state(port, keep_cache=keep_cache))
+                ofmsgs.extend(
+                    self._port_delete_flows_state(port, keep_cache=keep_cache)
+                )
 
         for vlan in vlans_with_deleted_ports:
             ofmsgs.extend(self.switch_manager.update_vlan(vlan))
 
         return ofmsgs
 
     def port_delete(self, port_num, keep_cache=False, other_valves=None):
         """Return flow messages that delete port from pipeline."""
-        return self.ports_delete([port_num], keep_cache=keep_cache, other_valves=other_valves)
+        return self.ports_delete(
+            [port_num], keep_cache=keep_cache, other_valves=other_valves
+        )
 
     def _reset_lacp_status(self, port):
         lacp_state = port.actor_state()
         lacp_role = port.lacp_port_state()
-        self._set_port_var('port_lacp_state', lacp_state, port)
-        self._inc_port_var('port_lacp_state_change_count', port)
-        self._set_port_var('lacp_port_id', port.lacp_port_id, port)
-        self._set_port_var('port_lacp_role', lacp_role, port)
+        self._set_port_var("port_lacp_state", lacp_state, port)
+        self._inc_port_var("port_lacp_state_change_count", port)
+        self._set_port_var("lacp_port_id", port.lacp_port_id, port)
+        self._set_port_var("port_lacp_role", lacp_role, port)
         self.notify(
-            {'LAG_CHANGE': {'port_no': port.number, 'state': lacp_state, 'role': lacp_role}})
-
-    def lacp_update(self, port, lacp_up, now=None, lacp_pkt=None,
-                    other_valves=None, cold_start=False):
+            {
+                "LAG_CHANGE": {
+                    "port_no": port.number,
+                    "state": lacp_state,
+                    "role": lacp_role,
+                }
+            }
+        )
+
+    def lacp_update(
+        self,
+        port,
+        lacp_up,
+        now=None,
+        lacp_pkt=None,
+        other_valves=None,
+        cold_start=False,
+    ):
         """Update the port's LACP states and enables/disables pipeline processing.
 
         Args:
             port: The port the packet is being received on
             lacp_up (bool): Whether the lacp actor is up
             now (float): The current time
             lacp_pkt (PacketMeta): The received LACP packet
@@ -851,21 +975,25 @@
             cold_start (bool): Whether port is cold starting.
         Returns:
             ofmsgs
         """
         ofmsgs = []
         if cold_start:
             self.switch_manager.lacp_update_actor_state(
-                port, lacp_up, now, lacp_pkt, cold_start=True)
+                port, lacp_up, now, lacp_pkt, cold_start=True
+            )
             self.switch_manager.lacp_update_port_selection_state(
-                port, self, other_valves, cold_start=True)
+                port, self, other_valves, cold_start=True
+            )
         updated = self.switch_manager.lacp_update_actor_state(
-            port, lacp_up, now, lacp_pkt, cold_start=False)
+            port, lacp_up, now, lacp_pkt, cold_start=False
+        )
         select_updated = self.switch_manager.lacp_update_port_selection_state(
-            port, self, other_valves, cold_start=False)
+            port, self, other_valves, cold_start=False
+        )
         if updated or select_updated:
             self._reset_lacp_status(port)
             if port.is_port_selected() and port.is_actor_up():
                 ofmsgs.extend(self.switch_manager.enable_forwarding(port))
                 ofmsgs.extend(self.add_vlans(port.vlans()))
             else:
                 ofmsgs.extend(self.switch_manager.disable_forwarding(port))
@@ -885,70 +1013,101 @@
             return {}
         pkt_meta.reparse_all()
         lldp_pkt = valve_packet.parse_lldp(pkt_meta.pkt)
         if not lldp_pkt:
             return {}
 
         port = pkt_meta.port
-        (remote_dp_id, remote_dp_name,
-         remote_port_id, remote_port_state) = valve_packet.parse_faucet_lldp(
-             lldp_pkt, self.dp.faucet_dp_mac)
+        (
+            remote_dp_id,
+            remote_dp_name,
+            remote_port_id,
+            remote_port_state,
+        ) = valve_packet.parse_faucet_lldp(lldp_pkt, self.dp.faucet_dp_mac)
 
         port.dyn_lldp_beacon_recv_time = now
         if port.dyn_lldp_beacon_recv_state != remote_port_state:
             chassis_id = str(self.dp.faucet_dp_mac)
             if remote_port_state:
                 self.logger.info(
-                    f'LLDP on {chassis_id}, {port} from {pkt_meta.eth_src} '
-                    f'(remote {valve_util.dpid_log(remote_dp_id)}, port {remote_port_id})'
-                    f' state {port.stack_state_name(remote_port_state)}')
+                    "LLDP on %s, %s from %s (remote %s, port %u) state %s"
+                    % (
+                        chassis_id,
+                        port,
+                        pkt_meta.eth_src,
+                        valve_util.dpid_log(remote_dp_id),
+                        remote_port_id,
+                        port.stack_state_name(remote_port_state),
+                    )
+                )
             port.dyn_lldp_beacon_recv_state = remote_port_state
 
         peer_mac_src = self.dp.ports[port.number].lldp_peer_mac
         if peer_mac_src and peer_mac_src != pkt_meta.eth_src:
-            self.logger.warning(f'Unexpected LLDP peer. Received pkt from {pkt_meta.eth_src} '
-                                f'instead of {peer_mac_src}')
+            self.logger.warning(
+                "Unexpected LLDP peer. Received pkt from %s instead of %s"
+                % (pkt_meta.eth_src, peer_mac_src)
+            )
         ofmsgs_by_valve = {}
         if remote_dp_id and remote_port_id:
-            self.logger.debug(f'FAUCET LLDP on {port} from {pkt_meta.eth_src} '
-                              f'(remote {valve_util.dpid_log(remote_dp_id)}, port {remote_port_id})')
-            ofmsgs_by_valve.update(self._lldp_manager.verify_lldp(
-                port, now, self, other_valves,
-                remote_dp_id, remote_dp_name,
-                remote_port_id, remote_port_state))
+            self.logger.debug(
+                "FAUCET LLDP on %s from %s (remote %s, port %u)"
+                % (
+                    port,
+                    pkt_meta.eth_src,
+                    valve_util.dpid_log(remote_dp_id),
+                    remote_port_id,
+                )
+            )
+            ofmsgs_by_valve.update(
+                self._lldp_manager.verify_lldp(
+                    port,
+                    now,
+                    self,
+                    other_valves,
+                    remote_dp_id,
+                    remote_dp_name,
+                    remote_port_id,
+                    remote_port_state,
+                )
+            )
         else:
-            self.logger.debug(f'LLDP on {port} from {pkt_meta.eth_src}: {str(lldp_pkt)}')
+            self.logger.debug(
+                "LLDP on %s from %s: %s" % (port, pkt_meta.eth_src, str(lldp_pkt))
+            )
         return ofmsgs_by_valve
 
     @staticmethod
     def _control_plane_handler(now, pkt_meta, route_manager):
         """Handle a packet probably destined to FAUCET's route managers.
 
         For example, next hop resolution or ICMP echo requests.
 
         Args:
             pkt_meta (PacketMeta): packet for control plane.
             route_manager (ValveRouteManager): route manager for this eth_type.
         Returns:
             list: OpenFlow messages, if any.
         """
-        if (pkt_meta.eth_dst == pkt_meta.vlan.faucet_mac
-                or not valve_packet.mac_addr_is_unicast(pkt_meta.eth_dst)):
+        if (
+            pkt_meta.eth_dst == pkt_meta.vlan.faucet_mac
+            or not valve_packet.mac_addr_is_unicast(pkt_meta.eth_dst)
+        ):
             return route_manager.control_plane_handler(now, pkt_meta)
         return []
 
     def rate_limit_packet_ins(self, now):
         """Return True if too many packet ins this second."""
         if self._last_packet_in_sec != now:
             self._last_packet_in_sec = now
             self._packet_in_count_sec = 0
         self._packet_in_count_sec += 1
         if self.dp.ignore_learn_ins:
             if self._packet_in_count_sec % self.dp.ignore_learn_ins == 0:
-                self._inc_var('of_ignored_packet_ins')
+                self._inc_var("of_ignored_packet_ins")
                 return True
         return False
 
     def learn_host(self, now, pkt_meta, other_valves):
         """Possibly learn a host on a port.
 
         Args:
@@ -957,57 +1116,73 @@
             other_valves (list): all Valves other than this one.
         Returns:
             list: OpenFlow messages, if any.
         """
         stacked_other_valves = set()
         if self.stack_manager:
             stacked_other_valves = self.stack_manager.stacked_valves(other_valves)
-        learn_port = self.switch_manager.edge_learn_port(
-            stacked_other_valves, pkt_meta)
+        learn_port = self.switch_manager.edge_learn_port(stacked_other_valves, pkt_meta)
         if learn_port is not None:
-            learn_flows, previous_port, update_cache = self.switch_manager.learn_host_on_vlan_ports(
-                now, learn_port, pkt_meta.vlan, pkt_meta.eth_src,
-                last_dp_coldstart_time=self.dp.dyn_last_coldstart_time)
+            (
+                learn_flows,
+                previous_port,
+                update_cache,
+            ) = self.switch_manager.learn_host_on_vlan_ports(
+                now,
+                learn_port,
+                pkt_meta.vlan,
+                pkt_meta.eth_src,
+                last_dp_coldstart_time=self.dp.dyn_last_coldstart_time,
+            )
             if update_cache:
                 pkt_meta.vlan.add_cache_host(pkt_meta.eth_src, learn_port, now)
                 if pkt_meta.l3_pkt is None:
                     pkt_meta.reparse_ip()
-                learn_log = f'L2 learned on {learn_port} {pkt_meta.log()} ' \
-                    f'({pkt_meta.vlan.hosts_count()} hosts total)'
+                learn_log = "L2 learned on %s %s (%u hosts total)" % (
+                    learn_port,
+                    pkt_meta.log(),
+                    pkt_meta.vlan.hosts_count(),
+                )
                 stack_descr = None
                 if pkt_meta.port.stack:
                     stack_descr = pkt_meta.port.stack_descr()
-                    learn_log += f' from {stack_descr}'
+                    learn_log += " from %s" % stack_descr
                 previous_port_no = None
                 if previous_port is not None:
                     previous_port_no = previous_port.number
                     if pkt_meta.port.number != previous_port_no:
-                        learn_log += f', moved from {previous_port}'
+                        learn_log += ", moved from %s" % previous_port
                         if previous_port.stack:
-                            learn_log += f' from {previous_port.stack_descr()}'
+                            learn_log += " from %s" % previous_port.stack_descr()
                 self.logger.info(learn_log)
-                learn_labels = dict(self.dp.base_prom_labels(), vid=pkt_meta.vlan.vid,
-                                    eth_src=pkt_meta.eth_src)
-                self._set_var('learned_l2_port', learn_port.number, labels=learn_labels)
+                learn_labels = dict(
+                    self.dp.base_prom_labels(),
+                    vid=pkt_meta.vlan.vid,
+                    eth_src=pkt_meta.eth_src,
+                )
+                self._set_var("learned_l2_port", learn_port.number, labels=learn_labels)
                 l2_learn_msg = {
-                    'port_no': learn_port.number,
-                    'previous_port_no': previous_port_no,
-                    'vid': pkt_meta.vlan.vid,
-                    'eth_src': pkt_meta.eth_src,
-                    'eth_dst': pkt_meta.eth_dst,
-                    'eth_type': pkt_meta.eth_type,
-                    'l3_src_ip': str(pkt_meta.l3_src),
-                    'l3_dst_ip': str(pkt_meta.l3_dst)}
+                    "port_no": learn_port.number,
+                    "previous_port_no": previous_port_no,
+                    "vid": pkt_meta.vlan.vid,
+                    "eth_src": pkt_meta.eth_src,
+                    "eth_dst": pkt_meta.eth_dst,
+                    "eth_type": pkt_meta.eth_type,
+                    "l3_src_ip": str(pkt_meta.l3_src),
+                    "l3_dst_ip": str(pkt_meta.l3_dst),
+                }
                 if stack_descr:
-                    l2_learn_msg.update({'stack_descr': stack_descr})
-                self.notify({'L2_LEARN': l2_learn_msg})
+                    l2_learn_msg.update({"stack_descr": stack_descr})
+                self.notify({"L2_LEARN": l2_learn_msg})
             return learn_flows
         return []
 
-    def parse_rcv_packet(self, in_port, vlan_vid, eth_type, data, orig_len, pkt, eth_pkt, vlan_pkt):
+    def parse_rcv_packet(
+        self, in_port, vlan_vid, eth_type, data, orig_len, pkt, eth_pkt, vlan_pkt
+    ):
         """Parse a received packet into a PacketMeta instance.
 
         Args:
             in_port (int): port packet was received on.
             vlan_vid (int): VLAN VID of port packet was received on.
             eth_type (int): Ethernet type of packet.
             data (bytes): Raw packet data.
@@ -1021,74 +1196,95 @@
         eth_src = eth_pkt.src
         eth_dst = eth_pkt.dst
         vlan = None
         if vlan_vid in self.dp.vlans:
             vlan = self.dp.vlans[vlan_vid]
         port = self.dp.ports[in_port]
         pkt_meta = valve_packet.PacketMeta(
-            data, orig_len, pkt, eth_pkt, vlan_pkt, port, vlan, eth_src, eth_dst, eth_type)
+            data,
+            orig_len,
+            pkt,
+            eth_pkt,
+            vlan_pkt,
+            port,
+            vlan,
+            eth_src,
+            eth_dst,
+            eth_type,
+        )
         if vlan_vid == self.dp.global_vlan:
             vlan_vid = valve_packet.int_from_mac(pkt_meta.eth_dst)
             vlan = self.dp.vlans.get(vlan_vid, None)
             pkt_meta.vlan = vlan
             if vlan is not None:
                 pkt_meta.eth_dst = vlan.faucet_mac
         return pkt_meta
 
     def parse_pkt_meta(self, msg):
         """Parse OF packet-in message to PacketMeta."""
         if not self.dp.dyn_running:
             return None
         if self.dp.strict_packet_in_cookie and self.dp.cookie != msg.cookie:
-            self.logger.info(f'got packet in with unknown cookie {msg.cookie}')
+            self.logger.info("got packet in with unknown cookie %s" % msg.cookie)
             return None
         # Drop any packet we didn't specifically ask for
         if msg.reason != valve_of.ofp.OFPR_ACTION:
             return None
         if not msg.match:
             return None
-        in_port = msg.match['in_port']
+        in_port = msg.match["in_port"]
         if not in_port or not self.dp.port_no_valid(in_port):
             return None
 
         if not msg.data:
             return None
         # Truncate packet in data (OVS > 2.5 does not honor max_len)
-        data = bytes(msg.data[:valve_of.MAX_PACKET_IN_BYTES])
+        data = bytes(msg.data[: valve_of.MAX_PACKET_IN_BYTES])
 
         # eth/VLAN header only
         pkt, eth_pkt, eth_type, vlan_pkt, vlan_vid = valve_packet.parse_packet_in_pkt(
-            data, max_len=valve_packet.ETH_VLAN_HEADER_SIZE)
+            data, max_len=valve_packet.ETH_VLAN_HEADER_SIZE
+        )
         if pkt is None or eth_pkt is None:
-            self.logger.info(
-                f'unparseable packet from port {in_port}')
+            self.logger.info("unparseable packet from port %u" % in_port)
             return None
-        if (vlan_vid is not None
-                and vlan_vid not in self.dp.vlans
-                and vlan_vid != self.dp.global_vlan):
-            self.logger.info(
-                f'packet for unknown VLAN {vlan_vid}')
+        if (
+            vlan_vid is not None
+            and vlan_vid not in self.dp.vlans
+            and vlan_vid != self.dp.global_vlan
+        ):
+            self.logger.info("packet for unknown VLAN %u" % vlan_vid)
             return None
         pkt_meta = self.parse_rcv_packet(
-            in_port, vlan_vid, eth_type, data, msg.total_len, pkt, eth_pkt, vlan_pkt)
+            in_port, vlan_vid, eth_type, data, msg.total_len, pkt, eth_pkt, vlan_pkt
+        )
         if not valve_packet.mac_addr_is_unicast(pkt_meta.eth_src):
             self.logger.info(
-                f'packet with non-unicast eth_src {pkt_meta.eth_src} port {in_port}')
+                "packet with non-unicast eth_src %s port %u"
+                % (pkt_meta.eth_src, in_port)
+            )
             return None
         if valve_packet.mac_addr_all_zeros(pkt_meta.eth_src):
             self.logger.info(
-                f'packet with all zeros eth_src {pkt_meta.eth_src} port {in_port}')
+                "packet with all zeros eth_src %s port %u" % (pkt_meta.eth_src, in_port)
+            )
             return None
         if self.dp.stack and self.dp.stack.graph:
-            if (not pkt_meta.port.stack
-                    and pkt_meta.vlan
-                    and pkt_meta.vlan not in pkt_meta.port.tagged_vlans
-                    and pkt_meta.vlan != pkt_meta.port.native_vlan):
-                self.logger.warning(f'packet from non-stack port number '
-                                    f'{pkt_meta.port.number} is not member of VLAN {pkt_meta.vlan.vid}')
+            if (
+                not pkt_meta.port.stack
+                and pkt_meta.vlan
+                and pkt_meta.vlan not in pkt_meta.port.tagged_vlans
+                and pkt_meta.vlan != pkt_meta.port.native_vlan
+            ):
+                self.logger.warning(
+                    (
+                        "packet from non-stack port number %u is not member of VLAN %u"
+                        % (pkt_meta.port.number, pkt_meta.vlan.vid)
+                    )
+                )
                 return None
         return pkt_meta
 
     def update_config_metrics(self):
         """Update table names for configuration."""
         self.metrics.reset_dpid(self.dp.base_prom_labels())
         self._reset_dp_status()
@@ -1104,76 +1300,95 @@
             # Also add table miss destination as possible next table, if set
             if table.table_config.miss_goto:
                 miss_table = table.table_config.miss_goto
                 if miss_table not in next_tables:
                     next_tables.append(miss_table)
 
             self._set_var(
-                'faucet_config_table_names',
+                "faucet_config_table_names",
                 table_id,
-                labels=dict(self.dp.base_prom_labels(), table_name=table.name,
-                            next_tables=",".join(next_tables)))
+                labels=dict(
+                    self.dp.base_prom_labels(),
+                    table_name=table.name,
+                    next_tables=",".join(next_tables),
+                ),
+            )
 
     def update_metrics(self, now, updated_port=None, rate_limited=False):
         """Update Gauge/metrics."""
 
         def _update_vlan(vlan, now, rate_limited):
             if vlan.dyn_last_updated_metrics_sec and rate_limited:
-                if now - vlan.dyn_last_updated_metrics_sec < self.dp.metrics_rate_limit_sec:
+                if (
+                    now - vlan.dyn_last_updated_metrics_sec
+                    < self.dp.metrics_rate_limit_sec
+                ):
                     return False
             vlan_labels = dict(self.dp.base_prom_labels(), vlan=vlan.vid)
-            self._set_var('vlan_hosts_learned', vlan.hosts_count(), labels=vlan_labels)
-            self._set_var('vlan_learn_bans', vlan.dyn_learn_ban_count, labels=vlan_labels)
+            self._set_var("vlan_hosts_learned", vlan.hosts_count(), labels=vlan_labels)
+            self._set_var(
+                "vlan_learn_bans", vlan.dyn_learn_ban_count, labels=vlan_labels
+            )
             for ipv in vlan.ipvs():
                 self._set_var(
-                    'vlan_neighbors',
+                    "vlan_neighbors",
                     vlan.neigh_cache_count_by_ipv(ipv),
-                    labels=dict(vlan_labels, ipv=ipv))
+                    labels=dict(vlan_labels, ipv=ipv),
+                )
             return True
 
         def _update_port(vlan, port):
             port_vlan_labels = self._port_vlan_labels(port, vlan)
             port_vlan_hosts_learned = port.hosts_count(vlans=[vlan])
-            self._set_port_var(
-                'port_learn_bans', port.dyn_learn_ban_count, port)
+            self._set_port_var("port_learn_bans", port.dyn_learn_ban_count, port)
             self._set_var(
-                'port_vlan_hosts_learned', port_vlan_hosts_learned, labels=port_vlan_labels)
+                "port_vlan_hosts_learned",
+                port_vlan_hosts_learned,
+                labels=port_vlan_labels,
+            )
             highwater = self._port_highwater[vlan.vid][port.number]
             stats_stale = vlan.dyn_host_cache_stats_stale.get(port.number, True)
             # No change in hosts learned on this VLAN, don't re-export MACs.
             if highwater == port_vlan_hosts_learned and not stats_stale:
                 return
             if highwater > port_vlan_hosts_learned:
                 for i in range(port_vlan_hosts_learned, highwater + 1):
-                    self._set_var('learned_macs', 0, dict(port_vlan_labels, n=i))
+                    self._set_var("learned_macs", 0, dict(port_vlan_labels, n=i))
             self._port_highwater[vlan.vid][port.number] = port_vlan_hosts_learned
             port_vlan_hosts = port.hosts(vlans=[vlan])
             assert port_vlan_hosts_learned == len(port_vlan_hosts)
             # TODO: make MAC table updates less expensive.
             for i, entry in enumerate(sorted(port_vlan_hosts)):
-                self._set_var('learned_macs', entry.eth_src_int, dict(port_vlan_labels, n=i))
+                self._set_var(
+                    "learned_macs", entry.eth_src_int, dict(port_vlan_labels, n=i)
+                )
             vlan.dyn_host_cache_stats_stale[port.number] = False
 
         if updated_port:
             for vlan in updated_port.vlans():
-                if not vlan.reserved_internal_vlan and _update_vlan(vlan, now, rate_limited):
+                if not vlan.reserved_internal_vlan and _update_vlan(
+                    vlan, now, rate_limited
+                ):
                     _update_port(vlan, updated_port)
                     vlan.dyn_last_updated_metrics_sec = now
         else:
             for vlan in self.dp.vlans.values():
-                if not vlan.reserved_internal_vlan and _update_vlan(vlan, now, rate_limited):
+                if not vlan.reserved_internal_vlan and _update_vlan(
+                    vlan, now, rate_limited
+                ):
                     for port in vlan.get_ports():
                         _update_port(vlan, port)
                     vlan.dyn_last_updated_metrics_sec = now
 
     def _non_vlan_rcv_packet(self, now, other_valves, pkt_meta):
-        self._inc_var('of_non_vlan_packet_ins')
+        self._inc_var("of_non_vlan_packet_ins")
         if pkt_meta.port.lacp:
             lacp_ofmsgs_by_valve = self.switch_manager.lacp_handler(
-                now, pkt_meta, self, other_valves, self.lacp_update)
+                now, pkt_meta, self, other_valves, self.lacp_update
+            )
             if lacp_ofmsgs_by_valve:
                 return lacp_ofmsgs_by_valve
         # TODO: verify LLDP message (e.g. org-specific authenticator TLV)
         return self.lldp_handler(now, pkt_meta, other_valves)
 
     def router_rcv_packet(self, now, pkt_meta):
         """Process packets destined for router or run resolver.
@@ -1182,54 +1397,54 @@
             now (float): current epoch time.
             pkt_meta (PacketMeta): packet for control plane.
         Returns:
             list: OpenFlow messages.
         """
         if not pkt_meta.vlan.faucet_vips:
             return []
-        route_manager = self._route_manager_by_eth_type.get(
-            pkt_meta.eth_type, None)
+        route_manager = self._route_manager_by_eth_type.get(pkt_meta.eth_type, None)
         if not (route_manager and route_manager.active):
             return []
         pkt_meta.reparse_ip()
         if not pkt_meta.l3_pkt:
             return []
-        control_plane_ofmsgs = self._control_plane_handler(
-            now, pkt_meta, route_manager)
+        control_plane_ofmsgs = self._control_plane_handler(now, pkt_meta, route_manager)
         ofmsgs = []
         if control_plane_ofmsgs:
             ofmsgs.extend(control_plane_ofmsgs)
         else:
-            ofmsgs.extend(
-                route_manager.add_host_fib_route_from_pkt(now, pkt_meta))
+            ofmsgs.extend(route_manager.add_host_fib_route_from_pkt(now, pkt_meta))
             # No CPN activity, run resolver.
             ofmsgs.extend(
-                route_manager.resolve_gateways(
-                    pkt_meta.vlan, now, resolve_all=False))
+                route_manager.resolve_gateways(pkt_meta.vlan, now, resolve_all=False)
+            )
             ofmsgs.extend(
                 route_manager.resolve_expire_hosts(
-                    pkt_meta.vlan, now, resolve_all=False))
+                    pkt_meta.vlan, now, resolve_all=False
+                )
+            )
         return ofmsgs
 
     def _vlan_rcv_packet(self, now, other_valves, pkt_meta):
         """Handle packet with VLAN tag across all Valves.
 
         Args:
             now (float): current epoch time.
             other_valves (list): all Valves other than this one.
             pkt_meta (PacketMeta): packet for control plane.
         Returns:
             dict: OpenFlow messages, if any by Valve.
         """
-        self._inc_var('of_vlan_packet_ins')
+        self._inc_var("of_vlan_packet_ins")
         ban_rules = self.switch_manager.ban_rules(pkt_meta)
         if ban_rules:
             return {self: ban_rules}
         ofmsgs_by_valve = self.switch_manager.learn_host_from_pkt(
-            self, now, pkt_meta, other_valves)
+            self, now, pkt_meta, other_valves
+        )
         return ofmsgs_by_valve
 
     def rcv_packet(self, now, other_valves, pkt_meta):
         """Handle a packet from the dataplane (eg to re/learn a host).
 
         The packet may be sent to us also in response to FAUCET
         initiating IPv6 neighbor discovery, or ARP, to resolve
@@ -1255,17 +1470,22 @@
             dict: OpenFlow messages, if any by Valve.
         """
         ofmsgs_by_valve = defaultdict(list)
         for lag, ports_up in self.dp.lags_up().items():
             for port in ports_up:
                 lacp_age = now - port.dyn_lacp_updated_time
                 if lacp_age > self.dp.lacp_timeout:
-                    self.logger.info(f'LAG {lag} {port} expired (age {lacp_age})')
-                    ofmsgs_by_valve[self].extend(self.lacp_update(
-                        port, False, now=now, other_valves=_other_valves))
+                    self.logger.info(
+                        "LAG %s %s expired (age %u)" % (lag, port, lacp_age)
+                    )
+                    ofmsgs_by_valve[self].extend(
+                        self.lacp_update(
+                            port, False, now=now, other_valves=_other_valves
+                        )
+                    )
         return ofmsgs_by_valve
 
     def state_expire(self, now, other_valves):
         """Expire controller caches/state (e.g. hosts learned).
 
         Args:
             now (float): current epoch time.
@@ -1277,47 +1497,59 @@
         if self.dp.dyn_running:
             ofmsgs_by_valve.update(self._lacp_state_expire(now, other_valves))
             for vlan in self.dp.vlans.values():
                 expired_hosts = self.switch_manager.expire_hosts_from_vlan(vlan, now)
                 if not self.dp.idle_dst:
                     for entry in expired_hosts:
                         ofmsgs_by_valve[self].extend(
-                            self.switch_manager.delete_host_from_vlan(entry.eth_src, vlan))
+                            self.switch_manager.delete_host_from_vlan(
+                                entry.eth_src, vlan
+                            )
+                        )
                 for entry in expired_hosts:
                     self._update_expired_host(entry, vlan)
                 for route_manager in self._route_manager_by_ipv.values():
-                    ofmsgs_by_valve[self].extend(route_manager.resolve_expire_hosts(vlan, now))
+                    ofmsgs_by_valve[self].extend(
+                        route_manager.resolve_expire_hosts(vlan, now)
+                    )
         return ofmsgs_by_valve
 
     def _update_expired_host(self, entry, vlan):
-        learn_labels = dict(self.dp.base_prom_labels(), vid=vlan.vid,
-                            eth_src=entry.eth_src)
-        self._remove_var('learned_l2_port', labels=learn_labels)
+        learn_labels = dict(
+            self.dp.base_prom_labels(), vid=vlan.vid, eth_src=entry.eth_src
+        )
+        self._remove_var("learned_l2_port", labels=learn_labels)
         self.notify(
-            {'L2_EXPIRE': {
-                'port_no': entry.port.number,
-                'vid': vlan.vid,
-                'eth_src': entry.eth_src}})
+            {
+                "L2_EXPIRE": {
+                    "port_no": entry.port.number,
+                    "vid": vlan.vid,
+                    "eth_src": entry.eth_src,
+                }
+            }
+        )
 
     def _pipeline_diff(self, new_dp):
         old_pipeline = self.dp.pipeline_str().splitlines()
         new_pipeline = new_dp.pipeline_str().splitlines()
         differ = difflib.Differ()
-        diff = '\n'.join(differ.compare(old_pipeline, new_pipeline))
-        self.logger.info(f'pipeline change: {diff}')
+        diff = "\n".join(differ.compare(old_pipeline, new_pipeline))
+        self.logger.info("pipeline change: %s" % diff)
 
     def _pipeline_change(self, new_dp):
         if new_dp:
             # With OVS/soft pipelines, only a change in allocated tables is significant.
             if self.dp.hardware != new_dp.hardware:
                 return True
             old_table_ids = self.dp.pipeline_tableids()
             new_table_ids = new_dp.pipeline_tableids()
             if old_table_ids != new_table_ids:
-                self.logger.info(f'table IDs changed, old {old_table_ids} new {new_table_ids}')
+                self.logger.info(
+                    "table IDs changed, old %s new %s" % (old_table_ids, new_table_ids)
+                )
                 return True
         return False
 
     def _apply_config_changes(self, new_dp, changes, valves=None):
         """Apply any detected configuration changes.
 
         Args:
@@ -1336,34 +1568,44 @@
                 added_meters: (set): added meter numbers.
             valves (list): List of other running valves
         Returns:
             tuple:
                 restart_type (string or None)
                 ofmsgs (list): OpenFlow messages.
         """
-        (deleted_ports, changed_ports, added_ports, changed_acl_ports,
-         deleted_vids, changed_vids, all_ports_changed,
-         _, deleted_meters, added_meters, changed_meters) = changes
-        restart_type = 'cold'
+        (
+            deleted_ports,
+            changed_ports,
+            added_ports,
+            changed_acl_ports,
+            deleted_vids,
+            changed_vids,
+            all_ports_changed,
+            _,
+            deleted_meters,
+            added_meters,
+            changed_meters,
+        ) = changes
+        restart_type = "cold"
         ofmsgs = []
 
         # If pipeline or all ports changed, default to cold start.
         if self._pipeline_change(new_dp):
             self.dp_init(new_dp, valves)
             return restart_type, ofmsgs
 
         if all_ports_changed:
-            self.logger.info('all ports changed')
+            self.logger.info("all ports changed")
             self.dp_init(new_dp, valves)
             return restart_type, ofmsgs
 
         restart_type = None
         for change in changes:
             if change:
-                restart_type = 'warm'
+                restart_type = "warm"
                 break
 
         # Nothing changed, nothing to check.
         if restart_type is None:
             self.dp_init(new_dp)
             return restart_type, ofmsgs
 
@@ -1391,21 +1633,21 @@
         self.dp_init(new_dp, valves)
 
         if self.acl_manager:
             if added_meters:
                 ofmsgs.extend(self.acl_manager.add_meters(added_meters))
         if added_ports:
             all_up_port_nos = [
-                port for port in added_ports
-                if port in self.dp.dyn_up_port_nos]
+                port for port in added_ports if port in self.dp.dyn_up_port_nos
+            ]
             ofmsgs.extend(self.ports_add(all_up_port_nos))
         if changed_ports:
             all_up_port_nos = [
-                port for port in changed_ports
-                if port in self.dp.dyn_up_port_nos]
+                port for port in changed_ports if port in self.dp.dyn_up_port_nos
+            ]
             ofmsgs.extend(self.ports_add(all_up_port_nos))
         if self.acl_manager and changed_acl_ports:
             for port_num in changed_acl_ports:
                 port = self.dp.ports[port_num]
                 ofmsgs.extend(self.acl_manager.cold_start_port(port))
         if changed_vids:
             changed_vlans = [self.dp.vlans[vid] for vid in changed_vids]
@@ -1432,26 +1674,29 @@
             now (float): current epoch time.
             new_dp (DP): new dataplane configuration.
             valves (list): List of all valves
         Returns:
             ofmsgs (list): OpenFlow messages.
         """
         restart_type, ofmsgs = self._apply_config_changes(
-            new_dp, self.dp.get_config_changes(self.logger, new_dp), valves)
+            new_dp, self.dp.get_config_changes(self.logger, new_dp), valves
+        )
         if restart_type is not None:
-            self._inc_var(f'faucet_config_reload_{restart_type}')
-            self.logger.info(f'{restart_type} starting')
-            if restart_type == 'cold':
-                self.logger.info('forcing DP reconnection to ensure ports are synchronized')
+            self._inc_var("faucet_config_reload_%s" % restart_type)
+            self.logger.info("%s starting" % restart_type)
+            if restart_type == "cold":
+                self.logger.info(
+                    "forcing DP reconnection to ensure ports are synchronized"
+                )
                 ofmsgs = None
-            elif restart_type == 'warm':
+            elif restart_type == "warm":
                 # DP not currently up, so no messages to send.
                 if not self.dp.dyn_running:
                     ofmsgs = []
-        self.notify({'CONFIG_CHANGE': {'restart_type': restart_type}})
+        self.notify({"CONFIG_CHANGE": {"restart_type": restart_type}})
         return ofmsgs
 
     def _warm_reconfig_port_native_vlans(self, port, new_dyn_dot1x_native_vlan):
         ofmsgs = []
         old_vlan = port.dyn_dot1x_native_vlan
         ofmsgs.extend(self.switch_manager.del_port(port))
         port.dyn_dot1x_native_vlan = new_dyn_dot1x_native_vlan
@@ -1508,58 +1753,69 @@
 
     def oferror(self, msg):
         """Correlate OFError message with flow we sent, if any.
 
         Args:
             msg (ryu.controller.ofp_event.EventOFPMsgBase): message from datapath.
         """
-        orig_msgs = [orig_msg for orig_msg in self.recent_ofmsgs if orig_msg.xid == msg.xid]
+        orig_msgs = [
+            orig_msg for orig_msg in self.recent_ofmsgs if orig_msg.xid == msg.xid
+        ]
         error_txt = msg
         if orig_msgs:
-            error_txt = f'{error_txt} caused by {orig_msgs[0]}'
-        error_type = 'UNKNOWN'
-        error_code = 'UNKNOWN'
+            error_txt = "%s caused by %s" % (error_txt, orig_msgs[0])
+        error_type = "UNKNOWN"
+        error_code = "UNKNOWN"
         try:
             error_tuple = valve_of.OFERROR_TYPE_CODE[msg.type]
             error_type = error_tuple[0]
             error_code = error_tuple[1][msg.code]
         except KeyError:
             pass
         if self.dp.group_table:
             # Unlike flows, adding an overwriting group (same group_id) is considered an error.
             # This "error" is expected with groups and redundant controllers, as one controller
             # may delete another's groups while they synchronize with new network state.
-            if (msg.type == valve_of.ofp.OFPET_GROUP_MOD_FAILED
-                    and msg.code == valve_of.ofp.OFPGMFC_GROUP_EXISTS):
+            if (
+                msg.type == valve_of.ofp.OFPET_GROUP_MOD_FAILED
+                and msg.code == valve_of.ofp.OFPGMFC_GROUP_EXISTS
+            ):
                 return
 
             # We output a flow referencing a group, that a redundant
             # controller deleted before sending its own copy of this flow.
-            if (msg.type == valve_of.ofp.OFPET_BAD_ACTION
-                    and msg.code == valve_of.ofp.OFPBAC_BAD_OUT_GROUP):
+            if (
+                msg.type == valve_of.ofp.OFPET_BAD_ACTION
+                and msg.code == valve_of.ofp.OFPBAC_BAD_OUT_GROUP
+            ):
                 return
-        if (msg.type == valve_of.ofp.OFPET_METER_MOD_FAILED
-                and msg.code == valve_of.ofp.OFPMMFC_METER_EXISTS):
+        if (
+            msg.type == valve_of.ofp.OFPET_METER_MOD_FAILED
+            and msg.code == valve_of.ofp.OFPMMFC_METER_EXISTS
+        ):
             # Same scenario as groups.
             return
-        self._inc_var('of_errors')
-        self.logger.error(f'OFError type: {error_type} code: {error_code} {error_txt}')
+        self._inc_var("of_errors")
+        self.logger.error(
+            "OFError type: %s code: %s %s" % (error_type, error_code, error_txt)
+        )
 
     def prepare_send_flows(self, flow_msgs):
         """Prepare to send flows to datapath.
 
         Args:
             flow_msgs (list): OpenFlow messages to send.
         """
         if flow_msgs is None:
             return flow_msgs
         reordered_flow_msgs = valve_of.valve_flowreorder(
-            flow_msgs, use_barriers=self.USE_BARRIERS)
+            flow_msgs, use_barriers=self.USE_BARRIERS
+        )
         self.ofchannel_log(reordered_flow_msgs)
-        self._inc_var('of_flowmsgs_sent', val=len(reordered_flow_msgs))
+        self._inc_var("of_flowmsgs_sent", val=len(reordered_flow_msgs))
         self.recent_ofmsgs.extend(reordered_flow_msgs)
         return reordered_flow_msgs
 
     def send_flows(self, ryu_dp, flow_msgs, now):
         """Send flows to datapath (or disconnect an OF session).
 
         Args:
@@ -1596,18 +1852,26 @@
 
     USE_OXM_IDS = True
     MAX_TABLE_ID = 0
     MIN_MAX_FLOWS = 0
     FILL_REQ = True
 
     def _pipeline_flows(self):
-        return [valve_of.table_features(
-            tfm_pipeline.load_tables(
-                self.dp, self, self.MAX_TABLE_ID, self.MIN_MAX_FLOWS,
-                self.USE_OXM_IDS, self.FILL_REQ))]
+        return [
+            valve_of.table_features(
+                tfm_pipeline.load_tables(
+                    self.dp,
+                    self,
+                    self.MAX_TABLE_ID,
+                    self.MIN_MAX_FLOWS,
+                    self.USE_OXM_IDS,
+                    self.FILL_REQ,
+                )
+            )
+        ]
 
     def _pipeline_change(self, new_dp):
         if new_dp:
             old_pipeline = self.dp.pipeline_str()
             new_pipeline = new_dp.pipeline_str()
             # TFM based pipelines, any pipeline change is significant.
             if old_pipeline != new_pipeline:
@@ -1668,26 +1932,26 @@
     """Valve implementation for NoviFlow with static pipeline."""
 
     STATIC_TABLE_IDS = True
     USE_BARRIERS = True
 
 
 SUPPORTED_HARDWARE = {
-    'Generic': Valve,
-    'GenericTFM': TfmValve,
-    'Allied-Telesis': AlliedTelesis,
-    'Aruba': ArubaValve,
-    'CiscoC9K': CiscoC9KValve,
-    'Lagopus': OVSValve,
-    'Netronome': OVSValve,
-    'NoviFlow': NoviFlowValve,
-    'Open vSwitch': OVSValve,
-    'Open vSwitch TFM': OVSTfmValve,
-    'ZodiacFX': OVSValve,
-    'ZodiacGX': OVSValve,
+    "Generic": Valve,
+    "GenericTFM": TfmValve,
+    "Allied-Telesis": AlliedTelesis,
+    "Aruba": ArubaValve,
+    "CiscoC9K": CiscoC9KValve,
+    "Lagopus": OVSValve,
+    "Netronome": OVSValve,
+    "NoviFlow": NoviFlowValve,
+    "Open vSwitch": OVSValve,
+    "Open vSwitch TFM": OVSTfmValve,
+    "ZodiacFX": OVSValve,
+    "ZodiacGX": OVSValve,
 }
 
 
 def valve_factory(dp):
     """Return a Valve object based dp's hardware configuration field.
 
     Args:
```

### Comparing `c65faucet-1.0.46/faucet/valve_acl.py` & `c65faucet-1.0.47/faucet/valve_acl.py`

 * *Files 8% similar despite different names*

```diff
@@ -24,340 +24,424 @@
 
 
 def push_vlan(acl_table, vlan_vid):
     """Push a VLAN tag with optional selection of eth type."""
     vid = vlan_vid
     vlan_eth_type = None
     if isinstance(vlan_vid, dict):
-        vid = vlan_vid['vid']
-        if 'eth_type' in vlan_vid:
-            vlan_eth_type = vlan_vid['eth_type']
+        vid = vlan_vid["vid"]
+        if "eth_type" in vlan_vid:
+            vlan_eth_type = vlan_vid["eth_type"]
     if vlan_eth_type is None:
         return valve_of.push_vlan_act(acl_table, vid)
-    return valve_of.push_vlan_act(
-        acl_table, vid, eth_type=vlan_eth_type)
+    return valve_of.push_vlan_act(acl_table, vid, eth_type=vlan_eth_type)
 
 
-def build_ordered_output_actions(acl_table, output_list, tunnel_rules=None, source_id=None):
+def build_ordered_output_actions(
+    acl_table, output_list, tunnel_rules=None, source_id=None
+):
     """Build actions from ordered ACL output list"""
     output_actions = []
     output_ports = []
     output_ofmsgs = []
     output_inst = []
     for action in output_list:
         for key, value in action.items():
-            if key == 'pop_vlans':
+            if key == "pop_vlans":
                 for _ in range(value):
                     output_actions.append(valve_of.pop_vlan())
-            if key == 'vlan_vid':
+            if key == "vlan_vid":
                 output_actions.extend(push_vlan(acl_table, value))
-            if key == 'swap_vid':
+            if key == "swap_vid":
                 output_actions.append(acl_table.set_vlan_vid(value))
-            if key == 'vlan_vids':
+            if key == "vlan_vids":
                 for vlan_vid in value:
                     output_actions.extend(push_vlan(acl_table, vlan_vid))
-            if key == 'set_fields':
+            if key == "set_fields":
                 for set_field in value:
                     output_actions.append(acl_table.set_field(**set_field))
-            if key == 'port':
+            if key == "port":
                 output_ports.append(value)
                 output_actions.append(valve_of.output_port(value))
-            if key == 'ports':
+            if key == "ports":
                 for output_port in value:
                     output_ports.append(output_port)
                     output_actions.append(valve_of.output_port(output_port))
-            if key == 'failover':
-                group_id = value['group_id']
+            if key == "failover":
+                group_id = value["group_id"]
                 buckets = []
-                for port in value['ports']:
-                    buckets.append(valve_of.bucket(
-                        watch_port=port, actions=[valve_of.output_port(port)]))
-                output_ofmsgs.extend(valve_of.groupadd_ff(group_id=group_id, buckets=buckets))
+                for port in value["ports"]:
+                    buckets.append(
+                        valve_of.bucket(
+                            watch_port=port, actions=[valve_of.output_port(port)]
+                        )
+                    )
+                output_ofmsgs.extend(
+                    valve_of.groupadd_ff(group_id=group_id, buckets=buckets)
+                )
                 output_actions.append(valve_of.group_act(group_id=group_id))
-            if key == 'tunnel' and tunnel_rules and source_id is not None:
+            if key == "tunnel" and tunnel_rules and source_id is not None:
                 source_rule = tunnel_rules[value][source_id]
-                _, tunnel_actions, tunnel_ofmsgs, tunnel_inst = build_output_actions(acl_table,
-                                                                                     source_rule)
+                _, tunnel_actions, tunnel_ofmsgs, tunnel_inst = build_output_actions(
+                    acl_table, source_rule
+                )
                 output_actions.extend(tunnel_actions)
                 output_ofmsgs.extend(tunnel_ofmsgs)
                 output_inst.extend(tunnel_inst)
-            if key == 'goto':
+            if key == "goto":
                 output_inst.append(valve_of.goto_table_id(value))
     return (output_ports, output_actions, output_ofmsgs, output_inst)
 
 
 def rewrite_vlan(acl_table, output_dict):
     """Implement actions to rewrite VLAN headers."""
     vlan_actions = []
-    if 'pop_vlans' in output_dict:
-        for _ in range(output_dict['pop_vlans']):
+    if "pop_vlans" in output_dict:
+        for _ in range(output_dict["pop_vlans"]):
             vlan_actions.append(valve_of.pop_vlan())
     # if vlan tag is specified, push it.
-    if 'vlan_vid' in output_dict:
-        vlan_actions.extend(push_vlan(acl_table, output_dict['vlan_vid']))
+    if "vlan_vid" in output_dict:
+        vlan_actions.extend(push_vlan(acl_table, output_dict["vlan_vid"]))
     # swap existing VID
-    elif 'swap_vid' in output_dict:
-        vlan_actions.append(
-            acl_table.set_vlan_vid(output_dict['swap_vid']))
+    elif "swap_vid" in output_dict:
+        vlan_actions.append(acl_table.set_vlan_vid(output_dict["swap_vid"]))
     # or, if a list, push them all (all with type Q).
-    elif 'vlan_vids' in output_dict:
-        for vlan_vid in output_dict['vlan_vids']:
+    elif "vlan_vids" in output_dict:
+        for vlan_vid in output_dict["vlan_vids"]:
             vlan_actions.extend(push_vlan(acl_table, vlan_vid))
     return vlan_actions
 
 
 def build_ct_actions(acl_table, ct_dict):
     """Build conntrack action from ACL rule"""
 
-    if 'clear' in ct_dict and ct_dict['clear']:
+    if "clear" in ct_dict and ct_dict["clear"]:
         return valve_of.ct_clear()
 
     ct_actions = []
 
-    if 'nat' in ct_dict:
+    if "nat" in ct_dict:
         nat_args = {
-            'flags': ct_dict['nat'].get('flags', 0),
-            'range_ipv4_min': ct_dict['nat'].get('range_ipv4_min', ''),
-            'range_ipv4_max': ct_dict['nat'].get('range_ipv4_max', ''),
-            'range_ipv6_min': ct_dict['nat'].get('range_ipv6_min', ''),
-            'range_ipv6_max': ct_dict['nat'].get('range_ipv6_max', ''),
-            'range_proto_min': ct_dict['nat'].get('range_proto_min', None),
-            'range_proto_max': ct_dict['nat'].get('range_proto_max', None),
+            "flags": ct_dict["nat"].get("flags", 0),
+            "range_ipv4_min": ct_dict["nat"].get("range_ipv4_min", ""),
+            "range_ipv4_max": ct_dict["nat"].get("range_ipv4_max", ""),
+            "range_ipv6_min": ct_dict["nat"].get("range_ipv6_min", ""),
+            "range_ipv6_max": ct_dict["nat"].get("range_ipv6_max", ""),
+            "range_proto_min": ct_dict["nat"].get("range_proto_min", None),
+            "range_proto_max": ct_dict["nat"].get("range_proto_max", None),
         }
         ct_actions.append(valve_of.ct_nat(**nat_args))
 
     ct_args = {
-        'flags': ct_dict.get('flags', 0),
-        'actions': ct_actions,
-        'alg': ct_dict.get('alg', 0),
-        'recirc_table': ct_dict.get('table'),
-        'zone_ofs_nbits': ct_dict.get('zone'),
-        'zone_src': ct_dict.get('zone_src', None),
+        "flags": ct_dict.get("flags", 0),
+        "actions": ct_actions,
+        "alg": ct_dict.get("alg", 0),
+        "recirc_table": ct_dict.get("table"),
+        "zone_ofs_nbits": ct_dict.get("zone"),
+        "zone_src": ct_dict.get("zone_src", None),
     }
 
     return valve_of.ct(**ct_args)
 
 
 def build_output_actions(acl_table, output_dict, tunnel_rules=None, source_id=None):
     """Implement actions to alter packet/output."""
     if isinstance(output_dict, (list, tuple)):
-        return build_ordered_output_actions(acl_table, output_dict, tunnel_rules, source_id)
+        return build_ordered_output_actions(
+            acl_table, output_dict, tunnel_rules, source_id
+        )
     output_actions = []
     output_inst = []
     output_port = None
     ofmsgs = []
     # rewrite any VLAN headers first always
     vlan_actions = rewrite_vlan(acl_table, output_dict)
     if vlan_actions:
         output_actions.extend(vlan_actions)
-    if 'set_fields' in output_dict:
-        for set_field in output_dict['set_fields']:
+    if "set_fields" in output_dict:
+        for set_field in output_dict["set_fields"]:
             output_actions.append(acl_table.set_field(**set_field))
-    if 'port' in output_dict:
-        output_port = output_dict['port']
+    if "port" in output_dict:
+        output_port = output_dict["port"]
         output_actions.append(valve_of.output_port(output_port))
-    if 'ports' in output_dict:
-        for output_port in output_dict['ports']:
+    if "ports" in output_dict:
+        for output_port in output_dict["ports"]:
             output_actions.append(valve_of.output_port(output_port))
-    if 'failover' in output_dict:
-        failover = output_dict['failover']
-        group_id = failover['group_id']
+    if "failover" in output_dict:
+        failover = output_dict["failover"]
+        group_id = failover["group_id"]
         buckets = []
-        for port in failover['ports']:
-            buckets.append(valve_of.bucket(
-                watch_port=port, actions=[valve_of.output_port(port)]))
+        for port in failover["ports"]:
+            buckets.append(
+                valve_of.bucket(watch_port=port, actions=[valve_of.output_port(port)])
+            )
         ofmsgs.extend(valve_of.groupadd_ff(group_id=group_id, buckets=buckets))
         output_actions.append(valve_of.group_act(group_id=group_id))
-    if 'tunnel' in output_dict and tunnel_rules and source_id is not None:
-        tunnel_id = output_dict['tunnel']
+    if "tunnel" in output_dict and tunnel_rules and source_id is not None:
+        tunnel_id = output_dict["tunnel"]
         source_rule = tunnel_rules[tunnel_id][source_id]
-        _, tunnel_actions, tunnel_ofmsgs, tunnel_inst = build_output_actions(acl_table, source_rule)
+        _, tunnel_actions, tunnel_ofmsgs, tunnel_inst = build_output_actions(
+            acl_table, source_rule
+        )
         output_actions.extend(tunnel_actions)
         tunnel_ofmsgs.extend(tunnel_ofmsgs)
         output_inst.extend(tunnel_inst)
-    if 'goto' in output_dict:
-        output_inst.append(valve_of.goto_table_id(output_dict['goto']))
+    if "goto" in output_dict:
+        output_inst.append(valve_of.goto_table_id(output_dict["goto"]))
     return (output_port, output_actions, ofmsgs, output_inst)
 
 
 def build_acl_entry(  # pylint: disable=too-many-arguments,too-many-branches,too-many-statements
-        acl_table, rule_conf, meters,
-        acl_allow_inst, acl_force_port_vlan_inst,
-        port_num=None, vlan_vid=None, tunnel_rules=None, source_id=None):
+    acl_table,
+    rule_conf,
+    meters,
+    acl_allow_inst,
+    acl_force_port_vlan_inst,
+    port_num=None,
+    vlan_vid=None,
+    tunnel_rules=None,
+    source_id=None,
+):
     """Build flow/groupmods for one ACL rule entry."""
     acl_inst = []
     acl_act = []
     acl_match_dict = {}
     acl_ofmsgs = []
     acl_cookie = None
     allow_inst = acl_allow_inst
 
     for attrib, attrib_value in rule_conf.items():
         # if attrib == 'in_port':
         #     continue
-        if attrib == 'cookie':
+        if attrib == "cookie":
             acl_cookie = attrib_value
             continue
-        if attrib == 'description':
+        if attrib == "description":
             continue
-        if attrib == 'actions':
+        if attrib == "actions":
             allow = False
             allow_specified = False
-            if 'allow' in attrib_value:
+            if "allow" in attrib_value:
                 allow_specified = True
-                if attrib_value['allow'] == 1:
+                if attrib_value["allow"] == 1:
                     allow = True
-            if 'force_port_vlan' in attrib_value:
-                if attrib_value['force_port_vlan'] == 1:
+            if "force_port_vlan" in attrib_value:
+                if attrib_value["force_port_vlan"] == 1:
                     allow_inst = acl_force_port_vlan_inst
-            if 'meter' in attrib_value:
-                meter_name = attrib_value['meter']
+            if "meter" in attrib_value:
+                meter_name = attrib_value["meter"]
                 acl_inst.append(valve_of.apply_meter(meters[meter_name].meter_id))
-            if 'mirror' in attrib_value:
-                port_no = attrib_value['mirror']
+            if "mirror" in attrib_value:
+                port_no = attrib_value["mirror"]
                 acl_act.append(valve_of.output_port(port_no))
                 if not allow_specified:
                     allow = True
-            if 'output' in attrib_value:
-                output_port, output_actions, output_ofmsgs, output_inst = build_output_actions(
-                    acl_table, attrib_value['output'], tunnel_rules, source_id)
+            if "output" in attrib_value:
+                (
+                    output_port,
+                    output_actions,
+                    output_ofmsgs,
+                    output_inst,
+                ) = build_output_actions(
+                    acl_table, attrib_value["output"], tunnel_rules, source_id
+                )
                 acl_act.extend(output_actions)
                 acl_ofmsgs.extend(output_ofmsgs)
                 acl_inst.extend(output_inst)
 
                 # if port specified, output packet now and exit pipeline.
                 if not allow and output_port is not None:
                     continue
-            if 'ct' in attrib_value:
-                ct_action = build_ct_actions(acl_table, attrib_value['ct'])
+            if "ct" in attrib_value:
+                ct_action = build_ct_actions(acl_table, attrib_value["ct"])
                 acl_act.append(ct_action)
 
             if allow:
                 acl_inst.extend(allow_inst)
         else:
             acl_match_dict[attrib] = attrib_value
     if port_num is not None:
         # This overwrites the `in_port` match if it is specified in the ACL config
-        acl_match_dict['in_port'] = port_num
+        acl_match_dict["in_port"] = port_num
     if vlan_vid is not None:
         # This overwrites the `vlan_vid` match if it is specified in the ACL config
-        acl_match_dict['vlan_vid'] = valve_of.vid_present(vlan_vid)
+        acl_match_dict["vlan_vid"] = valve_of.vid_present(vlan_vid)
     try:
         acl_match = valve_of.match_from_dict(acl_match_dict)
     except TypeError as type_error:
-        raise InvalidConfigError('invalid match type in ACL') from type_error
+        raise InvalidConfigError("invalid match type in ACL") from type_error
     if acl_act:
         acl_inst.append(valve_of.apply_actions(acl_act))
     return (acl_match, acl_inst, acl_cookie, acl_ofmsgs)
 
 
-def build_tunnel_ofmsgs(rule_conf, acl_table, priority,
-                        port_num=None, vlan_vid=None, flowdel=False,
-                        reverse=False):
+def build_tunnel_ofmsgs(
+    rule_conf,
+    acl_table,
+    priority,
+    port_num=None,
+    vlan_vid=None,
+    flowdel=False,
+    reverse=False,
+):
     """Build a specific tunnel only ofmsgs"""
     ofmsgs = []
     acl_inst = []
     acl_match = []
     acl_match_dict = {}
-    _, output_actions, output_ofmsgs, output_inst = build_output_actions(acl_table, rule_conf)
+    _, output_actions, output_ofmsgs, output_inst = build_output_actions(
+        acl_table, rule_conf
+    )
     ofmsgs.extend(output_ofmsgs)
     acl_inst.extend(output_inst)
     acl_inst.append(valve_of.apply_actions(output_actions))
     if port_num is not None:
-        acl_match_dict['in_port'] = port_num
+        acl_match_dict["in_port"] = port_num
     if vlan_vid is not None:
-        acl_match_dict['vlan_vid'] = valve_of.vid_present(vlan_vid)
+        acl_match_dict["vlan_vid"] = valve_of.vid_present(vlan_vid)
     if reverse:
-        acl_match_dict['vlan_pcp'] = valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG
+        acl_match_dict["vlan_pcp"] = valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG
     else:
-        acl_match_dict['vlan_pcp'] = valve_of.PCP_TUNNEL_FLAG
+        acl_match_dict["vlan_pcp"] = valve_of.PCP_TUNNEL_FLAG
     try:
         acl_match = valve_of.match_from_dict(acl_match_dict)
     except TypeError as type_error:
-        raise InvalidConfigError('invalid match type in ACL') from type_error
+        raise InvalidConfigError("invalid match type in ACL") from type_error
     flowmod = acl_table.flowmod(acl_match, priority=priority, inst=tuple(acl_inst))
     if flowdel:
-        ofmsgs.append(acl_table.flowdel(match=acl_match, priority=priority, strict=False))
+        ofmsgs.append(
+            acl_table.flowdel(match=acl_match, priority=priority, strict=False)
+        )
     ofmsgs.append(flowmod)
     return ofmsgs
 
 
-def build_rule_ofmsgs(rule_conf, acl_table,
-                      acl_allow_inst, acl_force_port_vlan_inst,
-                      highest_priority, acl_rule_priority, meters,
-                      exact_match, port_num=None, vlan_vid=None,
-                      tunnel_rules=None, source_id=None, flowdel=False):
+def build_rule_ofmsgs(
+    rule_conf,
+    acl_table,
+    acl_allow_inst,
+    acl_force_port_vlan_inst,
+    highest_priority,
+    acl_rule_priority,
+    meters,
+    exact_match,
+    port_num=None,
+    vlan_vid=None,
+    tunnel_rules=None,
+    source_id=None,
+    flowdel=False,
+):
     """Build an ACL rule and return OFMSGs"""
     ofmsgs = []
     acl_match, acl_inst, acl_cookie, acl_ofmsgs = build_acl_entry(
-        acl_table, rule_conf, meters,
-        acl_allow_inst, acl_force_port_vlan_inst,
-        port_num, vlan_vid, tunnel_rules, source_id)
+        acl_table,
+        rule_conf,
+        meters,
+        acl_allow_inst,
+        acl_force_port_vlan_inst,
+        port_num,
+        vlan_vid,
+        tunnel_rules,
+        source_id,
+    )
     ofmsgs.extend(acl_ofmsgs)
     priority = acl_rule_priority
     if exact_match:
         priority = highest_priority
     flowmod = acl_table.flowmod(
-        acl_match, priority=priority, inst=tuple(acl_inst), cookie=acl_cookie)
+        acl_match, priority=priority, inst=tuple(acl_inst), cookie=acl_cookie
+    )
     if flowdel:
         ofmsgs.append(acl_table.flowdel(match=acl_match, priority=priority))
     ofmsgs.append(flowmod)
     return ofmsgs
 
 
-def build_acl_ofmsgs(acls, acl_table,
-                     acl_allow_inst, acl_force_port_vlan_inst,
-                     highest_priority, meters,
-                     exact_match, port_num=None, vlan_vid=None,
-                     tunnel_rules=None, source_id=None, flowdel=False):
+def build_acl_ofmsgs(
+    acls,
+    acl_table,
+    acl_allow_inst,
+    acl_force_port_vlan_inst,
+    highest_priority,
+    meters,
+    exact_match,
+    port_num=None,
+    vlan_vid=None,
+    tunnel_rules=None,
+    source_id=None,
+    flowdel=False,
+):
     """Build flow/groupmods for all entries in an ACL."""
     ofmsgs = []
     acl_rule_priority = highest_priority
     for acl in acls:
         for rule_conf in acl.rules:
-            ofmsgs.extend(build_rule_ofmsgs(
-                rule_conf, acl_table,
-                acl_allow_inst, acl_force_port_vlan_inst,
-                highest_priority, acl_rule_priority, meters,
-                exact_match, port_num, vlan_vid,
-                tunnel_rules=tunnel_rules, source_id=source_id, flowdel=flowdel))
+            ofmsgs.extend(
+                build_rule_ofmsgs(
+                    rule_conf,
+                    acl_table,
+                    acl_allow_inst,
+                    acl_force_port_vlan_inst,
+                    highest_priority,
+                    acl_rule_priority,
+                    meters,
+                    exact_match,
+                    port_num,
+                    vlan_vid,
+                    tunnel_rules=tunnel_rules,
+                    source_id=source_id,
+                    flowdel=flowdel,
+                )
+            )
             acl_rule_priority -= 1
     return ofmsgs
 
 
 def build_acl_port_of_msgs(acl, vid, port_num, acl_table, goto_table, priority):
     """A Helper function for building Openflow Mod Messages for Port ACLs"""
     ofmsgs = None
     if acl.rules:
         ofmsgs = build_acl_ofmsgs(
-            [acl], acl_table,
+            [acl],
+            acl_table,
             [valve_of.goto_table(goto_table)],
             [valve_of.goto_table(goto_table)],
-            priority, acl.meter, acl.exact_match,
-            vlan_vid=vid, port_num=port_num)
+            priority,
+            acl.meter,
+            acl.exact_match,
+            vlan_vid=vid,
+            port_num=port_num,
+        )
     return ofmsgs
 
 
 def add_mac_address_to_match(match, eth_src):
     """Add or change the value of a match type"""
     # NOTE: This function has been created to work around for
     # OFPMatch.set_dl_src() not storing persistent changes
     if not eth_src:
         return match
 
     dict_match = dict(match.items())
-    dict_match['eth_src'] = eth_src
+    dict_match["eth_src"] = eth_src
     return valve_of.match_from_dict(dict_match)
 
 
 class ValveAclManager(ValveManagerBase):
     """Handle installation of ACLs on a DP"""
 
-    def __init__(self, port_acl_table, vlan_acl_table, egress_acl_table,
-                 pipeline, meters, dp_acls=None):
+    def __init__(
+        self,
+        port_acl_table,
+        vlan_acl_table,
+        egress_acl_table,
+        pipeline,
+        meters,
+        dp_acls=None,
+    ):
         self.dp_acls = dp_acls
         self.port_acl_table = port_acl_table
         self.vlan_acl_table = vlan_acl_table
         self.egress_acl_table = egress_acl_table
         self.pipeline = pipeline
         self.acl_priority = self._FILTER_PRIORITY
         self.override_priority = self.acl_priority + 1
@@ -367,41 +451,58 @@
 
     def initialise_tables(self):
         """Install dp acls if configured"""
         ofmsgs = []
         if self.dp_acls:
             acl_allow_inst = self.pipeline.accept_to_vlan()
             acl_force_port_vlan_inst = self.pipeline.accept_to_l2_forwarding()
-            ofmsgs.extend(build_acl_ofmsgs(
-                self.dp_acls, self.port_acl_table, acl_allow_inst,
-                acl_force_port_vlan_inst, self.acl_priority, self.meters,
-                False))
+            ofmsgs.extend(
+                build_acl_ofmsgs(
+                    self.dp_acls,
+                    self.port_acl_table,
+                    acl_allow_inst,
+                    acl_force_port_vlan_inst,
+                    self.acl_priority,
+                    self.meters,
+                    False,
+                )
+            )
         return ofmsgs
 
     def _port_acls_allowed(self, port):
         return not self.dp_acls and not port.output_only and self.port_acl_table
 
     def add_port(self, port):
         """Install port acls if configured"""
         ofmsgs = []
         if self._port_acls_allowed(port):
             in_port_match = self.port_acl_table.match(in_port=port.number)
             acl_allow_inst = self.pipeline.accept_to_vlan()
             acl_force_port_vlan_inst = self.pipeline.accept_to_l2_forwarding()
             if port.acls_in:
-                ofmsgs.extend(build_acl_ofmsgs(
-                    port.acls_in, self.port_acl_table,
-                    acl_allow_inst, acl_force_port_vlan_inst,
-                    self.acl_priority, self.meters,
-                    port.acls_in[0].exact_match, port_num=port.number))
+                ofmsgs.extend(
+                    build_acl_ofmsgs(
+                        port.acls_in,
+                        self.port_acl_table,
+                        acl_allow_inst,
+                        acl_force_port_vlan_inst,
+                        self.acl_priority,
+                        self.meters,
+                        port.acls_in[0].exact_match,
+                        port_num=port.number,
+                    )
+                )
             elif not port.dot1x:
-                ofmsgs.append(self.port_acl_table.flowmod(
-                    in_port_match,
-                    priority=self.acl_priority,
-                    inst=tuple(acl_allow_inst)))
+                ofmsgs.append(
+                    self.port_acl_table.flowmod(
+                        in_port_match,
+                        priority=self.acl_priority,
+                        inst=tuple(acl_allow_inst),
+                    )
+                )
         return ofmsgs
 
     def del_port(self, port):
         ofmsgs = []
         if self._port_acls_allowed(port):
             in_port_match = self.port_acl_table.match(in_port=port.number)
             ofmsgs.append(self.port_acl_table.flowdel(in_port_match, self.acl_priority))
@@ -426,125 +527,177 @@
     def add_vlan(self, vlan, cold_start):
         """Install VLAN ACL if configured."""
         ofmsgs = []
         if vlan.acls_in:
             acl_allow_inst = self.pipeline.accept_to_classification()
             acl_force_port_vlan_inst = self.pipeline.accept_to_l2_forwarding()
             ofmsgs = build_acl_ofmsgs(
-                vlan.acls_in, self.vlan_acl_table, acl_allow_inst,
-                acl_force_port_vlan_inst, self.acl_priority, self.meters,
-                vlan.acls_in[0].exact_match, vlan_vid=vlan.vid)
+                vlan.acls_in,
+                self.vlan_acl_table,
+                acl_allow_inst,
+                acl_force_port_vlan_inst,
+                self.acl_priority,
+                self.meters,
+                vlan.acls_in[0].exact_match,
+                vlan_vid=vlan.vid,
+            )
         if self.egress_acl_table is not None:
             egress_acl_allow_inst = self.pipeline.accept_to_egress()
             if vlan.acls_out:
-                ofmsgs.extend(build_acl_ofmsgs(
-                    vlan.acls_out, self.egress_acl_table, egress_acl_allow_inst,
-                    egress_acl_allow_inst, self.acl_priority, self.meters,
-                    vlan.acls_out[0].exact_match, vlan_vid=vlan.vid
-                ))
+                ofmsgs.extend(
+                    build_acl_ofmsgs(
+                        vlan.acls_out,
+                        self.egress_acl_table,
+                        egress_acl_allow_inst,
+                        egress_acl_allow_inst,
+                        self.acl_priority,
+                        self.meters,
+                        vlan.acls_out[0].exact_match,
+                        vlan_vid=vlan.vid,
+                    )
+                )
             else:
-                ofmsgs.append(self.egress_acl_table.flowmod(
-                    self.egress_acl_table.match(vlan=vlan),
-                    priority=self.acl_priority,
-                    inst=tuple(egress_acl_allow_inst),
-                ))
+                ofmsgs.append(
+                    self.egress_acl_table.flowmod(
+                        self.egress_acl_table.match(vlan=vlan),
+                        priority=self.acl_priority,
+                        inst=tuple(egress_acl_allow_inst),
+                    )
+                )
         return ofmsgs
 
     def add_authed_mac(self, port_num, mac):
         """Add authed mac address"""
-        return [self.port_acl_table.flowmod(
-            self.port_acl_table.match(in_port=port_num, eth_src=mac),
-            priority=self.auth_priority,
-            inst=tuple(self.pipeline.accept_to_vlan()))]
+        return [
+            self.port_acl_table.flowmod(
+                self.port_acl_table.match(in_port=port_num, eth_src=mac),
+                priority=self.auth_priority,
+                inst=tuple(self.pipeline.accept_to_vlan()),
+            )
+        ]
 
     def del_authed_mac(self, port_num, mac=None, strict=True):
         """remove authed mac address"""
         if mac:
-            return [self.port_acl_table.flowdel(
-                self.port_acl_table.match(in_port=port_num, eth_src=mac),
+            return [
+                self.port_acl_table.flowdel(
+                    self.port_acl_table.match(in_port=port_num, eth_src=mac),
+                    priority=self.auth_priority,
+                    strict=strict,
+                )
+            ]
+        return [
+            self.port_acl_table.flowdel(
+                self.port_acl_table.match(in_port=port_num),
                 priority=self.auth_priority,
-                strict=strict)]
-        return [self.port_acl_table.flowdel(
-            self.port_acl_table.match(in_port=port_num),
-            priority=self.auth_priority,
-            strict=strict)]
+                strict=strict,
+            )
+        ]
 
     def del_port_acl(self, acl, port_num, mac=None):
         """Delete ACL rules for Port"""
+
         def convert_to_flow_del(ofp_flowmods):
             flowdels = []
             for flowmod in ofp_flowmods:
-                flowdels.append(self.port_acl_table.flowdel(
-                    match=flowmod.match, priority=flowmod.priority))
+                flowdels.append(
+                    self.port_acl_table.flowdel(
+                        match=flowmod.match, priority=flowmod.priority
+                    )
+                )
 
             return flowdels
 
         pipeline_vlan_table = self.pipeline.vlan_table
-        flowmods = build_acl_port_of_msgs(acl, None, port_num, self.port_acl_table,
-                                          pipeline_vlan_table, self.auth_priority)
+        flowmods = build_acl_port_of_msgs(
+            acl,
+            None,
+            port_num,
+            self.port_acl_table,
+            pipeline_vlan_table,
+            self.auth_priority,
+        )
         for flow in flowmods:
             flow.match = add_mac_address_to_match(flow.match, mac)
 
         return convert_to_flow_del(flowmods)
 
     def add_port_acl(self, acl, port_num, mac=None):
         """Create ACL openflow rules for Port"""
         pipeline_vlan_table = self.pipeline.vlan_table
-        flowmods = build_acl_port_of_msgs(acl, None, port_num, self.port_acl_table,
-                                          pipeline_vlan_table, self.auth_priority)
+        flowmods = build_acl_port_of_msgs(
+            acl,
+            None,
+            port_num,
+            self.port_acl_table,
+            pipeline_vlan_table,
+            self.auth_priority,
+        )
 
         for flow in flowmods:
             flow.match = add_mac_address_to_match(flow.match, mac)
 
         return flowmods
 
     def create_dot1x_flow_pair(self, port_num, nfv_sw_port_num, mac):
         """Create dot1x flow pair"""
         ofmsgs = [
             self.port_acl_table.flowmod(
                 match=self.port_acl_table.match(
-                    in_port=port_num,
-                    eth_type=valve_packet.ETH_EAPOL),
+                    in_port=port_num, eth_type=valve_packet.ETH_EAPOL
+                ),
                 priority=self.override_priority,
-                inst=(valve_of.apply_actions([
-                    self.port_acl_table.set_field(eth_dst=mac),
-                    valve_of.output_port(nfv_sw_port_num)]),),
+                inst=(
+                    valve_of.apply_actions(
+                        [
+                            self.port_acl_table.set_field(eth_dst=mac),
+                            valve_of.output_port(nfv_sw_port_num),
+                        ]
+                    ),
+                ),
             ),
             self.port_acl_table.flowmod(
                 match=self.port_acl_table.match(
                     in_port=nfv_sw_port_num,
                     eth_type=valve_packet.ETH_EAPOL,
-                    eth_src=mac),
+                    eth_src=mac,
+                ),
                 priority=self.override_priority,
-                inst=(valve_of.apply_actions([
-                    self.port_acl_table.set_field(
-                        eth_src=valve_packet.EAPOL_ETH_DST),
-                    valve_of.output_port(port_num)
-                ]),),
-            )
+                inst=(
+                    valve_of.apply_actions(
+                        [
+                            self.port_acl_table.set_field(
+                                eth_src=valve_packet.EAPOL_ETH_DST
+                            ),
+                            valve_of.output_port(port_num),
+                        ]
+                    ),
+                ),
+            ),
         ]
 
         return ofmsgs
 
     def del_dot1x_flow_pair(self, port_num, nfv_sw_port_num, mac):
         """Deletes dot1x flow pair"""
         ofmsgs = [
             self.port_acl_table.flowdel(
                 match=self.port_acl_table.match(
                     in_port=nfv_sw_port_num,
                     eth_type=valve_packet.ETH_EAPOL,
-                    eth_src=mac),
+                    eth_src=mac,
+                ),
                 priority=self.override_priority,
             ),
             self.port_acl_table.flowdel(
                 match=self.port_acl_table.match(
-                    in_port=port_num,
-                    eth_type=valve_packet.ETH_EAPOL),
+                    in_port=port_num, eth_type=valve_packet.ETH_EAPOL
+                ),
                 priority=self.override_priority,
-            )
+            ),
         ]
         return ofmsgs
 
     def create_mab_flow(self, port_num, nfv_sw_port_num, mac):
         """
         Create MAB ACL for sending IP Activity to Chewie NFV
             Returns flowmods to send all IP traffic to Chewie
@@ -560,97 +713,149 @@
                 in_port=port_num,
                 eth_type=valve_of.ether.ETH_TYPE_IP,
                 nw_proto=valve_of.inet.IPPROTO_UDP,
                 udp_src=68,
                 udp_dst=67,
             ),
             priority=self.low_priority,
-            inst=(valve_of.apply_actions([
-                self.port_acl_table.set_field(eth_dst=mac),
-                valve_of.output_port(nfv_sw_port_num)]),),
+            inst=(
+                valve_of.apply_actions(
+                    [
+                        self.port_acl_table.set_field(eth_dst=mac),
+                        valve_of.output_port(nfv_sw_port_num),
+                    ]
+                ),
+            ),
         )
 
     def del_mab_flow(self, port_num, _nfv_sw_port_num, _mac):
         """
         Remove MAB ACL for sending IP Activity to Chewie NFV
             Returns flowmods to send all IP traffic to Chewie
 
         Args:
             port_num (int): Number of port in
             _nfv_sw_port_num(int): Number of port out
             _mac(str): MAC address of the valve/port combo
 
         """
-        return [self.port_acl_table.flowdel(
-            match=self.port_acl_table.match(
-                in_port=port_num,
-                eth_type=valve_of.ether.ETH_TYPE_IP,
-                nw_proto=valve_of.inet.IPPROTO_UDP,
-                udp_src=68,
-                udp_dst=67,
-            ),
-            priority=self.low_priority,
-            strict=True
-        )]
+        return [
+            self.port_acl_table.flowdel(
+                match=self.port_acl_table.match(
+                    in_port=port_num,
+                    eth_type=valve_of.ether.ETH_TYPE_IP,
+                    nw_proto=valve_of.inet.IPPROTO_UDP,
+                    udp_src=68,
+                    udp_dst=67,
+                ),
+                priority=self.low_priority,
+                strict=True,
+            )
+        ]
 
     def build_tunnel_rules_ofmsgs(self, source_id, tunnel_id, acl):
         """Build a tunnel only generated rule"""
         ofmsgs = []
         acl_table = self.port_acl_table
         priority = self.override_priority
-        reverse = acl.tunnel_dests[tunnel_id]['reverse']
-        ofmsgs.extend(build_tunnel_ofmsgs(
-            acl.dyn_tunnel_rules[tunnel_id][source_id], acl_table, priority,
-            None, tunnel_id, flowdel=True, reverse=reverse))
+        reverse = acl.tunnel_dests[tunnel_id]["reverse"]
+        ofmsgs.extend(
+            build_tunnel_ofmsgs(
+                acl.dyn_tunnel_rules[tunnel_id][source_id],
+                acl_table,
+                priority,
+                None,
+                tunnel_id,
+                flowdel=True,
+                reverse=reverse,
+            )
+        )
         return ofmsgs
 
     def build_reverse_tunnel_rules_ofmsgs(self, source_id, tunnel_id, acl):
         """Build a (reverse) tunnel only generated rule"""
         ofmsgs = []
         acl_table = self.port_acl_table
         priority = self.override_priority
         if acl.requires_reverse_tunnel(tunnel_id):
-            ofmsgs.extend(build_tunnel_ofmsgs(
-                acl.dyn_reverse_tunnel_rules[tunnel_id][source_id], acl_table, priority,
-                None, tunnel_id, flowdel=True, reverse=True))
+            ofmsgs.extend(
+                build_tunnel_ofmsgs(
+                    acl.dyn_reverse_tunnel_rules[tunnel_id][source_id],
+                    acl_table,
+                    priority,
+                    None,
+                    tunnel_id,
+                    flowdel=True,
+                    reverse=True,
+                )
+            )
         return ofmsgs
 
     def build_tunnel_acl_rule_ofmsgs(self, source_id, tunnel_id, acl):
         """Build a rule of an ACL that contains a tunnel"""
         ofmsgs = []
         acl_allow_inst = self.pipeline.accept_to_vlan()
         acl_table = self.port_acl_table
         rules = acl.get_tunnel_rules(tunnel_id)
         acl_force_port_vlan_inst = self.pipeline.accept_to_l2_forwarding()
         if self.dp_acls and acl in self.dp_acls:
-            ofmsgs.extend(build_acl_ofmsgs(
-                self.dp_acls, acl_table, acl_allow_inst,
-                acl_force_port_vlan_inst, self.acl_priority, self.meters,
-                False, tunnel_rules=acl.dyn_tunnel_rules, source_id=source_id, flowdel=True))
+            ofmsgs.extend(
+                build_acl_ofmsgs(
+                    self.dp_acls,
+                    acl_table,
+                    acl_allow_inst,
+                    acl_force_port_vlan_inst,
+                    self.acl_priority,
+                    self.meters,
+                    False,
+                    tunnel_rules=acl.dyn_tunnel_rules,
+                    source_id=source_id,
+                    flowdel=True,
+                )
+            )
         else:
             for rule_conf in rules:
                 rule_index = acl.rules.index(rule_conf)
                 priority = self.acl_priority - rule_index
-                in_port = acl.tunnel_sources[source_id]['port']
-                ofmsgs.extend(build_rule_ofmsgs(
-                    rule_conf, acl_table, acl_allow_inst, acl_force_port_vlan_inst,
-                    self.acl_priority, priority, self.meters,
-                    acl.exact_match, in_port, None, acl.dyn_tunnel_rules, source_id, flowdel=True))
+                in_port = acl.tunnel_sources[source_id]["port"]
+                ofmsgs.extend(
+                    build_rule_ofmsgs(
+                        rule_conf,
+                        acl_table,
+                        acl_allow_inst,
+                        acl_force_port_vlan_inst,
+                        self.acl_priority,
+                        priority,
+                        self.meters,
+                        acl.exact_match,
+                        in_port,
+                        None,
+                        acl.dyn_tunnel_rules,
+                        source_id,
+                        flowdel=True,
+                    )
+                )
         return ofmsgs
 
     def add_meters(self, added_meters):
         """Add new meters."""
         ofmsgs = []
         if added_meters:
             for added_meter in added_meters:
-                ofmsgs.append(valve_of.meteradd(
-                    self.meters.get(added_meter).entry, command=0))
+                ofmsgs.append(
+                    valve_of.meteradd(self.meters.get(added_meter).entry, command=0)
+                )
         return ofmsgs
 
     def del_meters(self, deleted_meters):
         ofmsgs = []
         if deleted_meters:
             deleted_meter_ids = [
-                self.meters[meter_key].meter_id for meter_key in deleted_meters]
-            ofmsgs.extend([
-                valve_of.meterdel(deleted_meter_id) for deleted_meter_id in deleted_meter_ids])
+                self.meters[meter_key].meter_id for meter_key in deleted_meters
+            ]
+            ofmsgs.extend(
+                [
+                    valve_of.meterdel(deleted_meter_id)
+                    for deleted_meter_id in deleted_meter_ids
+                ]
+            )
         return ofmsgs
```

### Comparing `c65faucet-1.0.46/faucet/valve_coprocessor.py` & `c65faucet-1.0.47/faucet/valve_coprocessor.py`

 * *Files 6% similar despite different names*

```diff
@@ -21,41 +21,60 @@
 from faucet.valve_manager_base import ValveManagerBase
 from faucet.vlan import OFVLAN
 
 
 class CoprocessorManager(ValveManagerBase):
     """Implementation of Valve coprocessor."""
 
-    def __init__(self, ports, copro_table, vlan_table, eth_src_table,  # pylint: disable=too-many-arguments
-                 output_table, low_priority, high_priority):
+    def __init__(
+        self,
+        ports,
+        copro_table,
+        vlan_table,
+        eth_src_table,  # pylint: disable=too-many-arguments
+        output_table,
+        low_priority,
+        high_priority,
+    ):
         self.ports = ports
         self.copro_table = copro_table
         self.vlan_table = vlan_table
         self.eth_src_table = eth_src_table
         self.output_table = output_table
         self.low_priority = low_priority
         self.high_priority = high_priority
 
     def add_port(self, port):
         """Add flows to allow coprocessor to inject or output packets."""
         ofmsgs = []
         if port.coprocessor:
-            ofmsgs.append(self.vlan_table.flowmod(
-                self.vlan_table.match(in_port=port.number),
-                priority=self.low_priority,
-                inst=(self.vlan_table.goto(self.copro_table),)))
-            ofmsgs.append(self.eth_src_table.flowmod(
-                match=self.eth_src_table.match(in_port=port.number),
-                priority=self.high_priority,
-                inst=(self.eth_src_table.goto(self.output_table),)))
+            ofmsgs.append(
+                self.vlan_table.flowmod(
+                    self.vlan_table.match(in_port=port.number),
+                    priority=self.low_priority,
+                    inst=(self.vlan_table.goto(self.copro_table),),
+                )
+            )
+            ofmsgs.append(
+                self.eth_src_table.flowmod(
+                    match=self.eth_src_table.match(in_port=port.number),
+                    priority=self.high_priority,
+                    inst=(self.eth_src_table.goto(self.output_table),),
+                )
+            )
             # TODO: add additional output port strategies (eg. MPLS) and tagged ports
-            vlan_vid_base = port.coprocessor.get('vlan_vid_base', 0)
+            vlan_vid_base = port.coprocessor.get("vlan_vid_base", 0)
             for port_number in self.ports:
-                inst = (valve_of.apply_actions((
-                    valve_of.pop_vlan(),
-                    valve_of.output_port(port_number))),)
+                inst = (
+                    valve_of.apply_actions(
+                        (valve_of.pop_vlan(), valve_of.output_port(port_number))
+                    ),
+                )
                 vid = vlan_vid_base + port_number
                 vlan = OFVLAN(str(vid), vid)
                 match = self.copro_table.match(vlan=vlan)
-                ofmsgs.append(self.copro_table.flowmod(
-                    match=match, priority=self.high_priority, inst=inst))
+                ofmsgs.append(
+                    self.copro_table.flowmod(
+                        match=match, priority=self.high_priority, inst=inst
+                    )
+                )
         return ofmsgs
```

### Comparing `c65faucet-1.0.46/faucet/valve_lldp.py` & `c65faucet-1.0.47/faucet/valve_lldp.py`

 * *Files 20% similar despite different names*

```diff
@@ -25,52 +25,75 @@
 from faucet import valve_packet
 from faucet.valve_manager_base import ValveManagerBase
 
 
 class ValveLLDPManager(ValveManagerBase):
     """Manage LLDP."""
 
-    def __init__(self, vlan_table, highest_priority, logger,
-                 notify, inc_var, set_var, set_port_var, stack_manager):
+    def __init__(
+        self,
+        vlan_table,
+        highest_priority,
+        logger,
+        notify,
+        inc_var,
+        set_var,
+        set_port_var,
+        stack_manager,
+    ):
         self.vlan_table = vlan_table
         self.highest_priority = highest_priority
         self.logger = logger
         self.notify = notify
         self._set_var = set_var
         self._inc_var = inc_var
         self._set_port_var = set_port_var
         self.stack_manager = stack_manager
 
     def _lldp_match(self, port):
         return self.vlan_table.match(
             in_port=port.number,
             eth_dst=valve_packet.LLDP_MAC_NEAREST_BRIDGE,
             eth_dst_mask=valve_packet.BRIDGE_GROUP_MASK,
-            eth_type=valve_of.ether.ETH_TYPE_LLDP)
+            eth_type=valve_of.ether.ETH_TYPE_LLDP,
+        )
 
     def add_port(self, port):
         ofmsgs = []
         if port.receive_lldp:
-            ofmsgs.append(self.vlan_table.flowcontroller(
-                match=self._lldp_match(port),
-                priority=self.highest_priority,
-                max_len=128))
+            ofmsgs.append(
+                self.vlan_table.flowcontroller(
+                    match=self._lldp_match(port),
+                    priority=self.highest_priority,
+                    max_len=128,
+                )
+            )
         return ofmsgs
 
     def del_port(self, port):
         ofmsgs = []
         if port.receive_lldp:
-            ofmsgs.append(self.vlan_table.flowdel(
-                match=self._lldp_match(port),
-                priority=self.highest_priority))
+            ofmsgs.append(
+                self.vlan_table.flowdel(
+                    match=self._lldp_match(port), priority=self.highest_priority
+                )
+            )
         return ofmsgs
 
-    def verify_lldp(self, port, now, valve, other_valves,
-                    remote_dp_id, remote_dp_name,
-                    remote_port_id, remote_port_state):
+    def verify_lldp(
+        self,
+        port,
+        now,
+        valve,
+        other_valves,
+        remote_dp_id,
+        remote_dp_name,
+        remote_port_id,
+        remote_port_state,
+    ):
         """
         Verify correct LLDP cabling, then update port to next state
 
         Args:
             port (Port): Port that received the LLDP
             now (float): Current time
             other_valves (list): Other valves in the topology
@@ -79,34 +102,44 @@
             remote_port_id (int): Recevied LLDP port ID
             remote_port_state (int): Received LLDP port state
         Returns:
             dict: Ofmsgs by valve
         """
         if not port.stack:
             return {}
-        remote_dp = port.stack['dp']
-        remote_port = port.stack['port']
+        remote_dp = port.stack["dp"]
+        remote_port = port.stack["port"]
         stack_correct = True
-        self._inc_var('stack_probes_received')
-        if (remote_dp_id != remote_dp.dp_id
-                or remote_dp_name != remote_dp.name
-                or remote_port_id != remote_port.number):
+        self._inc_var("stack_probes_received")
+        if (
+            remote_dp_id != remote_dp.dp_id
+            or remote_dp_name != remote_dp.name
+            or remote_port_id != remote_port.number
+        ):
             self.logger.error(
-                f'Stack {port} cabling incorrect, expected '
-                f'{valve_util.dpid_log(remote_dp.dp_id)}:{remote_dp.name}:{remote_port.number}, '
-                f'actual {valve_util.dpid_log(remote_dp_id)}:{remote_dp_name}:{remote_port_id}')
+                "Stack %s cabling incorrect, expected %s:%s:%u, actual %s:%s:%u"
+                % (
+                    port,
+                    valve_util.dpid_log(remote_dp.dp_id),
+                    remote_dp.name,
+                    remote_port.number,
+                    valve_util.dpid_log(remote_dp_id),
+                    remote_dp_name,
+                    remote_port_id,
+                )
+            )
             stack_correct = False
-            self._inc_var('stack_cabling_errors')
+            self._inc_var("stack_cabling_errors")
         port.dyn_stack_probe_info = {
-            'last_seen_lldp_time': now,
-            'stack_correct': stack_correct,
-            'remote_dp_id': remote_dp_id,
-            'remote_dp_name': remote_dp_name,
-            'remote_port_id': remote_port_id,
-            'remote_port_state': remote_port_state
+            "last_seen_lldp_time": now,
+            "stack_correct": stack_correct,
+            "remote_dp_id": remote_dp_id,
+            "remote_dp_name": remote_dp_name,
+            "remote_port_id": remote_port_id,
+            "remote_port_state": remote_port_state,
         }
         return self.update_stack_link_state([port], now, valve, other_valves)
 
     def update_stack_link_state(self, ports, now, valve, other_valves):
         """
         Update the stack link states of the set of provided stack ports
 
@@ -118,58 +151,83 @@
         Returns:
             dict: ofmsgs by valve
         """
         stack_changes = 0
         ofmsgs_by_valve = defaultdict(list)
         stacked_valves = set()
         if self.stack_manager:
-            stacked_valves = {valve}.union(self.stack_manager.stacked_valves(other_valves))
+            stacked_valves = {valve}.union(
+                self.stack_manager.stacked_valves(other_valves)
+            )
         for port in ports:
             before_state = port.stack_state()
             after_state, reason = port.stack_port_update(now)
             if before_state != after_state:
-                self._set_port_var('port_stack_state', after_state, port)
+                self._set_port_var("port_stack_state", after_state, port)
                 self._inc_var(
-                    'port_stack_state_change_count', labels=valve.dp.port_labels(port.number))
-                self.notify({'STACK_STATE': {
-                    'port': port.number,
-                    'state': after_state}})
-                self.logger.info(f'Stack {port} state {port.stack_state_name(after_state)} '
-                                 f'(previous state {port.stack_state_name(before_state)}): {reason}')
+                    "port_stack_state_change_count",
+                    labels=valve.dp.port_labels(port.number),
+                )
+                self.notify(
+                    {"STACK_STATE": {"port": port.number, "state": after_state}}
+                )
+                self.logger.info(
+                    "Stack %s state %s (previous state %s): %s"
+                    % (
+                        port,
+                        port.stack_state_name(after_state),
+                        port.stack_state_name(before_state),
+                        reason,
+                    )
+                )
                 stack_changes += 1
                 port_up = False
                 if port.is_stack_up():
                     port_up = True
-                elif port.is_stack_init() and port.stack['port'].is_stack_up():
+                elif port.is_stack_init() and port.stack["port"].is_stack_up():
                     port_up = True
                 for stack_valve in stacked_valves:
                     stack_valve.stack_manager.update_stack_topo(port_up, valve.dp, port)
         if stack_changes or valve.stale_root:
-            self.logger.info(f'{stack_changes} stack ports changed state, stale root {valve.stale_root}')
+            self.logger.info(
+                "%u stack ports changed state, stale root %s"
+                % (stack_changes, valve.stale_root)
+            )
             valve.stale_root = False
             notify_dps = {}
             for stack_valve in stacked_valves:
                 if not stack_valve.dp.dyn_running:
                     continue
                 ofmsgs_by_valve[stack_valve].extend(
-                    stack_valve.add_vlans(stack_valve.dp.vlans.values()))
+                    stack_valve.add_vlans(stack_valve.dp.vlans.values())
+                )
                 for port in stack_valve.dp.stack_ports():
                     ofmsgs_by_valve[stack_valve].extend(
-                        stack_valve.switch_manager.del_port(port))
+                        stack_valve.switch_manager.del_port(port)
+                    )
                 ofmsgs_by_valve[stack_valve].extend(
-                    stack_valve.stack_manager.add_tunnel_acls())
+                    stack_valve.stack_manager.add_tunnel_acls()
+                )
                 path_port = stack_valve.stack_manager.chosen_towards_port
                 path_port_number = path_port.number if path_port else 0.0
                 self._set_var(
-                    'dp_root_hop_port', path_port_number, labels=stack_valve.dp.base_prom_labels())
-                notify_dps.setdefault(stack_valve.dp.name, {})['root_hop_port'] = path_port_number
+                    "dp_root_hop_port",
+                    path_port_number,
+                    labels=stack_valve.dp.base_prom_labels(),
+                )
+                notify_dps.setdefault(stack_valve.dp.name, {})[
+                    "root_hop_port"
+                ] = path_port_number
             # Find the first valve with a valid stack and trigger notification.
             for stack_valve in stacked_valves:
                 if stack_valve.dp.stack.graph:
                     self.notify(
-                        {'STACK_TOPO_CHANGE': {
-                            'stack_root': stack_valve.dp.stack.root_name,
-                            'graph': stack_valve.dp.stack.get_node_link_data(),
-                            'dps': notify_dps
-                        }})
+                        {
+                            "STACK_TOPO_CHANGE": {
+                                "stack_root": stack_valve.dp.stack.root_name,
+                                "graph": stack_valve.dp.stack.get_node_link_data(),
+                                "dps": notify_dps,
+                            }
+                        }
+                    )
                     break
         return ofmsgs_by_valve
```

### Comparing `c65faucet-1.0.46/faucet/valve_manager_base.py` & `c65faucet-1.0.47/faucet/valve_manager_base.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/valve_of.py` & `c65faucet-1.0.47/faucet/valve_of.py`

 * *Files 11% similar despite different names*

```diff
@@ -23,177 +23,239 @@
 import ipaddress
 import random
 
 # pylint: disable=unused-import
 from os_ken.lib import mac  # noqa: F401
 from os_ken.lib import ofctl_v1_3 as ofctl
 from os_ken.lib.ofctl_utils import (
-    str_to_int, to_match_ip, to_match_masked_int, to_match_eth, to_match_vid, OFCtlUtil)
+    str_to_int,
+    to_match_ip,
+    to_match_masked_int,
+    to_match_eth,
+    to_match_vid,
+    OFCtlUtil,
+)
 from os_ken.ofproto import ether
+
 # pylint: disable=unused-import
 from os_ken.ofproto import inet  # noqa: F401
 from os_ken.ofproto import ofproto_v1_3 as ofp
 from os_ken.ofproto import ofproto_v1_3_parser as parser
 
 from faucet.conf import test_config_condition, InvalidConfigError
 from faucet.valve_of_old import OLD_MATCH_FIELDS
-from faucet.valve_util import LRU_MAX
 
 MIN_VID = 1
 MAX_VID = 4095
 VLAN_GROUP_OFFSET = MAX_VID + 1
 ROUTE_GROUP_OFFSET = VLAN_GROUP_OFFSET * 2
 OFP_VERSIONS = [ofp.OFP_VERSION]
 OFP_IN_PORT = ofp.OFPP_IN_PORT
 MAX_PACKET_IN_BYTES = 194  # largest packet is icmp6 echo req with 128 byte payload
 ECTP_ETH_TYPE = 0x9000
 
 # https://en.wikipedia.org/wiki/IEEE_P802.1p
 # Avoid use of PCP 1 which is BK priority (lowest)
 
-TUNNEL_INDICATOR_FIELD = 'vlan_pcp'
+TUNNEL_INDICATOR_FIELD = "vlan_pcp"
 # Used to indicate traffic in a one-to-one bi-directional tunnel is heading
 # in the reverse/return direction
 PCP_TUNNEL_REVERSE_DIRECTION_FLAG = 4
 # Used to indicate traffic belongs in a tunnel (for all cases not including
 # the 1-1 reverse bi-directional tunnel)
 PCP_TUNNEL_FLAG = 3
 
 PCP_EXT_PORT_FLAG = 2
 PCP_NONEXT_PORT_FLAG = 0
-EXTERNAL_FORWARDING_FIELD = 'vlan_pcp'
+EXTERNAL_FORWARDING_FIELD = "vlan_pcp"
 
 
 OFERROR_TYPE_CODE = {
-    ofp.OFPET_HELLO_FAILED: ('OFPET_HELLO_FAILED', {
-        ofp.OFPHFC_INCOMPATIBLE: 'OFPHFC_INCOMPATIBLE',
-        ofp.OFPHFC_EPERM: 'OFPHFC_EPERM'}),
-    ofp.OFPET_BAD_REQUEST: ('OFPET_BAD_REQUEST', {
-        ofp.OFPBRC_BAD_VERSION: 'OFPBRC_BAD_VERSION',
-        ofp.OFPBRC_BAD_TYPE: 'OFPBRC_BAD_TYPE',
-        ofp.OFPBRC_BAD_MULTIPART: 'OFPBRC_BAD_MULTIPART',
-        ofp.OFPBRC_BAD_EXPERIMENTER: 'OFPBRC_BAD_EXPERIMENTER',
-        ofp.OFPBRC_BAD_EXP_TYPE: 'OFPBRC_BAD_EXP_TYPE',
-        ofp.OFPBRC_EPERM: 'OFPBRC_EPERM',
-        ofp.OFPBRC_BAD_LEN: 'OFPBRC_BAD_LEN',
-        ofp.OFPBRC_BUFFER_EMPTY: 'OFPBRC_BUFFER_EMPTY',
-        ofp.OFPBRC_BUFFER_UNKNOWN: 'OFPBRC_BUFFER_UNKNOWN',
-        ofp.OFPBRC_BAD_TABLE_ID: 'OFPBRC_BAD_TABLE_ID',
-        ofp.OFPBRC_IS_SLAVE: 'OFPBRC_IS_SLAVE',
-        ofp.OFPBRC_BAD_PORT: 'OFPBRC_BAD_PORT',
-        ofp.OFPBRC_BAD_PACKET: 'OFPBRC_BAD_PACKET',
-        ofp.OFPBRC_MULTIPART_BUFFER_OVERFLOW: 'OFPBRC_MULTIPART_BUFFER_OVERFLOW'}),
-    ofp.OFPET_BAD_ACTION: ('OFPET_BAD_ACTION', {
-        ofp.OFPBAC_BAD_TYPE: 'OFPBAC_BAD_TYPE',
-        ofp.OFPBAC_BAD_LEN: 'OFPBAC_BAD_LEN',
-        ofp.OFPBAC_BAD_EXPERIMENTER: 'OFPBAC_BAD_EXPERIMENTER',
-        ofp.OFPBAC_BAD_EXP_TYPE: 'OFPBAC_BAD_EXP_TYPE',
-        ofp.OFPBAC_BAD_OUT_PORT: 'OFPBAC_BAD_OUT_PORT',
-        ofp.OFPBAC_BAD_ARGUMENT: 'OFPBAC_BAD_ARGUMENT',
-        ofp.OFPBAC_EPERM: 'OFPBAC_EPERM',
-        ofp.OFPBAC_TOO_MANY: 'OFPBAC_TOO_MANY',
-        ofp.OFPBAC_BAD_QUEUE: 'OFPBAC_BAD_QUEUE',
-        ofp.OFPBAC_BAD_OUT_GROUP: 'OFPBAC_BAD_OUT_GROUP',
-        ofp.OFPBAC_MATCH_INCONSISTENT: 'OFPBAC_MATCH_INCONSISTENT',
-        ofp.OFPBAC_UNSUPPORTED_ORDER: 'OFPBAC_UNSUPPORTED_ORDER',
-        ofp.OFPBAC_BAD_TAG: 'OFPBAC_BAD_TAG',
-        ofp.OFPBAC_BAD_SET_TYPE: 'OFPBAC_BAD_SET_TYPE',
-        ofp.OFPBAC_BAD_SET_LEN: 'OFPBAC_BAD_SET_LEN',
-        ofp.OFPBAC_BAD_SET_ARGUMENT: 'OFPBAC_BAD_SET_ARGUMENT'}),
-    ofp.OFPET_BAD_INSTRUCTION: ('OFPET_BAD_INSTRUCTION', {
-        ofp.OFPBIC_UNKNOWN_INST: 'OFPBIC_UNKNOWN_INST',
-        ofp.OFPBIC_UNSUP_INST: 'OFPBIC_UNSUP_INST',
-        ofp.OFPBIC_BAD_TABLE_ID: 'OFPBIC_BAD_TABLE_ID',
-        ofp.OFPBIC_UNSUP_METADATA: 'OFPBIC_UNSUP_METADATA',
-        ofp.OFPBIC_UNSUP_METADATA_MASK: 'OFPBIC_UNSUP_METADATA_MASK',
-        ofp.OFPBIC_BAD_EXPERIMENTER: 'OFPBIC_BAD_EXPERIMENTER',
-        ofp.OFPBIC_BAD_EXP_TYPE: 'OFPBIC_BAD_EXP_TYPE',
-        ofp.OFPBIC_BAD_LEN: 'OFPBIC_BAD_LEN',
-        ofp.OFPBIC_EPERM: 'OFPBIC_EPERM'}),
-    ofp.OFPET_BAD_MATCH: ('OFPET_BAD_MATCH', {
-        ofp.OFPBMC_BAD_TYPE: 'OFPBMC_BAD_TYPE',
-        ofp.OFPBMC_BAD_LEN: 'OFPBMC_BAD_LEN',
-        ofp.OFPBMC_BAD_TAG: 'OFPBMC_BAD_TAG',
-        ofp.OFPBMC_BAD_DL_ADDR_MASK: 'OFPBMC_BAD_DL_ADDR_MASK',
-        ofp.OFPBMC_BAD_NW_ADDR_MASK: 'OFPBMC_BAD_NW_ADDR_MASK',
-        ofp.OFPBMC_BAD_WILDCARDS: 'OFPBMC_BAD_WILDCARDS',
-        ofp.OFPBMC_BAD_FIELD: 'OFPBMC_BAD_FIELD',
-        ofp.OFPBMC_BAD_VALUE: 'OFPBMC_BAD_VALUE',
-        ofp.OFPBMC_BAD_MASK: 'OFPBMC_BAD_MASK',
-        ofp.OFPBMC_BAD_PREREQ: 'OFPBMC_BAD_PREREQ',
-        ofp.OFPBMC_DUP_FIELD: 'OFPBMC_DUP_FIELD',
-        ofp.OFPBMC_EPERM: 'OFPBMC_EPERM'}),
-    ofp.OFPET_FLOW_MOD_FAILED: ('OFPET_FLOW_MOD_FAILED', {
-        ofp.OFPFMFC_UNKNOWN: 'OFPFMFC_UNKNOWN',
-        ofp.OFPFMFC_TABLE_FULL: 'OFPFMFC_TABLE_FULL',
-        ofp.OFPFMFC_BAD_TABLE_ID: 'OFPFMFC_BAD_TABLE_ID',
-        ofp.OFPFMFC_OVERLAP: 'OFPFMFC_OVERLAP',
-        ofp.OFPFMFC_EPERM: 'OFPFMFC_EPERM',
-        ofp.OFPFMFC_BAD_TIMEOUT: 'OFPFMFC_BAD_TIMEOUT',
-        ofp.OFPFMFC_BAD_COMMAND: 'OFPFMFC_BAD_COMMAND',
-        ofp.OFPFMFC_BAD_FLAGS: 'OFPFMFC_BAD_FLAGS'}),
-    ofp.OFPET_GROUP_MOD_FAILED: ('OFPET_GROUP_MOD_FAILED', {
-        ofp.OFPGMFC_GROUP_EXISTS: 'OFPGMFC_GROUP_EXISTS',
-        ofp.OFPGMFC_INVALID_GROUP: 'OFPGMFC_INVALID_GROUP',
-        ofp.OFPGMFC_WEIGHT_UNSUPPORTED: 'OFPGMFC_WEIGHT_UNSUPPORTED',
-        ofp.OFPGMFC_OUT_OF_GROUPS: 'OFPGMFC_OUT_OF_GROUPS',
-        ofp.OFPGMFC_OUT_OF_BUCKETS: 'OFPGMFC_OUT_OF_BUCKETS',
-        ofp.OFPGMFC_CHAINING_UNSUPPORTED: 'OFPGMFC_CHAINING_UNSUPPORTED',
-        ofp.OFPGMFC_WATCH_UNSUPPORTED: 'OFPGMFC_WATCH_UNSUPPORTED',
-        ofp.OFPGMFC_LOOP: 'OFPGMFC_LOOP',
-        ofp.OFPGMFC_UNKNOWN_GROUP: 'OFPGMFC_UNKNOWN_GROUP',
-        ofp.OFPGMFC_CHAINED_GROUP: 'OFPGMFC_CHAINED_GROUP',
-        ofp.OFPGMFC_BAD_TYPE: 'OFPGMFC_BAD_TYPE',
-        ofp.OFPGMFC_BAD_COMMAND: 'OFPGMFC_BAD_COMMAND',
-        ofp.OFPGMFC_BAD_BUCKET: 'OFPGMFC_BAD_BUCKET',
-        ofp.OFPGMFC_BAD_WATCH: 'OFPGMFC_BAD_WATCH',
-        ofp.OFPGMFC_EPERM: 'OFPGMFC_EPERM'}),
-    ofp.OFPET_PORT_MOD_FAILED: ('OFPET_PORT_MOD_FAILED', {
-        ofp.OFPPMFC_BAD_PORT: 'OFPPMFC_BAD_PORT',
-        ofp.OFPPMFC_BAD_HW_ADDR: 'OFPPMFC_BAD_HW_ADDR',
-        ofp.OFPPMFC_BAD_CONFIG: 'OFPPMFC_BAD_CONFIG',
-        ofp.OFPPMFC_BAD_ADVERTISE: 'OFPPMFC_BAD_ADVERTISE',
-        ofp.OFPPMFC_EPERM: 'OFPPMFC_EPERM'}),
-    ofp.OFPET_TABLE_MOD_FAILED: ('OFPET_TABLE_MOD_FAILED', {
-        ofp.OFPTMFC_BAD_TABLE: 'OFPTMFC_BAD_TABLE',
-        ofp.OFPTMFC_BAD_CONFIG: 'OFPTMFC_BAD_CONFIG',
-        ofp.OFPTMFC_EPERM: 'OFPTMFC_EPERM'}),
-    ofp.OFPET_QUEUE_OP_FAILED: ('OFPET_QUEUE_OP_FAILED', {
-        ofp.OFPQOFC_BAD_PORT: 'OFPQOFC_BAD_PORT',
-        ofp.OFPQOFC_BAD_QUEUE: 'OFPQOFC_BAD_QUEUE',
-        ofp.OFPQOFC_EPERM: 'OFPQOFC_EPERM'}),
-    ofp.OFPET_SWITCH_CONFIG_FAILED: ('OFPET_SWITCH_CONFIG_FAILED', {
-        ofp.OFPSCFC_BAD_FLAGS: 'OFPSCFC_BAD_FLAGS',
-        ofp.OFPSCFC_BAD_LEN: 'OFPSCFC_BAD_LEN',
-        ofp.OFPSCFC_EPERM: 'OFPSCFC_EPERM'}),
-    ofp.OFPET_ROLE_REQUEST_FAILED: ('OFPET_ROLE_REQUEST_FAILED', {
-        ofp.OFPRRFC_STALE: 'OFPRRFC_STALE',
-        ofp.OFPRRFC_UNSUP: 'OFPRRFC_UNSUP',
-        ofp.OFPRRFC_BAD_ROLE: 'OFPRRFC_BAD_ROLE'}),
-    ofp.OFPET_METER_MOD_FAILED: ('OFPET_METER_MOD_FAILED', {
-        ofp.OFPMMFC_UNKNOWN: 'OFPMMFC_UNKNOWN',
-        ofp.OFPMMFC_METER_EXISTS: 'OFPMMFC_METER_EXISTS',
-        ofp.OFPMMFC_INVALID_METER: 'OFPMMFC_INVALID_METER',
-        ofp.OFPMMFC_UNKNOWN_METER: 'OFPMMFC_UNKNOWN_METER',
-        ofp.OFPMMFC_BAD_COMMAND: 'OFPMMFC_BAD_COMMAND',
-        ofp.OFPMMFC_BAD_FLAGS: 'OFPMMFC_BAD_FLAGS',
-        ofp.OFPMMFC_BAD_RATE: 'OFPMMFC_BAD_RATE',
-        ofp.OFPMMFC_BAD_BURST: 'OFPMMFC_BAD_BURST',
-        ofp.OFPMMFC_BAD_BAND: 'OFPMMFC_BAD_BAND',
-        ofp.OFPMMFC_BAD_BAND_VALUE: 'OFPMMFC_BAD_BAND_VALUE',
-        ofp.OFPMMFC_OUT_OF_METERS: 'OFPMMFC_OUT_OF_METERS',
-        ofp.OFPMMFC_OUT_OF_BANDS: 'OFPMMFC_OUT_OF_BANDS'}),
-    ofp.OFPET_TABLE_FEATURES_FAILED: ('OFPET_TABLE_FEATURES_FAILED', {
-        ofp.OFPTFFC_BAD_TABLE: 'OFPTFFC_BAD_TABLE',
-        ofp.OFPTFFC_BAD_METADATA: 'OFPTFFC_BAD_METADATA',
-        ofp.OFPTFFC_BAD_TYPE: 'OFPTFFC_BAD_TYPE',
-        ofp.OFPTFFC_BAD_LEN: 'OFPTFFC_BAD_LEN',
-        ofp.OFPTFFC_BAD_ARGUMENT: 'OFPTFFC_BAD_ARGUMENT',
-        ofp.OFPTFFC_EPERM: 'OFPTFFC_EPERM'}),
-    ofp.OFPET_EXPERIMENTER: ('OFPET_EXPERIMENTER', {}),
+    ofp.OFPET_HELLO_FAILED: (
+        "OFPET_HELLO_FAILED",
+        {
+            ofp.OFPHFC_INCOMPATIBLE: "OFPHFC_INCOMPATIBLE",
+            ofp.OFPHFC_EPERM: "OFPHFC_EPERM",
+        },
+    ),
+    ofp.OFPET_BAD_REQUEST: (
+        "OFPET_BAD_REQUEST",
+        {
+            ofp.OFPBRC_BAD_VERSION: "OFPBRC_BAD_VERSION",
+            ofp.OFPBRC_BAD_TYPE: "OFPBRC_BAD_TYPE",
+            ofp.OFPBRC_BAD_MULTIPART: "OFPBRC_BAD_MULTIPART",
+            ofp.OFPBRC_BAD_EXPERIMENTER: "OFPBRC_BAD_EXPERIMENTER",
+            ofp.OFPBRC_BAD_EXP_TYPE: "OFPBRC_BAD_EXP_TYPE",
+            ofp.OFPBRC_EPERM: "OFPBRC_EPERM",
+            ofp.OFPBRC_BAD_LEN: "OFPBRC_BAD_LEN",
+            ofp.OFPBRC_BUFFER_EMPTY: "OFPBRC_BUFFER_EMPTY",
+            ofp.OFPBRC_BUFFER_UNKNOWN: "OFPBRC_BUFFER_UNKNOWN",
+            ofp.OFPBRC_BAD_TABLE_ID: "OFPBRC_BAD_TABLE_ID",
+            ofp.OFPBRC_IS_SLAVE: "OFPBRC_IS_SLAVE",
+            ofp.OFPBRC_BAD_PORT: "OFPBRC_BAD_PORT",
+            ofp.OFPBRC_BAD_PACKET: "OFPBRC_BAD_PACKET",
+            ofp.OFPBRC_MULTIPART_BUFFER_OVERFLOW: "OFPBRC_MULTIPART_BUFFER_OVERFLOW",
+        },
+    ),
+    ofp.OFPET_BAD_ACTION: (
+        "OFPET_BAD_ACTION",
+        {
+            ofp.OFPBAC_BAD_TYPE: "OFPBAC_BAD_TYPE",
+            ofp.OFPBAC_BAD_LEN: "OFPBAC_BAD_LEN",
+            ofp.OFPBAC_BAD_EXPERIMENTER: "OFPBAC_BAD_EXPERIMENTER",
+            ofp.OFPBAC_BAD_EXP_TYPE: "OFPBAC_BAD_EXP_TYPE",
+            ofp.OFPBAC_BAD_OUT_PORT: "OFPBAC_BAD_OUT_PORT",
+            ofp.OFPBAC_BAD_ARGUMENT: "OFPBAC_BAD_ARGUMENT",
+            ofp.OFPBAC_EPERM: "OFPBAC_EPERM",
+            ofp.OFPBAC_TOO_MANY: "OFPBAC_TOO_MANY",
+            ofp.OFPBAC_BAD_QUEUE: "OFPBAC_BAD_QUEUE",
+            ofp.OFPBAC_BAD_OUT_GROUP: "OFPBAC_BAD_OUT_GROUP",
+            ofp.OFPBAC_MATCH_INCONSISTENT: "OFPBAC_MATCH_INCONSISTENT",
+            ofp.OFPBAC_UNSUPPORTED_ORDER: "OFPBAC_UNSUPPORTED_ORDER",
+            ofp.OFPBAC_BAD_TAG: "OFPBAC_BAD_TAG",
+            ofp.OFPBAC_BAD_SET_TYPE: "OFPBAC_BAD_SET_TYPE",
+            ofp.OFPBAC_BAD_SET_LEN: "OFPBAC_BAD_SET_LEN",
+            ofp.OFPBAC_BAD_SET_ARGUMENT: "OFPBAC_BAD_SET_ARGUMENT",
+        },
+    ),
+    ofp.OFPET_BAD_INSTRUCTION: (
+        "OFPET_BAD_INSTRUCTION",
+        {
+            ofp.OFPBIC_UNKNOWN_INST: "OFPBIC_UNKNOWN_INST",
+            ofp.OFPBIC_UNSUP_INST: "OFPBIC_UNSUP_INST",
+            ofp.OFPBIC_BAD_TABLE_ID: "OFPBIC_BAD_TABLE_ID",
+            ofp.OFPBIC_UNSUP_METADATA: "OFPBIC_UNSUP_METADATA",
+            ofp.OFPBIC_UNSUP_METADATA_MASK: "OFPBIC_UNSUP_METADATA_MASK",
+            ofp.OFPBIC_BAD_EXPERIMENTER: "OFPBIC_BAD_EXPERIMENTER",
+            ofp.OFPBIC_BAD_EXP_TYPE: "OFPBIC_BAD_EXP_TYPE",
+            ofp.OFPBIC_BAD_LEN: "OFPBIC_BAD_LEN",
+            ofp.OFPBIC_EPERM: "OFPBIC_EPERM",
+        },
+    ),
+    ofp.OFPET_BAD_MATCH: (
+        "OFPET_BAD_MATCH",
+        {
+            ofp.OFPBMC_BAD_TYPE: "OFPBMC_BAD_TYPE",
+            ofp.OFPBMC_BAD_LEN: "OFPBMC_BAD_LEN",
+            ofp.OFPBMC_BAD_TAG: "OFPBMC_BAD_TAG",
+            ofp.OFPBMC_BAD_DL_ADDR_MASK: "OFPBMC_BAD_DL_ADDR_MASK",
+            ofp.OFPBMC_BAD_NW_ADDR_MASK: "OFPBMC_BAD_NW_ADDR_MASK",
+            ofp.OFPBMC_BAD_WILDCARDS: "OFPBMC_BAD_WILDCARDS",
+            ofp.OFPBMC_BAD_FIELD: "OFPBMC_BAD_FIELD",
+            ofp.OFPBMC_BAD_VALUE: "OFPBMC_BAD_VALUE",
+            ofp.OFPBMC_BAD_MASK: "OFPBMC_BAD_MASK",
+            ofp.OFPBMC_BAD_PREREQ: "OFPBMC_BAD_PREREQ",
+            ofp.OFPBMC_DUP_FIELD: "OFPBMC_DUP_FIELD",
+            ofp.OFPBMC_EPERM: "OFPBMC_EPERM",
+        },
+    ),
+    ofp.OFPET_FLOW_MOD_FAILED: (
+        "OFPET_FLOW_MOD_FAILED",
+        {
+            ofp.OFPFMFC_UNKNOWN: "OFPFMFC_UNKNOWN",
+            ofp.OFPFMFC_TABLE_FULL: "OFPFMFC_TABLE_FULL",
+            ofp.OFPFMFC_BAD_TABLE_ID: "OFPFMFC_BAD_TABLE_ID",
+            ofp.OFPFMFC_OVERLAP: "OFPFMFC_OVERLAP",
+            ofp.OFPFMFC_EPERM: "OFPFMFC_EPERM",
+            ofp.OFPFMFC_BAD_TIMEOUT: "OFPFMFC_BAD_TIMEOUT",
+            ofp.OFPFMFC_BAD_COMMAND: "OFPFMFC_BAD_COMMAND",
+            ofp.OFPFMFC_BAD_FLAGS: "OFPFMFC_BAD_FLAGS",
+        },
+    ),
+    ofp.OFPET_GROUP_MOD_FAILED: (
+        "OFPET_GROUP_MOD_FAILED",
+        {
+            ofp.OFPGMFC_GROUP_EXISTS: "OFPGMFC_GROUP_EXISTS",
+            ofp.OFPGMFC_INVALID_GROUP: "OFPGMFC_INVALID_GROUP",
+            ofp.OFPGMFC_WEIGHT_UNSUPPORTED: "OFPGMFC_WEIGHT_UNSUPPORTED",
+            ofp.OFPGMFC_OUT_OF_GROUPS: "OFPGMFC_OUT_OF_GROUPS",
+            ofp.OFPGMFC_OUT_OF_BUCKETS: "OFPGMFC_OUT_OF_BUCKETS",
+            ofp.OFPGMFC_CHAINING_UNSUPPORTED: "OFPGMFC_CHAINING_UNSUPPORTED",
+            ofp.OFPGMFC_WATCH_UNSUPPORTED: "OFPGMFC_WATCH_UNSUPPORTED",
+            ofp.OFPGMFC_LOOP: "OFPGMFC_LOOP",
+            ofp.OFPGMFC_UNKNOWN_GROUP: "OFPGMFC_UNKNOWN_GROUP",
+            ofp.OFPGMFC_CHAINED_GROUP: "OFPGMFC_CHAINED_GROUP",
+            ofp.OFPGMFC_BAD_TYPE: "OFPGMFC_BAD_TYPE",
+            ofp.OFPGMFC_BAD_COMMAND: "OFPGMFC_BAD_COMMAND",
+            ofp.OFPGMFC_BAD_BUCKET: "OFPGMFC_BAD_BUCKET",
+            ofp.OFPGMFC_BAD_WATCH: "OFPGMFC_BAD_WATCH",
+            ofp.OFPGMFC_EPERM: "OFPGMFC_EPERM",
+        },
+    ),
+    ofp.OFPET_PORT_MOD_FAILED: (
+        "OFPET_PORT_MOD_FAILED",
+        {
+            ofp.OFPPMFC_BAD_PORT: "OFPPMFC_BAD_PORT",
+            ofp.OFPPMFC_BAD_HW_ADDR: "OFPPMFC_BAD_HW_ADDR",
+            ofp.OFPPMFC_BAD_CONFIG: "OFPPMFC_BAD_CONFIG",
+            ofp.OFPPMFC_BAD_ADVERTISE: "OFPPMFC_BAD_ADVERTISE",
+            ofp.OFPPMFC_EPERM: "OFPPMFC_EPERM",
+        },
+    ),
+    ofp.OFPET_TABLE_MOD_FAILED: (
+        "OFPET_TABLE_MOD_FAILED",
+        {
+            ofp.OFPTMFC_BAD_TABLE: "OFPTMFC_BAD_TABLE",
+            ofp.OFPTMFC_BAD_CONFIG: "OFPTMFC_BAD_CONFIG",
+            ofp.OFPTMFC_EPERM: "OFPTMFC_EPERM",
+        },
+    ),
+    ofp.OFPET_QUEUE_OP_FAILED: (
+        "OFPET_QUEUE_OP_FAILED",
+        {
+            ofp.OFPQOFC_BAD_PORT: "OFPQOFC_BAD_PORT",
+            ofp.OFPQOFC_BAD_QUEUE: "OFPQOFC_BAD_QUEUE",
+            ofp.OFPQOFC_EPERM: "OFPQOFC_EPERM",
+        },
+    ),
+    ofp.OFPET_SWITCH_CONFIG_FAILED: (
+        "OFPET_SWITCH_CONFIG_FAILED",
+        {
+            ofp.OFPSCFC_BAD_FLAGS: "OFPSCFC_BAD_FLAGS",
+            ofp.OFPSCFC_BAD_LEN: "OFPSCFC_BAD_LEN",
+            ofp.OFPSCFC_EPERM: "OFPSCFC_EPERM",
+        },
+    ),
+    ofp.OFPET_ROLE_REQUEST_FAILED: (
+        "OFPET_ROLE_REQUEST_FAILED",
+        {
+            ofp.OFPRRFC_STALE: "OFPRRFC_STALE",
+            ofp.OFPRRFC_UNSUP: "OFPRRFC_UNSUP",
+            ofp.OFPRRFC_BAD_ROLE: "OFPRRFC_BAD_ROLE",
+        },
+    ),
+    ofp.OFPET_METER_MOD_FAILED: (
+        "OFPET_METER_MOD_FAILED",
+        {
+            ofp.OFPMMFC_UNKNOWN: "OFPMMFC_UNKNOWN",
+            ofp.OFPMMFC_METER_EXISTS: "OFPMMFC_METER_EXISTS",
+            ofp.OFPMMFC_INVALID_METER: "OFPMMFC_INVALID_METER",
+            ofp.OFPMMFC_UNKNOWN_METER: "OFPMMFC_UNKNOWN_METER",
+            ofp.OFPMMFC_BAD_COMMAND: "OFPMMFC_BAD_COMMAND",
+            ofp.OFPMMFC_BAD_FLAGS: "OFPMMFC_BAD_FLAGS",
+            ofp.OFPMMFC_BAD_RATE: "OFPMMFC_BAD_RATE",
+            ofp.OFPMMFC_BAD_BURST: "OFPMMFC_BAD_BURST",
+            ofp.OFPMMFC_BAD_BAND: "OFPMMFC_BAD_BAND",
+            ofp.OFPMMFC_BAD_BAND_VALUE: "OFPMMFC_BAD_BAND_VALUE",
+            ofp.OFPMMFC_OUT_OF_METERS: "OFPMMFC_OUT_OF_METERS",
+            ofp.OFPMMFC_OUT_OF_BANDS: "OFPMMFC_OUT_OF_BANDS",
+        },
+    ),
+    ofp.OFPET_TABLE_FEATURES_FAILED: (
+        "OFPET_TABLE_FEATURES_FAILED",
+        {
+            ofp.OFPTFFC_BAD_TABLE: "OFPTFFC_BAD_TABLE",
+            ofp.OFPTFFC_BAD_METADATA: "OFPTFFC_BAD_METADATA",
+            ofp.OFPTFFC_BAD_TYPE: "OFPTFFC_BAD_TYPE",
+            ofp.OFPTFFC_BAD_LEN: "OFPTFFC_BAD_LEN",
+            ofp.OFPTFFC_BAD_ARGUMENT: "OFPTFFC_BAD_ARGUMENT",
+            ofp.OFPTFFC_EPERM: "OFPTFFC_EPERM",
+        },
+    ),
+    ofp.OFPET_EXPERIMENTER: ("OFPET_EXPERIMENTER", {}),
 }
 
 
 def ignore_port(port_num):
     """Return True if FAUCET should ignore this port.
 
     Args:
@@ -240,15 +302,18 @@
 
     Args:
         ofmsg: ryu.ofproto.ofproto_v1_3_parser message.
     Returns:
         bool: True if is a FlowMod, add or modify.
     """
     return isinstance(ofmsg, parser.OFPFlowMod) and ofmsg.command in (
-        ofp.OFPFC_ADD, ofp.OFPFC_MODIFY, ofp.OFPFC_MODIFY_STRICT)
+        ofp.OFPFC_ADD,
+        ofp.OFPFC_MODIFY,
+        ofp.OFPFC_MODIFY_STRICT,
+    )
 
 
 def is_groupmod(ofmsg):
     """Return True if OF message is a GroupMod.
 
     Args:
         ofmsg: ryu.ofproto.ofproto_v1_3_parser message.
@@ -295,83 +360,84 @@
     """Return True if flow message is a FlowMod and a delete.
 
     Args:
         ofmsg: ryu.ofproto.ofproto_v1_3_parser message.
     Returns:
         bool: True if is a FlowMod delete/strict.
     """
-    return is_flowmod(ofmsg) and ofmsg.command in (ofp.OFPFC_DELETE, ofp.OFPFC_DELETE_STRICT)
+    return is_flowmod(ofmsg) and ofmsg.command in (
+        ofp.OFPFC_DELETE,
+        ofp.OFPFC_DELETE_STRICT,
+    )
 
 
 def is_groupdel(ofmsg):
     """Return True if OF message is a GroupMod and command is delete.
 
     Args:
         ofmsg: ryu.ofproto.ofproto_v1_3_parser message.
     Returns:
         bool: True if is a GroupMod delete
     """
-    if (is_groupmod(ofmsg)
-            and (ofmsg.command == ofp.OFPGC_DELETE)):
+    if is_groupmod(ofmsg) and (ofmsg.command == ofp.OFPGC_DELETE):
         return True
     return False
 
 
 def is_meterdel(ofmsg):
     """Return True if OF message is a MeterMod and command is delete.
 
     Args:
         ofmsg: ryu.ofproto.ofproto_v1_3_parser message.
     Returns:
         bool: True if is a MeterMod delete
     """
-    if (is_metermod(ofmsg)
-            and (ofmsg.command == ofp.OFPMC_DELETE)):
+    if is_metermod(ofmsg) and (ofmsg.command == ofp.OFPMC_DELETE):
         return True
     return False
 
 
 def is_groupadd(ofmsg):
     """Return True if OF message is a GroupMod and command is add.
 
     Args:
         ofmsg: ryu.ofproto.ofproto_v1_3_parser message.
     Returns:
         bool: True if is a GroupMod add
     """
-    if (is_groupmod(ofmsg)
-            and (ofmsg.command == ofp.OFPGC_ADD)):
+    if is_groupmod(ofmsg) and (ofmsg.command == ofp.OFPGC_ADD):
         return True
     return False
 
 
 def is_meteradd(ofmsg):
     """Return True if OF message is a MeterMod and command is add.
 
     Args:
         ofmsg: ryu.ofproto.ofproto_v1_3_parser message.
     Returns:
         bool: True if is a MeterMod add
     """
-    if (is_metermod(ofmsg)
-            and (ofmsg.command == ofp.OFPMC_ADD)):
+    if is_metermod(ofmsg) and (ofmsg.command == ofp.OFPMC_ADD):
         return True
     return False
 
 
 def is_apply_actions(instruction):
     """Return True if an apply action.
 
     Args:
         instruction: OpenFlow instruction.
     Returns:
         bool: True if an apply action.
     """
-    return (isinstance(instruction, parser.OFPInstructionActions)
-            and instruction.type == ofp.OFPIT_APPLY_ACTIONS)
+    return (
+        isinstance(instruction, parser.OFPInstructionActions)
+        and instruction.type == ofp.OFPIT_APPLY_ACTIONS
+    )
 
 
 def is_meter(instruction):
     """Return True if a meter.
 
     Args:
         instruction: OpenFlow instruction.
@@ -390,15 +456,15 @@
 
 
 def apply_meter(meter_id):
     """Return instruction to apply a meter."""
     return parser.OFPInstructionMeter(meter_id, ofp.OFPIT_METER)
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache()
 def _apply_actions(actions):
     return parser.OFPInstructionActions(ofp.OFPIT_APPLY_ACTIONS, actions)
 
 
 def apply_actions(actions):
     """Return instruction that applies action list.
 
@@ -406,27 +472,27 @@
         actions (list): list of OpenFlow actions.
     Returns:
         ryu.ofproto.ofproto_v1_3_parser.OFPInstruction: instruction of actions.
     """
     return _apply_actions(tuple(actions))
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache()
 def goto_table(table):
     """Return instruction to goto table.
 
     Args:
         table (ValveTable): table to goto.
     Returns:
         ryu.ofproto.ofproto_v1_3_parser.OFPInstruction: goto instruction.
     """
     return parser.OFPInstructionGotoTable(table.table_id)
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache()
 def goto_table_id(table_id):
     """Return instruction to goto table by table ID.
 
     Args:
         table (int): table by ID to goto.
     Returns:
         ryu.ofproto.ofproto_v1_3_parser.OFPInstruction: goto instruction.
@@ -441,19 +507,19 @@
         metadata (int): metadata to write to packet
         maks (int): mask to apply to metadata
         table (ValveTable): table to goto.
     Returns:
         list of OFPInstructions"""
     return [
         parser.OFPInstructionWriteMetadata(metadata, mask),
-        parser.OFPInstructionGotoTable(table.table_id)
+        parser.OFPInstructionGotoTable(table.table_id),
     ]
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache()
 def set_field(**kwds):
     """Return action to set any field.
 
     Args:
         kwds (dict): exactly one field to set
     Returns:
         ryu.ofproto.ofproto_v1_3_parser.OFPActionSetField: set field action.
@@ -479,38 +545,40 @@
         vid (int): VLAN VID with VID_PRESENT.
     Returns:
         int: VLAN VID.
     """
     return vid ^ ofp.OFPVID_PRESENT
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def push_vlan_act(table, vlan_vid, eth_type=ether.ETH_TYPE_8021Q):
     """Return OpenFlow action list to push Ethernet 802.1Q header with VLAN VID.
 
     Args:
         vid (int): VLAN VID
     Returns:
         list: actions to push 802.1Q header with VLAN VID set.
     """
     return [
         parser.OFPActionPushVlan(eth_type),
         table.set_vlan_vid(vlan_vid),
     ]
 
 
+@functools.lru_cache()
 def dec_ip_ttl():
     """Return OpenFlow action to decrement IP TTL.
 
     Returns:
         ryu.ofproto.ofproto_v1_3_parser.OFPActionDecNwTtl: decrement IP TTL.
     """
     return parser.OFPActionDecNwTtl()
 
 
+@functools.lru_cache(maxsize=1024)
 def pop_vlan():
     """Return OpenFlow action to pop outermost Ethernet 802.1Q VLAN header.
 
     Returns:
         ryu.ofproto.ofproto_v1_3_parser.OFPActionPopVlan: Pop VLAN.
     """
     return parser.OFPActionPopVlan()
@@ -545,15 +613,15 @@
         kwds (dict): exactly one network address translation connection tracker action.
     Returns:
         ryu.ofproto.nx_actions.NXActionNAT: network address translation connection tracker action.
     """
     return parser.NXActionNAT(**kwds)  # pylint: disable=no-member
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def output_port(port_num, max_len=0):
     """Return OpenFlow action to output to a port.
 
     Args:
         port_num (int): port to output to.
         max_len (int): maximum length of packet to output (default no maximum).
     Returns:
@@ -581,15 +649,15 @@
     Returns:
         list of ryu.ofproto.ofproto_v1_3_parser.OFPActionOutput: output to port actions.
     """
     output_ports = ports_from_output_port_acts(output_port_acts)
     return [output_port(port) for port in sorted(output_ports)]
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def output_non_output_actions(flood_acts):
     """Split output actions into deduped actions, output ports, and non-output port actions.
 
     Args:
         list of ryu.ofproto.ofproto_v1_3_parser.OFPActions: flood actions.
     Returns:
         set of deduped actions, output ports, and non-output actions.
@@ -610,24 +678,25 @@
                 continue
             all_nonoutput_actions.add(str_act)
         deduped_acts.append(act)
     nonoutput_actions = all_nonoutput_actions - set([str(pop_vlan())])
     return (deduped_acts, output_ports, nonoutput_actions)
 
 
+@functools.lru_cache()
 def output_in_port():
     """Return OpenFlow action to output out input port.
 
     Returns:
        ryu.ofproto.ofproto_v1_3_parser.OFPActionOutput.
     """
     return output_port(OFP_IN_PORT)
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache()
 def output_controller(max_len=MAX_PACKET_IN_BYTES):
     """Return OpenFlow action to packet in to the controller.
 
     Args:
         max_len (int): max number of bytes from packet to output.
     Returns:
         ryu.ofproto.ofproto_v1_3_parser.OFPActionOutput: packet in action.
@@ -646,232 +715,257 @@
     """
     random.shuffle(port_nums)
     return parser.OFPPacketOut(
         datapath=None,
         buffer_id=ofp.OFP_NO_BUFFER,
         in_port=ofp.OFPP_CONTROLLER,
         actions=[output_port(port_num) for port_num in port_nums],
-        data=data)
+        data=data,
+    )
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache()
 def packetout(port_num, data):
     """Return OpenFlow action to packet out to dataplane from controller.
 
     Args:
         port_num (int): port to output to.
         data (str): raw packet to output.
     Returns:
         ryu.ofproto.ofproto_v1_3_parser.OFPActionOutput: packet out action.
     """
     return packetouts([port_num], data)
 
 
+@functools.lru_cache()
 def barrier():
     """Return OpenFlow barrier request.
 
     Returns:
         ryu.ofproto.ofproto_v1_3_parser.OFPBarrierRequest: barrier request.
     """
     return parser.OFPBarrierRequest(None)
 
 
 def table_features(body):
-    return parser.OFPTableFeaturesStatsRequest(
-        datapath=None, body=body)
+    return parser.OFPTableFeaturesStatsRequest(datapath=None, body=body)
 
 
 def match(match_fields):
     """Return OpenFlow matches from dict.
 
     Args:
         match_fields (dict): match fields and values.
     Returns:
         ryu.ofproto.ofproto_v1_3_parser.OFPMatch: matches.
     """
     return parser.OFPMatch(**match_fields)
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache()
 def valve_match_vid(value):
     return to_match_vid(value, ofp.OFPVID_PRESENT)
 
 
 MATCH_FIELDS = {
     # See 7.2.3.7 Flow Match Fields (OF 1.3.5)
-    'in_port': OFCtlUtil(ofp).ofp_port_from_user,
-    'in_phy_port': str_to_int,
-    'metadata': to_match_masked_int,
-    'eth_dst': to_match_eth,
-    'eth_src': to_match_eth,
-    'eth_type': str_to_int,
-    'vlan_vid': valve_match_vid,
-    'vlan_pcp': str_to_int,
-    'ip_dscp': str_to_int,
-    'ip_ecn': str_to_int,
-    'ip_proto': str_to_int,
-    'ipv4_src': to_match_ip,
-    'ipv4_dst': to_match_ip,
-    'tcp_src': to_match_masked_int,
-    'tcp_dst': to_match_masked_int,
-    'udp_src': to_match_masked_int,
-    'udp_dst': to_match_masked_int,
-    'sctp_src': to_match_masked_int,
-    'sctp_dst': to_match_masked_int,
-    'icmpv4_type': str_to_int,
-    'icmpv4_code': str_to_int,
-    'arp_op': str_to_int,
-    'arp_spa': to_match_ip,
-    'arp_tpa': to_match_ip,
-    'arp_sha': to_match_eth,
-    'arp_tha': to_match_eth,
-    'ipv6_src': to_match_ip,
-    'ipv6_dst': to_match_ip,
-    'ipv6_flabel': str_to_int,
-    'icmpv6_type': str_to_int,
-    'icmpv6_code': str_to_int,
-    'ipv6_nd_target': to_match_ip,
-    'ipv6_nd_sll': to_match_eth,
-    'ipv6_nd_tll': to_match_eth,
-    'mpls_label': str_to_int,
-    'mpls_tc': str_to_int,
-    'mpls_bos': str_to_int,
-    'pbb_isid': to_match_masked_int,
-    'tunnel_id': to_match_masked_int,
-    'ipv6_exthdr': to_match_masked_int,
-
+    "in_port": OFCtlUtil(ofp).ofp_port_from_user,
+    "in_phy_port": str_to_int,
+    "metadata": to_match_masked_int,
+    "eth_dst": to_match_eth,
+    "eth_src": to_match_eth,
+    "eth_type": str_to_int,
+    "vlan_vid": valve_match_vid,
+    "vlan_pcp": str_to_int,
+    "ip_dscp": str_to_int,
+    "ip_ecn": str_to_int,
+    "ip_proto": str_to_int,
+    "ipv4_src": to_match_ip,
+    "ipv4_dst": to_match_ip,
+    "tcp_src": to_match_masked_int,
+    "tcp_dst": to_match_masked_int,
+    "udp_src": to_match_masked_int,
+    "udp_dst": to_match_masked_int,
+    "sctp_src": to_match_masked_int,
+    "sctp_dst": to_match_masked_int,
+    "icmpv4_type": str_to_int,
+    "icmpv4_code": str_to_int,
+    "arp_op": str_to_int,
+    "arp_spa": to_match_ip,
+    "arp_tpa": to_match_ip,
+    "arp_sha": to_match_eth,
+    "arp_tha": to_match_eth,
+    "ipv6_src": to_match_ip,
+    "ipv6_dst": to_match_ip,
+    "ipv6_flabel": str_to_int,
+    "icmpv6_type": str_to_int,
+    "icmpv6_code": str_to_int,
+    "ipv6_nd_target": to_match_ip,
+    "ipv6_nd_sll": to_match_eth,
+    "ipv6_nd_tll": to_match_eth,
+    "mpls_label": str_to_int,
+    "mpls_tc": str_to_int,
+    "mpls_bos": str_to_int,
+    "pbb_isid": to_match_masked_int,
+    "tunnel_id": to_match_masked_int,
+    "ipv6_exthdr": to_match_masked_int,
     # Nicira extensions, see ovs-fields(7)
-    'ct_state': to_match_masked_int,
-    'ct_zone': str_to_int,
-    'ct_mark': to_match_masked_int,
-    'ct_label': to_match_masked_int
+    "ct_state": to_match_masked_int,
+    "ct_zone": str_to_int,
+    "ct_mark": to_match_masked_int,
+    "ct_label": to_match_masked_int,
 }
 
 
 def match_from_dict(match_dict):
     """Parse a match dict into a OFPMatch object"""
     kwargs = {}
     for of_match, field in match_dict.items():
         of_match = OLD_MATCH_FIELDS.get(of_match, of_match)
-        test_config_condition(of_match not in MATCH_FIELDS, 'Unknown match field: %s' % of_match)
+        test_config_condition(
+            of_match not in MATCH_FIELDS, "Unknown match field: %s" % of_match
+        )
         try:
             encoded_field = MATCH_FIELDS[of_match](field)
         except TypeError as type_error:
-            raise InvalidConfigError('%s cannot be type %s' %
-                                     (of_match, type(field))) from type_error
+            raise InvalidConfigError(
+                "%s cannot be type %s" % (of_match, type(field))
+            ) from type_error
         kwargs[of_match] = encoded_field
 
     return parser.OFPMatch(**kwargs)
 
 
 def _match_ip_masked(ipa):
     if isinstance(ipa, (ipaddress.IPv4Network, ipaddress.IPv6Network)):
         return (str(ipa.network_address), str(ipa.netmask))
     return (str(ipa.ip), str(ipa.netmask))
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
-def build_match_dict(in_port=None, vlan=None, eth_type=None, eth_src=None,
-                     eth_dst=None, eth_dst_mask=None, icmpv6_type=None,
-                     nw_proto=None, nw_dst=None, metadata=None,
-                     metadata_mask=None, vlan_pcp=None, udp_src=None, udp_dst=None):
+@functools.lru_cache(maxsize=1024)
+def build_match_dict(
+    in_port=None,
+    vlan=None,
+    eth_type=None,
+    eth_src=None,
+    eth_dst=None,
+    eth_dst_mask=None,
+    icmpv6_type=None,
+    nw_proto=None,
+    nw_dst=None,
+    metadata=None,
+    metadata_mask=None,
+    vlan_pcp=None,
+    udp_src=None,
+    udp_dst=None,
+):
     match_dict = {}
     if in_port is not None:
-        match_dict['in_port'] = in_port
+        match_dict["in_port"] = in_port
     if vlan is not None:
         if isinstance(vlan, int):
             vid = vlan
         else:
             vid = vlan.vid
         if vid == ofp.OFPVID_NONE:
-            match_dict['vlan_vid'] = int(ofp.OFPVID_NONE)
+            match_dict["vlan_vid"] = int(ofp.OFPVID_NONE)
         elif vid == ofp.OFPVID_PRESENT:
-            match_dict['vlan_vid'] = (ofp.OFPVID_PRESENT, ofp.OFPVID_PRESENT)
+            match_dict["vlan_vid"] = (ofp.OFPVID_PRESENT, ofp.OFPVID_PRESENT)
         else:
-            match_dict['vlan_vid'] = vid_present(vid)
+            match_dict["vlan_vid"] = vid_present(vid)
     if eth_src is not None:
-        match_dict['eth_src'] = eth_src
+        match_dict["eth_src"] = eth_src
     if eth_dst is not None:
         if eth_dst_mask is not None:
-            match_dict['eth_dst'] = (eth_dst, eth_dst_mask)
+            match_dict["eth_dst"] = (eth_dst, eth_dst_mask)
         else:
-            match_dict['eth_dst'] = eth_dst
+            match_dict["eth_dst"] = eth_dst
     if nw_proto is not None:
-        match_dict['ip_proto'] = nw_proto
+        match_dict["ip_proto"] = nw_proto
     if udp_dst is not None:
-        match_dict['udp_dst'] = udp_dst
+        match_dict["udp_dst"] = udp_dst
     if udp_src is not None:
-        match_dict['udp_src'] = udp_src
+        match_dict["udp_src"] = udp_src
     if icmpv6_type is not None:
-        match_dict['icmpv6_type'] = icmpv6_type
+        match_dict["icmpv6_type"] = icmpv6_type
     if nw_dst is not None:
         nw_dst_masked = _match_ip_masked(nw_dst)
         if eth_type == ether.ETH_TYPE_ARP:
-            match_dict['arp_tpa'] = str(nw_dst.ip)
+            match_dict["arp_tpa"] = str(nw_dst.ip)
         elif eth_type == ether.ETH_TYPE_IP:
-            match_dict['ipv4_dst'] = nw_dst_masked
+            match_dict["ipv4_dst"] = nw_dst_masked
         else:
-            match_dict['ipv6_dst'] = nw_dst_masked
+            match_dict["ipv6_dst"] = nw_dst_masked
     if eth_type is not None:
-        match_dict['eth_type'] = eth_type
+        match_dict["eth_type"] = eth_type
     if metadata is not None:
         if metadata_mask is not None:
-            match_dict['metadata'] = (metadata, metadata_mask)
+            match_dict["metadata"] = (metadata, metadata_mask)
         else:
-            match_dict['metadata'] = metadata
+            match_dict["metadata"] = metadata
     if vlan_pcp is not None:
-        match_dict['vlan_pcp'] = vlan_pcp
+        match_dict["vlan_pcp"] = vlan_pcp
     return match_dict
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
-def flowmod(cookie, command, table_id, priority, out_port, out_group,
-            match_fields, inst, hard_timeout, idle_timeout, flags=0):
+@functools.lru_cache()
+def flowmod(
+    cookie,
+    command,
+    table_id,
+    priority,
+    out_port,
+    out_group,
+    match_fields,
+    inst,
+    hard_timeout,
+    idle_timeout,
+    flags=0,
+):
     return parser.OFPFlowMod(
         datapath=None,
         cookie=cookie,
         command=command,
         table_id=table_id,
         priority=priority,
         out_port=out_port,
         out_group=out_group,
         match=match_fields,
         instructions=inst,
         hard_timeout=hard_timeout,
         idle_timeout=idle_timeout,
-        flags=flags)
+        flags=flags,
+    )
 
 
 class NullRyuDatapath:
     """Placeholder Ryu Datapath."""
+
     ofproto = ofp
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache()
 def verify_flowmod(flowmod_msg):
     """Verify flowmod can be serialized."""
     flowmod_msg.datapath = NullRyuDatapath()
     # Must be non-zero.
     flowmod_msg.set_xid(1)
     flowmod_msg.serialize()
 
 
 def group_act(group_id):
     """Return an action to run a group."""
     return parser.OFPActionGroup(group_id)
 
 
-def bucket(weight=0, watch_port=ofp.OFPP_ANY,
-           watch_group=ofp.OFPG_ANY, actions=None):
+def bucket(weight=0, watch_port=ofp.OFPP_ANY, watch_group=ofp.OFPG_ANY, actions=None):
     """Return a group action bucket with provided actions."""
     return parser.OFPBucket(
-        weight=weight,
-        watch_port=watch_port,
-        watch_group=watch_group,
-        actions=actions)
+        weight=weight, watch_port=watch_port, watch_group=watch_group, actions=actions
+    )
 
 
 def build_group_flood_buckets(vlan_flood_acts):
     """Return a list of group buckets to implement flooding on a VLAN."""
     buckets = []
     non_outputs = []
     for act in vlan_flood_acts:
@@ -880,40 +974,33 @@
         else:
             non_outputs.append(act)
     return buckets
 
 
 def groupdel(datapath=None, group_id=ofp.OFPG_ALL):
     """Delete a group (default all groups)."""
-    return parser.OFPGroupMod(
-        datapath,
-        ofp.OFPGC_DELETE,
-        0,
-        group_id)
+    return parser.OFPGroupMod(datapath, ofp.OFPGC_DELETE, 0, group_id)
 
 
 def groupadd(datapath=None, type_=ofp.OFPGT_ALL, group_id=0, buckets=None):
     """Add a group."""
     return [
         groupdel(datapath=datapath, group_id=group_id),
-        parser.OFPGroupMod(datapath, ofp.OFPGC_ADD, type_, group_id, buckets)]
+        parser.OFPGroupMod(datapath, ofp.OFPGC_ADD, type_, group_id, buckets),
+    ]
 
 
 def groupadd_ff(datapath=None, group_id=0, buckets=None):
     """Add a fast failover group."""
     return groupadd(datapath, type_=ofp.OFPGT_FF, group_id=group_id, buckets=buckets)
 
 
 def meterdel(datapath=None, meter_id=ofp.OFPM_ALL):
     """Delete a meter (default all meters)."""
-    return parser.OFPMeterMod(
-        datapath,
-        ofp.OFPMC_DELETE,
-        0,
-        meter_id)
+    return parser.OFPMeterMod(datapath, ofp.OFPMC_DELETE, 0, meter_id)
 
 
 def meteradd(meter_conf, command=ofp.OFPMC_ADD):
     """Add a meter based on YAML configuration."""
 
     class NoopDP:
         """Fake DP to be able to use ofctl to parse meter config."""
@@ -942,93 +1029,110 @@
 def controller_pps_meteradd(datapath=None, pps=0):
     """Add a PPS meter towards controller."""
     return parser.OFPMeterMod(
         datapath=datapath,
         command=ofp.OFPMC_ADD,
         flags=ofp.OFPMF_PKTPS,
         meter_id=ofp.OFPM_CONTROLLER,
-        bands=[parser.OFPMeterBandDrop(rate=pps)])
+        bands=[parser.OFPMeterBandDrop(rate=pps)],
+    )
 
 
 def controller_pps_meterdel(datapath=None):
     """Delete a PPS meter towards controller."""
     return parser.OFPMeterMod(
         datapath=datapath,
         command=ofp.OFPMC_DELETE,
         flags=ofp.OFPMF_PKTPS,
-        meter_id=ofp.OFPM_CONTROLLER)
+        meter_id=ofp.OFPM_CONTROLLER,
+    )
 
 
 def slowpath_pps_meteradd(datapath=None, pps=0):
     """Add a PPS meter towards controller."""
     return parser.OFPMeterMod(
         datapath=datapath,
         command=ofp.OFPMC_ADD,
         flags=ofp.OFPMF_PKTPS,
         meter_id=ofp.OFPM_SLOWPATH,
-        bands=[parser.OFPMeterBandDrop(rate=pps)])
+        bands=[parser.OFPMeterBandDrop(rate=pps)],
+    )
 
 
 def slowpath_pps_meterdel(datapath=None):
     """Delete a PPS meter towards controller."""
     return parser.OFPMeterMod(
         datapath=datapath,
         command=ofp.OFPMC_DELETE,
         flags=ofp.OFPMF_PKTPS,
-        meter_id=ofp.OFPM_SLOWPATH)
+        meter_id=ofp.OFPM_SLOWPATH,
+    )
 
 
 def is_global_flowdel(ofmsg):
     """Is a delete of all flows in all tables."""
-    return is_flowdel(ofmsg) and ofmsg.table_id == ofp.OFPTT_ALL and not ofmsg.match.items()
+    return (
+        is_flowdel(ofmsg)
+        and ofmsg.table_id == ofp.OFPTT_ALL
+        and not ofmsg.match.items()
+    )
 
 
 def is_global_groupdel(ofmsg):
     """Is a delete of all groups."""
     return is_groupdel(ofmsg) and ofmsg.group_id == ofp.OFPG_ALL
 
 
 def is_global_meterdel(ofmsg):
     """Is a delete of all meters."""
     return is_meterdel(ofmsg) and ofmsg.meter_id == ofp.OFPM_ALL
 
 
 # We can tell right away what kind of OF messages these are.
 _MSG_KINDS_TYPES = {
-    parser.OFPPacketOut: 'packetout',
-    parser.OFPTableFeaturesStatsRequest: 'tfm',
-    parser.OFPSetConfig: 'config',
-    parser.OFPSetAsync: 'config',
-    parser.OFPDescStatsRequest: 'config',
+    parser.OFPPacketOut: "packetout",
+    parser.OFPTableFeaturesStatsRequest: "tfm",
+    parser.OFPSetConfig: "config",
+    parser.OFPSetAsync: "config",
+    parser.OFPDescStatsRequest: "config",
 }
 
 
 # We need to examine the OF message more closely to classify it.
 _MSG_KINDS = {
     parser.OFPFlowMod: (
-        ('deleteglobal', is_global_flowdel), ('delete', is_flowdel), ('flowaddmod', is_flowaddmod)),
+        ("deleteglobal", is_global_flowdel),
+        ("delete", is_flowdel),
+        ("flowaddmod", is_flowaddmod),
+    ),
     parser.OFPGroupMod: (
-        ('deleteglobal', is_global_groupdel), ('delete', is_groupdel), ('groupadd', is_groupadd)),
+        ("deleteglobal", is_global_groupdel),
+        ("delete", is_groupdel),
+        ("groupadd", is_groupadd),
+    ),
     parser.OFPMeterMod: (
-        ('deleteglobal', is_global_meterdel), ('delete', is_meterdel), ('meteradd', is_meteradd)),
+        ("deleteglobal", is_global_meterdel),
+        ("delete", is_meterdel),
+        ("meteradd", is_meteradd),
+    ),
 }
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache()
 def _msg_kind(ofmsg):
     ofmsg_type = type(ofmsg)
     ofmsg_kind = _MSG_KINDS_TYPES.get(ofmsg_type, None)
     if ofmsg_kind:
         return ofmsg_kind
     kinds = _MSG_KINDS.get(ofmsg_type, None)
     if kinds:
         for kind, kind_func in kinds:
             if kind_func(ofmsg):
                 return kind
-    return 'other'
+    return "other"
 
 
 def _partition_ofmsgs(input_ofmsgs):
     """Partition input ofmsgs by kind."""
     by_kind = {}
     for ofmsg in input_ofmsgs:
         by_kind.setdefault(_msg_kind(ofmsg), []).append(ofmsg)
@@ -1047,16 +1151,19 @@
 
 
 def sort_flows(input_ofmsgs):
     """Sort flows in canonical order, descending table and priority."""
     return sorted(
         input_ofmsgs,
         key=lambda ofmsg: (
-            getattr(ofmsg, 'table_id', ofp.OFPTT_ALL),
-            getattr(ofmsg, 'priority', 2**16 + 1)), reverse=True)
+            getattr(ofmsg, "table_id", ofp.OFPTT_ALL),
+            getattr(ofmsg, "priority", 2**16 + 1),
+        ),
+        reverse=True,
+    )
 
 
 def dedupe_ofmsgs(input_ofmsgs, random_order, flowkey):
     """Return deduplicated ofmsg list."""
     # Built in comparison doesn't work until serialized() called
     # Can't use dict or json comparison as may be nested
     deduped_input_ofmsgs = {flowkey(ofmsg): ofmsg for ofmsg in input_ofmsgs}
@@ -1067,33 +1174,39 @@
     return sort_flows(deduped_input_ofmsgs.values())
 
 
 def dedupe_overlaps_ofmsgs(input_ofmsgs, random_order, flowkey):
     deduped_ofmsgs = dedupe_ofmsgs(input_ofmsgs, random_order, flowkey)
     ofmsgs_by_table = {}
     for ofmsg in deduped_ofmsgs:
-        table_id = getattr(ofmsg, 'table_id', None)
+        table_id = getattr(ofmsg, "table_id", None)
         ofmsgs_by_table.setdefault(table_id, []).append(ofmsg)
-    all_table_ids = {table_id for table_id in ofmsgs_by_table if isinstance(table_id, int)}
+    all_table_ids = {
+        table_id for table_id in ofmsgs_by_table if isinstance(table_id, int)
+    }
 
     # If priority-less deletes across all tables are detected, then remove any
     # overlapping deletes (e.g. if a delete all tables vlan=100 is deleted, then remove
     # all other table-specific deletes that have vlan=100).
     if ofp.OFPTT_ALL in all_table_ids:
         overlap_matches = {
-            tuple(ofmsg.match.items()) for ofmsg in ofmsgs_by_table[ofp.OFPTT_ALL]
-            if not ofmsg.priority}
+            tuple(ofmsg.match.items())
+            for ofmsg in ofmsgs_by_table[ofp.OFPTT_ALL]
+            if not ofmsg.priority
+        }
         table_ids = all_table_ids - {ofp.OFPTT_ALL}
         if overlap_matches and table_ids:
             for table_id in table_ids:
                 for overlap_match in overlap_matches:
                     overlap_match = set(overlap_match)
                     ofmsgs_by_table[table_id] = [
-                        ofmsg for ofmsg in ofmsgs_by_table[table_id]
-                        if not overlap_match.issubset(set(ofmsg.match.items()))]
+                        ofmsg
+                        for ofmsg in ofmsgs_by_table[table_id]
+                        if not overlap_match.issubset(set(ofmsg.match.items()))
+                    ]
             nooverlaps_ofmsgs = []
             for _, ofmsgs in sorted(ofmsgs_by_table.items(), reverse=True):
                 nooverlaps_ofmsgs.extend(ofmsgs)
             return nooverlaps_ofmsgs
 
     return deduped_ofmsgs
 
@@ -1107,52 +1220,55 @@
             continue
         input_ofmsgs_without_overlaps.append(ofmsg)
     return input_ofmsgs_without_overlaps
 
 
 # kind, random_order, suggest_barrier, flowkey
 _OFMSG_ORDER = (
-    ('config', False, True, str, dedupe_ofmsgs),
-    ('deleteglobal', False, True, str, dedupe_ofmsgs),
-    ('delete', False, True, str, dedupe_overlaps_ofmsgs),
-    ('tfm', False, True, str, dedupe_ofmsgs),
-    ('groupadd', False, True, str, dedupe_ofmsgs),
-    ('meteradd', False, True, str, dedupe_ofmsgs),
-    ('flowaddmod', False, False, _flowmodkey, dedupe_ofmsgs),
-    ('other', False, False, str, dedupe_ofmsgs),
-    ('packetout', True, False, str, dedupe_ofmsgs),
+    ("config", False, True, str, dedupe_ofmsgs),
+    ("deleteglobal", False, True, str, dedupe_ofmsgs),
+    ("delete", False, True, str, dedupe_overlaps_ofmsgs),
+    ("tfm", False, True, str, dedupe_ofmsgs),
+    ("groupadd", False, True, str, dedupe_ofmsgs),
+    ("meteradd", False, True, str, dedupe_ofmsgs),
+    ("flowaddmod", False, False, _flowmodkey, dedupe_ofmsgs),
+    ("other", False, False, str, dedupe_ofmsgs),
+    ("packetout", True, False, str, dedupe_ofmsgs),
 )
 
 
 def valve_flowreorder(input_ofmsgs, use_barriers=True):
     """Reorder flows for better OFA performance."""
     # Move all deletes to be first, and add one barrier,
     # while optionally randomizing order. Platforms that do
     # parallel delete will perform better and platforms that
     # don't will have at most only one barrier to deal with.
     output_ofmsgs = []
     by_kind = _partition_ofmsgs(input_ofmsgs)
 
     # Suppress all other relevant deletes if a global delete is present.
-    delete_global_ofmsgs = by_kind.get('deleteglobal', [])
+    delete_global_ofmsgs = by_kind.get("deleteglobal", [])
     if delete_global_ofmsgs:
         global_types = {type(ofmsg) for ofmsg in delete_global_ofmsgs}
         new_delete = [
-            ofmsg for ofmsg in by_kind.get('delete', []) if type(ofmsg) not in global_types]
-        by_kind['delete'] = new_delete
+            ofmsg
+            for ofmsg in by_kind.get("delete", [])
+            if type(ofmsg) not in global_types
+        ]
+        by_kind["delete"] = new_delete
 
     for kind, random_order, _suggest_barrier, flowkey, dedupe_func in _OFMSG_ORDER:
         ofmsgs = dedupe_func(by_kind.get(kind, []), random_order, flowkey)
         if ofmsgs:
             by_kind[kind] = ofmsgs
 
-    deletes = by_kind.get('delete', None)
-    addmod = by_kind.get('flowaddmod', None)
+    deletes = by_kind.get("delete", None)
+    addmod = by_kind.get("flowaddmod", None)
     if deletes and addmod:
-        by_kind['delete'] = remove_overlap_ofmsgs(deletes, addmod)
+        by_kind["delete"] = remove_overlap_ofmsgs(deletes, addmod)
 
     for kind, _random_order, suggest_barrier, _flowkey, dedupe_func in _OFMSG_ORDER:
         ofmsgs = by_kind.get(kind, [])
         if ofmsgs:
             output_ofmsgs.extend(ofmsgs)
             if use_barriers and suggest_barrier:
                 output_ofmsgs.append(barrier())
@@ -1160,15 +1276,17 @@
 
 
 def flood_tagged_port_outputs(ports, in_port=None, exclude_ports=None):
     """Return list of actions necessary to flood to list of tagged ports."""
     flood_acts = []
     in_port_mirror_output_ports = {}
     if in_port is not None:
-        in_port_mirror_output_ports = ports_from_output_port_acts(in_port.mirror_actions())
+        in_port_mirror_output_ports = ports_from_output_port_acts(
+            in_port.mirror_actions()
+        )
     if ports:
         for port in ports:
             if in_port is not None and port == in_port:
                 if in_port.hairpin:
                     flood_acts.append(output_in_port())
                 continue
             if exclude_ports and port in exclude_ports:
@@ -1182,48 +1300,52 @@
                 flood_acts.extend(mirror_actions)
     return dedupe_output_port_acts(flood_acts)
 
 
 def flood_untagged_port_outputs(ports, in_port=None, exclude_ports=None):
     """Return list of actions necessary to flood to list of untagged ports."""
     flood_acts = flood_tagged_port_outputs(
-        ports, in_port=in_port, exclude_ports=exclude_ports)
+        ports, in_port=in_port, exclude_ports=exclude_ports
+    )
     if flood_acts:
         flood_acts = [pop_vlan()] + flood_acts
     return flood_acts
 
 
 def flood_port_outputs(tagged_ports, untagged_ports, in_port=None, exclude_ports=None):
     """Return actions for both tagged and untagged ports."""
-    return (
-        flood_tagged_port_outputs(tagged_ports, in_port, exclude_ports)
-        + flood_untagged_port_outputs(untagged_ports, in_port, exclude_ports))
+    return flood_tagged_port_outputs(
+        tagged_ports, in_port, exclude_ports
+    ) + flood_untagged_port_outputs(untagged_ports, in_port, exclude_ports)
 
 
 def faucet_config(datapath=None):
     """Return switch config for FAUCET."""
     return parser.OFPSetConfig(datapath, ofp.OFPC_FRAG_NORMAL, 0)
 
 
-def faucet_async(datapath=None, notify_flow_removed=False, packet_in=True, port_status=True):
+def faucet_async(
+    datapath=None, notify_flow_removed=False, packet_in=True, port_status=True
+):
     """Return async message config for FAUCET/Gauge"""
     packet_in_mask = 0
     if packet_in:
         packet_in_mask = 1 << ofp.OFPR_ACTION
     port_status_mask = 0
     if port_status:
         port_status_mask = (
-            1 << ofp.OFPPR_ADD | 1 << ofp.OFPPR_DELETE | 1 << ofp.OFPPR_MODIFY)
+            1 << ofp.OFPPR_ADD | 1 << ofp.OFPPR_DELETE | 1 << ofp.OFPPR_MODIFY
+        )
     flow_removed_mask = 0
     if notify_flow_removed:
-        flow_removed_mask = (
-            1 << ofp.OFPRR_IDLE_TIMEOUT | 1 << ofp.OFPRR_HARD_TIMEOUT)
+        flow_removed_mask = 1 << ofp.OFPRR_IDLE_TIMEOUT | 1 << ofp.OFPRR_HARD_TIMEOUT
     return parser.OFPSetAsync(
         datapath,
         [packet_in_mask, packet_in_mask],
         [port_status_mask, port_status_mask],
-        [flow_removed_mask, flow_removed_mask])
+        [flow_removed_mask, flow_removed_mask],
+    )
 
 
 def desc_stats_request(datapath=None):
     """Query switch description."""
     return parser.OFPDescStatsRequest(datapath, 0)
```

### Comparing `c65faucet-1.0.46/faucet/valve_of_old.py` & `c65faucet-1.0.47/faucet/valve_of_old.py`

 * *Files 22% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 # Map of old field name, to new.
 OLD_MATCH_FIELDS = {
-    'dl_dst': 'eth_dst',
-    'dl_src': 'eth_src',
-    'dl_type': 'eth_type',
-    'dl_vlan': 'vlan_vid',
-    'nw_proto': 'ip_proto',
-    'nw_src': 'ipv4_src',
-    'nw_dst': 'ipv4_dst',
+    "dl_dst": "eth_dst",
+    "dl_src": "eth_src",
+    "dl_type": "eth_type",
+    "dl_vlan": "vlan_vid",
+    "nw_proto": "ip_proto",
+    "nw_src": "ipv4_src",
+    "nw_dst": "ipv4_dst",
 }
```

### Comparing `c65faucet-1.0.46/faucet/valve_outonly.py` & `c65faucet-1.0.47/faucet/valve_outonly.py`

 * *Files 6% similar despite different names*

```diff
@@ -26,19 +26,25 @@
     def __init__(self, vlan_table, highest_priority):
         self.vlan_table = vlan_table
         self.highest_priority = highest_priority
 
     def add_port(self, port):
         ofmsgs = []
         if port.output_only:
-            ofmsgs.append(self.vlan_table.flowdrop(
-                match=self.vlan_table.match(in_port=port.number),
-                priority=self.highest_priority))
+            ofmsgs.append(
+                self.vlan_table.flowdrop(
+                    match=self.vlan_table.match(in_port=port.number),
+                    priority=self.highest_priority,
+                )
+            )
         return ofmsgs
 
     def del_port(self, port):
         ofmsgs = []
         if port.output_only:
-            ofmsgs.append(self.vlan_table.flowdel(
-                match=self.vlan_table.match(in_port=port.number),
-                priority=self.highest_priority))
+            ofmsgs.append(
+                self.vlan_table.flowdel(
+                    match=self.vlan_table.match(in_port=port.number),
+                    priority=self.highest_priority,
+                )
+            )
         return ofmsgs
```

### Comparing `c65faucet-1.0.46/faucet/valve_packet.py` & `c65faucet-1.0.47/faucet/valve_packet.py`

 * *Files 3% similar despite different names*

```diff
@@ -22,110 +22,127 @@
 import socket
 import struct
 from netaddr import EUI
 
 from os_ken.lib import addrconv
 from os_ken.lib.mac import BROADCAST, DONTCARE, is_multicast, haddr_to_bin
 from os_ken.lib.packet import (
-    arp, bpdu, ethernet,
-    icmp, icmpv6, ipv4, ipv6,
-    lldp, slow, packet, vlan)
+    arp,
+    bpdu,
+    ethernet,
+    icmp,
+    icmpv6,
+    ipv4,
+    ipv6,
+    lldp,
+    slow,
+    packet,
+    vlan,
+)
 from os_ken.lib.packet.stream_parser import StreamParser
 
 from faucet import valve_util
 from faucet import valve_of
-from faucet.valve_util import LRU_MAX
 
-FAUCET_MAC = '0e:00:00:00:00:01'  # Default FAUCET MAC address
+FAUCET_MAC = "0e:00:00:00:00:01"  # Default FAUCET MAC address
 
 ETH_HEADER_SIZE = 14
-ETH_VLAN_HEADER_SIZE = ETH_HEADER_SIZE + 4  # https://en.wikipedia.org/wiki/IEEE_802.1Q#Frame_format
+ETH_VLAN_HEADER_SIZE = (
+    ETH_HEADER_SIZE + 4
+)  # https://en.wikipedia.org/wiki/IEEE_802.1Q#Frame_format
 IPV4_HEADER_SIZE = 20  # https://en.wikipedia.org/wiki/IPv4#Header
-ICMP_ECHO_REQ_SIZE = 8 + 128  # https://en.wikipedia.org/wiki/Ping_(networking_utility)#ICMP_packet
-ICMP6_ECHO_REQ_SIZE = 8 + 128  # https://en.wikipedia.org/wiki/Ping_(networking_utility)#ICMP_packet
+ICMP_ECHO_REQ_SIZE = (
+    8 + 128
+)  # https://en.wikipedia.org/wiki/Ping_(networking_utility)#ICMP_packet
+ICMP6_ECHO_REQ_SIZE = (
+    8 + 128
+)  # https://en.wikipedia.org/wiki/Ping_(networking_utility)#ICMP_packet
 IPV6_HEADER_SIZE = 40  # https://en.wikipedia.org/wiki/IPv6_packet#Fixed_header
 ARP_REQ_PKT_SIZE = 28
-ARP_PKT_SIZE = 46  # https://en.wikipedia.org/wiki/Address_Resolution_Protocol#Packet_structure
+ARP_PKT_SIZE = (
+    46  # https://en.wikipedia.org/wiki/Address_Resolution_Protocol#Packet_structure
+)
 VLAN_ARP_REQ_PKT_SIZE = ETH_VLAN_HEADER_SIZE + ARP_REQ_PKT_SIZE
 VLAN_ARP_PKT_SIZE = ETH_VLAN_HEADER_SIZE + ARP_PKT_SIZE
 VLAN_ICMP_ECHO_REQ_SIZE = ETH_VLAN_HEADER_SIZE + IPV4_HEADER_SIZE + ICMP_ECHO_REQ_SIZE
 VLAN_ICMP6_ECHO_REQ_SIZE = ETH_VLAN_HEADER_SIZE + IPV6_HEADER_SIZE + ICMP6_ECHO_REQ_SIZE
 
-ETH_EAPOL = 0x888e
-EAPOL_ETH_DST = '01:80:c2:00:00:03'
+ETH_EAPOL = 0x888E
+EAPOL_ETH_DST = "01:80:c2:00:00:03"
 SLOW_PROTOCOL_MULTICAST = slow.SLOW_PROTOCOL_MULTICAST
 BRIDGE_GROUP_ADDRESS = bpdu.BRIDGE_GROUP_ADDRESS
-BRIDGE_GROUP_MASK = 'ff:ff:ff:ff:ff:f0'
+BRIDGE_GROUP_MASK = "ff:ff:ff:ff:ff:f0"
 LLDP_MAC_NEAREST_BRIDGE = lldp.LLDP_MAC_NEAREST_BRIDGE
-CISCO_CDP_VTP_UDLD_ADDRESS = '01:00:0c:cc:cc:cc'
-CISCO_SPANNING_GROUP_ADDRESS = '01:00:0c:cc:cc:cd'
-IPV6_ALL_NODES_MCAST = '33:33:00:00:00:01'
-IPV6_ALL_ROUTERS_MCAST = '33:33:00:00:00:02'
-IPV6_ALL_NODES = ipaddress.IPv6Address('ff02::1')
+CISCO_CDP_VTP_UDLD_ADDRESS = "01:00:0c:cc:cc:cc"
+CISCO_SPANNING_GROUP_ADDRESS = "01:00:0c:cc:cc:cd"
+IPV6_ALL_NODES_MCAST = "33:33:00:00:00:01"
+IPV6_ALL_ROUTERS_MCAST = "33:33:00:00:00:02"
+IPV6_ALL_NODES = ipaddress.IPv6Address("ff02::1")
 IPV6_MAX_HOP_LIM = 255
 IPV6_RA_HOP_LIM = 64
 
 LLDP_FAUCET_DP_ID = 1
 LLDP_FAUCET_STACK_STATE = 2
 
 LACP_SIZE = 124
 
 EUI_BITS = len(EUI(0).packed * 8)
-MAC_MASK_BITMAP = {(2**EUI_BITS - 2**i): (EUI_BITS - i) for i in range(0, EUI_BITS + 1)}
+MAC_MASK_BITMAP = {
+    (2**EUI_BITS - 2**i): (EUI_BITS - i) for i in range(0, EUI_BITS + 1)
+}
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def mac_mask_bits(mac_mask):
     """Return number of bits in MAC mask or 0."""
     if mac_mask is not None:
         return MAC_MASK_BITMAP.get(EUI(mac_mask).value, 0)
     return 0
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def int_from_mac(mac):
-    int_hi, int_lo = [int(i, 16) for i in mac.split(':')[-2:]]
+    int_hi, int_lo = [int(i, 16) for i in mac.split(":")[-2:]]
     return (int_hi << 8) + int_lo
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def int_in_mac(mac, to_int):
-    int_mac = mac.split(':')[:4] + [
-        '%x' % (to_int >> 8), '%x' % (to_int & 0xff)]
-    return ':'.join(int_mac)
+    int_mac = mac.split(":")[:4] + ["%x" % (to_int >> 8), "%x" % (to_int & 0xFF)]
+    return ":".join(int_mac)
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def ipv4_parseable(ip_header_data):
     """Return True if an IPv4 packet we could parse."""
     # TODO: python library parsers are fragile
     # Perform sanity checking on the header to limit exposure of the parser
-    ipv4_header = struct.unpack('!BBHHHBBH4s4s', ip_header_data[:IPV4_HEADER_SIZE])
-    header_size = (ipv4_header[0] & 0xf) * 32 / 8
+    ipv4_header = struct.unpack("!BBHHHBBH4s4s", ip_header_data[:IPV4_HEADER_SIZE])
+    header_size = (ipv4_header[0] & 0xF) * 32 / 8
     if header_size < IPV4_HEADER_SIZE:
         return False
     flags = ipv4_header[4] >> 12
     # MF bit set
     if flags & 0x2:
         return False
     # fragment - discard
-    ip_off = ipv4_header[4] & 0xfff
+    ip_off = ipv4_header[4] & 0xFFF
     if ip_off:
         return False
     # not a protocol conservatively known to parse
     protocol = ipv4_header[6]
     if protocol not in (socket.IPPROTO_ICMP, socket.IPPROTO_UDP, socket.IPPROTO_TCP):
         return False
     return True
 
 
 def mac_byte_mask(mask_bytes=0):
     """Return a MAC address mask with n bytes masked out."""
     assert mask_bytes <= 6
-    return ':'.join(['ff'] * mask_bytes + (['00'] * (6 - mask_bytes)))
+    return ":".join(["ff"] * mask_bytes + (["00"] * (6 - mask_bytes)))
 
 
 def parse_eth_pkt(pkt):
     """Return parsed Ethernet packet.
 
     Args:
         pkt (ryu.lib.packet.packet): packet received from dataplane.
@@ -153,15 +170,15 @@
         pkt (ryu.lib.packet.packet): packet received from dataplane.
     Returns:
         ryu.lib.packet.lldp: LLDP packet.
     """
     return pkt.get_protocol(lldp.lldp)
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def parse_packet_in_pkt(data, max_len, eth_pkt=None, vlan_pkt=None):
     """Parse a packet received via packet in from the dataplane.
 
     Args:
         data (bytearray): packet data from dataplane.
         max_len (int): max number of packet data bytes to parse.
     Returns:
@@ -194,28 +211,28 @@
             pkt = packet.Packet(data)
     except (AttributeError, AssertionError, StreamParser.TooSmallException):
         pass
 
     return (pkt, eth_pkt, eth_type, vlan_pkt, vlan_vid)
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def mac_addr_all_zeros(mac_addr):
     """Returns True if mac_addr is all zeros.
 
     Args:
         mac_addr (str): MAC address.
     Returns:
         bool: True if all zeros.
     """
     mac_bin = haddr_to_bin(mac_addr)
     return mac_bin == DONTCARE
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def mac_addr_is_unicast(mac_addr):
     """Returns True if mac_addr is a unicast Ethernet address.
 
     Args:
         mac_addr (str): MAC address.
     Returns:
         bool: True if a unicast Ethernet address.
@@ -235,64 +252,65 @@
         eth_dst (str): destination Ethernet MAC address.
         dl_type (int): EtherType.
     Returns:
         ryu.lib.packet.ethernet: Ethernet packet with header.
     """
     pkt_header = packet.Packet()
     if vid is None:
-        eth_header = ethernet.ethernet(
-            eth_dst, eth_src, dl_type)
+        eth_header = ethernet.ethernet(eth_dst, eth_src, dl_type)
         pkt_header.add_protocol(eth_header)
     else:
-        eth_header = ethernet.ethernet(
-            eth_dst, eth_src, valve_of.ether.ETH_TYPE_8021Q)
+        eth_header = ethernet.ethernet(eth_dst, eth_src, valve_of.ether.ETH_TYPE_8021Q)
         pkt_header.add_protocol(eth_header)
         vlan_header = vlan.vlan(vid=vid, ethertype=dl_type)
         pkt_header.add_protocol(vlan_header)
     return pkt_header
 
 
-def lldp_beacon(eth_src, chassis_id, port_id, ttl, org_tlvs=None,
-                system_name=None, port_descr=None):
+def lldp_beacon(
+    eth_src, chassis_id, port_id, ttl, org_tlvs=None, system_name=None, port_descr=None
+):
     """Return an LLDP frame suitable for a host/access port.
 
     Args:
         eth_src (str): source Ethernet MAC address.
         chassis_id (str): Chassis ID.
         port_id (int): port ID,
         TTL (int): TTL for payload.
         org_tlvs (list): list of tuples of (OUI, subtype, info).
     Returns:
         ryu.lib.packet.ethernet: Ethernet packet with header.
     """
     pkt = build_pkt_header(
-        None, eth_src, lldp.LLDP_MAC_NEAREST_BRIDGE, valve_of.ether.ETH_TYPE_LLDP)
+        None, eth_src, lldp.LLDP_MAC_NEAREST_BRIDGE, valve_of.ether.ETH_TYPE_LLDP
+    )
     tlvs = [
         lldp.ChassisID(
             subtype=lldp.ChassisID.SUB_MAC_ADDRESS,
-            chassis_id=addrconv.mac.text_to_bin(chassis_id)),
+            chassis_id=addrconv.mac.text_to_bin(chassis_id),
+        ),
         lldp.PortID(
-            subtype=lldp.PortID.SUB_INTERFACE_NAME,
-            port_id=str(port_id).encode('utf-8')),
-        lldp.TTL(
-            ttl=ttl)
+            subtype=lldp.PortID.SUB_INTERFACE_NAME, port_id=str(port_id).encode("utf-8")
+        ),
+        lldp.TTL(ttl=ttl),
     ]
     for tlv, info_name, info in (
-            (lldp.SystemName, 'system_name', system_name),
-            (lldp.PortDescription, 'port_description', port_descr)):
+        (lldp.SystemName, "system_name", system_name),
+        (lldp.PortDescription, "port_description", port_descr),
+    ):
         if info is not None:
-            info_args = {info_name: info.encode('UTF-8')}
+            info_args = {info_name: info.encode("UTF-8")}
             tlvs.append(tlv(**info_args))
     if org_tlvs is not None:
         for tlv_oui, tlv_subtype, tlv_info in org_tlvs:
             tlvs.append(
                 lldp.OrganizationallySpecific(
-                    oui=tlv_oui,
-                    subtype=tlv_subtype,
-                    info=tlv_info))
+                    oui=tlv_oui, subtype=tlv_subtype, info=tlv_info
+                )
+            )
     tlvs.append(lldp.End())
     lldp_pkt = lldp.lldp(tlvs)
     pkt.add_protocol(lldp_pkt)
     pkt.serialize()
     return pkt
 
 
@@ -301,28 +319,31 @@
     return addrconv.mac.text_to_bin(mac)[:3]
 
 
 def faucet_lldp_tlvs(dp):
     """Return LLDP TLVs for a datapath."""
     tlvs = []
     tlvs.append(
-        (faucet_oui(dp.faucet_dp_mac), LLDP_FAUCET_DP_ID, str(dp.dp_id).encode('utf-8')))
+        (faucet_oui(dp.faucet_dp_mac), LLDP_FAUCET_DP_ID, str(dp.dp_id).encode("utf-8"))
+    )
     return tlvs
 
 
 def faucet_lldp_stack_state_tlvs(dp, port):
     """Return a LLDP TLV for state of a stack port."""
     tlvs = []
     if not port.stack:
         return []
     tlvs.append(
         (
             faucet_oui(dp.faucet_dp_mac),
             LLDP_FAUCET_STACK_STATE,
-            str(port.dyn_stack_current_state).encode('utf-8')))
+            str(port.dyn_stack_current_state).encode("utf-8"),
+        )
+    )
     return tlvs
 
 
 def tlvs_by_type(tlvs, tlv_type):
     """Return list of TLVs with matching type."""
     return [tlv for tlv in tlvs if tlv.tlv_type == tlv_type]
 
@@ -341,17 +362,19 @@
         except (AttributeError, ValueError, TypeError):
             pass
     return tlv_val
 
 
 def faucet_tlvs(lldp_pkt, faucet_dp_mac):
     """Return list of TLVs with FAUCET OUI."""
-    return [tlv for tlv in tlvs_by_type(
-        lldp_pkt.tlvs, lldp.LLDP_TLV_ORGANIZATIONALLY_SPECIFIC)
-        if tlv.oui == faucet_oui(faucet_dp_mac)]
+    return [
+        tlv
+        for tlv in tlvs_by_type(lldp_pkt.tlvs, lldp.LLDP_TLV_ORGANIZATIONALLY_SPECIFIC)
+        if tlv.oui == faucet_oui(faucet_dp_mac)
+    ]
 
 
 def parse_faucet_lldp(lldp_pkt, faucet_dp_mac):
     """Parse and return FAUCET TLVs from LLDP packet."""
     remote_dp_id = None
     remote_dp_name = None
     remote_port_id = None
@@ -359,51 +382,57 @@
 
     tlvs = faucet_tlvs(lldp_pkt, faucet_dp_mac)
     if tlvs:
         dp_id_tlvs = tlvs_by_subtype(tlvs, LLDP_FAUCET_DP_ID)
         dp_name_tlvs = tlvs_by_type(lldp_pkt.tlvs, lldp.LLDP_TLV_SYSTEM_NAME)
         port_id_tlvs = tlvs_by_type(lldp_pkt.tlvs, lldp.LLDP_TLV_PORT_ID)
         port_state_tlvs = tlvs_by_subtype(tlvs, LLDP_FAUCET_STACK_STATE)
-        remote_dp_id = tlv_cast(dp_id_tlvs, 'info', int)
-        remote_port_id = tlv_cast(port_id_tlvs, 'port_id', int)
-        remote_port_state = tlv_cast(port_state_tlvs, 'info', int)
-        remote_dp_name = tlv_cast(dp_name_tlvs, 'system_name', valve_util.utf8_decode)
+        remote_dp_id = tlv_cast(dp_id_tlvs, "info", int)
+        remote_port_id = tlv_cast(port_id_tlvs, "port_id", int)
+        remote_port_state = tlv_cast(port_state_tlvs, "info", int)
+        remote_dp_name = tlv_cast(dp_name_tlvs, "system_name", valve_util.utf8_decode)
     return (remote_dp_id, remote_dp_name, remote_port_id, remote_port_state)
 
 
 def lacp_actor_up(lacp_pkt):
     """Return 1 if remote LACP link is up."""
-    if (lacp_pkt.actor_state_synchronization
-            and lacp_pkt.actor_state_collecting
-            and lacp_pkt.actor_state_distributing):
+    if (
+        lacp_pkt.actor_state_synchronization
+        and lacp_pkt.actor_state_collecting
+        and lacp_pkt.actor_state_distributing
+    ):
         return 1
     return 0
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
-def lacp_reqreply(eth_src,
-                  actor_system, actor_key, actor_port,
-                  actor_port_priority=0,
-                  actor_state_synchronization=0,
-                  actor_state_activity=0,
-                  actor_state_collecting=1,
-                  actor_state_distributing=1,
-                  partner_system='00:00:00:00:00:00',
-                  partner_key=0,
-                  partner_port=0,
-                  partner_system_priority=0,
-                  partner_port_priority=0,
-                  partner_state_defaulted=0,
-                  partner_state_expired=0,
-                  partner_state_timeout=0,
-                  partner_state_collecting=0,
-                  partner_state_distributing=0,
-                  partner_state_aggregation=0,
-                  partner_state_synchronization=0,
-                  partner_state_activity=0):
+@functools.lru_cache(maxsize=1024)
+def lacp_reqreply(
+    eth_src,
+    actor_system,
+    actor_key,
+    actor_port,
+    actor_port_priority=0,
+    actor_state_synchronization=0,
+    actor_state_activity=0,
+    actor_state_collecting=1,
+    actor_state_distributing=1,
+    partner_system="00:00:00:00:00:00",
+    partner_key=0,
+    partner_port=0,
+    partner_system_priority=0,
+    partner_port_priority=0,
+    partner_state_defaulted=0,
+    partner_state_expired=0,
+    partner_state_timeout=0,
+    partner_state_collecting=0,
+    partner_state_distributing=0,
+    partner_state_aggregation=0,
+    partner_state_synchronization=0,
+    partner_state_activity=0,
+):
     """Return a LACP frame.
 
     Args:
         eth_src (str): source Ethernet MAC address.
         actor_system (str): actor system ID (MAC address)
         actor_key (int): actor's LACP key assigned to this port.
         actor_port (int): actor port number.
@@ -424,15 +453,16 @@
         partner_state_aggregation (int): 1 if partner can aggregate this link.
         partner_state_synchronization (int): 1 if partner will use this link.
         partner_state_activity (int): 1 if partner actively sends LACP.
     Returns:
         ryu.lib.packet.ethernet: Ethernet packet with header.
     """
     pkt = build_pkt_header(
-        None, eth_src, slow.SLOW_PROTOCOL_MULTICAST, valve_of.ether.ETH_TYPE_SLOW)
+        None, eth_src, slow.SLOW_PROTOCOL_MULTICAST, valve_of.ether.ETH_TYPE_SLOW
+    )
     lacp_pkt = slow.lacp(
         version=1,
         actor_system=actor_system,
         actor_port=actor_port,
         partner_system=partner_system,
         partner_port=partner_port,
         actor_key=actor_key,
@@ -452,60 +482,68 @@
         actor_state_distributing=actor_state_distributing,
         partner_state_distributing=partner_state_distributing,
         actor_state_aggregation=1,
         partner_state_aggregation=partner_state_aggregation,
         actor_state_synchronization=actor_state_synchronization,
         partner_state_synchronization=partner_state_synchronization,
         actor_state_activity=actor_state_activity,
-        partner_state_activity=partner_state_activity)
+        partner_state_activity=partner_state_activity,
+    )
     pkt.add_protocol(lacp_pkt)
     pkt.serialize()
     return pkt
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def arp_request(vid, eth_src, eth_dst, src_ip, dst_ip):
     """Return an ARP request packet.
 
     Args:
         vid (int or None): VLAN VID to use (or None).
         eth_src (str): Ethernet source address.
         eth_dst (str): Ethernet destination address.
         src_ip (ipaddress.IPv4Address): source IPv4 address.
         dst_ip (ipaddress.IPv4Address): requested IPv4 address.
     Returns:
         ryu.lib.packet.arp: serialized ARP request packet.
     """
-    pkt = build_pkt_header(
-        vid, eth_src, eth_dst, valve_of.ether.ETH_TYPE_ARP)
+    pkt = build_pkt_header(vid, eth_src, eth_dst, valve_of.ether.ETH_TYPE_ARP)
     arp_pkt = arp.arp(
-        opcode=arp.ARP_REQUEST, src_mac=eth_src,
-        src_ip=str(src_ip), dst_mac=valve_of.mac.DONTCARE_STR, dst_ip=str(dst_ip))
+        opcode=arp.ARP_REQUEST,
+        src_mac=eth_src,
+        src_ip=str(src_ip),
+        dst_mac=valve_of.mac.DONTCARE_STR,
+        dst_ip=str(dst_ip),
+    )
     pkt.add_protocol(arp_pkt)
     pkt.serialize()
     return pkt
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def arp_reply(vid, eth_src, eth_dst, src_ip, dst_ip):
     """Return an ARP reply packet.
 
     Args:
         vid (int or None): VLAN VID to use (or None).
         eth_src (str): Ethernet source address.
         eth_dst (str): destination Ethernet MAC address.
         src_ip (ipaddress.IPv4Address): source IPv4 address.
         dst_ip (ipaddress.IPv4Address): destination IPv4 address.
     Returns:
         ryu.lib.packet.arp: serialized ARP reply packet.
     """
     pkt = build_pkt_header(vid, eth_src, eth_dst, valve_of.ether.ETH_TYPE_ARP)
     arp_pkt = arp.arp(
-        opcode=arp.ARP_REPLY, src_mac=eth_src,
-        src_ip=src_ip, dst_mac=eth_dst, dst_ip=dst_ip)
+        opcode=arp.ARP_REPLY,
+        src_mac=eth_src,
+        src_ip=src_ip,
+        dst_mac=eth_dst,
+        dst_ip=dst_ip,
+    )
     pkt.add_protocol(arp_pkt)
     pkt.serialize()
     return pkt
 
 
 def echo_reply(vid, eth_src, eth_dst, src_ip, dst_ip, data):
     """Return an ICMP echo reply packet.
@@ -516,59 +554,58 @@
         eth_dst (str): destination Ethernet MAC address.
         src_ip (ipaddress.IPv4Address): source IPv4 address.
         dst_ip (ipaddress.IPv4Address): destination IPv4 address.
     Returns:
         ryu.lib.packet.icmp: serialized ICMP echo reply packet.
     """
     pkt = build_pkt_header(vid, eth_src, eth_dst, valve_of.ether.ETH_TYPE_IP)
-    ipv4_pkt = ipv4.ipv4(
-        dst=dst_ip, src=src_ip, proto=valve_of.inet.IPPROTO_ICMP)
+    ipv4_pkt = ipv4.ipv4(dst=dst_ip, src=src_ip, proto=valve_of.inet.IPPROTO_ICMP)
     pkt.add_protocol(ipv4_pkt)
     icmp_pkt = icmp.icmp(
-        type_=icmp.ICMP_ECHO_REPLY, code=icmp.ICMP_ECHO_REPLY_CODE,
-        data=data)
+        type_=icmp.ICMP_ECHO_REPLY, code=icmp.ICMP_ECHO_REPLY_CODE, data=data
+    )
     pkt.add_protocol(icmp_pkt)
     pkt.serialize()
     return pkt
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def ipv6_link_eth_mcast(dst_ip):
     """Return an Ethernet multicast address from an IPv6 address.
 
     See RFC 2464 section 7.
 
     Args:
         dst_ip (ipaddress.IPv6Address): IPv6 address.
     Returns:
         str: Ethernet multicast address.
     """
-    mcast_mac_bytes = b'\x33\x33\xff' + dst_ip.packed[-3:]
-    mcast_mac = ':'.join(['%02X' % x for x in mcast_mac_bytes])
+    mcast_mac_bytes = b"\x33\x33\xff" + dst_ip.packed[-3:]
+    mcast_mac = ":".join(["%02X" % x for x in mcast_mac_bytes])
     return mcast_mac
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def ipv6_solicited_node_from_ucast(ucast):
     """Return IPv6 solicited node multicast address from IPv6 unicast address.
 
     See RFC 3513 section 2.7.1.
 
     Args:
        ucast (ipaddress.IPv6Address): IPv6 unicast address.
     Returns:
        ipaddress.IPv6Address: IPv6 solicited node multicast address.
     """
-    link_mcast_prefix = ipaddress.ip_interface('ff02::1:ff00:0/104')
+    link_mcast_prefix = ipaddress.ip_interface("ff02::1:ff00:0/104")
     mcast_bytes = link_mcast_prefix.packed[:13] + ucast.packed[-3:]
     link_mcast = ipaddress.IPv6Address(mcast_bytes)
     return link_mcast
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def nd_request(vid, eth_src, eth_dst, src_ip, dst_ip):
     """Return IPv6 neighbor discovery request packet.
 
     Args:
         vid (int or None): VLAN VID to use (or None).
         eth_src (str): source Ethernet MAC address.
         eth_dst (str): Ethernet destination address.
@@ -580,153 +617,148 @@
     if mac_addr_is_unicast(eth_dst):
         nd_mac = eth_dst
         nd_ip = dst_ip
     else:
         nd_mac = ipv6_link_eth_mcast(dst_ip)
         nd_ip = ipv6_solicited_node_from_ucast(dst_ip)
     pkt = build_pkt_header(vid, eth_src, nd_mac, valve_of.ether.ETH_TYPE_IPV6)
-    ipv6_pkt = ipv6.ipv6(
-        src=str(src_ip), dst=nd_ip, nxt=valve_of.inet.IPPROTO_ICMPV6)
+    ipv6_pkt = ipv6.ipv6(src=str(src_ip), dst=nd_ip, nxt=valve_of.inet.IPPROTO_ICMPV6)
     pkt.add_protocol(ipv6_pkt)
     icmpv6_pkt = icmpv6.icmpv6(
         type_=icmpv6.ND_NEIGHBOR_SOLICIT,
         data=icmpv6.nd_neighbor(
-            dst=dst_ip,
-            option=icmpv6.nd_option_sla(hw_src=eth_src)))
+            dst=dst_ip, option=icmpv6.nd_option_sla(hw_src=eth_src)
+        ),
+    )
     pkt.add_protocol(icmpv6_pkt)
     pkt.serialize()
     return pkt
 
 
-@functools.lru_cache(maxsize=LRU_MAX)
+@functools.lru_cache(maxsize=1024)
 def nd_advert(vid, eth_src, eth_dst, src_ip, dst_ip):
     """Return IPv6 neighbor avertisement packet.
 
     Args:
         vid (int or None): VLAN VID to use (or None).
         eth_src (str): source Ethernet MAC address.
         eth_dst (str): destination Ethernet MAC address.
         src_ip (ipaddress.IPv6Address): source IPv6 address.
         dst_ip (ipaddress.IPv6Address): destination IPv6 address.
     Returns:
         ryu.lib.packet.ethernet: Serialized IPv6 neighbor discovery packet.
     """
-    pkt = build_pkt_header(
-        vid, eth_src, eth_dst, valve_of.ether.ETH_TYPE_IPV6)
+    pkt = build_pkt_header(vid, eth_src, eth_dst, valve_of.ether.ETH_TYPE_IPV6)
     ipv6_icmp6 = ipv6.ipv6(
         src=src_ip,
         dst=dst_ip,
         nxt=valve_of.inet.IPPROTO_ICMPV6,
-        hop_limit=IPV6_MAX_HOP_LIM)
+        hop_limit=IPV6_MAX_HOP_LIM,
+    )
     pkt.add_protocol(ipv6_icmp6)
     icmpv6_nd_advert = icmpv6.icmpv6(
         type_=icmpv6.ND_NEIGHBOR_ADVERT,
         data=icmpv6.nd_neighbor(
-            dst=src_ip,
-            option=icmpv6.nd_option_tla(hw_src=eth_src), res=7))
+            dst=src_ip, option=icmpv6.nd_option_tla(hw_src=eth_src), res=7
+        ),
+    )
     pkt.add_protocol(icmpv6_nd_advert)
     pkt.serialize()
     return pkt
 
 
-def icmpv6_echo_reply(vid, eth_src, eth_dst, src_ip, dst_ip, hop_limit,
-                      id_, seq, data):
+def icmpv6_echo_reply(vid, eth_src, eth_dst, src_ip, dst_ip, hop_limit, id_, seq, data):
     r"""Return IPv6 ICMP echo reply packet.
 
-        Args:
-            vid (int or None): VLAN VID to use (or None).
-            eth_src (str): source Ethernet MAC address.
-            eth_dst (str): destination Ethernet MAC address.
-            src_ip (ipaddress.IPv6Address): source IPv6 address.
-            dst_ip (ipaddress.IPv6Address): destination IPv6 address.
-            hop_limit (int): IPv6 hop limit.
-            id_ (int): identifier for echo reply.
-            seq (int): sequence number for echo reply.
-            data (str): payload for echo reply.
-        Returns:
-            ryu.lib.packet.ethernet: Serialized IPv6 ICMP echo reply packet.
+    Args:
+        vid (int or None): VLAN VID to use (or None).
+        eth_src (str): source Ethernet MAC address.
+        eth_dst (str): destination Ethernet MAC address.
+        src_ip (ipaddress.IPv6Address): source IPv6 address.
+        dst_ip (ipaddress.IPv6Address): destination IPv6 address.
+        hop_limit (int): IPv6 hop limit.
+        id_ (int): identifier for echo reply.
+        seq (int): sequence number for echo reply.
+        data (str): payload for echo reply.
+    Returns:
+        ryu.lib.packet.ethernet: Serialized IPv6 ICMP echo reply packet.
     """
-    pkt = build_pkt_header(
-        vid, eth_src, eth_dst, valve_of.ether.ETH_TYPE_IPV6)
+    pkt = build_pkt_header(vid, eth_src, eth_dst, valve_of.ether.ETH_TYPE_IPV6)
     ipv6_reply = ipv6.ipv6(
-        src=src_ip,
-        dst=dst_ip,
-        nxt=valve_of.inet.IPPROTO_ICMPV6,
-        hop_limit=hop_limit)
+        src=src_ip, dst=dst_ip, nxt=valve_of.inet.IPPROTO_ICMPV6, hop_limit=hop_limit
+    )
     pkt.add_protocol(ipv6_reply)
     icmpv6_reply = icmpv6.icmpv6(
-        type_=icmpv6.ICMPV6_ECHO_REPLY,
-        data=icmpv6.echo(id_=id_, seq=seq, data=data))
+        type_=icmpv6.ICMPV6_ECHO_REPLY, data=icmpv6.echo(id_=id_, seq=seq, data=data)
+    )
     pkt.add_protocol(icmpv6_reply)
     pkt.serialize()
     return pkt
 
 
-def router_advert(vid, eth_src, eth_dst, src_ip, dst_ip,
-                  vips, pi_flags=0x6):
+def router_advert(vid, eth_src, eth_dst, src_ip, dst_ip, vips, pi_flags=0x6):
     """Return IPv6 ICMP Router Advert.
 
     Args:
         vid (int or None): VLAN VID to use (or None).
         eth_src (str): source Ethernet MAC address.
         eth_dst (str): dest Ethernet MAC address.
         src_ip (ipaddress.IPv6Address): source IPv6 address.
         vips (list): prefixes (ipaddress.IPv6Address) to advertise.
         pi_flags (int): flags to set in prefix information field (default set A and L)
     Returns:
         ryu.lib.packet.ethernet: Serialized IPv6 ICMP RA packet.
     """
-    pkt = build_pkt_header(
-        vid, eth_src, eth_dst, valve_of.ether.ETH_TYPE_IPV6)
+    pkt = build_pkt_header(vid, eth_src, eth_dst, valve_of.ether.ETH_TYPE_IPV6)
     ipv6_pkt = ipv6.ipv6(
         src=src_ip,
         dst=dst_ip,
         nxt=valve_of.inet.IPPROTO_ICMPV6,
-        hop_limit=IPV6_MAX_HOP_LIM)
+        hop_limit=IPV6_MAX_HOP_LIM,
+    )
     pkt.add_protocol(ipv6_pkt)
     options = []
     for vip in vips:
         options.append(
             icmpv6.nd_option_pi(
                 prefix=vip.network.network_address,
                 pl=vip.network.prefixlen,
                 res1=pi_flags,
                 val_l=86400,
                 pre_l=14400,
-            ))
+            )
+        )
     options.append(icmpv6.nd_option_sla(hw_src=eth_src))
     # https://tools.ietf.org/html/rfc4861#section-4.6.2
     icmpv6_ra_pkt = icmpv6.icmpv6(
         type_=icmpv6.ND_ROUTER_ADVERT,
-        data=icmpv6.nd_router_advert(
-            rou_l=1800,
-            ch_l=IPV6_RA_HOP_LIM,
-            options=options))
+        data=icmpv6.nd_router_advert(rou_l=1800, ch_l=IPV6_RA_HOP_LIM, options=options),
+    )
     pkt.add_protocol(icmpv6_ra_pkt)
     pkt.serialize()
     return pkt
 
 
 class PacketMeta:
     """Original, and parsed Ethernet packet metadata."""
 
     __slots__ = [
-        'data',
-        'orig_len',
-        'pkt',
-        'eth_pkt',
-        'vlan_pkt',
-        'port',
-        'vlan',
-        'eth_src',
-        'eth_dst',
-        'eth_type',
-        'l3_pkt',
-        'l3_src',
-        'l3_dst',
+        "data",
+        "orig_len",
+        "pkt",
+        "eth_pkt",
+        "vlan_pkt",
+        "port",
+        "vlan",
+        "eth_src",
+        "eth_dst",
+        "eth_type",
+        "l3_pkt",
+        "l3_src",
+        "l3_dst",
     ]
 
     ETH_TYPES_PARSERS = {
         valve_of.ether.ETH_TYPE_IP: (4, ipv4_parseable, ipv4.ipv4),
         valve_of.ether.ETH_TYPE_ARP: (None, None, arp.arp),
         valve_of.ether.ETH_TYPE_IPV6: (6, None, ipv6.ipv6),
     }
@@ -738,16 +770,27 @@
     }
 
     MAX_ETH_TYPE_PKT_SIZE = {
         valve_of.ether.ETH_TYPE_ARP: VLAN_ARP_PKT_SIZE,
         valve_of.ether.ETH_TYPE_IP: VLAN_ICMP_ECHO_REQ_SIZE,
     }
 
-    def __init__(self, data, orig_len, pkt, eth_pkt, vlan_pkt, port, valve_vlan,
-                 eth_src, eth_dst, eth_type):
+    def __init__(
+        self,
+        data,
+        orig_len,
+        pkt,
+        eth_pkt,
+        vlan_pkt,
+        port,
+        valve_vlan,
+        eth_src,
+        eth_dst,
+        eth_type,
+    ):
         self.data = data
         self.orig_len = orig_len
         self.pkt = pkt
         self.eth_pkt = eth_pkt
         self.vlan_pkt = vlan_pkt
         self.port = port
         self.vlan = valve_vlan
@@ -755,25 +798,32 @@
         self.eth_dst = eth_dst
         self.eth_type = eth_type
         self.l3_pkt = None
         self.l3_src = None
         self.l3_dst = None
 
     def log(self):
-        vlan_msg = ''
+        vlan_msg = ""
         if self.vlan:
-            vlan_msg = f'VLAN {self.vlan.vid}'
-        return '%s (L2 type 0x%4.4x, L2 dst %s, L3 src %s, L3 dst %s) %s %s' % (
-            self.eth_src, self.eth_type, self.eth_dst, self.l3_src, self.l3_dst,
-            self.port, vlan_msg)
+            vlan_msg = "VLAN %u" % self.vlan.vid
+        return "%s (L2 type 0x%4.4x, L2 dst %s, L3 src %s, L3 dst %s) %s %s" % (
+            self.eth_src,
+            self.eth_type,
+            self.eth_dst,
+            self.l3_src,
+            self.l3_dst,
+            self.port,
+            vlan_msg,
+        )
 
     def reparse(self, max_len):
         """Reparse packet using data up to the specified maximum length."""
         pkt, eth_pkt, eth_type, vlan_pkt, _ = parse_packet_in_pkt(
-            self.data, max_len, eth_pkt=self.eth_pkt, vlan_pkt=self.vlan_pkt)
+            self.data, max_len, eth_pkt=self.eth_pkt, vlan_pkt=self.vlan_pkt
+        )
         if pkt is None or eth_type is None:
             return
         right_size = self.MAX_ETH_TYPE_PKT_SIZE.get(eth_type, len(self.data))
         if len(self.data) > right_size:
             self.data = self.data[:right_size]
         self.pkt = pkt
         self.eth_pkt = eth_pkt
@@ -804,18 +854,18 @@
                 ip_header_data = self.data[ETH_VLAN_HEADER_SIZE:]
                 if ip_parseable is not None and not ip_parseable(ip_header_data):
                     return
             parse_limit = header_size + payload
             self.reparse(parse_limit)
             self.l3_pkt = self.pkt.get_protocol(pkt_parser)
             if self.l3_pkt:
-                if hasattr(self.l3_pkt, 'src'):
+                if hasattr(self.l3_pkt, "src"):
                     self.l3_src = self.l3_pkt.src
                     self.l3_dst = self.l3_pkt.dst
-                elif hasattr(self.l3_pkt, 'src_ip'):
+                elif hasattr(self.l3_pkt, "src_ip"):
                     self.l3_src = self.l3_pkt.src_ip
                     self.l3_dst = self.l3_pkt.dst_ip
                 self.l3_src = ipaddress.ip_address(self.l3_src)
                 self.l3_dst = ipaddress.ip_address(self.l3_dst)
 
     def packet_complete(self):
         """True if we have the complete packet."""
```

### Comparing `c65faucet-1.0.46/faucet/valve_pipeline.py` & `c65faucet-1.0.47/faucet/valve_pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,15 +18,14 @@
 # limitations under the License.
 
 import copy
 import functools
 import faucet.faucet_metadata as faucet_md
 from faucet import valve_of
 from faucet.valve_manager_base import ValveManagerBase
-from faucet.valve_util import LRU_MAX
 
 
 class ValvePipeline(ValveManagerBase):
     """Responsible for maintaing the integrity of the Faucet pipeline for a
     single valve.
 
     Controls what packets a module sees in its tables and how it can pass
@@ -45,55 +44,55 @@
         if dp.egress_pipeline:
             self.egress_table = dp.tables['egress']
             self.egress_acl_table = dp.tables.get('egress_acl')
         self.filter_priority = self._FILTER_PRIORITY
         self.select_priority = self._HIGH_PRIORITY
 
     @staticmethod
-    @functools.lru_cache(maxsize=LRU_MAX)
+    @functools.lru_cache(maxsize=1024)
     def _accept_to_table(table, actions):
         inst = [table.goto_this()]
         if actions:
             inst.append(valve_of.apply_actions(actions))
         return tuple(inst)
 
-    @functools.lru_cache(maxsize=LRU_MAX)
+    @functools.lru_cache(maxsize=1024)
     def accept_to_vlan(self, actions=None):
         """Get instructions to forward packet through the pipeline to
         vlan table.
         args:
             actions: (optional) list of actions to apply to packet.
         returns:
             list of instructions
         """
         return self._accept_to_table(self.vlan_table, actions)
 
-    @functools.lru_cache(maxsize=LRU_MAX)
+    @functools.lru_cache(maxsize=1024)
     def accept_to_classification(self, actions=None):
         """Get instructions to forward packet through the pipeline to
         classification table.
         args:
             actions: (optional) list of actions to apply to packet.
         returns:
             list of instructions
         """
         return self._accept_to_table(self.classification_table, actions)
 
-    @functools.lru_cache(maxsize=LRU_MAX)
+    @functools.lru_cache(maxsize=1024)
     def accept_to_l2_forwarding(self, actions=None):
         """Get instructions to forward packet through the pipeline to l2
         forwarding.
         args:
             actions: (optional) list of actions to apply to packet.
         returns:
             list of instructions
         """
         return self._accept_to_table(self.output_table, actions)
 
-    @functools.lru_cache(maxsize=LRU_MAX)
+    @functools.lru_cache(maxsize=1024)
     def accept_to_egress(self, actions=None):
         """Get instructions to forward packet through the pipeline to egress
         table
 
         Raises an assertion error if egress pipeline is not configured
 
         args:
```

### Comparing `c65faucet-1.0.46/faucet/valve_route.py` & `c65faucet-1.0.47/faucet/valve_route.py`

 * *Files 5% similar despite different names*

```diff
@@ -39,20 +39,20 @@
         self.vid = vid
 
 
 class NextHop:
     """Describes a directly connected (at layer 2) nexthop."""
 
     __slots__ = [
-        'cache_time',
-        'eth_src',
-        'last_retry_time',
-        'next_retry_time',
-        'resolve_retries',
-        'port',
+        "cache_time",
+        "eth_src",
+        "last_retry_time",
+        "next_retry_time",
+        "resolve_retries",
+        "port",
     ]
 
     def __init__(self, eth_src, port, now):
         self.eth_src = eth_src
         self.port = port
         self.cache_time = now
         self.resolve_retries = 0
@@ -71,67 +71,82 @@
 
     def next_retry(self, now, max_resolve_backoff_time):
         """Increment state for next retry."""
         self.resolve_retries += 1
         self.last_retry_time = now
         self.next_retry_time = now + min(
             (2**self.resolve_retries + random.randint(0, self.resolve_retries)),
-            max_resolve_backoff_time)
+            max_resolve_backoff_time,
+        )
 
     def resolution_due(self, now, max_age):
         """Return True if this nexthop is due to be re resolved/retried."""
         if self.eth_src is not None and self.age(now) < max_age:
             return False
         if self.next_retry_time is None or self.next_retry_time < now:
             return True
         return False
 
     def __str__(self):
-        return '%s %s' % (self.eth_src, self.port)
+        return "%s %s" % (self.eth_src, self.port)
 
     def __repr__(self):
         return self.__str__()
 
 
 class ValveRouteManager(ValveManagerBase):
     """Base class to implement RIB/FIB."""
 
     __slots__ = [
-        'active',
-        'neighbor_timeout',
-        'dec_ttl',
-        'fib_table',
-        'pipeline',
-        'multi_out',
-        'notify',
-        'global_vlan',
-        'global_routing',
-        'logger',
-        'max_host_fib_retry_count',
-        'max_hosts_per_resolve_cycle',
-        'max_resolve_backoff_time',
-        'proactive_learn',
-        'route_priority',
-        'routers',
-        'vip_table',
-        'switch_manager',
+        "active",
+        "neighbor_timeout",
+        "dec_ttl",
+        "fib_table",
+        "pipeline",
+        "multi_out",
+        "notify",
+        "global_vlan",
+        "global_routing",
+        "logger",
+        "max_host_fib_retry_count",
+        "max_hosts_per_resolve_cycle",
+        "max_resolve_backoff_time",
+        "proactive_learn",
+        "route_priority",
+        "routers",
+        "vip_table",
+        "switch_manager",
     ]
 
     IPV = 0
     ETH_TYPE = None
     ICMP_TYPE = None
     ICMP_SIZE = None
     MAX_PACKET_IN_SIZE = valve_of.MAX_PACKET_IN_BYTES
     CONTROL_ETH_TYPES = ()  # type: ignore
     IP_PKT = None
 
-    def __init__(self, logger, notify, global_vlan, neighbor_timeout,
-                 max_hosts_per_resolve_cycle, max_host_fib_retry_count,
-                 max_resolve_backoff_time, proactive_learn, dec_ttl, multi_out,
-                 fib_table, vip_table, pipeline, routers, stack_manager):
+    def __init__(
+        self,
+        logger,
+        notify,
+        global_vlan,
+        neighbor_timeout,
+        max_hosts_per_resolve_cycle,
+        max_host_fib_retry_count,
+        max_resolve_backoff_time,
+        proactive_learn,
+        dec_ttl,
+        multi_out,
+        fib_table,
+        vip_table,
+        pipeline,
+        routers,
+        stack_manager,
+    ):
         self.notify = notify
         self.logger = logger
         self.global_vlan = AnonVLAN(global_vlan)
         self.neighbor_timeout = neighbor_timeout
         self.max_hosts_per_resolve_cycle = max_hosts_per_resolve_cycle
         self.max_host_fib_retry_count = max_host_fib_retry_count
         self.max_resolve_backoff_time = max_resolve_backoff_time
@@ -143,33 +158,40 @@
         self.pipeline = pipeline
         self.route_priority = self._LPM_PRIORITY
         self.routers = routers
         self.active = False
         self.global_routing = self._global_routing()
         self.stack_manager = stack_manager
         if self.global_routing:
-            self.logger.info('global routing enabled')
+            self.logger.info("global routing enabled")
 
     def notify_learn(self, pkt_meta):
-        self.notify({'L3_LEARN': {
-            'eth_src': pkt_meta.eth_src,
-            'l3_src_ip': str(pkt_meta.l3_src),
-            'port_no': pkt_meta.port.number,
-            'vid': pkt_meta.vlan.vid}})
+        self.notify(
+            {
+                "L3_LEARN": {
+                    "eth_src": pkt_meta.eth_src,
+                    "l3_src_ip": str(pkt_meta.l3_src),
+                    "port_no": pkt_meta.port.number,
+                    "vid": pkt_meta.vlan.vid,
+                }
+            }
+        )
 
     def nexthop_dead(self, nexthop_cache_entry):
         """Returns true if the nexthop_cache_entry is considered dead"""
         return nexthop_cache_entry.dead(self.max_host_fib_retry_count)
 
     @staticmethod
     def _unicast_to_vip(pkt_meta):
         """Return true if packet is from a src in the connected network and dst ip is
-            a faucet vip. I.e: Packet is traffic bound for a VIP"""
-        return (pkt_meta.eth_dst == pkt_meta.vlan.faucet_mac
-                and pkt_meta.vlan.from_connected_to_vip(pkt_meta.l3_src, pkt_meta.l3_dst))
+        a faucet vip. I.e: Packet is traffic bound for a VIP"""
+        return (
+            pkt_meta.eth_dst == pkt_meta.vlan.faucet_mac
+            and pkt_meta.vlan.from_connected_to_vip(pkt_meta.l3_src, pkt_meta.l3_dst)
+        )
 
     @staticmethod
     def _gw_resolve_pkt():
         return None
 
     @staticmethod
     def _gw_respond_pkt():
@@ -177,96 +199,123 @@
 
     def _flood_stack_links(self, pkt_builder, vlan, multi_out, *args):
         """Return flood packet-out actions to stack ports for gw resolving"""
         ofmsgs = []
         if self.stack_manager:
             ports = []
             if self.stack_manager.stack.is_root():
-                ports = list(self.stack_manager.away_ports
-                             - self.stack_manager.inactive_away_ports
-                             - self.stack_manager.pruned_away_ports)
+                ports = list(
+                    self.stack_manager.away_ports
+                    - self.stack_manager.inactive_away_ports
+                    - self.stack_manager.pruned_away_ports
+                )
             else:
                 if self.stack_manager.chosen_towards_port is not None:
                     ports = [self.stack_manager.chosen_towards_port]
             if ports:
                 running_port_nos = [port.number for port in ports if port.running()]
                 pkt = pkt_builder(vlan.vid, *args)
                 if running_port_nos:
                     random.shuffle(running_port_nos)
                     if multi_out:
-                        ofmsgs.append(valve_of.packetouts(running_port_nos, bytes(pkt.data)))
+                        ofmsgs.append(
+                            valve_of.packetouts(running_port_nos, bytes(pkt.data))
+                        )
                     else:
                         ofmsgs.extend(
-                            [valve_of.packetout(port_no, bytes(pkt.data)) for
-                             port_no in running_port_nos])
+                            [
+                                valve_of.packetout(port_no, bytes(pkt.data))
+                                for port_no in running_port_nos
+                            ]
+                        )
         return ofmsgs
 
     def _resolve_gw_on_vlan(self, vlan, faucet_vip, ip_gw):
         """Return flood packet-out actions for gw resolving"""
         ofmsgs = []
         stack_ofmsgs = self._flood_stack_links(
-            self._gw_resolve_pkt(), vlan, self.multi_out,
-            vlan.faucet_mac, valve_of.mac.BROADCAST_STR, faucet_vip.ip, ip_gw)
+            self._gw_resolve_pkt(),
+            vlan,
+            self.multi_out,
+            vlan.faucet_mac,
+            valve_of.mac.BROADCAST_STR,
+            faucet_vip.ip,
+            ip_gw,
+        )
         if stack_ofmsgs:
             ofmsgs.extend(stack_ofmsgs)
         vlan_ofmsgs = vlan.flood_pkt(
-            self._gw_resolve_pkt(), self.multi_out,
-            vlan.faucet_mac, valve_of.mac.BROADCAST_STR, faucet_vip.ip, ip_gw)
+            self._gw_resolve_pkt(),
+            self.multi_out,
+            vlan.faucet_mac,
+            valve_of.mac.BROADCAST_STR,
+            faucet_vip.ip,
+            ip_gw,
+        )
         if vlan_ofmsgs:
             ofmsgs.extend(vlan_ofmsgs)
         return ofmsgs
 
     def _resolve_gw_on_port(self, vlan, port, faucet_vip, ip_gw, eth_dst):
         """Return packet-out actions for outputting to a specific port"""
         return vlan.pkt_out_port(
-            self._gw_resolve_pkt(),
-            port, vlan.faucet_mac, eth_dst, faucet_vip.ip, ip_gw)
+            self._gw_resolve_pkt(), port, vlan.faucet_mac, eth_dst, faucet_vip.ip, ip_gw
+        )
 
     def _controller_and_flood(self):
         """Return instructions to forward packet to l2-forwarding"""
         return self.pipeline.accept_to_l2_forwarding(
-            actions=(valve_of.output_controller(max_len=self.MAX_PACKET_IN_SIZE),))
+            actions=(valve_of.output_controller(max_len=self.MAX_PACKET_IN_SIZE),)
+        )
 
     def _resolve_vip_response(self, pkt_meta, solicited_ip, now):
         """Learn host requesting for router, and return packet-out ofmsgs router response"""
         ofmsgs = []
         vlan = pkt_meta.vlan
-        if (pkt_meta.vlan.is_faucet_vip(solicited_ip)
-                and pkt_meta.vlan.ip_in_vip_subnet(pkt_meta.l3_src)):
+        if pkt_meta.vlan.is_faucet_vip(solicited_ip) and pkt_meta.vlan.ip_in_vip_subnet(
+            pkt_meta.l3_src
+        ):
             src_ip = pkt_meta.l3_src
             eth_src = pkt_meta.eth_src
             port = pkt_meta.port
             if self._stateful_gw(vlan, src_ip):
-                ofmsgs.extend(
-                    self._add_host_fib_route(vlan, src_ip, blackhole=False))
-                ofmsgs.extend(self._update_nexthop(
-                    now, vlan, port, eth_src, src_ip))
+                ofmsgs.extend(self._add_host_fib_route(vlan, src_ip, blackhole=False))
+                ofmsgs.extend(self._update_nexthop(now, vlan, port, eth_src, src_ip))
                 if ofmsgs:
                     self.logger.info(
-                        'Resolve response to %s from %s' % (
-                            solicited_ip, pkt_meta.log()))
+                        "Resolve response to %s from %s"
+                        % (solicited_ip, pkt_meta.log())
+                    )
             ofmsgs.append(
                 vlan.pkt_out_port(
-                    self._gw_respond_pkt(), port,
-                    vlan.faucet_mac, eth_src,
-                    solicited_ip, src_ip))
+                    self._gw_respond_pkt(),
+                    port,
+                    vlan.faucet_mac,
+                    eth_src,
+                    solicited_ip,
+                    src_ip,
+                )
+            )
         return ofmsgs
 
     def _gw_advert(self, pkt_meta, target_ip, now):
         """Receive an advert, so update nexthop information"""
         ofmsgs = []
         vlan = pkt_meta.vlan
         if vlan.ip_in_vip_subnet(target_ip):
             if self._stateful_gw(vlan, target_ip):
-                ofmsgs.extend(self._update_nexthop(
-                    now, vlan, pkt_meta.port, pkt_meta.eth_src, target_ip))
+                ofmsgs.extend(
+                    self._update_nexthop(
+                        now, vlan, pkt_meta.port, pkt_meta.eth_src, target_ip
+                    )
+                )
                 if ofmsgs:
                     self.logger.info(
-                        'Received advert for %s from %s' % (
-                            target_ip, pkt_meta.log()))
+                        "Received advert for %s from %s" % (target_ip, pkt_meta.log())
+                    )
         return ofmsgs
 
     def _vlan_routes(self, vlan):
         """Return vlan routes"""
         return vlan.routes_by_ipv(self.IPV)
 
     def _vlan_nexthop_cache(self, vlan):
@@ -276,20 +325,27 @@
     def expire_port_nexthops(self, port):
         """Expire all hosts on a port"""
         ofmsgs = []
         now = time.time()
         for vlan in port.vlans():
             nexthop_cache = self._vlan_nexthop_cache(vlan)
             dead_nexthops = [
-                (ip_gw, nexthop_cache_entry) for ip_gw, nexthop_cache_entry in nexthop_cache.items()
-                if nexthop_cache_entry and nexthop_cache_entry.port
-                and port.number == nexthop_cache_entry.port.number]
+                (ip_gw, nexthop_cache_entry)
+                for ip_gw, nexthop_cache_entry in nexthop_cache.items()
+                if nexthop_cache_entry
+                and nexthop_cache_entry.port
+                and port.number == nexthop_cache_entry.port.number
+            ]
             for ip_gw, nexthop_cache_entry in dead_nexthops:
-                self.logger.info('marking %s as a dead nexthop' % nexthop_cache_entry.eth_src)
-                ofmsgs.extend(self._expire_gateway_flows(ip_gw, nexthop_cache_entry, vlan, now))
+                self.logger.info(
+                    "marking %s as a dead nexthop" % nexthop_cache_entry.eth_src
+                )
+                ofmsgs.extend(
+                    self._expire_gateway_flows(ip_gw, nexthop_cache_entry, vlan, now)
+                )
         return ofmsgs
 
     def _vlan_nexthop_cache_entry(self, vlan, ip_gw):
         """Return nexthop cache entry"""
         nexthop_cache = self._vlan_nexthop_cache(vlan)
         return nexthop_cache.get(ip_gw, None)
 
@@ -298,17 +354,20 @@
         del nexthop_cache[ip_gw]
 
     def _nexthop_actions(self, eth_dst, vlan):
         """Return flowrule actions for fib entry"""
         actions = []
         if self.routers:
             actions.append(self.fib_table.set_vlan_vid(vlan.vid))
-        actions.extend([
-            self.fib_table.set_field(eth_src=vlan.faucet_mac),
-            self.fib_table.set_field(eth_dst=eth_dst)])
+        actions.extend(
+            [
+                self.fib_table.set_field(eth_src=vlan.faucet_mac),
+                self.fib_table.set_field(eth_dst=eth_dst),
+            ]
+        )
         if self.dec_ttl:
             actions.append(valve_of.dec_ip_ttl())
         return tuple(actions)
 
     def _route_match(self, vlan, ip_dst):
         """Return vid, dst, eth_type flowrule match for fib entry"""
         return self.fib_table.match(vlan=vlan, eth_type=self.ETH_TYPE, nw_dst=ip_dst)
@@ -351,112 +410,137 @@
         learn_connected_priority = self.route_priority + faucet_vip.network.prefixlen
         faucet_mac = vlan.faucet_mac
         actions = None
         if self.global_routing:
             vlan_mac = valve_packet.int_in_mac(faucet_mac, vlan.vid)
             actions = [
                 self.fib_table.set_field(eth_dst=vlan_mac),
-                self.fib_table.set_vlan_vid(self.global_vlan.vid)
+                self.fib_table.set_vlan_vid(self.global_vlan.vid),
             ]
-        ofmsgs.extend(self.pipeline.select_packets(
-            self.fib_table,
-            {'eth_type': self.ETH_TYPE, 'eth_dst': faucet_mac, 'vlan': vlan},
-            actions
-        ))
+        ofmsgs.extend(
+            self.pipeline.select_packets(
+                self.fib_table,
+                {"eth_type": self.ETH_TYPE, "eth_dst": faucet_mac, "vlan": vlan},
+                actions,
+            )
+        )
         if self.global_routing:
             vlan = self.global_vlan
-        ofmsgs.append(self.fib_table.flowmod(
-            self._route_match(vlan, faucet_vip_host),
-            priority=priority,
-            inst=(self.fib_table.goto(self.vip_table),)))
+        ofmsgs.append(
+            self.fib_table.flowmod(
+                self._route_match(vlan, faucet_vip_host),
+                priority=priority,
+                inst=(self.fib_table.goto(self.vip_table),),
+            )
+        )
         if self.proactive_learn and not faucet_vip.ip.is_link_local:
             routed_vlans = self._routed_vlans(vlan)
             for routed_vlan in routed_vlans:
-                ofmsgs.append(self.fib_table.flowmod(
-                    self._route_match(routed_vlan, faucet_vip),
-                    priority=learn_connected_priority,
-                    inst=(self.fib_table.goto(self.vip_table),)))
+                ofmsgs.append(
+                    self.fib_table.flowmod(
+                        self._route_match(routed_vlan, faucet_vip),
+                        priority=learn_connected_priority,
+                        inst=(self.fib_table.goto(self.vip_table),),
+                    )
+                )
             # Unicast ICMP to us.
             priority -= 1
-            ofmsgs.append(self.vip_table.flowcontroller(
-                self.vip_table.match(
-                    eth_type=self.ETH_TYPE,
-                    eth_dst=faucet_mac,
-                    nw_proto=self.ICMP_TYPE),
-                priority=priority,
-                max_len=self.ICMP_SIZE))
+            ofmsgs.append(
+                self.vip_table.flowcontroller(
+                    self.vip_table.match(
+                        eth_type=self.ETH_TYPE,
+                        eth_dst=faucet_mac,
+                        nw_proto=self.ICMP_TYPE,
+                    ),
+                    priority=priority,
+                    max_len=self.ICMP_SIZE,
+                )
+            )
             # Learn + flood other ICMP not unicast to us.
             priority -= 1
-            ofmsgs.append(self.vip_table.flowmod(
-                self.vip_table.match(
-                    eth_type=self.ETH_TYPE,
-                    nw_proto=self.ICMP_TYPE),
-                priority=priority,
-                inst=self._controller_and_flood()))
+            ofmsgs.append(
+                self.vip_table.flowmod(
+                    self.vip_table.match(
+                        eth_type=self.ETH_TYPE, nw_proto=self.ICMP_TYPE
+                    ),
+                    priority=priority,
+                    inst=self._controller_and_flood(),
+                )
+            )
             # Learn from other IP traffic unicast to us.
             priority -= 1
-            ofmsgs.append(self.vip_table.flowcontroller(
-                self.vip_table.match(
-                    eth_type=self.ETH_TYPE,
-                    eth_dst=faucet_mac),
-                priority=priority,
-                max_len=self.MAX_PACKET_IN_SIZE))
+            ofmsgs.append(
+                self.vip_table.flowcontroller(
+                    self.vip_table.match(eth_type=self.ETH_TYPE, eth_dst=faucet_mac),
+                    priority=priority,
+                    max_len=self.MAX_PACKET_IN_SIZE,
+                )
+            )
             # Learn + flood IP traffic not unicast to us.
             priority -= 1
-            ofmsgs.append(self.vip_table.flowmod(
-                self.vip_table.match(
-                    eth_type=self.ETH_TYPE),
-                priority=priority,
-                inst=self._controller_and_flood()))
+            ofmsgs.append(
+                self.vip_table.flowmod(
+                    self.vip_table.match(eth_type=self.ETH_TYPE),
+                    priority=priority,
+                    inst=self._controller_and_flood(),
+                )
+            )
         return ofmsgs
 
     def _add_faucet_vip_nd(self, vlan, priority, faucet_vip, faucet_vip_host):
         raise NotImplementedError  # pragma: no cover
 
     def add_vlan(self, vlan, cold_start):
         """Add a VLAN."""
         ofmsgs = []
         # add controller IPs if configured.
         for faucet_vip in vlan.faucet_vips_by_ipv(self.IPV):
             max_prefixlen = faucet_vip.ip.max_prefixlen
             faucet_vip_host = self._host_from_faucet_vip(faucet_vip)
             priority = self.route_priority + max_prefixlen
-            ofmsgs.extend(self._add_faucet_vip_nd(
-                vlan, priority, faucet_vip, faucet_vip_host))
-            ofmsgs.extend(self._add_faucet_fib_to_vip(
-                vlan, priority, faucet_vip, faucet_vip_host))
+            ofmsgs.extend(
+                self._add_faucet_vip_nd(vlan, priority, faucet_vip, faucet_vip_host)
+            )
+            ofmsgs.extend(
+                self._add_faucet_fib_to_vip(vlan, priority, faucet_vip, faucet_vip_host)
+            )
         return ofmsgs
 
     def del_vlan(self, vlan):
         """Delete a VLAN."""
         ofmsgs = []
         if vlan.faucet_vips_by_ipv:
-            ofmsgs.append(self.fib_table.flowdel(
-                match=self.fib_table.match(vlan=vlan)))
+            ofmsgs.append(self.fib_table.flowdel(match=self.fib_table.match(vlan=vlan)))
         return ofmsgs
 
     def _add_resolved_route(self, vlan, ip_gw, ip_dst, eth_dst, is_updated):
         """Return flowmods for enabling routing of a resolved nexthop"""
         ofmsgs = []
         if is_updated:
             self.logger.info(
-                'Updating next hop for route %s via %s (%s) on VLAN %u' % (
-                    ip_dst, ip_gw, eth_dst, vlan.vid))
+                "Updating next hop for route %s via %s (%s) on VLAN %u"
+                % (ip_dst, ip_gw, eth_dst, vlan.vid)
+            )
             ofmsgs.extend(self._del_route_flows(vlan, ip_dst))
         else:
             self.logger.info(
-                'Adding new route %s via %s (%s) on VLAN %u' % (
-                    ip_dst, ip_gw, eth_dst, vlan.vid))
+                "Adding new route %s via %s (%s) on VLAN %u"
+                % (ip_dst, ip_gw, eth_dst, vlan.vid)
+            )
         inst = self.pipeline.accept_to_l2_forwarding(
-            actions=self._nexthop_actions(eth_dst, vlan))
+            actions=self._nexthop_actions(eth_dst, vlan)
+        )
         routed_vlans = self._routed_vlans(vlan)
         for routed_vlan in routed_vlans:
             in_match = self._route_match(routed_vlan, ip_dst)
-            ofmsgs.append(self.fib_table.flowmod(
-                in_match, priority=self._route_priority(ip_dst), inst=inst))
+            ofmsgs.append(
+                self.fib_table.flowmod(
+                    in_match, priority=self._route_priority(ip_dst), inst=inst
+                )
+            )
         return ofmsgs
 
     def _update_nexthop_cache(self, now, vlan, eth_src, port, ip_gw):
         """Add information to the nexthop cache and return the new object"""
         nexthop = NextHop(eth_src, port, now)
         nexthop_cache = self._vlan_nexthop_cache(vlan)
         nexthop_cache[ip_gw] = nexthop
@@ -476,16 +560,19 @@
         """
         ofmsgs = []
         cached_eth_dst = self._cached_nexthop_eth_dst(vlan, resolved_ip_gw)
 
         if cached_eth_dst != eth_src:
             is_updated = cached_eth_dst is not None
             for ip_dst in vlan.ip_dsts_for_ip_gw(resolved_ip_gw):
-                ofmsgs.extend(self._add_resolved_route(
-                    vlan, resolved_ip_gw, ip_dst, eth_src, is_updated))
+                ofmsgs.extend(
+                    self._add_resolved_route(
+                        vlan, resolved_ip_gw, ip_dst, eth_src, is_updated
+                    )
+                )
 
         self._update_nexthop_cache(now, vlan, eth_src, port, resolved_ip_gw)
         return ofmsgs
 
     def _vlan_unresolved_nexthops(self, vlan, ip_gws, now):
         """Return unresolved or expired IP gateways, never tried/oldest first.
 
@@ -494,18 +581,21 @@
            ip_gws (list): tuple, IP gateway and controller IP in same subnet.
            now (float): seconds since epoch.
         Returns:
            list: prioritized list of gateways.
         """
         vlan_nexthop_cache = self._vlan_nexthop_cache(vlan)
         nexthop_entries = [
-            (ip_gw, vlan_nexthop_cache.get(ip_gw, None)) for ip_gw in ip_gws]
+            (ip_gw, vlan_nexthop_cache.get(ip_gw, None)) for ip_gw in ip_gws
+        ]
         not_fresh_nexthops = [
-            (ip_gw, entry) for ip_gw, entry in nexthop_entries
-            if entry is None or entry.resolution_due(now, self.neighbor_timeout)]
+            (ip_gw, entry)
+            for ip_gw, entry in nexthop_entries
+            if entry is None or entry.resolution_due(now, self.neighbor_timeout)
+        ]
         unresolved_nexthops_by_retries = defaultdict(list)
         for ip_gw, entry in not_fresh_nexthops:
             if entry is None:
                 entry = self._update_nexthop_cache(now, vlan, None, None, ip_gw)
             unresolved_nexthops_by_retries[entry.resolve_retries].append(ip_gw)
         unresolved_nexthops = deque()
         for _retries, nexthops in sorted(unresolved_nexthops_by_retries.items()):
@@ -516,64 +606,76 @@
     def advertise(self, vlan):
         raise NotImplementedError  # pragma: no cover
 
     def _resolve_gateway_flows(self, ip_gw, nexthop_cache_entry, vlan, now):
         """Return packet-out ofmsgs using ARP/ND to resolve for nexthop"""
         faucet_vip = vlan.vip_map(ip_gw)
         if not faucet_vip:
-            self.logger.info('Not resolving %s (not in connected network)' % ip_gw)
+            self.logger.info("Not resolving %s (not in connected network)" % ip_gw)
             return []
         resolve_flows = []
         last_retry_time = nexthop_cache_entry.last_retry_time
         nexthop_cache_entry.next_retry(now, self.max_resolve_backoff_time)
-        if (vlan.targeted_gw_resolution
-                and last_retry_time is None and nexthop_cache_entry.port is not None):
+        if (
+            vlan.targeted_gw_resolution
+            and last_retry_time is None
+            and nexthop_cache_entry.port is not None
+        ):
             port = nexthop_cache_entry.port
             eth_dst = nexthop_cache_entry.eth_src
-            resolve_flows = [self._resolve_gw_on_port(
-                vlan, port, faucet_vip, ip_gw, eth_dst)]
+            resolve_flows = [
+                self._resolve_gw_on_port(vlan, port, faucet_vip, ip_gw, eth_dst)
+            ]
         else:
             resolve_flows = self._resolve_gw_on_vlan(vlan, faucet_vip, ip_gw)
         if resolve_flows:
             if last_retry_time is None:
                 self.logger.info(
-                    'resolving %s (%u flows) on VLAN %u' % (ip_gw, len(resolve_flows), vlan.vid))
+                    "resolving %s (%u flows) on VLAN %u"
+                    % (ip_gw, len(resolve_flows), vlan.vid)
+                )
             else:
                 self.logger.info(
-                    'resolving %s retry %u (last attempt was %us ago; %u flows) on VLAN %u' % (
+                    "resolving %s retry %u (last attempt was %us ago; %u flows) on VLAN %u"
+                    % (
                         ip_gw,
                         nexthop_cache_entry.resolve_retries,
                         now - last_retry_time,
                         len(resolve_flows),
-                        vlan.vid))
+                        vlan.vid,
+                    )
+                )
         return resolve_flows
 
     def _expire_gateway_flows(self, ip_gw, nexthop_cache_entry, vlan, now):
         """Return ofmsgs deleting the expired nexthop information"""
         expire_flows = []
         self.logger.info(
-            'expiring dead route %s (age %us) on %s' % (
-                ip_gw, nexthop_cache_entry.age(now), vlan))
+            "expiring dead route %s (age %us) on %s"
+            % (ip_gw, nexthop_cache_entry.age(now), vlan)
+        )
         port = nexthop_cache_entry.port
         self._del_vlan_nexthop_cache_entry(vlan, ip_gw)
         expire_flows = self._del_host_fib_route(
-            vlan, ipaddress.ip_network(ip_gw.exploded))
+            vlan, ipaddress.ip_network(ip_gw.exploded)
+        )
         if port is None:
             expire_flows = []
         return expire_flows
 
     def _resolve_expire_gateway_flows(self, ip_gw, nexthop_cache_entry, vlan, now):
         """If cache entry is dead then delete related flows
         otherwise return packet-out ofmsgs to resolve nexthops"""
         if self.nexthop_dead(nexthop_cache_entry):
             return self._expire_gateway_flows(ip_gw, nexthop_cache_entry, vlan, now)
         return self._resolve_gateway_flows(ip_gw, nexthop_cache_entry, vlan, now)
 
-    def _resolve_gateways_flows(self, resolve_handler, vlan, now,
-                                unresolved_nexthops, remaining_attempts):
+    def _resolve_gateways_flows(
+        self, resolve_handler, vlan, now, unresolved_nexthops, remaining_attempts
+    ):
         """Resolve for nexthops using the resolve_handler
         Return packet-out ofmsgs using V4 ARP/V6 ND to resolve nexthops
         """
         ofmsgs = []
         for ip_gw in unresolved_nexthops:
             if remaining_attempts == 0:
                 break
@@ -597,44 +699,58 @@
             resolve_all (bool): attempt to resolve all unresolved gateways.
         Returns:
             list: OpenFlow messages.
         """
         unresolved_gateways = []
         if resolve_all:
             unresolved_gateways = self._vlan_unresolved_nexthops(
-                vlan, vlan.dyn_route_gws_by_ipv[self.IPV], now)
+                vlan, vlan.dyn_route_gws_by_ipv[self.IPV], now
+            )
             vlan.dyn_unresolved_route_ip_gws[self.IPV] = unresolved_gateways
         else:
             if vlan.dyn_unresolved_route_ip_gws[self.IPV]:
-                unresolved_gateways = [vlan.dyn_unresolved_route_ip_gws[self.IPV].popleft()]
+                unresolved_gateways = [
+                    vlan.dyn_unresolved_route_ip_gws[self.IPV].popleft()
+                ]
         return self._resolve_gateways_flows(
-            self._resolve_gateway_flows, vlan, now,
-            unresolved_gateways, self.max_hosts_per_resolve_cycle)
+            self._resolve_gateway_flows,
+            vlan,
+            now,
+            unresolved_gateways,
+            self.max_hosts_per_resolve_cycle,
+        )
 
     def resolve_expire_hosts(self, vlan, now, resolve_all=True):
         """Re/resolve hosts.
 
         Args:
             vlan (vlan): VLAN containing this RIB/FIB.
             now (float): seconds since epoch.
             resolve_all (bool): attempt to resolve all unresolved gateways.
         Returns:
             list: OpenFlow messages.
         """
         unresolved_gateways = []
         if resolve_all:
             unresolved_gateways = self._vlan_unresolved_nexthops(
-                vlan, vlan.dyn_host_gws_by_ipv[self.IPV], now)
+                vlan, vlan.dyn_host_gws_by_ipv[self.IPV], now
+            )
             vlan.dyn_unresolved_host_ip_gws[self.IPV] = unresolved_gateways
         else:
             if vlan.dyn_unresolved_host_ip_gws[self.IPV]:
-                unresolved_gateways = [vlan.dyn_unresolved_host_ip_gws[self.IPV].popleft()]
+                unresolved_gateways = [
+                    vlan.dyn_unresolved_host_ip_gws[self.IPV].popleft()
+                ]
         return self._resolve_gateways_flows(
-            self._resolve_expire_gateway_flows, vlan, now,
-            unresolved_gateways, self.max_hosts_per_resolve_cycle)
+            self._resolve_expire_gateway_flows,
+            vlan,
+            now,
+            unresolved_gateways,
+            self.max_hosts_per_resolve_cycle,
+        )
 
     def _cached_nexthop_eth_dst(self, vlan, ip_gw):
         """Return nexthop cache entry eth_dst for the ip_gw"""
         entry = self._vlan_nexthop_cache_entry(vlan, ip_gw)
         if entry is not None and entry.eth_src is not None:
             return entry.eth_src
         return None
@@ -656,29 +772,41 @@
         ofmsgs = []
         if self.proactive_learn:
             router = self._router_for_vlan(vlan)
             if router is None:
                 faucet_vip = vlan.vip_map(dst_ip)
             else:
                 vlan, faucet_vip = router.vip_map(dst_ip)
-            if (vlan and vlan.ip_in_vip_subnet(dst_ip, faucet_vip)
-                    and faucet_vip.ip != dst_ip and self._stateful_gw(vlan, dst_ip)):
+            if (
+                vlan
+                and vlan.ip_in_vip_subnet(dst_ip, faucet_vip)
+                and faucet_vip.ip != dst_ip
+                and self._stateful_gw(vlan, dst_ip)
+            ):
                 limit = self._vlan_nexthop_cache_limit(vlan)
                 if limit is None or len(self._vlan_nexthop_cache(vlan)) < limit:
                     # TODO: avoid relearning L3 source if same L3 source tries
                     # multiple L3 destinations quickly.
                     ofmsgs.extend(self.add_host_fib_route_from_pkt(now, pkt_meta))
-                    resolution_in_progress = dst_ip in vlan.dyn_host_gws_by_ipv[self.IPV]
-                    ofmsgs.extend(self._add_host_fib_route(vlan, dst_ip, blackhole=True))
+                    resolution_in_progress = (
+                        dst_ip in vlan.dyn_host_gws_by_ipv[self.IPV]
+                    )
+                    ofmsgs.extend(
+                        self._add_host_fib_route(vlan, dst_ip, blackhole=True)
+                    )
                     nexthop_cache_entry = self._update_nexthop_cache(
-                        now, vlan, None, None, dst_ip)
+                        now, vlan, None, None, dst_ip
+                    )
                     if not resolution_in_progress:
                         resolve_flows = self._resolve_gateway_flows(
-                            dst_ip, nexthop_cache_entry, vlan,
-                            nexthop_cache_entry.cache_time)
+                            dst_ip,
+                            nexthop_cache_entry,
+                            vlan,
+                            nexthop_cache_entry.cache_time,
+                        )
                         ofmsgs.extend(resolve_flows)
         return ofmsgs
 
     def router_vlan_for_ip_gw(self, vlan, ip_gw):
         """Return router VLAN for IP gateway (or None).
 
         Args:
@@ -705,32 +833,38 @@
         Returns:
             list: OpenFlow messages.
         """
         ofmsgs = []
         vlan = self.router_vlan_for_ip_gw(vlan, ip_gw)
         if vlan is None:
             self.logger.error(
-                ('Cannot resolve destination VLAN for gateway %s '
-                 '(not in global router?)' % ip_gw))
+                (
+                    "Cannot resolve destination VLAN for gateway %s "
+                    "(not in global router?)" % ip_gw
+                )
+            )
             return ofmsgs
         if vlan.is_faucet_vip(ip_dst):
             return ofmsgs
         routes = self._vlan_routes(vlan)
         if routes.get(ip_dst, None) == ip_gw:
             return ofmsgs
 
         vlan.add_route(ip_dst, ip_gw)
         cached_eth_dst = self._cached_nexthop_eth_dst(vlan, ip_gw)
         if cached_eth_dst is not None:
-            ofmsgs.extend(self._add_resolved_route(
-                vlan=vlan,
-                ip_gw=ip_gw,
-                ip_dst=ip_dst,
-                eth_dst=cached_eth_dst,
-                is_updated=False))
+            ofmsgs.extend(
+                self._add_resolved_route(
+                    vlan=vlan,
+                    ip_gw=ip_gw,
+                    ip_dst=ip_dst,
+                    eth_dst=cached_eth_dst,
+                    is_updated=False,
+                )
+            )
         return ofmsgs
 
     def _add_host_fib_route(self, vlan, host_ip, blackhole=False):
         """Add a host FIB route.
 
         Args:
             vlan (vlan): VLAN containing this RIB.
@@ -740,20 +874,24 @@
         """
         ofmsgs = []
         if blackhole:
             priority = self._route_priority(host_ip)
             host_int = self._host_ip_to_host_int(host_ip)
             timeout = (
                 self.max_resolve_backoff_time * self.max_host_fib_retry_count
-                + random.randint(0, self.max_resolve_backoff_time * 2))
+                + random.randint(0, self.max_resolve_backoff_time * 2)
+            )
             routed_vlans = self._routed_vlans(vlan)
             for routed_vlan in routed_vlans:
                 in_match = self._route_match(routed_vlan, host_int)
-                ofmsgs.append(self.fib_table.flowmod(
-                    in_match, priority=priority, hard_timeout=timeout))
+                ofmsgs.append(
+                    self.fib_table.flowmod(
+                        in_match, priority=priority, hard_timeout=timeout
+                    )
+                )
         host_route = ipaddress.ip_network(host_ip.exploded)
         ofmsgs.extend(self.add_route(vlan, host_ip, host_route))
         return ofmsgs
 
     def _del_host_fib_route(self, vlan, host_ip):
         """Delete a host FIB route.
 
@@ -783,32 +921,42 @@
             now (float): seconds since epoch.
             pkt_meta (PacketMeta): received packet.
         Returns:
             list: OpenFlow messages.
         """
         src_ip = pkt_meta.l3_src
         ofmsgs = []
-        if (src_ip and pkt_meta.vlan.ip_in_vip_subnet(src_ip)
-                and self._stateful_gw(pkt_meta.vlan, src_ip)):
+        if (
+            src_ip
+            and pkt_meta.vlan.ip_in_vip_subnet(src_ip)
+            and self._stateful_gw(pkt_meta.vlan, src_ip)
+        ):
             ip_pkt = self._ip_pkt(pkt_meta.pkt)
             if ip_pkt:
                 ofmsgs.extend(
-                    self._add_host_fib_route(pkt_meta.vlan, src_ip, blackhole=False))
-                ofmsgs.extend(self._update_nexthop(
-                    now, pkt_meta.vlan, pkt_meta.port, pkt_meta.eth_src, src_ip))
+                    self._add_host_fib_route(pkt_meta.vlan, src_ip, blackhole=False)
+                )
+                ofmsgs.extend(
+                    self._update_nexthop(
+                        now, pkt_meta.vlan, pkt_meta.port, pkt_meta.eth_src, src_ip
+                    )
+                )
         return ofmsgs
 
     def _del_route_flows(self, vlan, ip_dst):
         """Delete all flows matching the vlan and ip_dst"""
         ofmsgs = []
         routed_vlans = self._routed_vlans(vlan)
         for routed_vlan in routed_vlans:
             route_match = self._route_match(routed_vlan, ip_dst)
-            ofmsgs.append(self.fib_table.flowdel(
-                route_match, priority=self._route_priority(ip_dst), strict=True))
+            ofmsgs.append(
+                self.fib_table.flowdel(
+                    route_match, priority=self._route_priority(ip_dst), strict=True
+                )
+            )
         return ofmsgs
 
     def del_route(self, vlan, ip_dst):
         """Delete a route from the RIB.
 
         Only one route with this exact destination is supported.
 
@@ -854,54 +1002,69 @@
 
     def _vlan_nexthop_cache_limit(self, vlan):
         return vlan.proactive_arp_limit
 
     def _add_faucet_vip_nd(self, vlan, priority, faucet_vip, faucet_vip_host):
         ofmsgs = []
         # ARP
-        ofmsgs.extend(self.pipeline.select_packets(
-            self.vip_table,
-            {'eth_type': valve_of.ether.ETH_TYPE_ARP, 'vlan': vlan}
-        ))
+        ofmsgs.extend(
+            self.pipeline.select_packets(
+                self.vip_table, {"eth_type": valve_of.ether.ETH_TYPE_ARP, "vlan": vlan}
+            )
+        )
         # ARP for FAUCET VIP
-        ofmsgs.append(self.vip_table.flowcontroller(
-            self.vip_table.match(
-                eth_type=valve_of.ether.ETH_TYPE_ARP,
-                eth_dst=valve_of.mac.BROADCAST_STR,
-                nw_dst=faucet_vip_host),
-            priority=priority,
-            max_len=valve_packet.VLAN_ARP_PKT_SIZE))
+        ofmsgs.append(
+            self.vip_table.flowcontroller(
+                self.vip_table.match(
+                    eth_type=valve_of.ether.ETH_TYPE_ARP,
+                    eth_dst=valve_of.mac.BROADCAST_STR,
+                    nw_dst=faucet_vip_host,
+                ),
+                priority=priority,
+                max_len=valve_packet.VLAN_ARP_PKT_SIZE,
+            )
+        )
         # ARP reply to FAUCET VIP
-        ofmsgs.append(self.vip_table.flowcontroller(
-            self.vip_table.match(
-                eth_type=valve_of.ether.ETH_TYPE_ARP,
-                eth_dst=vlan.faucet_mac),
-            priority=priority,
-            max_len=valve_packet.VLAN_ARP_PKT_SIZE))
+        ofmsgs.append(
+            self.vip_table.flowcontroller(
+                self.vip_table.match(
+                    eth_type=valve_of.ether.ETH_TYPE_ARP, eth_dst=vlan.faucet_mac
+                ),
+                priority=priority,
+                max_len=valve_packet.VLAN_ARP_PKT_SIZE,
+            )
+        )
         priority -= 1
         # Other ARP
-        ofmsgs.append(self.vip_table.flowmod(
-            self.vip_table.match(
-                eth_type=valve_of.ether.ETH_TYPE_ARP),
-            priority=priority,
-            inst=self.pipeline.accept_to_l2_forwarding()))
+        ofmsgs.append(
+            self.vip_table.flowmod(
+                self.vip_table.match(eth_type=valve_of.ether.ETH_TYPE_ARP),
+                priority=priority,
+                inst=self.pipeline.accept_to_l2_forwarding(),
+            )
+        )
         return ofmsgs
 
     def _control_plane_arp_handler(self, now, pkt_meta):
         """Handle ARP packets destined for the router"""
         ofmsgs = []
         if not pkt_meta.eth_type == valve_of.ether.ETH_TYPE_ARP:
             return ofmsgs
         arp_pkt = pkt_meta.pkt.get_protocol(arp.arp)
         if arp_pkt is None:
             return ofmsgs
         opcode = arp_pkt.opcode
         if opcode == arp.ARP_REQUEST:
-            if pkt_meta.eth_dst in (valve_of.mac.BROADCAST_STR, pkt_meta.vlan.faucet_mac):
-                ofmsgs.extend(self._resolve_vip_response(pkt_meta, pkt_meta.l3_dst, now))
+            if pkt_meta.eth_dst in (
+                valve_of.mac.BROADCAST_STR,
+                pkt_meta.vlan.faucet_mac,
+            ):
+                ofmsgs.extend(
+                    self._resolve_vip_response(pkt_meta, pkt_meta.l3_dst, now)
+                )
         elif opcode == arp.ARP_REPLY:
             if pkt_meta.eth_dst == pkt_meta.vlan.faucet_mac:
                 ofmsgs.extend(self._gw_advert(pkt_meta, pkt_meta.l3_src, now))
         self.notify_learn(pkt_meta)
         return ofmsgs
 
     def _control_plane_icmp_handler(self, now, pkt_meta, ipv4_pkt):
@@ -913,18 +1076,23 @@
             pkt_meta.reparse_all()
             icmp_pkt = pkt_meta.pkt.get_protocol(icmp.icmp)
             if icmp_pkt is None:
                 return ofmsgs
             if icmp_pkt.type == icmp.ICMP_ECHO_REQUEST:
                 ofmsgs.append(
                     pkt_meta.vlan.pkt_out_port(
-                        valve_packet.echo_reply, pkt_meta.port,
-                        pkt_meta.vlan.faucet_mac, pkt_meta.eth_src,
-                        pkt_meta.l3_dst, pkt_meta.l3_src,
-                        icmp_pkt.data))
+                        valve_packet.echo_reply,
+                        pkt_meta.port,
+                        pkt_meta.vlan.faucet_mac,
+                        pkt_meta.eth_src,
+                        pkt_meta.l3_dst,
+                        pkt_meta.l3_src,
+                        icmp_pkt.data,
+                    )
+                )
                 # ping but no previous ARP request for FAUCET VIP
                 # from this host. Missed ARP request or host has
                 # static ARP entry for us?
                 if self._cached_nexthop_eth_dst(pkt_meta.vlan, pkt_meta.l3_src) is None:
                     ofmsgs.extend(self.add_host_fib_route_from_pkt(now, pkt_meta))
         return ofmsgs
 
@@ -933,16 +1101,15 @@
         if pkt_meta.packet_complete():
             arp_replies = self._control_plane_arp_handler(now, pkt_meta)
             if arp_replies:
                 return arp_replies
             ipv4_pkt = self._ip_pkt(pkt_meta.pkt)
             if ipv4_pkt is None:
                 return []
-            icmp_replies = self._control_plane_icmp_handler(
-                now, pkt_meta, ipv4_pkt)
+            icmp_replies = self._control_plane_icmp_handler(now, pkt_meta, ipv4_pkt)
             if icmp_replies:
                 return icmp_replies
         return super().control_plane_handler(now, pkt_meta)
 
 
 class ValveIPv6RouteManager(ValveRouteManager):
     """Implement IPv6 FIB."""
@@ -963,77 +1130,100 @@
         return valve_packet.nd_advert
 
     def _vlan_nexthop_cache_limit(self, vlan):
         return vlan.proactive_nd_limit
 
     def _add_faucet_vip_nd(self, vlan, priority, faucet_vip, faucet_vip_host):
         faucet_vip_host_nd_mcast = valve_packet.ipv6_link_eth_mcast(
-            valve_packet.ipv6_solicited_node_from_ucast(faucet_vip.ip))
+            valve_packet.ipv6_solicited_node_from_ucast(faucet_vip.ip)
+        )
         ofmsgs = []
         # RA if this is a link local FAUCET VIP
         if faucet_vip.ip.is_link_local:
             match = {
-                'eth_type': self.ETH_TYPE,
-                'eth_dst': valve_packet.IPV6_ALL_ROUTERS_MCAST,
-                'vlan': vlan
+                "eth_type": self.ETH_TYPE,
+                "eth_dst": valve_packet.IPV6_ALL_ROUTERS_MCAST,
+                "vlan": vlan,
             }
             ofmsgs.extend(self.pipeline.select_packets(self.vip_table, match))
-            ofmsgs.append(self.vip_table.flowmod(
+            ofmsgs.append(
+                self.vip_table.flowmod(
+                    self.vip_table.match(
+                        eth_type=self.ETH_TYPE,
+                        eth_dst=valve_packet.IPV6_ALL_ROUTERS_MCAST,
+                        nw_proto=valve_of.inet.IPPROTO_ICMPV6,
+                        icmpv6_type=icmpv6.ND_ROUTER_SOLICIT,
+                    ),
+                    priority=priority,
+                    inst=self._controller_and_flood(),
+                )
+            )
+        # IPv6 ping unicast to FAUCET
+        ofmsgs.append(
+            self.vip_table.flowcontroller(
                 self.vip_table.match(
                     eth_type=self.ETH_TYPE,
-                    eth_dst=valve_packet.IPV6_ALL_ROUTERS_MCAST,
+                    eth_dst=vlan.faucet_mac,
                     nw_proto=valve_of.inet.IPPROTO_ICMPV6,
-                    icmpv6_type=icmpv6.ND_ROUTER_SOLICIT),
+                    icmpv6_type=icmpv6.ICMPV6_ECHO_REQUEST,
+                ),
                 priority=priority,
-                inst=self._controller_and_flood()))
-        # IPv6 ping unicast to FAUCET
-        ofmsgs.append(self.vip_table.flowcontroller(
-            self.vip_table.match(
-                eth_type=self.ETH_TYPE,
-                eth_dst=vlan.faucet_mac,
-                nw_proto=valve_of.inet.IPPROTO_ICMPV6,
-                icmpv6_type=icmpv6.ICMPV6_ECHO_REQUEST),
-            priority=priority,
-            max_len=self.ICMP_SIZE))
+                max_len=self.ICMP_SIZE,
+            )
+        )
         # IPv6 NA unicast to FAUCET.
-        ofmsgs.append(self.vip_table.flowcontroller(
-            self.vip_table.match(
-                eth_type=self.ETH_TYPE,
-                eth_dst=vlan.faucet_mac,
-                nw_proto=valve_of.inet.IPPROTO_ICMPV6,
-                icmpv6_type=icmpv6.ND_NEIGHBOR_ADVERT),
-            priority=priority,
-            max_len=self.ICMP_SIZE))
+        ofmsgs.append(
+            self.vip_table.flowcontroller(
+                self.vip_table.match(
+                    eth_type=self.ETH_TYPE,
+                    eth_dst=vlan.faucet_mac,
+                    nw_proto=valve_of.inet.IPPROTO_ICMPV6,
+                    icmpv6_type=icmpv6.ND_NEIGHBOR_ADVERT,
+                ),
+                priority=priority,
+                max_len=self.ICMP_SIZE,
+            )
+        )
         # IPv6 NS for FAUCET VIP
         match = {
-            'eth_type': self.ETH_TYPE,
-            'eth_dst': faucet_vip_host_nd_mcast,
-            'vlan': vlan
+            "eth_type": self.ETH_TYPE,
+            "eth_dst": faucet_vip_host_nd_mcast,
+            "vlan": vlan,
         }
         ofmsgs.extend(self.pipeline.select_packets(self.vip_table, match))
-        ofmsgs.append(self.vip_table.flowmod(
-            self.vip_table.match(
-                eth_type=self.ETH_TYPE,
-                eth_dst=faucet_vip_host_nd_mcast,
-                nw_proto=valve_of.inet.IPPROTO_ICMPV6,
-                icmpv6_type=icmpv6.ND_NEIGHBOR_SOLICIT),
-            priority=priority,
-            inst=self._controller_and_flood()))
+        ofmsgs.append(
+            self.vip_table.flowmod(
+                self.vip_table.match(
+                    eth_type=self.ETH_TYPE,
+                    eth_dst=faucet_vip_host_nd_mcast,
+                    nw_proto=valve_of.inet.IPPROTO_ICMPV6,
+                    icmpv6_type=icmpv6.ND_NEIGHBOR_SOLICIT,
+                ),
+                priority=priority,
+                inst=self._controller_and_flood(),
+            )
+        )
         return ofmsgs
 
     def _add_faucet_fib_to_vip(self, vlan, priority, faucet_vip, faucet_vip_host):
         ofmsgs = super()._add_faucet_fib_to_vip(
-            vlan, priority, faucet_vip, faucet_vip_host)
-        faucet_vip_broadcast = ipaddress.IPv6Interface(faucet_vip.network.broadcast_address)
+            vlan, priority, faucet_vip, faucet_vip_host
+        )
+        faucet_vip_broadcast = ipaddress.IPv6Interface(
+            faucet_vip.network.broadcast_address
+        )
         if self.global_routing:
             vlan = self.global_vlan
-        ofmsgs.append(self.fib_table.flowmod(
-            self._route_match(vlan, faucet_vip_broadcast),
-            priority=priority,
-            inst=(self.fib_table.goto(self.vip_table),)))
+        ofmsgs.append(
+            self.fib_table.flowmod(
+                self._route_match(vlan, faucet_vip_broadcast),
+                priority=priority,
+                inst=(self.fib_table.goto(self.vip_table),),
+            )
+        )
         return ofmsgs
 
     def _nd_solicit_handler(self, now, pkt_meta, _ipv6_pkt, icmpv6_pkt):
         ofmsgs = []
         solicited_ip = ipaddress.ip_address(icmpv6_pkt.data.dst)
         ofmsgs.extend(self._resolve_vip_response(pkt_meta, solicited_ip, now))
         self.notify_learn(pkt_meta)
@@ -1049,33 +1239,47 @@
     def _router_solicit_handler(self, _now, pkt_meta, _ipv6_pkt, _icmpv6_pkt):
         ofmsgs = []
         link_local_vips, other_vips = pkt_meta.vlan.link_and_other_vips(self.IPV)
         for vip in link_local_vips:
             if pkt_meta.l3_src in vip.network:
                 ofmsgs.append(
                     pkt_meta.vlan.pkt_out_port(
-                        valve_packet.router_advert, pkt_meta.port,
-                        pkt_meta.vlan.faucet_mac, pkt_meta.eth_src,
-                        vip.ip, pkt_meta.l3_src, other_vips))
+                        valve_packet.router_advert,
+                        pkt_meta.port,
+                        pkt_meta.vlan.faucet_mac,
+                        pkt_meta.eth_src,
+                        vip.ip,
+                        pkt_meta.l3_src,
+                        other_vips,
+                    )
+                )
                 self.logger.info(
-                    'Responded to RS solicit from %s (%s)' % (
-                        pkt_meta.l3_src, pkt_meta.log()))
+                    "Responded to RS solicit from %s (%s)"
+                    % (pkt_meta.l3_src, pkt_meta.log())
+                )
                 break
         return ofmsgs
 
     def _echo_request_handler(self, now, pkt_meta, ipv6_pkt, icmpv6_pkt):
         ofmsgs = []
         if self._unicast_to_vip(pkt_meta):
             ofmsgs.append(
                 pkt_meta.vlan.pkt_out_port(
-                    valve_packet.icmpv6_echo_reply, pkt_meta.port,
-                    pkt_meta.vlan.faucet_mac, pkt_meta.eth_src,
-                    pkt_meta.l3_dst, pkt_meta.l3_src, ipv6_pkt.hop_limit,
-                    icmpv6_pkt.data.id, icmpv6_pkt.data.seq,
-                    icmpv6_pkt.data.data))
+                    valve_packet.icmpv6_echo_reply,
+                    pkt_meta.port,
+                    pkt_meta.vlan.faucet_mac,
+                    pkt_meta.eth_src,
+                    pkt_meta.l3_dst,
+                    pkt_meta.l3_src,
+                    ipv6_pkt.hop_limit,
+                    icmpv6_pkt.data.id,
+                    icmpv6_pkt.data.seq,
+                    icmpv6_pkt.data.data,
+                )
+            )
             # ping but no previous ND request for FAUCET VIP
             # from this host. Missed ND request or host has
             # static ND entry for us?
             if self._cached_nexthop_eth_dst(pkt_meta.vlan, pkt_meta.l3_src) is None:
                 ofmsgs.extend(self.add_host_fib_route_from_pkt(now, pkt_meta))
         return ofmsgs
 
@@ -1100,44 +1304,53 @@
             return ofmsgs
         reparse_size = 32
         pkt_meta.reparse_ip(payload=reparse_size)
         icmpv6_pkt = pkt_meta.pkt.get_protocol(icmpv6.icmpv6)
         if icmpv6_pkt is None:
             return ofmsgs
         icmpv6_type = icmpv6_pkt.type_
-        if (ipv6_pkt.hop_limit != valve_packet.IPV6_MAX_HOP_LIM
-                and icmpv6_type != icmpv6.ICMPV6_ECHO_REQUEST):
+        if (
+            ipv6_pkt.hop_limit != valve_packet.IPV6_MAX_HOP_LIM
+            and icmpv6_type != icmpv6.ICMPV6_ECHO_REQUEST
+        ):
             return ofmsgs
         handler, payload_type, type_reparse_size = self._icmpv6_handlers.get(
-            icmpv6_type, (None, None, None))
+            icmpv6_type, (None, None, None)
+        )
         if handler is not None and (
-                payload_type is None
-                or isinstance(icmpv6_pkt.data, payload_type)):
+            payload_type is None or isinstance(icmpv6_pkt.data, payload_type)
+        ):
             if type_reparse_size != reparse_size:
                 pkt_meta.reparse_ip(payload=type_reparse_size)
                 icmpv6_pkt = pkt_meta.pkt.get_protocol(icmpv6.icmpv6)
             ofmsgs = handler(self, now, pkt_meta, ipv6_pkt, icmpv6_pkt)
         return ofmsgs
 
     def control_plane_handler(self, now, pkt_meta):
         """Resolve packets destined for router or proactively learn host information"""
         if pkt_meta.packet_complete():
             ipv6_pkt = self._ip_pkt(pkt_meta.pkt)
             if ipv6_pkt is not None:
                 icmp_replies = self._control_plane_icmpv6_handler(
-                    now, pkt_meta, ipv6_pkt)
+                    now, pkt_meta, ipv6_pkt
+                )
                 if icmp_replies:
                     return icmp_replies
         return super().control_plane_handler(now, pkt_meta)
 
     def advertise(self, vlan):
         ofmsgs = []
         link_local_vips, other_vips = vlan.link_and_other_vips(self.IPV)
         for link_local_vip in link_local_vips:
             # https://tools.ietf.org/html/rfc4861#section-6.1.2
-            ofmsgs.extend(vlan.flood_pkt(
-                valve_packet.router_advert, self.multi_out,
-                vlan.faucet_mac,
-                valve_packet.IPV6_ALL_NODES_MCAST,
-                link_local_vip.ip, valve_packet.IPV6_ALL_NODES,
-                other_vips))
+            ofmsgs.extend(
+                vlan.flood_pkt(
+                    valve_packet.router_advert,
+                    self.multi_out,
+                    vlan.faucet_mac,
+                    valve_packet.IPV6_ALL_NODES_MCAST,
+                    link_local_vip.ip,
+                    valve_packet.IPV6_ALL_NODES,
+                    other_vips,
+                )
+            )
         return ofmsgs
```

### Comparing `c65faucet-1.0.46/faucet/valve_ryuapp.py` & `c65faucet-1.0.47/faucet/valve_ryuapp.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/valve_stack.py` & `c65faucet-1.0.47/faucet/valve_stack.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,17 +4,19 @@
 from collections import defaultdict
 
 from faucet.valve_manager_base import ValveManagerBase
 
 
 class ValveStackManager(ValveManagerBase):
     """Implement stack manager, this handles the more higher-order stack functions.
-This includes port nominations and flood directionality."""
+    This includes port nominations and flood directionality."""
 
-    def __init__(self, logger, dp, stack, tunnel_acls, acl_manager, output_table, **_kwargs):
+    def __init__(
+        self, logger, dp, stack, tunnel_acls, acl_manager, output_table, **_kwargs
+    ):
         """
         Initialize variables and set up peer distances
 
         Args:
             stack (Stack): Stack object of the DP on the Valve being managed
         """
         # Logger for logging
@@ -44,15 +46,17 @@
         self.pruned_away_ports = None
 
         self.reset_peer_distances()
 
     @staticmethod
     def stacked_valves(valves):
         """Return set of valves that have stacking enabled"""
-        return {valve for valve in valves if valve.dp.stack and valve.dp.stack.root_name}
+        return {
+            valve for valve in valves if valve.dp.stack and valve.dp.stack.root_name
+        }
 
     def reset_peer_distances(self):
         """Recalculates the towards and away ports for this node"""
         self.towards_root_ports = set()
         self.chosen_towards_ports = set()
         self.chosen_towards_port = None
 
@@ -61,66 +65,82 @@
         self.pruned_away_ports = set()
 
         all_peer_ports = set(self.stack.canonical_up_ports())
         if self.stack.is_root():
             self.away_ports = all_peer_ports
         else:
             port_peer_distances = {
-                port: len(port.stack['dp'].stack.shortest_path_to_root())
-                for port in all_peer_ports}
+                port: len(port.stack["dp"].stack.shortest_path_to_root())
+                for port in all_peer_ports
+            }
             shortest_peer_distance = None
             for port, port_peer_distance in port_peer_distances.items():
                 if shortest_peer_distance is None:
                     shortest_peer_distance = port_peer_distance
                     continue
                 shortest_peer_distance = min(shortest_peer_distance, port_peer_distance)
             self.towards_root_ports = {
-                port for port, port_peer_distance in port_peer_distances.items()
-                if port_peer_distance == shortest_peer_distance}
+                port
+                for port, port_peer_distance in port_peer_distances.items()
+                if port_peer_distance == shortest_peer_distance
+            }
 
             self.away_ports = all_peer_ports - self.towards_root_ports
 
             if self.towards_root_ports:
                 # Generate a shortest path to calculate the chosen connection to root
                 shortest_path = self.stack.shortest_path_to_root()
                 # Choose the port that is connected to peer DP
                 if shortest_path and len(shortest_path) > 1:
                     first_peer_dp = shortest_path[1]
                 else:
                     first_peer_port = self.stack.canonical_port_order(
-                        self.towards_root_ports)[0]
-                    first_peer_dp = first_peer_port.stack['dp'].name
+                        self.towards_root_ports
+                    )[0]
+                    first_peer_dp = first_peer_port.stack["dp"].name
                 # The chosen towards ports are the ports through the chosen peer DP
                 self.chosen_towards_ports = {
-                    port for port in self.towards_root_ports
-                    if port.stack['dp'].name == first_peer_dp}  # pytype: disable=attribute-error
+                    port
+                    for port in self.towards_root_ports
+                    if port.stack["dp"].name == first_peer_dp
+                }  # pytype: disable=attribute-error
 
             if self.chosen_towards_ports:
                 self.chosen_towards_port = self.stack.canonical_up_ports(
-                    self.chosen_towards_ports)[0]
+                    self.chosen_towards_ports
+                )[0]
 
             # Away ports are all the remaining (non-towards) ports
             self.away_ports = all_peer_ports - self.towards_root_ports
 
         if self.away_ports:
             # Get inactive away ports, ports whose peers have a better path to root
             self.inactive_away_ports = {
-                port for port in self.away_ports
-                if not self.stack.is_in_path(port.stack['dp'].name, self.stack.root_name)}
+                port
+                for port in self.away_ports
+                if not self.stack.is_in_path(
+                    port.stack["dp"].name, self.stack.root_name
+                )
+            }
 
             # Get pruned away ports, redundant ports for each adjacent DP
             ports_by_dp = defaultdict(list)
             for port in self.away_ports:
-                ports_by_dp[port.stack['dp']].append(port)
+                ports_by_dp[port.stack["dp"]].append(port)
             for ports in ports_by_dp.values():
                 remote_away_ports = self.stack.canonical_up_ports(
-                    [port.stack['port'] for port in ports])
-                self.pruned_away_ports.update([
-                    port.stack['port'] for port in remote_away_ports
-                    if port != remote_away_ports[0]])
+                    [port.stack["port"] for port in ports]
+                )
+                self.pruned_away_ports.update(
+                    [
+                        port.stack["port"]
+                        for port in remote_away_ports
+                        if port != remote_away_ports[0]
+                    ]
+                )
 
         return self.chosen_towards_ports
 
     def update_stack_topo(self, event, dp, port):
         """
         Update the stack topo according to the event.
 
@@ -128,17 +148,17 @@
             event (bool): True if the port is UP
             dp (DP): DP object
             port (Port): The port being brought UP/DOWN
         """
         self.stack.modify_link(dp, port, event)
         towards_ports = self.reset_peer_distances()
         if towards_ports:
-            self.logger.info('shortest path to root is via %s' % towards_ports)
+            self.logger.info("shortest path to root is via %s" % towards_ports)
         else:
-            self.logger.info('no path available to root')
+            self.logger.info("no path available to root")
 
     def default_port_towards(self, dp_name):
         """
         Default shortest path towards the provided destination, via direct shortest path
 
         Args:
             dp_name (str): Destination DP
@@ -164,15 +184,15 @@
             # Current node is the destination node, use default
             return self.default_port_towards(dp_name)
         path_to_root = self.stack.shortest_path_to_root(dp_name)
         if path_to_root and self.stack.name in path_to_root:
             # Current node is a transit node between root & destination, direct path to destination
             away_dp = path_to_root[path_to_root.index(self.stack.name) - 1]
             for port in self.away_ports:
-                if port.stack['dp'].name == away_dp and not self.is_pruned_port(port):
+                if port.stack["dp"].name == away_dp and not self.is_pruned_port(port):
                     return port
             return None
         # Otherwise, head towards the root, path to destination via root
         return self.chosen_towards_port
 
     def edge_learn_port_towards(self, pkt_meta, edge_dp):
         """
@@ -220,23 +240,26 @@
             now (float): Current time
             last_live_times (dict): Last live time value for each DP
             update_time (int): Stack root update interval time
         Returns:
             bool: True if current stack node is healthy
         """
         prev_health = self.stack.dyn_healthy_info
-        new_health, reason = self.stack.update_health(
-            now, last_live_times, update_time)
+        new_health, reason = self.stack.update_health(now, last_live_times, update_time)
         if prev_health != self.stack.dyn_healthy_info:
-            health = 'HEALTHY' if new_health else 'UNHEALTHY'
-            self.logger.info('Stack node %s %s (%s)' % (self.stack.name, health, reason))
+            health = "HEALTHY" if new_health else "UNHEALTHY"
+            self.logger.info(
+                "Stack node %s %s (%s)" % (self.stack.name, health, reason)
+            )
         return new_health
 
     @staticmethod
-    def nominate_stack_root(root_valve, other_valves, now, last_live_times, update_time):
+    def nominate_stack_root(
+        root_valve, other_valves, now, last_live_times, update_time
+    ):
         """
         Nominate a new stack root
 
         Args:
             root_valve (Valve): Previous/current root Valve object
             other_valves (list): List of other valves (not including previous root)
             now (float): Current time
@@ -250,15 +273,17 @@
             stack_valves = {root_valve}.union(stack_valves)
 
         # Create lists of healthy and unhealthy root candidates
         healthy_valves = []
         unhealthy_valves = []
         for valve in stack_valves:
             if valve.dp.stack.is_root_candidate():
-                healthy = valve.stack_manager.update_health(now, last_live_times, update_time)
+                healthy = valve.stack_manager.update_health(
+                    now, last_live_times, update_time
+                )
                 if healthy:
                     healthy_valves.append(valve)
                 elif valve.dp.stack.dyn_healthy_info[0]:
                     unhealthy_valves.append(valve)
 
         if not healthy_valves and not unhealthy_valves:
             # No root candidates/stack valves, so no nomination
@@ -325,74 +350,81 @@
             if self.pruned_away_ports:
                 return port in self.pruned_away_ports
             return False
         return True
 
     def adjacent_stack_ports(self, peer_dp):
         """Return list of ports that connect to an adjacent DP"""
-        return [port for port in self.stack.ports if port.stack['dp'] == peer_dp]
+        return [port for port in self.stack.ports if port.stack["dp"] == peer_dp]
 
     def acl_update_tunnel(self, acl):
         """Return ofmsgs for all tunnels in an ACL with a tunnel rule"""
         ofmsgs = []
         source_vids = defaultdict(list)
         for _id, tunnel_dest in acl.tunnel_dests.items():
-            dst_dp, dst_port = tunnel_dest['dst_dp'], tunnel_dest['dst_port']
+            dst_dp, dst_port = tunnel_dest["dst_dp"], tunnel_dest["dst_port"]
             # Update the tunnel rules for each tunnel action specified
             updated_sources = []
             updated_reverse_sources = []
             for source_id, source in acl.tunnel_sources.items():
                 # We loop through each tunnel source in a single ACL instance and update the info
-                src_dp, src_port = source['dp'], source['port']
-                in_port = self.tunnel_outport(
-                    dst_dp, src_dp, src_port)
-                out_port = self.tunnel_outport(
-                    src_dp, dst_dp, dst_port)
+                src_dp, src_port = source["dp"], source["port"]
+                in_port = self.tunnel_outport(dst_dp, src_dp, src_port)
+                out_port = self.tunnel_outport(src_dp, dst_dp, dst_port)
                 updated = False
                 if out_port is None and dst_port is None and dst_dp == self.dp.name:
                     # Will need to update at most once, to ensure the correct rules
                     # get populated in the destination DP for a tunnel that outputs
                     # to just a DP
                     updated = acl.update_source_tunnel_rules(
-                        self.stack.name, source_id, _id, out_port, self.output_table)
+                        self.stack.name, source_id, _id, out_port, self.output_table
+                    )
                 elif out_port:
                     updated = acl.update_source_tunnel_rules(
-                        self.stack.name, source_id, _id, out_port, self.output_table)
+                        self.stack.name, source_id, _id, out_port, self.output_table
+                    )
                 if updated:
                     if self.stack.name == src_dp:
                         # We need to re-build and apply the whole ACL
                         source_vids[source_id].append(_id)
                     else:
                         # We only need to re-build and apply the tunnel
                         updated_sources.append(source_id)
                 reverse_updated = False
                 if src_port is None and in_port is None and src_dp == self.dp.name:
                     reverse_updated = acl.update_reverse_tunnel_rules(
-                        self.stack.name, source_id, _id, in_port, self.output_table)
+                        self.stack.name, source_id, _id, in_port, self.output_table
+                    )
                 elif in_port:
                     reverse_updated = acl.update_reverse_tunnel_rules(
-                        self.stack.name, source_id, _id, in_port, self.output_table)
+                        self.stack.name, source_id, _id, in_port, self.output_table
+                    )
                 if reverse_updated:
                     if acl.requires_reverse_tunnel(_id):
                         # Update the reverse tunnel rules if the tunnel is configured to have them
                         updated_reverse_sources.append(source_id)
             # The tunnel in the ACL does not have a source on this stack instance, so
             #   we only need to re-build the special tunnel forwarding rule.
             for source_id in updated_sources:
-                ofmsgs.extend(self.acl_manager.build_tunnel_rules_ofmsgs(
-                    source_id, _id, acl))
+                ofmsgs.extend(
+                    self.acl_manager.build_tunnel_rules_ofmsgs(source_id, _id, acl)
+                )
             for source_id in updated_reverse_sources:
-                ofmsgs.extend(self.acl_manager.build_reverse_tunnel_rules_ofmsgs(
-                    source_id, _id, acl))
+                ofmsgs.extend(
+                    self.acl_manager.build_reverse_tunnel_rules_ofmsgs(
+                        source_id, _id, acl
+                    )
+                )
         # If a tunnel is updated, but the source is configured as the current DP
         #   then we will also need to re-build the rest of the ACL rules aswell.
         for source_id, vids in source_vids.items():
             for vid in vids:
-                ofmsgs.extend(self.acl_manager.build_tunnel_acl_rule_ofmsgs(
-                    source_id, vid, acl))
+                ofmsgs.extend(
+                    self.acl_manager.build_tunnel_acl_rule_ofmsgs(source_id, vid, acl)
+                )
         return ofmsgs
 
     def add_tunnel_acls(self):
         """Returns ofmsgs installing the tunnel path rules"""
         ofmsgs = []
         if self.tunnel_acls:
             for acl in self.tunnel_acls:
```

### Comparing `c65faucet-1.0.46/faucet/valve_switch.py` & `c65faucet-1.0.47/faucet/valve_switch.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/faucet/valve_switch_stack.py` & `c65faucet-1.0.47/faucet/valve_switch_stack.py`

 * *Files 8% similar despite different names*

```diff
@@ -35,20 +35,26 @@
 
         self.stack_manager = stack_manager
 
         self._set_ext_port_flag = ()
         self._set_nonext_port_flag = ()
         self.external_root_only = False
         if self.has_externals:
-            self.logger.info('external ports present, using loop protection')
-            self._set_ext_port_flag = (self.flood_table.set_external_forwarding_requested(),)
-            self._set_nonext_port_flag = (self.flood_table.set_no_external_forwarding_requested(),)
-            if (not self.stack_manager.stack.is_root()
-                    and self.stack_manager.stack.is_root_candidate()):
-                self.logger.info('external flooding on root only')
+            self.logger.info("external ports present, using loop protection")
+            self._set_ext_port_flag = (
+                self.flood_table.set_external_forwarding_requested(),
+            )
+            self._set_nonext_port_flag = (
+                self.flood_table.set_no_external_forwarding_requested(),
+            )
+            if (
+                not self.stack_manager.stack.is_root()
+                and self.stack_manager.stack.is_root_candidate()
+            ):
+                self.logger.info("external flooding on root only")
                 self.external_root_only = True
 
     @staticmethod
     def _non_stack_learned(other_valves, pkt_meta):
         """
         Obtain DP that has learnt the host that sent the packet
 
@@ -89,141 +95,246 @@
         if self.has_externals:
             if port.tagged_vlans and port.loop_protect_external:
                 external_forwarding_requested = False
             elif not port.stack:
                 external_forwarding_requested = True
         return external_forwarding_requested
 
-    def _build_flood_acts_for_port(self, vlan, exclude_unicast, port,  # pylint: disable=too-many-arguments
-                                   exclude_all_external=False,
-                                   exclude_restricted_bcast_arpnd=False):
+    def _build_flood_acts_for_port(
+        self,
+        vlan,
+        exclude_unicast,
+        port,
+        exclude_all_external=False,
+        exclude_restricted_bcast_arpnd=False,
+    ):  # pylint: disable=too-many-arguments
         if self.external_root_only:
             exclude_all_external = True
         return super()._build_flood_acts_for_port(
-            vlan, exclude_unicast, port,
+            vlan,
+            exclude_unicast,
+            port,
             exclude_all_external=exclude_all_external,
-            exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd)
+            exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd,
+        )
 
-    def _flood_actions(self, in_port, external_ports,  # pylint: disable=too-many-arguments
-                       away_flood_actions, toward_flood_actions, local_flood_actions):
+    def _flood_actions(
+        self,
+        in_port,
+        external_ports,
+        away_flood_actions,
+        toward_flood_actions,
+        local_flood_actions,
+    ):  # pylint: disable=too-many-arguments
         raise NotImplementedError
 
-    def _build_flood_rule_actions(self, vlan, exclude_unicast, in_port,  # pylint: disable=too-many-arguments
-                                  exclude_all_external=False, exclude_restricted_bcast_arpnd=False):
+    def _build_flood_rule_actions(
+        self,
+        vlan,
+        exclude_unicast,
+        in_port,
+        exclude_all_external=False,
+        exclude_restricted_bcast_arpnd=False,
+    ):  # pylint: disable=too-many-arguments
         """Compiles all the possible flood rule actions for a port on a stack node"""
         exclude_ports = list(self.stack_manager.inactive_away_ports)
         external_ports = vlan.loop_protect_external_ports()
         if in_port and self.stack_manager.is_stack_port(in_port):
-            in_port_peer_dp = in_port.stack['dp']
-            exclude_ports = exclude_ports + self.stack_manager.adjacent_stack_ports(in_port_peer_dp)
-        local_flood_actions = tuple(self._build_flood_local_rule_actions(
-            vlan, exclude_unicast, in_port, exclude_all_external, exclude_restricted_bcast_arpnd))
-        away_flood_actions = tuple(valve_of.flood_tagged_port_outputs(
-            self.stack_manager.away_ports, in_port, exclude_ports=exclude_ports))
-        toward_flood_actions = tuple(valve_of.flood_tagged_port_outputs(
-            self.stack_manager.chosen_towards_ports, in_port))
+            in_port_peer_dp = in_port.stack["dp"]
+            exclude_ports = exclude_ports + self.stack_manager.adjacent_stack_ports(
+                in_port_peer_dp
+            )
+        local_flood_actions = tuple(
+            self._build_flood_local_rule_actions(
+                vlan,
+                exclude_unicast,
+                in_port,
+                exclude_all_external,
+                exclude_restricted_bcast_arpnd,
+            )
+        )
+        away_flood_actions = tuple(
+            valve_of.flood_tagged_port_outputs(
+                self.stack_manager.away_ports, in_port, exclude_ports=exclude_ports
+            )
+        )
+        toward_flood_actions = tuple(
+            valve_of.flood_tagged_port_outputs(
+                self.stack_manager.chosen_towards_ports, in_port
+            )
+        )
         flood_acts = self._flood_actions(
-            in_port, external_ports, away_flood_actions,
-            toward_flood_actions, local_flood_actions)
+            in_port,
+            external_ports,
+            away_flood_actions,
+            toward_flood_actions,
+            local_flood_actions,
+        )
         return flood_acts
 
-    def _build_mask_flood_rules_filters(self, port, vlan, eth_dst, eth_dst_mask, prune):
+    def _build_mask_flood_rules_filters(
+        self, port, vlan, eth_dst, eth_dst_mask, prune
+    ):  # pylint: disable=too-many-arguments
         """Builds filter for the input table to filter packets on ports that are pruned"""
         ofmsgs = []
 
-        match = {'in_port': port.number, 'vlan': vlan}
+        match = {"in_port": port.number, "vlan": vlan}
         if eth_dst is not None:
-            match.update({'eth_dst': eth_dst, 'eth_dst_mask': eth_dst_mask})
+            match.update({"eth_dst": eth_dst, "eth_dst_mask": eth_dst_mask})
 
-        replace_priority_offset = (self.classification_offset - (
-            self.pipeline.filter_priority - self.pipeline.select_priority))
+        replace_priority_offset = self.classification_offset - (
+            self.pipeline.filter_priority - self.pipeline.select_priority
+        )
 
         priority_offset = replace_priority_offset
         if eth_dst is None:
             priority_offset -= 1
 
         if prune:
             # Allow the prune rule to be replaced with OF strict matching if
             # this port is unpruned later.
-            ofmsgs.extend(self.pipeline.filter_packets(
-                match, priority_offset=priority_offset))
+            ofmsgs.extend(
+                self.pipeline.filter_packets(match, priority_offset=priority_offset)
+            )
         else:
-            ofmsgs.extend(self.pipeline.remove_filter(
-                match, priority_offset=priority_offset))
+            ofmsgs.extend(
+                self.pipeline.remove_filter(match, priority_offset=priority_offset)
+            )
             # Control learning from multicast/broadcast on non-root DPs.
-            if (not self.stack_manager.stack.is_root()
-                    and eth_dst is not None and self._USES_REFLECTION):
+            if (
+                not self.stack_manager.stack.is_root()
+                and eth_dst is not None
+                and self._USES_REFLECTION
+            ):
                 # If this is an edge DP, we don't have to learn from
                 # hosts that only broadcast.  If we're an intermediate
                 # DP, only learn from broadcasts further away from
                 # the root (and ignore the reflected broadcast for
                 # learning purposes).
-                if self.stack_manager.stack.is_edge() or self.stack_manager.is_towards_root(port):
-                    ofmsgs.extend(self.pipeline.select_packets(
-                        self.flood_table, match,
-                        priority_offset=self.classification_offset))
+                if (
+                    self.stack_manager.stack.is_edge()
+                    or self.stack_manager.is_towards_root(port)
+                ):
+                    ofmsgs.extend(
+                        self.pipeline.select_packets(
+                            self.flood_table,
+                            match,
+                            priority_offset=self.classification_offset,
+                        )
+                    )
         return ofmsgs
 
-    def _build_mask_flood_rules_flood_acts(self, vlan, eth_type, eth_dst, eth_dst_mask,
-                                           exclude_unicast, exclude_restricted_bcast_arpnd,
-                                           command, cold_start, prune, port):  # pylint: disable=unused-argument
+    def _build_mask_flood_rules_flood_acts(
+        self,
+        vlan,
+        eth_type,
+        eth_dst,
+        eth_dst_mask,
+        exclude_unicast,
+        exclude_restricted_bcast_arpnd,
+        command,
+        cold_start,
+        prune,
+        port,
+    ):  # pylint: disable=unused-argument,disable=too-many-arguments
         """Builds the flood rules for the flood table to forward packets along the stack topology"""
         ofmsgs = []
         flood_acts = []
         if self.has_externals:
             # If external flag is set, flood to external ports, otherwise exclude them.
             for ext_port_flag, exclude_all_external in (
-                    (valve_of.PCP_NONEXT_PORT_FLAG, True),
-                    (valve_of.PCP_EXT_PORT_FLAG, False)):
+                (valve_of.PCP_NONEXT_PORT_FLAG, True),
+                (valve_of.PCP_EXT_PORT_FLAG, False),
+            ):
                 if not prune:
                     flood_acts, _, _ = self._build_flood_acts_for_port(
-                        vlan, exclude_unicast, port,
+                        vlan,
+                        exclude_unicast,
+                        port,
                         exclude_all_external=exclude_all_external,
-                        exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd)
+                        exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd,
+                    )
                 port_flood_ofmsg = self._build_flood_rule_for_port(
-                    vlan, eth_type, eth_dst, eth_dst_mask, command, port, flood_acts,
-                    add_match={valve_of.EXTERNAL_FORWARDING_FIELD: ext_port_flag})
+                    vlan,
+                    eth_type,
+                    eth_dst,
+                    eth_dst_mask,
+                    command,
+                    port,
+                    flood_acts,
+                    add_match={valve_of.EXTERNAL_FORWARDING_FIELD: ext_port_flag},
+                )
                 ofmsgs.append(port_flood_ofmsg)
         else:
             if not prune:
                 flood_acts, _, _ = self._build_flood_acts_for_port(
-                    vlan, exclude_unicast, port,
-                    exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd)
+                    vlan,
+                    exclude_unicast,
+                    port,
+                    exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd,
+                )
             port_flood_ofmsg = self._build_flood_rule_for_port(
-                vlan, eth_type, eth_dst, eth_dst_mask, command, port, flood_acts)
+                vlan, eth_type, eth_dst, eth_dst_mask, command, port, flood_acts
+            )
             ofmsgs.append(port_flood_ofmsg)
         return ofmsgs
 
-    def _build_mask_flood_rules(self, vlan, eth_type, eth_dst, eth_dst_mask,  # pylint: disable=too-many-arguments
-                                exclude_unicast, exclude_restricted_bcast_arpnd,
-                                command, cold_start):
+    def _build_mask_flood_rules(
+        self,
+        vlan,
+        eth_type,
+        eth_dst,
+        eth_dst_mask,
+        exclude_unicast,
+        exclude_restricted_bcast_arpnd,
+        command,
+        cold_start,
+    ):  # pylint: disable=too-many-arguments
         """Builds that flood rules for each mask for each port in the stack.
         This takes into account the pruned and non-pruned ports and returns
             the appropriate flood rule actions"""
         # Stack ports aren't in VLANs, so need special rules to cause flooding from them.
         ofmsgs = super()._build_mask_flood_rules(
-            vlan, eth_type, eth_dst, eth_dst_mask,
-            exclude_unicast, exclude_restricted_bcast_arpnd,
-            command, cold_start)
+            vlan,
+            eth_type,
+            eth_dst,
+            eth_dst_mask,
+            exclude_unicast,
+            exclude_restricted_bcast_arpnd,
+            command,
+            cold_start,
+        )
 
         for port in self.stack_manager.stack_ports():
-
             if eth_dst is not None:
                 # Prune broadcast flooding where multiply connected to same DP
                 prune = self.stack_manager.is_pruned_port(port)
             else:
                 # Do not prune unicast, may be reply from directly connected DP.
                 prune = False
 
-            ofmsgs.extend(self._build_mask_flood_rules_filters(
-                port, vlan, eth_dst, eth_dst_mask, prune))
-            ofmsgs.extend(self._build_mask_flood_rules_flood_acts(
-                vlan, eth_type, eth_dst, eth_dst_mask,
-                exclude_unicast, exclude_restricted_bcast_arpnd,
-                command, cold_start, prune, port))
+            ofmsgs.extend(
+                self._build_mask_flood_rules_filters(
+                    port, vlan, eth_dst, eth_dst_mask, prune
+                )
+            )
+            ofmsgs.extend(
+                self._build_mask_flood_rules_flood_acts(
+                    vlan,
+                    eth_type,
+                    eth_dst,
+                    eth_dst_mask,
+                    exclude_unicast,
+                    exclude_restricted_bcast_arpnd,
+                    command,
+                    cold_start,
+                    prune,
+                    port,
+                )
+            )
 
         return ofmsgs
 
     def edge_learn_port(self, other_valves, pkt_meta):
         """
         Find a port towards the edge DP where the packet originated from
 
@@ -246,16 +357,15 @@
         # If learning on an external port, check another DP hasn't
         # already learned on a local/non-external port.
         if pkt_meta.port.loop_protect_external:
             edge_dp = self._non_stack_learned(other_valves, pkt_meta)
             if edge_dp:
                 return self.stack_manager.edge_learn_port_towards(pkt_meta, edge_dp)
         # Locally learn.
-        return super().edge_learn_port(
-            other_valves, pkt_meta)
+        return super().edge_learn_port(other_valves, pkt_meta)
 
     def _edge_dp_for_host(self, other_valves, pkt_meta):
         """Simple distributed unicast learning.
 
         Args:
             other_valves (list): All Valves other than this one.
             pkt_meta (PacketMeta): PacketMeta instance for packet received.
@@ -268,32 +378,39 @@
         """Install rules to drop spoofed faucet mac"""
         # antispoof for FAUCET's MAC address
         # TODO: antispoof for controller IPs on this VLAN, too.
         ofmsgs = []
         if self.drop_spoofed_faucet_mac:
             for port in self.ports.values():
                 if not port.stack:
-                    ofmsgs.extend(self.pipeline.filter_packets(
-                        {'eth_src': vlan.faucet_mac, 'in_port': port.number}))
+                    ofmsgs.extend(
+                        self.pipeline.filter_packets(
+                            {"eth_src": vlan.faucet_mac, "in_port": port.number}
+                        )
+                    )
         return ofmsgs
 
     def add_port(self, port):
         ofmsgs = super().add_port(port)
         # If this is a stacking port, accept all VLANs (came from another FAUCET)
         if port.stack:
             # Actual stack traffic will have VLAN tags.
-            ofmsgs.append(self.vlan_table.flowdrop(
-                match=self.vlan_table.match(
-                    in_port=port.number,
-                    vlan=NullVLAN()),
-                priority=self.low_priority + 1))
-            ofmsgs.append(self.vlan_table.flowmod(
-                match=self.vlan_table.match(in_port=port.number),
-                priority=self.low_priority,
-                inst=self.pipeline.accept_to_classification()))
+            ofmsgs.append(
+                self.vlan_table.flowdrop(
+                    match=self.vlan_table.match(in_port=port.number, vlan=NullVLAN()),
+                    priority=self.low_priority + 1,
+                )
+            )
+            ofmsgs.append(
+                self.vlan_table.flowmod(
+                    match=self.vlan_table.match(in_port=port.number),
+                    priority=self.low_priority,
+                    inst=self.pipeline.accept_to_classification(),
+                )
+            )
         return ofmsgs
 
     def del_port(self, port):
         ofmsgs = super().del_port(port)
         if port.stack:
             for vlan in self.vlans.values():
                 vlan.clear_cache_hosts_on_port(port)
@@ -311,15 +428,15 @@
         Args:
             lacp_id: The LACP LAG ID
             other_valves (list): list of other valves
         Returns:
             nominated_dpid, reason
         """
         if not other_valves:
-            return None, ''
+            return None, ""
         stacked_other_valves = self.stack_manager.stacked_valves(other_valves)
         all_stacked_valves = {valve}.union(stacked_other_valves)
         ports = {}
         no_sync_ports = {}
         root_dpid = None
         for stack_valve in all_stacked_valves:
             all_lags = stack_valve.dp.lags_up()
@@ -328,240 +445,307 @@
             nosync_lags = stack_valve.dp.lags_nosync()
             for l_id in nosync_lags:
                 ports.setdefault(stack_valve.dp.dp_id, 0)
                 no_sync_ports[stack_valve.dp.dp_id] = len(nosync_lags.get(l_id, 0))
             if stack_valve.dp.stack.is_root():
                 root_dpid = stack_valve.dp.dp_id
         # Order by number of ports
-        port_order = sorted(ports,
-                            key=lambda port: (ports.get(port, 0), no_sync_ports.get(port, 0)),
-                            reverse=True)
+        port_order = sorted(
+            ports,
+            key=lambda port: (ports.get(port, 0), no_sync_ports.get(port, 0)),
+            reverse=True,
+        )
         if not port_order:
-            return None, ''
+            return None, ""
         most_ports_dpid = port_order[0]
-        most_ports_dpids = [dpid for dpid, num in ports.items() if num == ports[most_ports_dpid]]
+        most_ports_dpids = [
+            dpid for dpid, num in ports.items() if num == ports[most_ports_dpid]
+        ]
         if len(most_ports_dpids) > 1:
             # There are several dpids that have the same number of lags
             if root_dpid in most_ports_dpids:
                 # root_dpid is the chosen DPID
-                return root_dpid, 'root dp'
+                return root_dpid, "root dp"
             # Order by lowest DPID
-            return sorted(most_ports_dpids), 'lowest dpid'
+            return sorted(most_ports_dpids), "lowest dpid"
         # Most_ports_dpid is the chosen DPID
-        return most_ports_dpid, 'most LAG ports'
+        return most_ports_dpid, "most LAG ports"
 
     def _learn_host_intervlan_routing_flows(self, port, vlan, eth_src, eth_dst):
         """Returns flows for the eth_src_table that enable packets that have been
            routed to be accepted from an adjacent DP and then switched to the destination.
            Eth_src_table flow rule to match on port, eth_src, eth_dst and vlan
 
         Args:
             port (Port): Port to match on.
             vlan (VLAN): VLAN to match on
             eth_src: source MAC address (should be the router MAC)
             eth_dst: destination MAC address
         """
         ofmsgs = []
-        (src_rule_idle_timeout, src_rule_hard_timeout, _) = self._learn_host_timeouts(port, eth_src)
-        src_match = self.eth_src_table.match(vlan=vlan, eth_src=eth_src, eth_dst=eth_dst)
+        (src_rule_idle_timeout, src_rule_hard_timeout, _) = self._learn_host_timeouts(
+            port, eth_src
+        )
+        src_match = self.eth_src_table.match(
+            vlan=vlan, eth_src=eth_src, eth_dst=eth_dst
+        )
         src_priority = self.host_priority - 1
         inst = (self.eth_src_table.goto(self.output_table),)
-        ofmsgs.extend([self.eth_src_table.flowmod(
-            match=src_match,
-            priority=src_priority,
-            inst=inst,
-            hard_timeout=src_rule_hard_timeout,
-            idle_timeout=src_rule_idle_timeout)])
+        ofmsgs.extend(
+            [
+                self.eth_src_table.flowmod(
+                    match=src_match,
+                    priority=src_priority,
+                    inst=inst,
+                    hard_timeout=src_rule_hard_timeout,
+                    idle_timeout=src_rule_idle_timeout,
+                )
+            ]
+        )
         return ofmsgs
 
     def _valve_learn_host_from_pkt(self, valve, now, pkt_meta, other_valves):
         """Add L3 forwarding rule if necessary for inter-VLAN routing."""
         ofmsgs_by_valve = super().learn_host_from_pkt(
-            valve, now, pkt_meta, other_valves)
-        if self.stack_manager.stack.route_learning and not self.stack_manager.stack.is_root():
+            valve, now, pkt_meta, other_valves
+        )
+        if (
+            self.stack_manager.stack.route_learning
+            and not self.stack_manager.stack.is_root()
+        ):
             if pkt_meta.eth_src == pkt_meta.vlan.faucet_mac:
-                ofmsgs_by_valve[valve].extend(self._learn_host_intervlan_routing_flows(
-                    pkt_meta.port, pkt_meta.vlan, pkt_meta.eth_src, pkt_meta.eth_dst))
+                ofmsgs_by_valve[valve].extend(
+                    self._learn_host_intervlan_routing_flows(
+                        pkt_meta.port, pkt_meta.vlan, pkt_meta.eth_src, pkt_meta.eth_dst
+                    )
+                )
             elif pkt_meta.eth_dst == pkt_meta.vlan.faucet_mac:
-                ofmsgs_by_valve[valve].extend(self._learn_host_intervlan_routing_flows(
-                    pkt_meta.port, pkt_meta.vlan, pkt_meta.eth_dst, pkt_meta.eth_src))
+                ofmsgs_by_valve[valve].extend(
+                    self._learn_host_intervlan_routing_flows(
+                        pkt_meta.port, pkt_meta.vlan, pkt_meta.eth_dst, pkt_meta.eth_src
+                    )
+                )
         return ofmsgs_by_valve
 
     def learn_host_from_pkt(self, valve, now, pkt_meta, other_valves):
         ofmsgs_by_valve = {}
 
         if self.stack_manager.stack.route_learning:
             stacked_other_valves = self.stack_manager.stacked_valves(other_valves)
             all_stacked_valves = {valve}.union(stacked_other_valves)
 
             # NOTE: multi DP routing requires learning from directly attached switch first.
             if pkt_meta.port.stack:
-                peer_dp = pkt_meta.port.stack['dp']
+                peer_dp = pkt_meta.port.stack["dp"]
                 if peer_dp.dyn_running:
                     faucet_macs = {pkt_meta.vlan.faucet_mac}.union(
-                        {valve.dp.faucet_dp_mac for valve in all_stacked_valves})
+                        {valve.dp.faucet_dp_mac for valve in all_stacked_valves}
+                    )
                     # Must always learn FAUCET VIP, but rely on neighbor
                     # to learn other hosts first.
                     if pkt_meta.eth_src not in faucet_macs:
                         return ofmsgs_by_valve
 
             for other_valve in stacked_other_valves:
                 stack_port = other_valve.stack_manager.relative_port_towards(
-                    self.stack_manager.stack.name)
+                    self.stack_manager.stack.name
+                )
                 valve_vlan = other_valve.dp.vlans.get(pkt_meta.vlan.vid, None)
                 if stack_port and valve_vlan:
                     valve_pkt_meta = copy.copy(pkt_meta)
                     valve_pkt_meta.vlan = valve_vlan
                     valve_pkt_meta.port = stack_port
                     valve_other_valves = all_stacked_valves - {other_valve}
-                    ofmsgs_by_valve.update(self._valve_learn_host_from_pkt(
-                        other_valve, now, valve_pkt_meta, valve_other_valves))
+                    ofmsgs_by_valve.update(
+                        self._valve_learn_host_from_pkt(
+                            other_valve, now, valve_pkt_meta, valve_other_valves
+                        )
+                    )
 
         ofmsgs_by_valve.update(
-            self._valve_learn_host_from_pkt(valve, now, pkt_meta, other_valves))
+            self._valve_learn_host_from_pkt(valve, now, pkt_meta, other_valves)
+        )
         return ofmsgs_by_valve
 
 
 class ValveSwitchStackManagerNoReflection(ValveSwitchStackManagerBase):
     """Stacks of size 2 - all switches directly connected to root.
 
     Root switch simply floods to all other switches.
 
     Non-root switches simply flood to the root.
     """
 
-    def _flood_actions(self, in_port, external_ports,  # pylint: disable=too-many-arguments
-                       away_flood_actions, toward_flood_actions, local_flood_actions):
+    def _flood_actions(
+        self,
+        in_port,
+        external_ports,
+        away_flood_actions,
+        toward_flood_actions,
+        local_flood_actions,
+    ):  # pylint: disable=too-many-arguments
         if not in_port or self.stack_manager.is_stack_port(in_port):
             flood_prefix = ()
         else:
             if external_ports:
                 flood_prefix = self._set_nonext_port_flag
             else:
                 flood_prefix = self._set_ext_port_flag
 
         flood_actions = (
-            flood_prefix + toward_flood_actions + away_flood_actions + local_flood_actions)
+            flood_prefix
+            + toward_flood_actions
+            + away_flood_actions
+            + local_flood_actions
+        )
 
         return flood_actions
 
     def _edge_dp_for_host(self, other_valves, pkt_meta):
         """Size 2 means root shortest path is always directly connected."""
-        peer_dp = pkt_meta.port.stack['dp']
+        peer_dp = pkt_meta.port.stack["dp"]
         if peer_dp.dyn_running:
             return self._non_stack_learned(other_valves, pkt_meta)
         # Fall back to assuming peer knows if we are not the peer's controller.
         return peer_dp
 
 
 class ValveSwitchStackManagerReflection(ValveSwitchStackManagerBase):
     """Stacks size > 2 reflect floods off of root (selective flooding).
 
-       .. code-block:: none
+    .. code-block:: none
 
-                                Hosts
-                                 ||||
-                                 ||||
-                   +----+       +----+       +----+
-                ---+1   |       |1234|       |   1+---
-          Hosts ---+2   |       |    |       |   2+--- Hosts
-                ---+3   |       |    |       |   3+---
-                ---+4  5+-------+5  6+-------+5  4+---
-                   +----+       +----+       +----+
-
-                   Root DP
-
-       Non-root switches flood only to the root. The root switch
-       reflects incoming floods back out. Non-root switches flood
-       packets from the root locally and to switches further away
-       from the root. Flooding is entirely implemented in the dataplane.
-
-       A host connected to a non-root switch can receive a copy of its
-       own flooded packet (because the non-root switch does not know
-       it has seen the packet already).
-
-       A host connected to the root switch does not have this problem
-       (because flooding is always away from the root). Therefore,
-       connections to other non-FAUCET stacking networks should only
-       be made to the root.
-
-       On the root switch (left), flood destinations are:
-
-       1: 2 3 4 5(s)
-       2: 1 3 4 5(s)
-       3: 1 2 4 5(s)
-       4: 1 2 3 5(s)
-       5: 1 2 3 4 5(s, note reflection)
-
-       On the middle switch:
-
-       1: 5(s)
-       2: 5(s)
-       3: 5(s)
-       4: 5(s)
-       5: 1 2 3 4 6(s)
-       6: 5(s)
-
-       On the rightmost switch:
-
-       1: 5(s)
-       2: 5(s)
-       3: 5(s)
-       4: 5(s)
-       5: 1 2 3 4
+                             Hosts
+                              ||||
+                              ||||
+                +----+       +----+       +----+
+             ---+1   |       |1234|       |   1+---
+       Hosts ---+2   |       |    |       |   2+--- Hosts
+             ---+3   |       |    |       |   3+---
+             ---+4  5+-------+5  6+-------+5  4+---
+                +----+       +----+       +----+
+
+                Root DP
+
+    Non-root switches flood only to the root. The root switch
+    reflects incoming floods back out. Non-root switches flood
+    packets from the root locally and to switches further away
+    from the root. Flooding is entirely implemented in the dataplane.
+
+    A host connected to a non-root switch can receive a copy of its
+    own flooded packet (because the non-root switch does not know
+    it has seen the packet already).
+
+    A host connected to the root switch does not have this problem
+    (because flooding is always away from the root). Therefore,
+    connections to other non-FAUCET stacking networks should only
+    be made to the root.
+
+    On the root switch (left), flood destinations are:
+
+    1: 2 3 4 5(s)
+    2: 1 3 4 5(s)
+    3: 1 2 4 5(s)
+    4: 1 2 3 5(s)
+    5: 1 2 3 4 5(s, note reflection)
+
+    On the middle switch:
+
+    1: 5(s)
+    2: 5(s)
+    3: 5(s)
+    4: 5(s)
+    5: 1 2 3 4 6(s)
+    6: 5(s)
+
+    On the rightmost switch:
+
+    1: 5(s)
+    2: 5(s)
+    3: 5(s)
+    4: 5(s)
+    5: 1 2 3 4
     """
 
     # Indicate to base class use of reflection required.
     _USES_REFLECTION = True
 
-    def _learn_cache_check(self, entry, vlan, now, eth_src, port, ofmsgs,
-                           cache_port, cache_age,
-                           delete_existing, refresh_rules):
+    def _learn_cache_check(
+        self,
+        entry,
+        vlan,
+        now,
+        eth_src,
+        port,
+        ofmsgs,
+        cache_port,
+        cache_age,
+        delete_existing,
+        refresh_rules,
+    ):  # pylint: disable=too-many-arguments
         learn_exit = False
         update_cache = True
         if cache_port is not None:
             # packet was received on same member of a LAG.
-            same_lag = (port.lacp and port.lacp == cache_port.lacp)
+            same_lag = port.lacp and port.lacp == cache_port.lacp
             # stacks of size > 2 will have an unknown MAC flooded towards the root,
             # and flooded down again. If we learned the MAC on a local port and
             # heard the reflected flooded copy, discard the reflection.
             local_stack_learn = port.stack and not cache_port.stack
             guard_time = self.cache_update_guard_time
             if cache_port == port or same_lag or local_stack_learn:
                 port_cache_valid = (
-                    port.dyn_update_time is not None and port.dyn_update_time <= entry.cache_time)
+                    port.dyn_update_time is not None
+                    and port.dyn_update_time <= entry.cache_time
+                )
                 # aggressively re-learn on LAGs, and prefer recently learned
                 # locally learned hosts on a stack.
                 if same_lag or local_stack_learn:
                     guard_time = 2
                 # port didn't change status, and recent cache update, don't do anything.
                 if cache_age < guard_time and port_cache_valid:
                     update_cache = False
                     learn_exit = True
                 # skip delete if host didn't change ports or on same LAG.
                 elif cache_port == port or same_lag:
                     delete_existing = False
                     if port_cache_valid:
                         refresh_rules = True
-        return (learn_exit, ofmsgs, cache_port, update_cache, delete_existing, refresh_rules)
-
-    def _flood_actions(self, in_port, external_ports,  # pylint: disable=too-many-arguments
-                       away_flood_actions, toward_flood_actions, local_flood_actions):
+        return (
+            learn_exit,
+            ofmsgs,
+            cache_port,
+            update_cache,
+            delete_existing,
+            refresh_rules,
+        )
+
+    def _flood_actions(
+        self,
+        in_port,
+        external_ports,
+        away_flood_actions,
+        toward_flood_actions,
+        local_flood_actions,
+    ):  # pylint: disable=too-many-arguments
         if self.stack_manager.stack.is_root():
             if external_ports:
                 flood_prefix = self._set_nonext_port_flag
             else:
                 flood_prefix = self._set_ext_port_flag
-            flood_actions = (away_flood_actions + local_flood_actions)
+            flood_actions = away_flood_actions + local_flood_actions
 
             if in_port and self.stack_manager.is_away(in_port):
                 # Packet from a non-root switch, flood locally and to all non-root switches
                 # (reflect it).
                 flood_actions = (
-                    away_flood_actions + (valve_of.output_in_port(),) + local_flood_actions)
+                    away_flood_actions
+                    + (valve_of.output_in_port(),)
+                    + local_flood_actions
+                )
 
             flood_actions = flood_prefix + flood_actions
         else:
             # Default non-root strategy is flood towards root.
             if external_ports:
                 flood_actions = self._set_nonext_port_flag + toward_flood_actions
             else:
@@ -574,18 +758,24 @@
                 # Packet from the root.
                 elif self.stack_manager.is_towards_root(in_port):
                     # If we have external ports, and packet hasn't already been flooded
                     # externally, flood it externally before passing it to further away switches,
                     # and mark it flooded.
                     if external_ports:
                         flood_actions = (
-                            self._set_nonext_port_flag + away_flood_actions + local_flood_actions)
+                            self._set_nonext_port_flag
+                            + away_flood_actions
+                            + local_flood_actions
+                        )
                     else:
                         flood_actions = (
-                            away_flood_actions + self._set_nonext_port_flag + local_flood_actions)
+                            away_flood_actions
+                            + self._set_nonext_port_flag
+                            + local_flood_actions
+                        )
                 # Packet from external port, locally. Mark it already flooded externally and
                 # flood to root (it came from an external switch so keep it within the stack).
                 elif in_port.loop_protect_external:
                     flood_actions = self._set_nonext_port_flag + toward_flood_actions
                 else:
                     flood_actions = self._set_ext_port_flag + toward_flood_actions
 
@@ -597,15 +787,15 @@
         # in the stack to keep each DP's graph consistent.
         # TODO: simplest possible unicast learning.
         # We find just one port that is the shortest unicast path to
         # the destination. We could use other factors (eg we could
         # load balance over multiple ports based on destination MAC).
         # Find port that forwards closer to destination DP that
         # has already learned this host (if any).
-        peer_dp = pkt_meta.port.stack['dp']
+        peer_dp = pkt_meta.port.stack["dp"]
         if peer_dp.dyn_running:
             return self._non_stack_learned(other_valves, pkt_meta)
         # Fall back to peer knows if edge or root if we are not the peer's controller.
         if peer_dp.stack.is_edge() or peer_dp.stack.is_root():
             return peer_dp
         # No DP has learned this host, yet. Take no action to allow remote learning to occur.
         return None
```

### Comparing `c65faucet-1.0.46/faucet/valve_switch_standalone.py` & `c65faucet-1.0.47/faucet/valve_switch_standalone.py`

 * *Files 22% similar despite different names*

```diff
@@ -22,51 +22,107 @@
 import copy
 import functools
 from collections import defaultdict
 from faucet import valve_of
 from faucet import valve_packet
 from faucet.valve_manager_base import ValveManagerBase
 from faucet.vlan import NullVLAN
-from faucet.valve_util import LRU_MAX
 
 
 class ValveSwitchManager(ValveManagerBase):  # pylint: disable=too-many-public-methods
     """Implement dataplane based flooding/learning for standalone dataplanes."""
 
     # Enumerate possible eth_dst flood destinations.
     # First bool says whether to flood this destination if the VLAN
     # has unicast flooding enabled (if unicast flooding is enabled,
     # then we flood all destination eth_dsts).
     FLOOD_DSTS = (
         (True, None, None, None),
-        (False, None, valve_packet.BRIDGE_GROUP_ADDRESS, valve_packet.mac_byte_mask(3)),  # 802.x
-        (False, None, '01:00:5E:00:00:00', valve_packet.mac_byte_mask(3)),  # IPv4 multicast
-        (False, None, '33:33:00:00:00:00', valve_packet.mac_byte_mask(2)),  # IPv6 multicast
-        (False, None, valve_of.mac.BROADCAST_STR, valve_packet.mac_byte_mask(6)),  # eth broadcasts
+        (
+            False,
+            None,
+            valve_packet.BRIDGE_GROUP_ADDRESS,
+            valve_packet.mac_byte_mask(3),
+        ),  # 802.x
+        (
+            False,
+            None,
+            "01:00:5E:00:00:00",
+            valve_packet.mac_byte_mask(3),
+        ),  # IPv4 multicast
+        (
+            False,
+            None,
+            "33:33:00:00:00:00",
+            valve_packet.mac_byte_mask(2),
+        ),  # IPv6 multicast
+        (
+            False,
+            None,
+            valve_of.mac.BROADCAST_STR,
+            valve_packet.mac_byte_mask(6),
+        ),  # eth broadcasts
     )
     # Ports with restricted broadcast enabled may only receive these broadcasts.
     RESTRICTED_FLOOD_DISTS = (
-        (False, valve_of.ether.ETH_TYPE_ARP,
-         valve_of.mac.BROADCAST_STR, valve_packet.mac_byte_mask(6)),  # ARP
-        (False, valve_of.ether.ETH_TYPE_IPV6,
-         '33:33:FF:00:00:00', valve_packet.mac_byte_mask(3)),  # IPv6 multicast for ND
-        (False, valve_of.ether.ETH_TYPE_IPV6,
-         valve_packet.IPV6_ALL_ROUTERS_MCAST, valve_packet.mac_byte_mask(6)),  # IPV6 all routers
-        (False, valve_of.ether.ETH_TYPE_IPV6,
-         valve_packet.IPV6_ALL_NODES_MCAST, valve_packet.mac_byte_mask(6)),  # IPv6 all nodes
+        (
+            False,
+            valve_of.ether.ETH_TYPE_ARP,
+            valve_of.mac.BROADCAST_STR,
+            valve_packet.mac_byte_mask(6),
+        ),  # ARP
+        (
+            False,
+            valve_of.ether.ETH_TYPE_IPV6,
+            "33:33:FF:00:00:00",
+            valve_packet.mac_byte_mask(3),
+        ),  # IPv6 multicast for ND
+        (
+            False,
+            valve_of.ether.ETH_TYPE_IPV6,
+            valve_packet.IPV6_ALL_ROUTERS_MCAST,
+            valve_packet.mac_byte_mask(6),
+        ),  # IPV6 all routers
+        (
+            False,
+            valve_of.ether.ETH_TYPE_IPV6,
+            valve_packet.IPV6_ALL_NODES_MCAST,
+            valve_packet.mac_byte_mask(6),
+        ),  # IPv6 all nodes
     )
 
-    def __init__(self, logger, ports, vlans,  # pylint: disable=too-many-arguments
-                 vlan_table, vlan_acl_table, eth_src_table, eth_dst_table,
-                 eth_dst_hairpin_table, flood_table, classification_table,
-                 pipeline, use_group_table, groups, combinatorial_port_flood,
-                 canonical_port_order, restricted_bcast_arpnd, has_externals,
-                 learn_ban_timeout, learn_timeout, learn_jitter, cache_update_guard_time,
-                 idle_dst, dp_high_priority, dp_highest_priority, faucet_dp_mac,
-                 drop_spoofed_faucet_mac):
+    def __init__(
+        self,
+        logger,
+        ports,
+        vlans,
+        vlan_table,
+        vlan_acl_table,
+        eth_src_table,
+        eth_dst_table,
+        eth_dst_hairpin_table,
+        flood_table,
+        classification_table,
+        pipeline,
+        use_group_table,
+        groups,
+        combinatorial_port_flood,
+        canonical_port_order,
+        restricted_bcast_arpnd,
+        has_externals,
+        learn_ban_timeout,
+        learn_timeout,
+        learn_jitter,
+        cache_update_guard_time,
+        idle_dst,
+        dp_high_priority,
+        dp_highest_priority,
+        faucet_dp_mac,
+        drop_spoofed_faucet_mac,
+    ):  # pylint: disable=too-many-arguments
         self.logger = logger
         self.ports = ports
         self.vlans = vlans
         self.vlan_table = vlan_table
         self.vlan_acl_table = vlan_acl_table
         self.eth_src_table = eth_src_table
         self.eth_dst_table = eth_dst_table
@@ -103,274 +159,426 @@
         self.faucet_dp_mac = faucet_dp_mac
         self.drop_spoofed_faucet_mac = drop_spoofed_faucet_mac
 
     def initialise_tables(self):
         """Initialise the flood table with filtering flows."""
         ofmsgs = []
         for eth_dst, eth_dst_mask in (
-                (valve_packet.CISCO_CDP_VTP_UDLD_ADDRESS, valve_packet.mac_byte_mask(6)),
-                (valve_packet.CISCO_SPANNING_GROUP_ADDRESS, valve_packet.mac_byte_mask(6)),
-                (valve_packet.BRIDGE_GROUP_ADDRESS, valve_packet.BRIDGE_GROUP_MASK)):
-            ofmsgs.append(self.flood_table.flowdrop(
-                self.flood_table.match(eth_dst=eth_dst, eth_dst_mask=eth_dst_mask),
-                priority=self._mask_flood_priority(eth_dst_mask)))
+            (valve_packet.CISCO_CDP_VTP_UDLD_ADDRESS, valve_packet.mac_byte_mask(6)),
+            (valve_packet.CISCO_SPANNING_GROUP_ADDRESS, valve_packet.mac_byte_mask(6)),
+            (valve_packet.BRIDGE_GROUP_ADDRESS, valve_packet.BRIDGE_GROUP_MASK),
+        ):
+            ofmsgs.append(
+                self.flood_table.flowdrop(
+                    self.flood_table.match(eth_dst=eth_dst, eth_dst_mask=eth_dst_mask),
+                    priority=self._mask_flood_priority(eth_dst_mask),
+                )
+            )
         return ofmsgs
 
     @staticmethod
     def floods_to_root(_dp_obj):
         """Return True if the given dp floods (only) to root switch"""
         return False
 
-    @functools.lru_cache(maxsize=LRU_MAX)
+    @functools.lru_cache(maxsize=1024)
     def _mask_flood_priority(self, eth_dst_mask):
         return self.flood_priority + valve_packet.mac_mask_bits(eth_dst_mask)
 
     @staticmethod
     def _vlan_all_ports(vlan, exclude_unicast):
         """Return list of all ports that should be flooded to on a VLAN."""
         return list(vlan.flood_ports(vlan.get_ports(), exclude_unicast))
 
-    def _build_flood_local_rule_actions(self, vlan, exclude_unicast, in_port,  # pylint: disable=too-many-arguments
-                                        exclude_all_external, exclude_restricted_bcast_arpnd):
+    def _build_flood_local_rule_actions(
+        self,
+        vlan,
+        exclude_unicast,
+        in_port,
+        exclude_all_external,
+        exclude_restricted_bcast_arpnd,
+    ):  # pylint: disable=too-many-arguments
         """Return a list of flood actions to flood packets from a port."""
-        external_ports = self.canonical_port_order(vlan.loop_protect_external_ports_up())
+        external_ports = self.canonical_port_order(
+            vlan.loop_protect_external_ports_up()
+        )
         exclude_ports = vlan.excluded_lag_ports(in_port)
         exclude_ports.update(vlan.exclude_native_if_dot1x())
-        if exclude_all_external or (in_port is not None and in_port.loop_protect_external):
+        if exclude_all_external or (
+            in_port is not None and in_port.loop_protect_external
+        ):
             exclude_ports.update(set(external_ports))
         else:
             exclude_ports.update(set(external_ports[1:]))
         if exclude_restricted_bcast_arpnd:
             exclude_ports.update(set(vlan.restricted_bcast_arpnd_ports()))
         return valve_of.flood_port_outputs(
             vlan.tagged_flood_ports(exclude_unicast),
             vlan.untagged_flood_ports(exclude_unicast),
             in_port=in_port,
-            exclude_ports=exclude_ports)
+            exclude_ports=exclude_ports,
+        )
 
-    def _build_flood_rule_actions(self, vlan, exclude_unicast, in_port,  # pylint: disable=too-many-arguments
-                                  exclude_all_external=False, exclude_restricted_bcast_arpnd=False):
+    def _build_flood_rule_actions(
+        self,
+        vlan,
+        exclude_unicast,
+        in_port,
+        exclude_all_external=False,
+        exclude_restricted_bcast_arpnd=False,
+    ):  # pylint: disable=too-many-arguments
         actions = []
-        if vlan.loop_protect_external_ports() and vlan.tagged_flood_ports(exclude_unicast):
+        if vlan.loop_protect_external_ports() and vlan.tagged_flood_ports(
+            exclude_unicast
+        ):
             actions.append(self.flood_table.set_external_forwarding_requested())
-        actions.extend(self._build_flood_local_rule_actions(
-            vlan, exclude_unicast, in_port, exclude_all_external, exclude_restricted_bcast_arpnd))
+        actions.extend(
+            self._build_flood_local_rule_actions(
+                vlan,
+                exclude_unicast,
+                in_port,
+                exclude_all_external,
+                exclude_restricted_bcast_arpnd,
+            )
+        )
         return tuple(actions)
 
     def _build_flood_rule(self, match, command, flood_acts, flood_priority):
         return self.flood_table.flowmod(
             match=match,
             command=command,
             inst=(valve_of.apply_actions(flood_acts),),
-            priority=flood_priority)
+            priority=flood_priority,
+        )
 
-    @functools.lru_cache(maxsize=LRU_MAX)
+    @functools.lru_cache(maxsize=1024)
     def _vlan_flood_priority(self, eth_type, eth_dst_mask):
         priority = self._mask_flood_priority(eth_dst_mask)
         if eth_type:
             priority += eth_type
         return priority
 
-    def _build_flood_rule_for_vlan(self, vlan, eth_type, eth_dst, eth_dst_mask,  # pylint: disable=too-many-arguments
-                                   exclude_unicast, command):
+    def _build_flood_rule_for_vlan(
+        self,
+        vlan,
+        eth_type,
+        eth_dst,
+        eth_dst_mask,
+        exclude_unicast,
+        command,
+    ):  # pylint: disable=too-many-arguments
         flood_priority = self._vlan_flood_priority(eth_type, eth_dst_mask)
         match = self.flood_table.match(
-            vlan=vlan, eth_type=eth_type, eth_dst=eth_dst, eth_dst_mask=eth_dst_mask)
+            vlan=vlan, eth_type=eth_type, eth_dst=eth_dst, eth_dst_mask=eth_dst_mask
+        )
         # TODO: optimization - drop all general flood dsts if all ports are restricted.
         exclude_restricted_bcast_arpnd = True
         flood_acts = self._build_flood_rule_actions(
-            vlan, exclude_unicast, None,
-            exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd)
-        return (self._build_flood_rule(match, command, flood_acts, flood_priority), flood_acts)
-
-    def _build_flood_acts_for_port(self, vlan, exclude_unicast, port,  # pylint: disable=too-many-arguments
-                                   exclude_all_external=False,
-                                   exclude_restricted_bcast_arpnd=False):
+            vlan,
+            exclude_unicast,
+            None,
+            exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd,
+        )
+        return (
+            self._build_flood_rule(match, command, flood_acts, flood_priority),
+            flood_acts,
+        )
+
+    def _build_flood_acts_for_port(
+        self,
+        vlan,
+        exclude_unicast,
+        port,
+        exclude_all_external=False,
+        exclude_restricted_bcast_arpnd=False,
+    ):  # pylint: disable=too-many-arguments
         flood_acts = ()
         port_output_ports = []
         port_non_output_acts = []
         if port.dyn_phys_up:
             if exclude_restricted_bcast_arpnd:
                 flood_acts = self._build_flood_rule_actions(
-                    vlan, exclude_unicast, port, exclude_all_external, port.restricted_bcast_arpnd)
+                    vlan,
+                    exclude_unicast,
+                    port,
+                    exclude_all_external,
+                    port.restricted_bcast_arpnd,
+                )
             else:
                 flood_acts = self._build_flood_rule_actions(
-                    vlan, exclude_unicast, port, exclude_all_external, False)
-            (flood_acts,
-             port_output_ports,
-             port_non_output_acts) = valve_of.output_non_output_actions(flood_acts)
+                    vlan, exclude_unicast, port, exclude_all_external, False
+                )
+            (
+                flood_acts,
+                port_output_ports,
+                port_non_output_acts,
+            ) = valve_of.output_non_output_actions(flood_acts)
             if not port_output_ports:
                 flood_acts = ()
                 port_non_output_acts = ()
         return (flood_acts, port_output_ports, port_non_output_acts)
 
-    def _build_flood_match_priority(self, port, vlan, eth_type,  # pylint: disable=too-many-arguments
-                                    eth_dst, eth_dst_mask, add_match):
+    def _build_flood_match_priority(
+        self,
+        port,
+        vlan,
+        eth_type,
+        eth_dst,
+        eth_dst_mask,
+        add_match,
+    ):  # pylint: disable=too-many-arguments
         flood_priority = self._vlan_flood_priority(eth_type, eth_dst_mask) + 1
         if add_match is None:
             add_match = {}
         match = self.flood_table.match(
-            vlan=vlan, in_port=port.number,
-            eth_type=eth_type, eth_dst=eth_dst, eth_dst_mask=eth_dst_mask,
-            **add_match)
+            vlan=vlan,
+            in_port=port.number,
+            eth_type=eth_type,
+            eth_dst=eth_dst,
+            eth_dst_mask=eth_dst_mask,
+            **add_match
+        )
         return (flood_priority, match)
 
-    def _build_flood_rule_for_port(self, vlan, eth_type, eth_dst, eth_dst_mask,  # pylint: disable=too-many-arguments
-                                   command, port, flood_acts, add_match=None):
+    def _build_flood_rule_for_port(
+        self,
+        vlan,
+        eth_type,
+        eth_dst,
+        eth_dst_mask,
+        command,
+        port,
+        flood_acts,
+        add_match=None,
+    ):  # pylint: disable=too-many-arguments
         flood_priority, match = self._build_flood_match_priority(
-            port, vlan, eth_type, eth_dst, eth_dst_mask, add_match)
+            port, vlan, eth_type, eth_dst, eth_dst_mask, add_match
+        )
         return self._build_flood_rule(match, command, flood_acts, flood_priority)
 
-    def _build_mask_flood_rules(self, vlan, eth_type, eth_dst, eth_dst_mask,  # pylint: disable=too-many-arguments
-                                exclude_unicast, exclude_restricted_bcast_arpnd,
-                                command, cold_start):
+    def _build_mask_flood_rules(
+        self,
+        vlan,
+        eth_type,
+        eth_dst,
+        eth_dst_mask,
+        exclude_unicast,
+        exclude_restricted_bcast_arpnd,
+        command,
+        cold_start,
+    ):  # pylint: disable=too-many-arguments
         ofmsgs = []
         if self.combinatorial_port_flood:
             for port in self._vlan_all_ports(vlan, exclude_unicast):
                 flood_acts, _, _ = self._build_flood_acts_for_port(
-                    vlan, exclude_unicast, port,
-                    exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd)
+                    vlan,
+                    exclude_unicast,
+                    port,
+                    exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd,
+                )
                 if flood_acts:
-                    ofmsgs.append(self._build_flood_rule_for_port(
-                        vlan, eth_type, eth_dst, eth_dst_mask, command, port, flood_acts))
+                    ofmsgs.append(
+                        self._build_flood_rule_for_port(
+                            vlan,
+                            eth_type,
+                            eth_dst,
+                            eth_dst_mask,
+                            command,
+                            port,
+                            flood_acts,
+                        )
+                    )
         else:
             vlan_flood_ofmsg, vlan_flood_acts = self._build_flood_rule_for_vlan(
-                vlan, eth_type, eth_dst, eth_dst_mask, exclude_unicast, command)
+                vlan, eth_type, eth_dst, eth_dst_mask, exclude_unicast, command
+            )
             if not self.use_group_table:
                 ofmsgs.append(vlan_flood_ofmsg)
-            (flood_acts,
-             vlan_output_ports,
-             vlan_non_output_acts) = valve_of.output_non_output_actions(vlan_flood_acts)
+            (
+                flood_acts,
+                vlan_output_ports,
+                vlan_non_output_acts,
+            ) = valve_of.output_non_output_actions(vlan_flood_acts)
             for port in self._vlan_all_ports(vlan, exclude_unicast):
-                (flood_acts,
-                 port_output_ports,
-                 port_non_output_acts) = self._build_flood_acts_for_port(
-                     vlan, exclude_unicast, port,
-                     exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd)
+                (
+                    flood_acts,
+                    port_output_ports,
+                    port_non_output_acts,
+                ) = self._build_flood_acts_for_port(
+                    vlan,
+                    exclude_unicast,
+                    port,
+                    exclude_restricted_bcast_arpnd=exclude_restricted_bcast_arpnd,
+                )
                 if not flood_acts:
                     continue
-                if (vlan_output_ports - set([port.number]) == port_output_ports
-                        and vlan_non_output_acts == port_non_output_acts):
+                if (
+                    vlan_output_ports - set([port.number]) == port_output_ports
+                    and vlan_non_output_acts == port_non_output_acts
+                ):
                     # Delete a potentially existing port specific flow
                     # TODO: optimize, avoid generating delete for port if no existing flow.
                     if not cold_start:
                         flood_priority, match = self._build_flood_match_priority(
-                            port, vlan, eth_type, eth_dst, eth_dst_mask, add_match=None)
-                        ofmsgs.append(self.flood_table.flowdel(
-                            match=match, priority=flood_priority))
+                            port, vlan, eth_type, eth_dst, eth_dst_mask, add_match=None
+                        )
+                        ofmsgs.append(
+                            self.flood_table.flowdel(
+                                match=match, priority=flood_priority
+                            )
+                        )
                 else:
-                    ofmsgs.append(self._build_flood_rule_for_port(
-                        vlan, eth_type, eth_dst, eth_dst_mask, command, port, flood_acts))
+                    ofmsgs.append(
+                        self._build_flood_rule_for_port(
+                            vlan,
+                            eth_type,
+                            eth_dst,
+                            eth_dst_mask,
+                            command,
+                            port,
+                            flood_acts,
+                        )
+                    )
         return ofmsgs
 
     def _build_multiout_flood_rules(self, vlan, command, cold_start):
         """Build flooding rules for a VLAN without using groups."""
         ofmsgs = []
         for unicast_eth_dst, eth_type, eth_dst, eth_dst_mask in self.flood_dsts:
             if unicast_eth_dst and not vlan.unicast_flood:
                 continue
             exclude_restricted_bcast_arpnd = eth_type is None
-            ofmsgs.extend(self._build_mask_flood_rules(
-                vlan, eth_type, eth_dst, eth_dst_mask,
-                unicast_eth_dst, exclude_restricted_bcast_arpnd,
-                command, cold_start))
+            ofmsgs.extend(
+                self._build_mask_flood_rules(
+                    vlan,
+                    eth_type,
+                    eth_dst,
+                    eth_dst_mask,
+                    unicast_eth_dst,
+                    exclude_restricted_bcast_arpnd,
+                    command,
+                    cold_start,
+                )
+            )
         return ofmsgs
 
     def _build_group_flood_rules(self, vlan, command):
         """Build flooding rules for a VLAN using groups."""
         _, vlan_flood_acts = self._build_flood_rule_for_vlan(
-            vlan, None, None, None, False, command)
+            vlan, None, None, None, False, command
+        )
         group_id = vlan.vid
         group = self.groups.get_entry(
-            group_id, valve_of.build_group_flood_buckets(vlan_flood_acts))
+            group_id, valve_of.build_group_flood_buckets(vlan_flood_acts)
+        )
         groups_by_unicast_eth = {False: group, True: group}
         ofmsgs = []
 
         # Only configure unicast flooding group if has different output
         # actions to non unicast flooding.
         _, unicast_eth_vlan_flood_acts = self._build_flood_rule_for_vlan(
-            vlan, None, None, None, True, command)
-        unicast_eth_vlan_flood_acts, unicast_output_ports, _ = valve_of.output_non_output_actions(
-            unicast_eth_vlan_flood_acts)
-        vlan_flood_acts, vlan_output_ports, _ = valve_of.output_non_output_actions(vlan_flood_acts)
+            vlan, None, None, None, True, command
+        )
+        (
+            unicast_eth_vlan_flood_acts,
+            unicast_output_ports,
+            _,
+        ) = valve_of.output_non_output_actions(unicast_eth_vlan_flood_acts)
+        vlan_flood_acts, vlan_output_ports, _ = valve_of.output_non_output_actions(
+            vlan_flood_acts
+        )
         if unicast_output_ports != vlan_output_ports:
             group_id += valve_of.VLAN_GROUP_OFFSET
             group = self.groups.get_entry(
-                group_id, valve_of.build_group_flood_buckets(unicast_eth_vlan_flood_acts))
+                group_id,
+                valve_of.build_group_flood_buckets(unicast_eth_vlan_flood_acts),
+            )
             groups_by_unicast_eth[True] = group
 
         for group in groups_by_unicast_eth.values():
             ofmsgs.extend(group.add())
 
         for unicast_eth_dst, eth_type, eth_dst, eth_dst_mask in self.flood_dsts:
             if unicast_eth_dst and not vlan.unicast_flood:
                 continue
             group = groups_by_unicast_eth[unicast_eth_dst]
             match = self.flood_table.match(
-                vlan=vlan, eth_type=eth_type, eth_dst=eth_dst, eth_dst_mask=eth_dst_mask)
+                vlan=vlan, eth_type=eth_type, eth_dst=eth_dst, eth_dst_mask=eth_dst_mask
+            )
             flood_priority = self._vlan_flood_priority(eth_type, eth_dst_mask)
-            ofmsgs.append(self.flood_table.flowmod(
-                match=match,
-                command=command,
-                inst=(valve_of.apply_actions((valve_of.group_act(group.group_id),)),),
-                priority=flood_priority))
+            ofmsgs.append(
+                self.flood_table.flowmod(
+                    match=match,
+                    command=command,
+                    inst=(
+                        valve_of.apply_actions((valve_of.group_act(group.group_id),)),
+                    ),
+                    priority=flood_priority,
+                )
+            )
         return ofmsgs
 
     def add_drop_spoofed_faucet_mac_rules(self, vlan):
         """Install rules to drop spoofed faucet mac"""
         # antispoof for FAUCET's MAC address
         # TODO: antispoof for controller IPs on this VLAN, too.
         ofmsgs = []
         if self.drop_spoofed_faucet_mac:
-            ofmsgs.extend(self.pipeline.filter_packets({'eth_src': vlan.faucet_mac}))
+            ofmsgs.extend(self.pipeline.filter_packets({"eth_src": vlan.faucet_mac}))
         return ofmsgs
 
     def add_vlan(self, vlan, cold_start):
         ofmsgs = []
         ofmsgs.extend(self.add_drop_spoofed_faucet_mac_rules(vlan))
-        ofmsgs.append(self.eth_src_table.flowcontroller(
-            match=self.eth_src_table.match(vlan=vlan),
-            priority=self.low_priority,
-            inst=(self.eth_src_table.goto(self.output_table),)))
+        ofmsgs.append(
+            self.eth_src_table.flowcontroller(
+                match=self.eth_src_table.match(vlan=vlan),
+                priority=self.low_priority,
+                inst=(self.eth_src_table.goto(self.output_table),),
+            )
+        )
         ofmsgs.extend(self._build_flood_rules(vlan, cold_start))
         return ofmsgs
 
     def del_vlan(self, vlan):
         return [
-            self.flood_table.flowdel(
-                match=self.flood_table.match(vlan=vlan)),
+            self.flood_table.flowdel(match=self.flood_table.match(vlan=vlan)),
             self.eth_src_table.flowdel(
-                match=self.eth_src_table.match(vlan=vlan), priority=self.low_priority)]
+                match=self.eth_src_table.match(vlan=vlan), priority=self.low_priority
+            ),
+        ]
 
     def update_vlan(self, vlan):
         return self._build_flood_rules(vlan, cold_start=False, modify=True)
 
     def _find_forwarding_table(self, vlan):
         if vlan.acls_in:
             return self.vlan_acl_table
         return self.classification_table()
 
     def _port_add_vlan_rules(self, port, vlan, mirror_act, push_vlan=True):
         actions = copy.copy(mirror_act)
         match_vlan = vlan
         if push_vlan:
-            actions.extend(valve_of.push_vlan_act(
-                self.vlan_table, vlan.vid))
+            actions.extend(valve_of.push_vlan_act(self.vlan_table, vlan.vid))
             match_vlan = NullVLAN()
         if self.has_externals:
             if port.loop_protect_external:
                 actions.append(self.vlan_table.set_no_external_forwarding_requested())
             else:
                 actions.append(self.vlan_table.set_external_forwarding_requested())
         inst = (
             valve_of.apply_actions(actions),
-            self.vlan_table.goto(self._find_forwarding_table(vlan)))
+            self.vlan_table.goto(self._find_forwarding_table(vlan)),
+        )
         return self.vlan_table.flowmod(
             self.vlan_table.match(in_port=port.number, vlan=match_vlan),
-            priority=self.low_priority, inst=inst)
+            priority=self.low_priority,
+            inst=inst,
+        )
 
     @staticmethod
     def _native_vlan(port):
         for native_vlan in (port.dyn_dot1x_native_vlan, port.native_vlan):
             if native_vlan is not None:
                 return native_vlan
         return None
@@ -382,68 +590,84 @@
             ofmsgs.extend(self.lacp_req_reply(port.dyn_last_lacp_pkt, port))
         return ofmsgs
 
     def _lacp_match(self, port):
         return self.vlan_table.match(
             in_port=port.number,
             eth_type=valve_of.ether.ETH_TYPE_SLOW,
-            eth_dst=valve_packet.SLOW_PROTOCOL_MULTICAST)
+            eth_dst=valve_packet.SLOW_PROTOCOL_MULTICAST,
+        )
 
     def add_port(self, port):
         ofmsgs = []
         if port.vlans():
             mirror_act = port.mirror_actions()
             for vlan in port.tagged_vlans:
-                ofmsgs.append(self._port_add_vlan_rules(
-                    port, vlan, mirror_act, push_vlan=False))
+                ofmsgs.append(
+                    self._port_add_vlan_rules(port, vlan, mirror_act, push_vlan=False)
+                )
             native_vlan = self._native_vlan(port)
             if native_vlan is not None:
-                ofmsgs.append(self._port_add_vlan_rules(
-                    port, native_vlan, mirror_act))
+                ofmsgs.append(self._port_add_vlan_rules(port, native_vlan, mirror_act))
             # If no untagged VLANs, add explicit drop rule for untagged packets.
             elif port.count_untag_vlan_miss:
-                ofmsgs.append(self.vlan_table.flowdrop(
-                    self.vlan_table.match(in_port=port.number, vlan=NullVLAN()),
-                    priority=self.low_priority))
+                ofmsgs.append(
+                    self.vlan_table.flowdrop(
+                        self.vlan_table.match(in_port=port.number, vlan=NullVLAN()),
+                        priority=self.low_priority,
+                    )
+                )
             if port.lacp:
-                ofmsgs.append(self.vlan_table.flowcontroller(
-                    self._lacp_match(port),
-                    priority=self.dp_highest_priority,
-                    max_len=valve_packet.LACP_SIZE))
+                ofmsgs.append(
+                    self.vlan_table.flowcontroller(
+                        self._lacp_match(port),
+                        priority=self.dp_highest_priority,
+                        max_len=valve_packet.LACP_SIZE,
+                    )
+                )
                 ofmsgs.extend(self.lacp_advertise(port))
         return ofmsgs
 
     def _del_host_flows(self, port):
         ofmsgs = []
         ofmsgs.append(
-            self.eth_src_table.flowdel(self.eth_src_table.match(in_port=port.number)))
+            self.eth_src_table.flowdel(self.eth_src_table.match(in_port=port.number))
+        )
         for table in (self.eth_dst_table, self.eth_dst_hairpin_table):
             if table:
                 # per OF 1.3.5 B.6.23, the OFA will match flows
                 # that have an action targeting this port.
                 ofmsgs.append(table.flowdel(out_port=port.number))
         return ofmsgs
 
     def del_port(self, port):
         ofmsgs = []
         if port.vlans():
             ofmsgs.extend(self._del_host_flows(port))
             native_vlan = self._native_vlan(port)
             if native_vlan is not None or port.count_untag_vlan_miss:
-                ofmsgs.append(self.vlan_table.flowdel(
-                    self.vlan_table.match(in_port=port.number, vlan=NullVLAN()),
-                    priority=self.low_priority))
+                ofmsgs.append(
+                    self.vlan_table.flowdel(
+                        self.vlan_table.match(in_port=port.number, vlan=NullVLAN()),
+                        priority=self.low_priority,
+                    )
+                )
             for vlan in port.tagged_vlans:
-                ofmsgs.append(self.vlan_table.flowdel(
-                    self.vlan_table.match(in_port=port.number, vlan=vlan),
-                    priority=self.low_priority))
+                ofmsgs.append(
+                    self.vlan_table.flowdel(
+                        self.vlan_table.match(in_port=port.number, vlan=vlan),
+                        priority=self.low_priority,
+                    )
+                )
             if port.lacp:
-                ofmsgs.append(self.vlan_table.flowdel(
-                    self._lacp_match(port),
-                    priority=self.dp_highest_priority))
+                ofmsgs.append(
+                    self.vlan_table.flowdel(
+                        self._lacp_match(port), priority=self.dp_highest_priority
+                    )
+                )
         return ofmsgs
 
     def _build_flood_rules(self, vlan, cold_start, modify=False):
         """Add flows to flood packets to unknown destinations on a VLAN."""
         command = valve_of.ofp.OFPFC_ADD
         if modify:
             command = valve_of.ofp.OFPFC_MODIFY_STRICT
@@ -478,54 +702,68 @@
         eth_src = pkt_meta.eth_src
         vlan = pkt_meta.vlan
 
         entry = vlan.cached_host(eth_src)
         if entry is None:
             if port.max_hosts:
                 if port.hosts_count() == port.max_hosts:
-                    ofmsgs.append(self._temp_ban_host_learning(
-                        self.eth_src_table.match(in_port=port.number)))
+                    ofmsgs.append(
+                        self._temp_ban_host_learning(
+                            self.eth_src_table.match(in_port=port.number)
+                        )
+                    )
                     port.dyn_learn_ban_count += 1
                     self.logger.info(
-                        f'max hosts {port.max_hosts} reached on {port}, '
-                        f'temporarily banning learning on this port, '
-                        f'and not learning {eth_src}')
+                        "max hosts %u reached on %s, "
+                        "temporarily banning learning on this port, "
+                        "and not learning %s" % (port.max_hosts, port, eth_src)
+                    )
             if vlan is not None and vlan.max_hosts:
                 hosts_count = vlan.hosts_count()
                 if hosts_count == vlan.max_hosts:
-                    ofmsgs.append(self._temp_ban_host_learning(self.eth_src_table.match(vlan=vlan)))
+                    ofmsgs.append(
+                        self._temp_ban_host_learning(
+                            self.eth_src_table.match(vlan=vlan)
+                        )
+                    )
                     vlan.dyn_learn_ban_count += 1
                     self.logger.info(
-                        f'max hosts {vlan.max_hosts} reached on VLAN {vlan.vid}, '
-                        f'temporarily banning learning on this VLAN, '
-                        f'and not learning {eth_src} on {port}')
+                        "max hosts %u reached on VLAN %u, "
+                        "temporarily banning learning on this VLAN, "
+                        "and not learning %s on %s"
+                        % (vlan.max_hosts, vlan.vid, eth_src, port)
+                    )
         return ofmsgs
 
     def _temp_ban_host_learning(self, match):
         return self.eth_src_table.flowdrop(
-            match,
-            priority=(self.low_priority + 1),
-            hard_timeout=self.learn_ban_timeout)
+            match, priority=(self.low_priority + 1), hard_timeout=self.learn_ban_timeout
+        )
 
     def delete_host_from_vlan(self, eth_src, vlan):
         """Delete a host from a VLAN."""
-        ofmsgs = [self.eth_src_table.flowdel(
-            self.eth_src_table.match(vlan=vlan, eth_src=eth_src))]
+        ofmsgs = [
+            self.eth_src_table.flowdel(
+                self.eth_src_table.match(vlan=vlan, eth_src=eth_src)
+            )
+        ]
         for table in (self.eth_dst_table, self.eth_dst_hairpin_table):
             if table:
                 ofmsgs.append(table.flowdel(table.match(vlan=vlan, eth_dst=eth_src)))
         return ofmsgs
 
     def expire_hosts_from_vlan(self, vlan, now):
         """Expire hosts from VLAN cache."""
         expired_hosts = vlan.expire_cache_hosts(now, self.learn_timeout)
         if expired_hosts:
             vlan.dyn_last_time_hosts_expired = now
             self.logger.info(
-                f'{vlan.hosts_count()} recently active hosts on VLAN {vlan.vid}, expired {expired_hosts}')
+                "%u recently active hosts on VLAN %u, expired %s"
+                % (vlan.hosts_count(), vlan.vid, expired_hosts)
+            )
         return expired_hosts
 
     def _jitter_learn_timeout(self, base_learn_timeout, port, eth_dst):
         """Calculate jittered learning timeout to avoid synchronized host timeouts."""
         # Hosts on this port never timeout.
         if port.permanent_learn:
             return 0
@@ -558,125 +796,202 @@
         return (src_rule_idle_timeout, src_rule_hard_timeout, dst_rule_idle_timeout)
 
     def _external_forwarding_requested(self, port):  # pylint: disable=unused-argument
         if self.has_externals:
             return True
         return None
 
-    def learn_host_on_vlan_port_flows(self, port, vlan, eth_src,
-                                      delete_existing, refresh_rules,
-                                      src_rule_idle_timeout,
-                                      src_rule_hard_timeout,
-                                      dst_rule_idle_timeout):
+    def learn_host_on_vlan_port_flows(
+        self,
+        port,
+        vlan,
+        eth_src,
+        delete_existing,
+        refresh_rules,
+        src_rule_idle_timeout,
+        src_rule_hard_timeout,
+        dst_rule_idle_timeout,
+    ):  # pylint: disable=too-many-arguments
         """Return flows that implement learning a host on a port."""
         ofmsgs = []
 
         # Delete any existing entries for MAC.
         if delete_existing:
             ofmsgs.extend(self.delete_host_from_vlan(eth_src, vlan))
 
         # Associate this MAC with source port.
         src_match = self.eth_src_table.match(
-            in_port=port.number, vlan=vlan, eth_src=eth_src)
+            in_port=port.number, vlan=vlan, eth_src=eth_src
+        )
         src_priority = self.host_priority - 1
 
         inst = (self.eth_src_table.goto(self.output_table),)
-        ofmsgs.append(self.eth_src_table.flowmod(
-            match=src_match,
-            priority=src_priority,
-            inst=inst,
-            hard_timeout=src_rule_hard_timeout,
-            idle_timeout=src_rule_idle_timeout))
+        ofmsgs.append(
+            self.eth_src_table.flowmod(
+                match=src_match,
+                priority=src_priority,
+                inst=inst,
+                hard_timeout=src_rule_hard_timeout,
+                idle_timeout=src_rule_idle_timeout,
+            )
+        )
 
         hairpinning = port.hairpin or port.hairpin_unicast
         # If we are refreshing only and not in hairpin mode, leave existing eth_dst alone.
         if refresh_rules and not hairpinning:
             return ofmsgs
 
         match_dict = {
-            'vlan': vlan, 'eth_dst': eth_src, valve_of.EXTERNAL_FORWARDING_FIELD: None}
+            "vlan": vlan,
+            "eth_dst": eth_src,
+            valve_of.EXTERNAL_FORWARDING_FIELD: None,
+        }
         if self.has_externals:
-            match_dict.update({
-                valve_of.EXTERNAL_FORWARDING_FIELD: valve_of.PCP_EXT_PORT_FLAG})
+            match_dict.update(
+                {valve_of.EXTERNAL_FORWARDING_FIELD: valve_of.PCP_EXT_PORT_FLAG}
+            )
 
         inst = self.pipeline.output(
-            port, vlan, external_forwarding_requested=self._external_forwarding_requested(port))
+            port,
+            vlan,
+            external_forwarding_requested=self._external_forwarding_requested(port),
+        )
 
         # Output packets for this MAC to specified port.
-        ofmsgs.append(self.eth_dst_table.flowmod(
-            self.eth_dst_table.match(**match_dict),
-            priority=self.host_priority,
-            inst=inst,
-            idle_timeout=dst_rule_idle_timeout))
-
-        if self.has_externals and not port.loop_protect_external:
-            match_dict.update({
-                valve_of.EXTERNAL_FORWARDING_FIELD: valve_of.PCP_NONEXT_PORT_FLAG})
-            ofmsgs.append(self.eth_dst_table.flowmod(
+        ofmsgs.append(
+            self.eth_dst_table.flowmod(
                 self.eth_dst_table.match(**match_dict),
                 priority=self.host_priority,
                 inst=inst,
-                idle_timeout=dst_rule_idle_timeout))
+                idle_timeout=dst_rule_idle_timeout,
+            )
+        )
+
+        if self.has_externals and not port.loop_protect_external:
+            match_dict.update(
+                {valve_of.EXTERNAL_FORWARDING_FIELD: valve_of.PCP_NONEXT_PORT_FLAG}
+            )
+            ofmsgs.append(
+                self.eth_dst_table.flowmod(
+                    self.eth_dst_table.match(**match_dict),
+                    priority=self.host_priority,
+                    inst=inst,
+                    idle_timeout=dst_rule_idle_timeout,
+                )
+            )
 
         # If port is in hairpin mode, install a special rule
         # that outputs packets destined to this MAC back out the same
         # port they came in (e.g. multiple hosts on same WiFi AP,
         # and FAUCET is switching between them on the same port).
         if hairpinning:
-            ofmsgs.append(self.eth_dst_hairpin_table.flowmod(
-                self.eth_dst_hairpin_table.match(in_port=port.number, vlan=vlan, eth_dst=eth_src),
-                priority=self.host_priority,
-                inst=self.pipeline.output(port, vlan, hairpin=True),
-                idle_timeout=dst_rule_idle_timeout))
+            ofmsgs.append(
+                self.eth_dst_hairpin_table.flowmod(
+                    self.eth_dst_hairpin_table.match(
+                        in_port=port.number, vlan=vlan, eth_dst=eth_src
+                    ),
+                    priority=self.host_priority,
+                    inst=self.pipeline.output(port, vlan, hairpin=True),
+                    idle_timeout=dst_rule_idle_timeout,
+                )
+            )
 
         return ofmsgs
 
-    # pylint: disable=unused-argument
-    def _perm_learn_check(self, entry, vlan, now, eth_src, port, ofmsgs,
-                          cache_port, cache_age,
-                          delete_existing, refresh_rules):
+    def _perm_learn_check(
+        self,
+        entry,
+        vlan,
+        now,
+        eth_src,
+        port,
+        ofmsgs,
+        cache_port,
+        cache_age,
+        delete_existing,
+        refresh_rules,
+    ):  # pylint: disable=too-many-arguments,disable=unused-argument
         learn_exit = False
         update_cache = True
         if entry is not None and entry.port.permanent_learn:
             if entry.port != port:
-                ofmsgs.extend(self.pipeline.filter_packets(
-                    {'eth_src': eth_src, 'in_port': port.number}))
+                ofmsgs.extend(
+                    self.pipeline.filter_packets(
+                        {"eth_src": eth_src, "in_port": port.number}
+                    )
+                )
             learn_exit = True
             update_cache = False
-        return (learn_exit, ofmsgs, cache_port, update_cache, delete_existing, refresh_rules)
-
-    def _learn_cache_check(self, entry, vlan, now, eth_src, port, ofmsgs,  # pylint: disable=unused-argument
-                           cache_port, cache_age,
-                           delete_existing, refresh_rules):
+        return (
+            learn_exit,
+            ofmsgs,
+            cache_port,
+            update_cache,
+            delete_existing,
+            refresh_rules,
+        )
+
+    def _learn_cache_check(
+        self,
+        entry,
+        vlan,
+        now,
+        eth_src,
+        port,
+        ofmsgs,
+        cache_port,
+        cache_age,
+        delete_existing,
+        refresh_rules,
+    ):  # pylint: disable=unused-argument,disable=too-many-arguments
         learn_exit = False
         update_cache = True
         if cache_port is not None:
             # packet was received on same member of a LAG.
-            same_lag = (port.lacp and port.lacp == cache_port.lacp)
+            same_lag = port.lacp and port.lacp == cache_port.lacp
             guard_time = self.cache_update_guard_time
             if cache_port == port or same_lag:
                 port_cache_valid = (
-                    port.dyn_update_time is not None and port.dyn_update_time <= entry.cache_time)
+                    port.dyn_update_time is not None
+                    and port.dyn_update_time <= entry.cache_time
+                )
                 # aggressively re-learn on LAGs
                 if same_lag:
                     guard_time = 2
                 # port didn't change status, and recent cache update, don't do anything.
                 if cache_age < guard_time and port_cache_valid:
                     update_cache = False
                     learn_exit = True
                 # skip delete if host didn't change ports or on same LAG.
                 elif cache_port == port or same_lag:
                     delete_existing = False
                     if port_cache_valid:
                         refresh_rules = True
-        return (learn_exit, ofmsgs, cache_port, update_cache, delete_existing, refresh_rules)
-
-    def _loop_protect_check(self, entry, vlan, now, eth_src, port, ofmsgs,  # pylint: disable=unused-argument
-                            cache_port, cache_age,
-                            delete_existing, refresh_rules):
+        return (
+            learn_exit,
+            ofmsgs,
+            cache_port,
+            update_cache,
+            delete_existing,
+            refresh_rules,
+        )
+
+    def _loop_protect_check(
+        self,
+        entry,
+        vlan,
+        now,
+        eth_src,
+        port,
+        ofmsgs,
+        cache_port,
+        cache_age,
+        delete_existing,
+        refresh_rules,
+    ):  # pylint: disable=unused-argument,disable=too-many-arguments
         learn_exit = False
         update_cache = True
         if port.loop_protect:
             ban_age = None
             learn_ban = False
 
             # if recently in loop protect mode and still receiving packets,
@@ -687,114 +1002,192 @@
                     learn_ban = True
 
             # if not in protect mode and we get a rapid move, enact protect mode
             if not learn_ban and entry is not None:
                 if port != cache_port and cache_age < self.cache_update_guard_time:
                     learn_ban = True
                     port.dyn_learn_ban_count += 1
-                    self.logger.info(f'rapid move of {eth_src} from {cache_port} '
-                                     f'to {port}, temp loop ban {port}')
+                    self.logger.info(
+                        "rapid move of %s from %s to %s, temp loop ban %s"
+                        % (eth_src, cache_port, port, port)
+                    )
 
             # already, or newly in protect mode, apply the ban rules.
             if learn_ban:
                 port.dyn_last_ban_time = now
-                ofmsgs.append(self._temp_ban_host_learning(
-                    self.eth_src_table.match(in_port=port.number)))
+                ofmsgs.append(
+                    self._temp_ban_host_learning(
+                        self.eth_src_table.match(in_port=port.number)
+                    )
+                )
                 learn_exit = True
-        return (learn_exit, ofmsgs, cache_port, update_cache, delete_existing, refresh_rules)
-
-    # pylint: disable=unused-argument
-    def _learn_check(self, entry, vlan, now, eth_src, port, ofmsgs,
-                     cache_port, cache_age,
-                     delete_existing, refresh_rules):
+        return (
+            learn_exit,
+            ofmsgs,
+            cache_port,
+            update_cache,
+            delete_existing,
+            refresh_rules,
+        )
+
+    def _learn_check(
+        self,
+        entry,
+        vlan,
+        now,
+        eth_src,
+        port,
+        ofmsgs,
+        cache_port,
+        cache_age,
+        delete_existing,
+        refresh_rules,
+    ):  # pylint: disable=unused-argument,disable=too-many-arguments
         learn_exit = True
         update_cache = True
-        (src_rule_idle_timeout,
-         src_rule_hard_timeout,
-         dst_rule_idle_timeout) = self._learn_host_timeouts(port, eth_src)
-
-        ofmsgs.extend(self.learn_host_on_vlan_port_flows(
-            port, vlan, eth_src, delete_existing, refresh_rules,
-            src_rule_idle_timeout, src_rule_hard_timeout,
-            dst_rule_idle_timeout))
-        return (learn_exit, ofmsgs, cache_port, update_cache, delete_existing, refresh_rules)
-
-    def learn_host_on_vlan_ports(self, now, port, vlan, eth_src,
-                                 delete_existing=True,
-                                 last_dp_coldstart_time=None):
+        (
+            src_rule_idle_timeout,
+            src_rule_hard_timeout,
+            dst_rule_idle_timeout,
+        ) = self._learn_host_timeouts(port, eth_src)
+
+        ofmsgs.extend(
+            self.learn_host_on_vlan_port_flows(
+                port,
+                vlan,
+                eth_src,
+                delete_existing,
+                refresh_rules,
+                src_rule_idle_timeout,
+                src_rule_hard_timeout,
+                dst_rule_idle_timeout,
+            )
+        )
+        return (
+            learn_exit,
+            ofmsgs,
+            cache_port,
+            update_cache,
+            delete_existing,
+            refresh_rules,
+        )
+
+    def learn_host_on_vlan_ports(
+        self,
+        now,
+        port,
+        vlan,
+        eth_src,
+        delete_existing=True,
+        last_dp_coldstart_time=None,
+    ):  # pylint: disable=unused-argument,disable=too-many-arguments
         """Learn a host on a port."""
         ofmsgs = []
         cache_port = None
         cache_age = None
         refresh_rules = False
         update_cache = True
         entry = vlan.cached_host(eth_src)
 
         # Host not cached, and no hosts expired since we cold started
         # Enable faster learning by assuming there's no previous host to delete
         if entry is None:
-            if (last_dp_coldstart_time
-                    and (vlan.dyn_last_time_hosts_expired is None
-                         or vlan.dyn_last_time_hosts_expired < last_dp_coldstart_time)):
+            if last_dp_coldstart_time and (
+                vlan.dyn_last_time_hosts_expired is None
+                or vlan.dyn_last_time_hosts_expired < last_dp_coldstart_time
+            ):
                 delete_existing = False
         else:
             cache_age = now - entry.cache_time
             cache_port = entry.port
 
         for learn_func in (
-                self._perm_learn_check, self._learn_cache_check,
-                self._loop_protect_check, self._learn_check):
-            (learn_exit, ofmsgs, cache_port, update_cache,
-             delete_existing, refresh_rules) = learn_func(
-                 entry, vlan, now, eth_src, port, ofmsgs, cache_port, cache_age,
-                 delete_existing, refresh_rules)
+            self._perm_learn_check,
+            self._learn_cache_check,
+            self._loop_protect_check,
+            self._learn_check,
+        ):
+            (
+                learn_exit,
+                ofmsgs,
+                cache_port,
+                update_cache,
+                delete_existing,
+                refresh_rules,
+            ) = learn_func(
+                entry,
+                vlan,
+                now,
+                eth_src,
+                port,
+                ofmsgs,
+                cache_port,
+                cache_age,
+                delete_existing,
+                refresh_rules,
+            )
             if learn_exit:
                 break
 
         return (ofmsgs, cache_port, update_cache)
 
     @staticmethod
     def flow_timeout(_now, _table_id, _match):
         """Handle a flow timed out message from dataplane."""
         return []
 
-    def lacp_update_actor_state(self, port, lacp_up, now=None, lacp_pkt=None, cold_start=False):
+    def lacp_update_actor_state(
+        self, port, lacp_up, now=None, lacp_pkt=None, cold_start=False
+    ):  # pylint: disable=too-many-arguments
         """Updates a LAG actor state.
 
         Args:
             port: LACP port
             lacp_up (bool): Whether LACP is going UP or DOWN
             now (float): Current epoch time
             lacp_pkt (PacketMeta): LACP packet
             cold_start (bool): Whether the port is being cold started
         Returns:
             bool: True if LACP state changed
         """
         prev_actor_state = port.actor_state()
         new_actor_state = port.lacp_actor_update(
-            lacp_up, now=now, lacp_pkt=lacp_pkt,
-            cold_start=cold_start)
+            lacp_up, now=now, lacp_pkt=lacp_pkt, cold_start=cold_start
+        )
         if prev_actor_state != new_actor_state:
-            self.logger.info(f'LAG {port.lacp} {port} actor state '
-                             f'{port.actor_state_name(new_actor_state)} (previous state '
-                             f'{port.actor_state_name(prev_actor_state)})')
+            self.logger.info(
+                "LAG %u %s actor state %s (previous state %s)"
+                % (
+                    port.lacp,
+                    port,
+                    port.actor_state_name(new_actor_state),
+                    port.actor_state_name(prev_actor_state),
+                )
+            )
         return prev_actor_state != new_actor_state
 
     def enable_forwarding(self, port):
         ofmsgs = []
-        ofmsgs.append(self.vlan_table.flowdel(
-            match=self.vlan_table.match(in_port=port.number),
-            priority=self.dp_high_priority, strict=True))
+        ofmsgs.append(
+            self.vlan_table.flowdel(
+                match=self.vlan_table.match(in_port=port.number),
+                priority=self.dp_high_priority,
+                strict=True,
+            )
+        )
         return ofmsgs
 
     def disable_forwarding(self, port):
         ofmsgs = []
-        ofmsgs.append(self.vlan_table.flowdrop(
-            match=self.vlan_table.match(in_port=port.number),
-            priority=self.dp_high_priority))
+        ofmsgs.append(
+            self.vlan_table.flowdrop(
+                match=self.vlan_table.match(in_port=port.number),
+                priority=self.dp_high_priority,
+            )
+        )
         return ofmsgs
 
     def lacp_req_reply(self, lacp_pkt, port):
         """
         Constructs a LACP req-reply packet.
 
         Args:
@@ -805,133 +1198,184 @@
         Returns:
             list packetout OpenFlow msgs.
         """
         if port.lacp_passthrough:
             for peer_num in port.lacp_passthrough:
                 lacp_peer = self.ports.get(peer_num, None)
                 if not lacp_peer.dyn_lacp_up:
-                    self.logger.warning(f'Suppressing LACP LAG {port.lacp} on '
-                                        f'{port}, peer {lacp_peer} link is down')
+                    self.logger.warning(
+                        "Suppressing LACP LAG %s on %s, peer %s link is down"
+                        % (port.lacp, port, lacp_peer)
+                    )
                     return []
         actor_state_activity = 0
         if port.lacp_active:
             actor_state_activity = 1
         actor_state_sync, actor_state_col, actor_state_dist = port.get_lacp_flags()
         if lacp_pkt:
             pkt = valve_packet.lacp_reqreply(
-                self.faucet_dp_mac, self.faucet_dp_mac,
-                port.lacp, port.lacp_port_id, port.lacp_port_priority,
-                actor_state_sync, actor_state_activity,
-                actor_state_col, actor_state_dist,
-                lacp_pkt.actor_system, lacp_pkt.actor_key, lacp_pkt.actor_port,
-                lacp_pkt.actor_system_priority, lacp_pkt.actor_port_priority,
+                self.faucet_dp_mac,
+                self.faucet_dp_mac,
+                port.lacp,
+                port.lacp_port_id,
+                port.lacp_port_priority,
+                actor_state_sync,
+                actor_state_activity,
+                actor_state_col,
+                actor_state_dist,
+                lacp_pkt.actor_system,
+                lacp_pkt.actor_key,
+                lacp_pkt.actor_port,
+                lacp_pkt.actor_system_priority,
+                lacp_pkt.actor_port_priority,
                 lacp_pkt.actor_state_defaulted,
                 lacp_pkt.actor_state_expired,
                 lacp_pkt.actor_state_timeout,
                 lacp_pkt.actor_state_collecting,
                 lacp_pkt.actor_state_distributing,
                 lacp_pkt.actor_state_aggregation,
                 lacp_pkt.actor_state_synchronization,
-                lacp_pkt.actor_state_activity)
+                lacp_pkt.actor_state_activity,
+            )
         else:
             pkt = valve_packet.lacp_reqreply(
-                self.faucet_dp_mac, self.faucet_dp_mac,
-                port.lacp, port.lacp_port_id, port.lacp_port_priority,
+                self.faucet_dp_mac,
+                self.faucet_dp_mac,
+                port.lacp,
+                port.lacp_port_id,
+                port.lacp_port_priority,
                 actor_state_synchronization=actor_state_sync,
                 actor_state_activity=actor_state_activity,
                 actor_state_collecting=actor_state_col,
-                actor_state_distributing=actor_state_dist)
-        self.logger.debug(f'Sending LACP {pkt} on {port} activity {actor_state_activity}')
+                actor_state_distributing=actor_state_dist,
+            )
+        self.logger.debug(
+            "Sending LACP %s on %s activity %s" % (pkt, port, actor_state_activity)
+        )
         return [valve_of.packetout(port.number, bytes(pkt.data))]
 
     @staticmethod
-    def get_lacp_dpid_nomination(lacp_id, valve, other_valves):  # pylint: disable=unused-argument
+    def get_lacp_dpid_nomination(
+        lacp_id, valve, other_valves
+    ):  # pylint: disable=unused-argument
         """Chooses the DP for a given LAG.
 
         The DP will be nominated by the following conditions in order:
             1) Number of LAG ports
             2) Root DP
             3) Lowest DPID
 
         Args:
             lacp_id: The LACP LAG ID
             other_valves (list): list of other valves
         Returns:
             nominated_dpid, reason
         """
-        return (valve.dp.dp_id, 'standalone')
+        return (valve.dp.dp_id, "standalone")
 
-    def lacp_update_port_selection_state(self, port, valve, other_valves=None, cold_start=False):
+    def lacp_update_port_selection_state(
+        self, port, valve, other_valves=None, cold_start=False
+    ):
         """Update the LACP port selection state.
 
         Args:
             port (Port): LACP port
             other_valves (list): List of other valves
             cold_start (bool): Whether the port is being cold started
         Returns:
             bool: True if port state changed
         """
-        nominated_dpid, _ = self.get_lacp_dpid_nomination(port.lacp, valve, other_valves)
+        nominated_dpid, _ = self.get_lacp_dpid_nomination(
+            port.lacp, valve, other_valves
+        )
         prev_state = port.lacp_port_state()
-        new_state = port.lacp_port_update(valve.dp.dp_id == nominated_dpid, cold_start=cold_start)
+        new_state = port.lacp_port_update(
+            valve.dp.dp_id == nominated_dpid, cold_start=cold_start
+        )
         if new_state != prev_state:
-            self.logger.info(f'LAG {port.lacp} {port} '
-                             f'{port.port_role_name(new_state)} '
-                             f'(previous state{port.port_role_name(prev_state)})')
+            self.logger.info(
+                "LAG %u %s %s (previous state %s)"
+                % (
+                    port.lacp,
+                    port,
+                    port.port_role_name(new_state),
+                    port.port_role_name(prev_state),
+                )
+            )
         return new_state != prev_state
 
-    def lacp_handler(self, now, pkt_meta, valve, other_valves, lacp_update):
+    def lacp_handler(
+        self, now, pkt_meta, valve, other_valves, lacp_update
+    ):  # pylint: disable=too-many-arguments
         """
         Handle receiving an LACP packet
         Args:
             now (float): current epoch time
             pkt_meta (PacketMeta): packet for control plane
             valve (Valve): valve instance
             other_valves (list): all other valves
             lacp_update: callable to signal LACP state changes
         Returns
             dict: OpenFlow messages, if any by Valve
         """
         ofmsgs_by_valve = defaultdict(list)
-        if (pkt_meta.eth_dst == valve_packet.SLOW_PROTOCOL_MULTICAST
-                and pkt_meta.eth_type == valve_of.ether.ETH_TYPE_SLOW
-                and pkt_meta.port.lacp):
+        if (
+            pkt_meta.eth_dst == valve_packet.SLOW_PROTOCOL_MULTICAST
+            and pkt_meta.eth_type == valve_of.ether.ETH_TYPE_SLOW
+            and pkt_meta.port.lacp
+        ):
             # LACP packet so reparse
-            pkt_meta.data = pkt_meta.data[:valve_packet.LACP_SIZE]
+            pkt_meta.data = pkt_meta.data[: valve_packet.LACP_SIZE]
             pkt_meta.reparse_all()
             lacp_pkt = valve_packet.parse_lacp_pkt(pkt_meta.pkt)
             if lacp_pkt:
-                self.logger.debug(f'receive LACP {lacp_pkt} on {pkt_meta.port}')
+                self.logger.debug("receive LACP %s on %s" % (lacp_pkt, pkt_meta.port))
                 # Respond to new LACP packet or if we haven't sent anything in a while
                 age = None
                 if pkt_meta.port.dyn_lacp_last_resp_time:
                     age = now - pkt_meta.port.dyn_lacp_last_resp_time
-                lacp_pkt_change = (
-                    pkt_meta.port.dyn_last_lacp_pkt is None
-                    or str(lacp_pkt) != str(pkt_meta.port.dyn_last_lacp_pkt))
+                lacp_pkt_change = pkt_meta.port.dyn_last_lacp_pkt is None or str(
+                    lacp_pkt
+                ) != str(pkt_meta.port.dyn_last_lacp_pkt)
                 lacp_resp_interval = pkt_meta.port.lacp_resp_interval
                 if lacp_pkt_change or (age is not None and age > lacp_resp_interval):
                     ofmsgs_by_valve[valve].extend(
-                        self.lacp_req_reply(lacp_pkt, pkt_meta.port))
+                        self.lacp_req_reply(lacp_pkt, pkt_meta.port)
+                    )
                     pkt_meta.port.dyn_lacp_last_resp_time = now
                 # Update the LACP information
                 actor_up = lacp_pkt.actor_state_synchronization
-                ofmsgs_by_valve[valve].extend(lacp_update(
-                    pkt_meta.port, actor_up, now=now, lacp_pkt=lacp_pkt, other_valves=other_valves))
+                ofmsgs_by_valve[valve].extend(
+                    lacp_update(
+                        pkt_meta.port,
+                        actor_up,
+                        now=now,
+                        lacp_pkt=lacp_pkt,
+                        other_valves=other_valves,
+                    )
+                )
                 # Determine if LACP ports with the same ID have met different actor systems
                 other_lag_ports = [
-                    port for port in self.ports.values()
-                    if port.lacp == pkt_meta.port.lacp and port.dyn_last_lacp_pkt]
+                    port
+                    for port in self.ports.values()
+                    if port.lacp == pkt_meta.port.lacp and port.dyn_last_lacp_pkt
+                ]
                 actor_system = lacp_pkt.actor_system
                 for other_lag_port in other_lag_ports:
                     other_actor_system = other_lag_port.dyn_last_lacp_pkt.actor_system
                     if actor_system != other_actor_system:
                         self.logger.error(
-                            f'LACP actor system mismatch {pkt_meta.port}: '
-                            f'{actor_system}, {other_lag_port} {other_actor_system}')
+                            "LACP actor system mismatch %s: %s, %s %s"
+                            % (
+                                pkt_meta.port,
+                                actor_system,
+                                other_lag_port,
+                                other_actor_system,
+                            )
+                        )
         return ofmsgs_by_valve
 
     @staticmethod
     def learn_host_from_pkt(valve, now, pkt_meta, other_valves):
         """Learn host from packet."""
         ofmsgs = []
         ofmsgs.extend(valve.learn_host(now, pkt_meta, other_valves))
@@ -946,25 +1390,25 @@
 
         not currently reliable.
     """
 
     def flow_timeout(self, now, table_id, match):
         ofmsgs = []
         if table_id in (self.eth_src_table.table_id, self.eth_dst_table.table_id):
-            if 'vlan_vid' in match:
-                vlan = self.vlans[valve_of.devid_present(match['vlan_vid'])]
+            if "vlan_vid" in match:
+                vlan = self.vlans[valve_of.devid_present(match["vlan_vid"])]
                 in_port = None
                 eth_src = None
                 eth_dst = None
                 for field, value in match.items():
-                    if field == 'in_port':
+                    if field == "in_port":
                         in_port = value
-                    elif field == 'eth_src':
+                    elif field == "eth_src":
                         eth_src = value
-                    elif field == 'eth_dst':
+                    elif field == "eth_dst":
                         eth_dst = value
                 if eth_src and in_port:
                     port = self.ports[in_port]
                     ofmsgs.extend(self._src_rule_expire(vlan, port, eth_src))
                 elif eth_dst:
                     ofmsgs.extend(self._dst_rule_expire(now, vlan, eth_dst))
         return ofmsgs
@@ -985,22 +1429,24 @@
     def _src_rule_expire(self, vlan, port, eth_src):
         """When a src rule expires, the host is probably inactive or active in
         receiving but not sending. We mark just mark the host as expired."""
         ofmsgs = []
         entry = vlan.cached_host_on_port(eth_src, port)
         if entry is not None:
             vlan.expire_cache_host(eth_src)
-            self.logger.info(f'expired src_rule for host {eth_src}')
+            self.logger.info("expired src_rule for host %s" % eth_src)
         return ofmsgs
 
     def _dst_rule_expire(self, now, vlan, eth_dst):
         """Expiring a dst rule may indicate that the host is actively sending
         traffic but not receving. If the src rule not yet expires, we reinstall
         host rules."""
         ofmsgs = []
         entry = vlan.cached_host(eth_dst)
         if entry is not None:
-            ofmsgs.extend(self.learn_host_on_vlan_ports(
-                now, entry.port, vlan, eth_dst, delete_existing=False))
-            self.logger.info(
-                f'refreshing host {eth_dst} from VLAN {vlan.vid}')
+            ofmsgs.extend(
+                self.learn_host_on_vlan_ports(
+                    now, entry.port, vlan, eth_dst, delete_existing=False
+                )
+            )
+            self.logger.info("refreshing host %s from VLAN %u" % (eth_dst, vlan.vid))
         return ofmsgs
```

### Comparing `c65faucet-1.0.46/faucet/valve_table.py` & `c65faucet-1.0.47/faucet/valve_table.py`

 * *Files 12% similar despite different names*

```diff
@@ -17,22 +17,27 @@
 # limitations under the License.
 
 import functools
 import hashlib
 import struct
 from faucet import valve_of
 from faucet.faucet_pipeline import ValveTableConfig
-from faucet.valve_util import LRU_MAX
 
 
 class ValveTable:  # pylint: disable=too-many-arguments,too-many-instance-attributes
     """Wrapper for an OpenFlow table."""
 
-    def __init__(self, name, table_config,
-                 flow_cookie, notify_flow_removed=False, next_tables=None):
+    def __init__(
+        self,
+        name,
+        table_config,
+        flow_cookie,
+        notify_flow_removed=False,
+        next_tables=None,
+    ):
         self.name = name
         self.table_config = table_config
         self.table_id = self.table_config.table_id
         self.set_fields = self.table_config.set_fields
         self.exact_match = self.table_config.exact_match
         self.match_types = None
         self.metadata_match = self.table_config.metadata_match
@@ -46,136 +51,196 @@
             for field, mask in self.table_config.match_types:
                 self.match_types[field] = mask
         self.flow_cookie = flow_cookie
         self.notify_flow_removed = notify_flow_removed
 
     def goto(self, next_table):
         """Add goto next table instruction."""
-        assert next_table.name in self.table_config.next_tables, (
-            f'{next_table.name} not configured as next table in {self.name}')
+        assert (
+            next_table.name in self.table_config.next_tables
+        ), "%s not configured as next table in %s" % (next_table.name, self.name)
         return valve_of.goto_table(next_table)
 
     def goto_this(self):
         return valve_of.goto_table(self)
 
     def goto_miss(self, next_table):
         """Add miss goto table instruction."""
-        assert next_table.name == self.table_config.miss_goto, (
-            f'{next_table.name} not configured as miss table in {self.name}')
+        assert (
+            next_table.name == self.table_config.miss_goto
+        ), "%s not configured as miss table in %s" % (next_table.name, self.name)
         return valve_of.goto_table(next_table)
 
     @staticmethod
     def set_field(**kwds):
         """Return set field action."""
         # raise exception if unknown set field.
         valve_of.match_from_dict(kwds)
         return valve_of.set_field(**kwds)
 
     def set_external_forwarding_requested(self):
         """Set field for external forwarding requested."""
-        return self.set_field(**{valve_of.EXTERNAL_FORWARDING_FIELD: valve_of.PCP_EXT_PORT_FLAG})
+        return self.set_field(
+            **{valve_of.EXTERNAL_FORWARDING_FIELD: valve_of.PCP_EXT_PORT_FLAG}
+        )
 
     def set_no_external_forwarding_requested(self):
         """Set field for no external forwarding requested."""
-        return self.set_field(**{valve_of.EXTERNAL_FORWARDING_FIELD: valve_of.PCP_NONEXT_PORT_FLAG})
+        return self.set_field(
+            **{valve_of.EXTERNAL_FORWARDING_FIELD: valve_of.PCP_NONEXT_PORT_FLAG}
+        )
 
     def set_vlan_vid(self, vlan_vid):
         """Set VLAN VID with VID_PRESENT flag set.
 
         Args:
             vid (int): VLAN VID
         Returns:
             ryu.ofproto.ofproto_v1_3_parser.OFPActionSetField: set VID with VID_PRESENT.
         """
         return self.set_field(vlan_vid=valve_of.vid_present(vlan_vid))
 
     # TODO: verify actions
     @staticmethod
-    @functools.lru_cache(maxsize=LRU_MAX)
-    def match(in_port=None, vlan=None,  # pylint: disable=too-many-arguments
-              eth_type=None, eth_src=None, eth_dst=None, eth_dst_mask=None,
-              icmpv6_type=None, nw_proto=None, nw_dst=None, metadata=None,
-              metadata_mask=None, vlan_pcp=None, udp_src=None, udp_dst=None):
+    @functools.lru_cache(maxsize=1024)
+    def match(
+        in_port=None,
+        vlan=None,  # pylint: disable=too-many-arguments
+        eth_type=None,
+        eth_src=None,
+        eth_dst=None,
+        eth_dst_mask=None,
+        icmpv6_type=None,
+        nw_proto=None,
+        nw_dst=None,
+        metadata=None,
+        metadata_mask=None,
+        vlan_pcp=None,
+        udp_src=None,
+        udp_dst=None,
+    ):
         """Compose an OpenFlow match rule."""
         match_dict = valve_of.build_match_dict(
-            in_port, vlan, eth_type, eth_src,
-            eth_dst, eth_dst_mask, icmpv6_type,
-            nw_proto, nw_dst, metadata, metadata_mask,
-            vlan_pcp, udp_src, udp_dst)
+            in_port,
+            vlan,
+            eth_type,
+            eth_src,
+            eth_dst,
+            eth_dst_mask,
+            icmpv6_type,
+            nw_proto,
+            nw_dst,
+            metadata,
+            metadata_mask,
+            vlan_pcp,
+            udp_src,
+            udp_dst,
+        )
         return valve_of.match(match_dict)
 
-    @functools.lru_cache(maxsize=LRU_MAX)
     def _verify_flowmod(self, flowmod):
         match_fields = flowmod.match.items()
         if valve_of.is_flowdel(flowmod):
             if self.table_id != valve_of.ofp.OFPTT_ALL:
                 for match_type, match_field in match_fields:
-                    assert match_type in self.match_types, (
-                        f'{match_type} match in table {self.name}')
+                    assert match_type in self.match_types, "%s match in table %s" % (
+                        match_type,
+                        self.name,
+                    )
         else:
             # TODO: ACL builder should not use ALL table.
             if self.table_id == valve_of.ofp.OFPTT_ALL:
                 return
-            assert not (flowmod.priority == 0 and match_fields), (
-                f'default flow cannot have matches on table {self.name}: {flowmod}')
+            assert not (
+                flowmod.priority == 0 and match_fields
+            ), "default flow cannot have matches on table %s: %s" % (self.name, flowmod)
             for match_type, match_field in match_fields:
-                assert match_type in self.match_types, (
-                    f'{match_type} match in table {self.name}')
+                assert match_type in self.match_types, "%s match in table %s" % (
+                    match_type,
+                    self.name,
+                )
                 config_mask = self.match_types[match_type]
                 flow_mask = isinstance(match_field, tuple)
-                assert config_mask or (not config_mask and not flow_mask), (
-                    f'{match_type} configured mask {config_mask} but flow mask '
-                    f'{flow_mask} in table {self.name} ({flowmod})')
+                assert config_mask or (
+                    not config_mask and not flow_mask
+                ), "%s configured mask %s but flow mask %s in table %s (%s)" % (
+                    match_type,
+                    config_mask,
+                    flow_mask,
+                    self.name,
+                    flowmod,
+                )
                 if self.exact_match and match_fields:
                     assert len(self.match_types) == len(match_fields), (
-                        f'exact match table {self.name} matches {self.match_types} '
-                        f'do not match flow matches {match_fields} ({flowmod})')
+                        "exact match table %s matches %s do not match flow matches %s (%s)"
+                        % (self.name, self.match_types, match_fields, flowmod)
+                    )
 
     def _trim_actions(self, actions):
         new_actions = []
         pending_actions = []
         for action in actions:
             if action.type in (valve_of.ofp.OFPAT_GROUP, valve_of.ofp.OFPAT_OUTPUT):
                 new_actions.extend(pending_actions)
                 new_actions.append(action)
                 pending_actions = []
             else:
                 pending_actions.append(action)
-        if self.table_id != valve_of.ofp.OFPTT_ALL:
-            set_fields = {action.key for action in new_actions if valve_of.is_set_field(action)}
-            assert not set_fields or set_fields.issubset(self.set_fields), (
-                f'unexpected set fields {set_fields} configured {self.set_fields} in {self.name}')
+        set_fields = {
+            action.key for action in new_actions if valve_of.is_set_field(action)
+        }
+        if self.table_id != valve_of.ofp.OFPTT_ALL and set_fields:
+            assert set_fields.issubset(
+                self.set_fields
+            ), "unexpected set fields %s configured %s in %s" % (
+                set_fields,
+                self.set_fields,
+                self.name,
+            )
         return new_actions
 
-    @functools.lru_cache(maxsize=LRU_MAX)
+    @functools.lru_cache()
     def _trim_inst(self, inst):
         """Discard empty/actions on packets that are not output and not goto another table."""
         inst_types = {instruction.type for instruction in inst}
         if valve_of.ofp.OFPIT_APPLY_ACTIONS in inst_types:
             goto_present = valve_of.ofp.OFPIT_GOTO_TABLE in inst_types
             new_inst = []
             for instruction in inst:
                 if instruction.type == valve_of.ofp.OFPIT_APPLY_ACTIONS:
-                    recirc_present = any((
-                        True for action in instruction.actions
-                        if valve_of.is_ct(action) and hasattr(action, 'recirc_table')
-                    ))
+                    recirc_present = any(
+                        (
+                            True
+                            for action in instruction.actions
+                            if valve_of.is_ct(action)
+                            and hasattr(action, "recirc_table")
+                        )
+                    )
                     # If no goto present, this is the last set of actions that can take place
                     if not goto_present and not recirc_present:
                         instruction.actions = self._trim_actions(instruction.actions)
                     # Always drop an apply actions instruction with no actions.
                     if not instruction.actions:
                         continue
                 new_inst.append(instruction)
             return tuple(new_inst)
         return inst
 
-    def flowmod(self, match=None, priority=None,  # pylint: disable=too-many-arguments
-                inst=None, command=valve_of.ofp.OFPFC_ADD, out_port=0,
-                out_group=0, hard_timeout=0, idle_timeout=0, cookie=None):
+    def flowmod(
+        self,
+        match=None,
+        priority=None,  # pylint: disable=too-many-arguments
+        inst=None,
+        command=valve_of.ofp.OFPFC_ADD,
+        out_port=0,
+        out_group=0,
+        hard_timeout=0,
+        idle_timeout=0,
+        cookie=None,
+    ):
         """Helper function to construct a flow mod message with cookie."""
         if priority is None:
             priority = 0  # self.dp.lowest_priority
         if not match:
             match = self.match()
         if inst is None:
             inst = ()
@@ -190,47 +255,53 @@
             cookie,
             command,
             self.table_id,
             priority,
             out_port,
             out_group,
             match,
-            inst,
+            tuple(inst),
             hard_timeout,
             idle_timeout,
-            flags)
+            flags,
+        )
         self._verify_flowmod(flowmod)
         return flowmod
 
-    def flowdel(self, match=None, priority=None, out_port=valve_of.ofp.OFPP_ANY, strict=False):
+    def flowdel(
+        self, match=None, priority=None, out_port=valve_of.ofp.OFPP_ANY, strict=False
+    ):
         """Delete matching flows from a table."""
         command = valve_of.ofp.OFPFC_DELETE
         if strict:
             command = valve_of.ofp.OFPFC_DELETE_STRICT
         return self.flowmod(
-            match=match, priority=priority, command=command,
-            out_port=out_port, out_group=valve_of.ofp.OFPG_ANY)
+            match=match,
+            priority=priority,
+            command=command,
+            out_port=out_port,
+            out_group=valve_of.ofp.OFPG_ANY,
+        )
 
     def flowdrop(self, match=None, priority=None, hard_timeout=0):
         """Add drop matching flow to a table."""
         return self.flowmod(
-            match=match,
-            priority=priority,
-            hard_timeout=hard_timeout,
-            inst=())
+            match=match, priority=priority, hard_timeout=hard_timeout, inst=()
+        )
 
     def flowcontroller(self, match=None, priority=None, inst=None, max_len=96):
         """Add flow outputting to controller."""
         if inst is None:
             inst = ()
         return self.flowmod(
             match=match,
             priority=priority,
-            inst=(valve_of.apply_actions(
-                (valve_of.output_controller(max_len),)),) + inst)
+            inst=(valve_of.apply_actions((valve_of.output_controller(max_len),)),)
+            + inst,
+        )
 
 
 class ValveGroupEntry:
     """Abstraction for a single OpenFlow group entry."""
 
     def __init__(self, table, group_id, buckets):
         self.table = table
@@ -240,16 +311,15 @@
     def update_buckets(self, buckets):
         """Update entry with new buckets."""
         self.buckets = tuple(buckets)
 
     def add(self):
         """Return flows to add this entry to the group table."""
         ofmsgs = []
-        ofmsgs.extend(valve_of.groupadd(
-            group_id=self.group_id, buckets=self.buckets))
+        ofmsgs.extend(valve_of.groupadd(group_id=self.group_id, buckets=self.buckets))
         self.table.entries[self.group_id] = self
         return ofmsgs
 
     def delete(self):
         """Return flow to delete an existing group entry."""
         if self.group_id in self.table.entries:
             del self.table.entries[self.group_id]
@@ -265,27 +335,27 @@
         """Constructs a new object"""
         self.entries = {}
 
     @staticmethod
     def group_id_from_str(key_str):
         """Return a group ID based on a string key."""
         # TODO: does not handle collisions
-        digest = hashlib.sha256(key_str.encode('utf-8')).digest()
-        return struct.unpack('<L', digest[:4])[0]
+        digest = hashlib.sha256(key_str.encode("utf-8")).digest()
+        return struct.unpack("<L", digest[:4])[0]
 
     def get_entry(self, group_id, buckets):
         """Update entry with group_id with buckets, and return the entry."""
         if group_id in self.entries:
             self.entries[group_id].update_buckets(buckets)
         else:
-            self.entries[group_id] = ValveGroupEntry(
-                self, group_id, buckets)
+            self.entries[group_id] = ValveGroupEntry(self, group_id, buckets)
         return self.entries[group_id]
 
     def delete_all(self):
         """Delete all groups."""
         self.entries = {}
         return valve_of.groupdel()
 
 
 wildcard_table = ValveTable(
-    'all', ValveTableConfig('all', valve_of.ofp.OFPTT_ALL), flow_cookie=0)
+    "all", ValveTableConfig("all", valve_of.ofp.OFPTT_ALL), flow_cookie=0
+)
```

### Comparing `c65faucet-1.0.46/faucet/valve_util.py` & `c65faucet-1.0.47/faucet/valve_util.py`

 * *Files 14% similar despite different names*

```diff
@@ -19,85 +19,88 @@
 import logging
 from logging.handlers import WatchedFileHandler
 import os
 import signal
 import sys
 from functools import wraps
 
-LRU_MAX = 4096
-
 
 def kill_on_exception(logname):
     """decorator to ensure functions will kill ryu when an unhandled exception
     occurs"""
+
     def _koe(func):
         @wraps(func)
         def __koe(*args, **kwargs):
             try:
                 func(*args, **kwargs)
             except Exception:  # pylint: disable=broad-except
-                logging.getLogger(logname).exception('Unhandled exception, killing RYU')
+                logging.getLogger(logname).exception("Unhandled exception, killing RYU")
                 logging.shutdown()
                 os.kill(os.getpid(), signal.SIGTERM)
+
         return __koe
+
     return _koe
 
 
 def utf8_decode(msg_str):
     """Gracefully decode a possibly UTF-8 string."""
-    return msg_str.decode('utf-8', errors='replace')
+    return msg_str.decode("utf-8", errors="replace")
 
 
 def get_sys_prefix():
     """Returns an additional prefix for log and configuration files when used in
     a virtual environment"""
 
     # Find the appropriate prefix for config and log file default locations
     # in case Faucet is run in a virtual environment. virtualenv marks the
     # original path in sys.real_prefix. If this value exists, and is
     # different from sys.prefix, then we are most likely running in a
     # virtualenv. Also check for Py3.3+ pyvenv.
-    sysprefix = ''
-    if (getattr(sys, 'real_prefix', sys.prefix) != sys.prefix
-            or getattr(sys, 'base_prefix', sys.prefix) != sys.prefix):
+    sysprefix = ""
+    if (
+        getattr(sys, "real_prefix", sys.prefix) != sys.prefix
+        or getattr(sys, "base_prefix", sys.prefix) != sys.prefix
+    ):
         sysprefix = sys.prefix
 
     return sysprefix
 
 
 _PREFIX = get_sys_prefix()
 # To specify a boolean-only setting, set the default value to a bool type.
 DEFAULTS = {
-    'FAUCET_CONFIG': ''.join((
-        _PREFIX,
-        '/etc/faucet/faucet.yaml',
-        ':',
-        _PREFIX,
-        '/etc/ryu/faucet/faucet.yaml')),
-    'FAUCET_STACK_ROOT_STATE_UPDATE_TIME': 10,
-    'FAUCET_CONFIG_STAT_RELOAD': False,
-    'FAUCET_CONFIG_AUTO_REVERT': False,
-    'FAUCET_LOG_LEVEL': 'INFO',
-    'FAUCET_LOG': _PREFIX + '/var/log/faucet/faucet.log',
-    'FAUCET_EVENT_SOCK': '',  # Special-case, see get_setting().
-    'FAUCET_EVENT_SOCK_HEARTBEAT': 0,  # Special-case, see get_setting().
-    'FAUCET_EXCEPTION_LOG': _PREFIX + '/var/log/faucet/faucet_exception.log',
-    'FAUCET_PROMETHEUS_PORT': '9302',
-    'FAUCET_PROMETHEUS_ADDR': '0.0.0.0',
-    'GAUGE_CONFIG': ''.join((
-        _PREFIX,
-        '/etc/faucet/gauge.yaml',
-        ':',
-        _PREFIX,
-        '/etc/ryu/faucet/gauge.yaml')),
-    'GAUGE_CONFIG_STAT_RELOAD': False,
-    'GAUGE_LOG_LEVEL': 'INFO',
-    'GAUGE_PROMETHEUS_ADDR': '0.0.0.0',
-    'GAUGE_EXCEPTION_LOG': _PREFIX + '/var/log/faucet/gauge_exception.log',
-    'GAUGE_LOG': _PREFIX + '/var/log/faucet/gauge.log'
+    "FAUCET_CONFIG": "".join(
+        (
+            _PREFIX,
+            "/etc/faucet/faucet.yaml",
+            ":",
+            _PREFIX,
+            "/etc/ryu/faucet/faucet.yaml",
+        )
+    ),
+    "FAUCET_STACK_ROOT_STATE_UPDATE_TIME": 10,
+    "FAUCET_CONFIG_STAT_RELOAD": False,
+    "FAUCET_CONFIG_AUTO_REVERT": False,
+    "FAUCET_LOG_LEVEL": "INFO",
+    "FAUCET_LOG": _PREFIX + "/var/log/faucet/faucet.log",
+    "FAUCET_EVENT_SOCK": "",  # Special-case, see get_setting().
+    "FAUCET_EVENT_SOCK_HEARTBEAT": 0,  # Special-case, see get_setting().
+    "FAUCET_EXCEPTION_LOG": _PREFIX + "/var/log/faucet/faucet_exception.log",
+    "FAUCET_PROMETHEUS_PORT": "9302",
+    "FAUCET_PROMETHEUS_ADDR": "0.0.0.0",
+    "GAUGE_CONFIG": "".join(
+        (_PREFIX, "/etc/faucet/gauge.yaml", ":", _PREFIX, "/etc/ryu/faucet/gauge.yaml")
+    ),
+    "GAUGE_CONFIG_STAT_RELOAD": False,
+    "GAUGE_LOG_LEVEL": "INFO",
+    "GAUGE_PROMETHEUS_ADDR": "0.0.0.0",
+    "GAUGE_EXCEPTION_LOG": _PREFIX + "/var/log/faucet/gauge_exception.log",
+    "GAUGE_LOG": _PREFIX + "/var/log/faucet/gauge.log",
 }
 
 
 def _cast_bool(value):
     """Return True if value is a non-zero int."""
     try:
         return int(value) != 0
@@ -106,62 +109,63 @@
 
 
 def get_setting(name, path_eval=False):
     """Returns value of specified configuration setting."""
     default_value = DEFAULTS[name]
     result = os.getenv(name, default_value)
     # split on ':' and find the first suitable path
-    if (path_eval
-            and isinstance(result, str)
-            and isinstance(default_value, str) and not
-            isinstance(default_value, bool)):
+    if (
+        path_eval
+        and isinstance(result, str)
+        and isinstance(default_value, str)
+        and not isinstance(default_value, bool)
+    ):
         locations = result.split(":")
         result = None
         for loc in locations:
             if os.path.isfile(loc):
                 result = loc
                 break
         if result is None:
             result = locations[0]
     # Check for setting that expects a boolean result.
     if isinstance(default_value, bool):
         return _cast_bool(result)
     # Special default for FAUCET_EVENT_SOCK.
-    if name == 'FAUCET_EVENT_SOCK':
-        if result == '0':
-            return ''
+    if name == "FAUCET_EVENT_SOCK":
+        if result == "0":
+            return ""
         if _cast_bool(result):
-            return _PREFIX + '/var/run/faucet/faucet.sock'
-    if name == 'FAUCET_EVENT_SOCK_HEARTBEAT':
-        if result == '0':
+            return _PREFIX + "/var/run/faucet/faucet.sock"
+    if name == "FAUCET_EVENT_SOCK_HEARTBEAT":
+        if result == "0":
             return 0
     return result
 
 
 def get_logger(logname, logfile, loglevel, propagate):
     """Create and return a logger object."""
 
     stream_handlers = {
-        'STDOUT': sys.stdout,
-        'STDERR': sys.stderr,
+        "STDOUT": sys.stdout,
+        "STDERR": sys.stderr,
     }
 
     try:
         if logfile in stream_handlers:
             logger_handler = logging.StreamHandler(stream_handlers[logfile])
         else:
             logger_handler = WatchedFileHandler(logfile)
     except (PermissionError, FileNotFoundError) as err:  # pytype: disable=name-error
         print(err)
         sys.exit(-1)
 
     logger = logging.getLogger(logname)
-    log_fmt = '%(asctime)s %(name)-6s %(levelname)-8s %(message)s'
-    logger_handler.setFormatter(
-        logging.Formatter(log_fmt, '%b %d %H:%M:%S'))
+    log_fmt = "%(asctime)s %(name)-6s %(levelname)-8s %(message)s"
+    logger_handler.setFormatter(logging.Formatter(log_fmt, "%b %d %H:%M:%S"))
     logger.addHandler(logger_handler)
     logger.propagate = propagate
     logger.setLevel(loglevel)
     return logger
 
 
 def close_logger(logger):
@@ -172,24 +176,25 @@
         handler.close()
         logger.removeHandler(handler)
 
 
 def dpid_log(dpid):
     """Log a DP ID as hex/decimal."""
     if dpid is None:
-        return 'DPID None (NoneType)'
-    return 'DPID %u (0x%x)' % (dpid, dpid)
+        return "DPID None (NoneType)"
+    return "DPID %u (0x%x)" % (dpid, dpid)
 
 
 def stat_config_files(config_hashes):
     """Return dict of a subset of stat attributes on config files."""
     config_files_stats = {}
     for config_file in list(config_hashes.keys()):
         try:
             config_file_stat = os.stat(config_file)
         except OSError:
             continue
         config_files_stats[config_file] = (
             config_file_stat.st_size,
             config_file_stat.st_mtime,
-            config_file_stat.st_ctime)
+            config_file_stat.st_ctime,
+        )
     return config_files_stats
```

### Comparing `c65faucet-1.0.46/faucet/valves_manager.py` & `c65faucet-1.0.47/faucet/valves_manager.py`

 * *Files 3% similar despite different names*

```diff
@@ -71,16 +71,25 @@
         if new_config_hashes:
             self.config_hashes = new_config_hashes
 
 
 class ValvesManager:
     """Manage a collection of Valves."""
 
-    def __init__(self, logname, logger, metrics, notifier, bgp,
-                 dot1x, config_auto_revert, send_flows_to_dp_by_id):
+    def __init__(
+        self,
+        logname,
+        logger,
+        metrics,
+        notifier,
+        bgp,
+        dot1x,
+        config_auto_revert,
+        send_flows_to_dp_by_id,
+    ):
         """Initialize ValvesManager.
 
         Args:
             logname (str): log name to use in logging.
             logger (logging.logging): logger instance to use for logging.
             metrics (FaucetMetrics): metrics instance.
             notifier (FaucetEvent): event notifier instance.
@@ -120,16 +129,18 @@
         """
         dps = dp_preparsed_parser(self.meta_dp_state.top_conf, self.meta_dp_state)
         self._apply_configs(dps, now, None)
 
     def valves_by_name(self):
         """Return a name/valve dict of all the stacking valves"""
         return {
-            valve.dp.name: valve for valve in self.valves.values()
-            if valve.stack_manager}
+            valve.dp.name: valve
+            for valve in self.valves.values()
+            if valve.stack_manager
+        }
 
     def maintain_stack_root(self, now, update_time):
         """
         Maintain current stack root
 
         Args:
             now (float): Current time
@@ -142,19 +153,22 @@
         if not valves_by_name:
             return False
 
         prev_root_name = self.meta_dp_state.stack_root_name
         prev_root_valve = valves_by_name.get(prev_root_name, None)
 
         prev_other_valves = [
-            valve for valve in self.valves.values()
-            if valve != prev_root_valve]
+            valve for valve in self.valves.values() if valve != prev_root_valve
+        ]
 
-        new_root_name = list(valves_by_name.values())[0].stack_manager.nominate_stack_root(
-            prev_root_valve, prev_other_valves, now, last_live_times, update_time)
+        new_root_name = list(valves_by_name.values())[
+            0
+        ].stack_manager.nominate_stack_root(
+            prev_root_valve, prev_other_valves, now, last_live_times, update_time
+        )
         return self.set_stack_root(now, new_root_name)
 
     def set_stack_root(self, now, new_root_name):
         """
         Set stack root
 
         Args:
@@ -166,34 +180,37 @@
         prev_root_valve = valves_by_name.get(prev_root_name, None)
         new_root_valve = valves_by_name.get(new_root_name, None)
 
         stack_change = False
         if new_root_valve:
             if prev_root_name != new_root_name:
                 # Current stack root is not the new stack root
-                self.logger.info('Stack root %s (previous %s)' % (
-                    new_root_name, prev_root_name))
+                self.logger.info(
+                    "Stack root %s (previous %s)" % (new_root_name, prev_root_name)
+                )
                 if prev_root_valve:
                     labels = prev_root_valve.dp.base_prom_labels()
                     self.metrics.is_dp_stack_root.labels(**labels).set(0)
                 self.meta_dp_state.stack_root_name = new_root_name
                 self.metrics.faucet_stack_root_dpid.set(new_root_valve.dp.dp_id)
                 self.metrics.stack_root_change_count.inc(1)
                 self.reload_stack_root_config(now)
                 if prev_root_name:
                     stack_change = True
             else:
                 # Current stack root does not change, however ensure that the current stack root
                 #   is known for all DPs
-                new_other_valves = [valve for valve in self.valves.values()
-                                    if valve != new_root_valve]
+                new_other_valves = [
+                    valve for valve in self.valves.values() if valve != new_root_valve
+                ]
                 inconsistent_dps = not new_root_valve.stack_manager.consistent_roots(
-                    prev_root_name, new_root_valve, new_other_valves)
+                    prev_root_name, new_root_valve, new_other_valves
+                )
                 if inconsistent_dps:
-                    self.logger.info('Reloading stack DPs (stack root inconsistent)')
+                    self.logger.info("Reloading stack DPs (stack root inconsistent)")
                     self.reload_stack_root_config(now)
                     stack_change = True
             labels = new_root_valve.dp.base_prom_labels()
             self.metrics.is_dp_stack_root.labels(**labels).set(1)
             for valve in valves_by_name.values():
                 labels = valve.dp.base_prom_labels()
                 path_port = valve.stack_manager.chosen_towards_port
@@ -202,90 +219,105 @@
         if stack_change:
             for valve in valves_by_name.values():
                 valve.stale_root = True
         return stack_change
 
     def event_socket_heartbeat(self):
         """raises event for event sock heartbeat"""
-        self._notify({'EVENT_SOCK_HEARTBEAT': None})
+        self._notify({"EVENT_SOCK_HEARTBEAT": None})
 
     def revert_config(self):
         """Attempt to revert config to last known good version."""
-        for config_file_name, config_content in self.meta_dp_state.last_good_config.items():
-            self.logger.info('attempting to revert to last good config: %s' % config_file_name)
+        for (
+            config_file_name,
+            config_content,
+        ) in self.meta_dp_state.last_good_config.items():
+            self.logger.info(
+                "attempting to revert to last good config: %s" % config_file_name
+            )
             try:
-                with open(config_file_name, 'w', encoding='utf-8') as config_file:
+                with open(config_file_name, "w", encoding="utf-8") as config_file:
                     config_file.write(str(config_content))
             except (FileNotFoundError, OSError, PermissionError) as err:
-                self.logger.error('could not revert %s: %s' % (config_file_name, err))
+                self.logger.error("could not revert %s: %s" % (config_file_name, err))
                 return
-        self.logger.info('successfully reverted to last good config')
+        self.logger.info("successfully reverted to last good config")
 
     def parse_configs(self, new_config_file):
         """Return parsed configs for Valves, or None."""
         self.metrics.faucet_config_hash_func.labels(algorithm=CONFIG_HASH_FUNC)
         try:
             new_conf_hashes, new_config_content, new_dps, top_conf = dp_parser(
-                new_config_file, self.logname, self.meta_dp_state)
+                new_config_file, self.logname, self.meta_dp_state
+            )
             new_present_conf_hashes = [
-                (conf_file, conf_hash) for conf_file, conf_hash in sorted(new_conf_hashes.items())
-                if conf_hash is not None]
+                (conf_file, conf_hash)
+                for conf_file, conf_hash in sorted(new_conf_hashes.items())
+                if conf_hash is not None
+            ]
             conf_files = [conf_file for conf_file, _ in new_present_conf_hashes]
             conf_hashes = [conf_hash for _, conf_hash in new_present_conf_hashes]
             self.config_watcher.update(new_config_file, new_conf_hashes)
             self.meta_dp_state.top_conf = top_conf
             self.meta_dp_state.last_good_config = new_config_content
             self.meta_dp_state.config_hash_info = dict(
-                config_files=','.join(conf_files), hashes=','.join(conf_hashes), error='')
+                config_files=",".join(conf_files),
+                hashes=",".join(conf_hashes),
+                error="",
+            )
             self.metrics.faucet_config_hash.info(self.meta_dp_state.config_hash_info)
             self.metrics.faucet_config_load_error.set(0)
         except InvalidConfigError as err:
-            self.logger.error('New config bad (%s) - rejecting', err)
+            self.logger.error("New config bad (%s) - rejecting", err)
             # If the config was reverted, let the watcher notice.
             if self.config_auto_revert:
                 self.revert_config()
             self.config_watcher.update(new_config_file)
             self.meta_dp_state.config_hash_info = dict(
-                config_files=new_config_file, hashes='', error=str(err))
+                config_files=new_config_file, hashes="", error=str(err)
+            )
             self.metrics.faucet_config_hash.info(self.meta_dp_state.config_hash_info)
             self.metrics.faucet_config_load_error.set(1)
             new_dps = None
         return new_dps
 
     def new_valve(self, new_dp):
         valve_cl = valve_factory(new_dp)
         if valve_cl is not None:
-            return valve_cl(new_dp, self.logname, self.metrics, self.notifier, self.dot1x)
+            return valve_cl(
+                new_dp, self.logname, self.metrics, self.notifier, self.dot1x
+            )
         self.logger.error(
-            '%s hardware %s must be one of %s',
+            "%s hardware %s must be one of %s",
             new_dp.name,
             new_dp.hardware,
-            sorted(list(SUPPORTED_HARDWARE.keys())))
+            sorted(list(SUPPORTED_HARDWARE.keys())),
+        )
         return None
 
     def _apply_configs(self, new_dps, now, delete_dp):
         self.update_config_applied(reset=True)
         if new_dps is None:
             return False
         deleted_dpids = set(self.valves) - {dp.dp_id for dp in new_dps}
         sent = {}
         for new_dp in new_dps:
             dp_id = new_dp.dp_id
             if dp_id in self.valves:
-                self.logger.info('Reconfiguring existing datapath %s', dpid_log(dp_id))
+                self.logger.info("Reconfiguring existing datapath %s", dpid_log(dp_id))
                 valve = self.valves[dp_id]
                 ofmsgs = valve.reload_config(now, new_dp, list(self.valves.values()))
                 self.send_flows_to_dp_by_id(valve, ofmsgs)
                 sent[dp_id] = valve.dp.dyn_running
             else:
-                self.logger.info('Add new datapath %s', dpid_log(new_dp.dp_id))
+                self.logger.info("Add new datapath %s", dpid_log(new_dp.dp_id))
                 valve = self.new_valve(new_dp)
                 if valve is None:
                     continue
-                self._notify({'CONFIG_CHANGE': {'restart_type': 'new'}}, dp=new_dp)
+                self._notify({"CONFIG_CHANGE": {"restart_type": "new"}}, dp=new_dp)
             valve.update_config_metrics()
             self.valves[dp_id] = valve
         if delete_dp is not None:
             for deleted_dp in deleted_dpids:
                 delete_dp(deleted_dp)
                 del self.valves[deleted_dp]
         self.bgp.reset(self.valves)
@@ -308,91 +340,118 @@
             self.notifier.notify(dp.dp_id, dp.name, event_dict)
         else:
             self.notifier.notify(0, str(0), event_dict)
 
     def request_reload_configs(self, now, new_config_file, delete_dp=None):
         """Process a request to load config changes."""
         if self.config_watcher.content_changed(new_config_file):
-            self.logger.info('configuration %s changed, analyzing differences', new_config_file)
+            self.logger.info(
+                "configuration %s changed, analyzing differences", new_config_file
+            )
             result = self.load_configs(now, new_config_file, delete_dp=delete_dp)
-            self._notify({'CONFIG_CHANGE':
-                          {'success': result,
-                           'config_hash_info': self.meta_dp_state.config_hash_info}})
+            self._notify(
+                {
+                    "CONFIG_CHANGE": {
+                        "success": result,
+                        "config_hash_info": self.meta_dp_state.config_hash_info,
+                    }
+                }
+            )
         else:
-            self.logger.info('configuration is unchanged, not reloading')
+            self.logger.info("configuration is unchanged, not reloading")
             self.metrics.faucet_config_load_error.set(0)
         self.metrics.faucet_config_reload_requests.inc()  # pylint: disable=no-member
 
     def update_metrics(self, now):
         """Update metrics in all Valves."""
         for valve in self.valves.values():
             valve.update_metrics(now, rate_limited=False)
         self.bgp.update_metrics(now)
 
     def valve_flow_services(self, now, valve_service):
         """Call a method on all Valves and send any resulting flows."""
         ofmsgs_by_valve = defaultdict(list)
         for valve in self.valves.values():
             other_valves = self._other_running_valves(valve)
-            valve_service_labels = dict(valve.dp.base_prom_labels(), valve_service=valve_service)
+            valve_service_labels = dict(
+                valve.dp.base_prom_labels(), valve_service=valve_service
+            )
             valve_service_func = getattr(valve, valve_service)
             with self.metrics.faucet_valve_service_secs.labels(  # pylint: disable=no-member
-                    **valve_service_labels).time():
-                for service_valve, ofmsgs in valve_service_func(now, other_valves).items():
+                **valve_service_labels
+            ).time():
+                for service_valve, ofmsgs in valve_service_func(
+                    now, other_valves
+                ).items():
                     # Since we are calling all Valves, keep only the ofmsgs
                     # provided by the last Valve called (eventual consistency).
                     if service_valve in ofmsgs_by_valve:
                         ofmsgs_by_valve[service_valve] = []
                     ofmsgs_by_valve[service_valve].extend(ofmsgs)
         self._send_ofmsgs_by_valve(ofmsgs_by_valve)
 
     def _other_running_valves(self, valve):
-        return [other_valve for other_valve in self.valves.values()
-                if valve != other_valve and other_valve.dp.dyn_running]
+        return [
+            other_valve
+            for other_valve in self.valves.values()
+            if valve != other_valve and other_valve.dp.dyn_running
+        ]
 
     def port_desc_stats_reply_handler(self, valve, msg, now):
         """Handle a port desc stats reply message."""
         ofmsgs_by_valve = valve.port_desc_stats_reply_handler(
-            msg.body, self._other_running_valves(valve), now)
+            msg.body, self._other_running_valves(valve), now
+        )
         self._send_ofmsgs_by_valve(ofmsgs_by_valve)
 
     def port_status_handler(self, valve, msg, now):
         """Handle a port status change message."""
         ofmsgs_by_valve = valve.port_status_handler(
-            msg.desc.port_no, msg.reason, msg.desc.state, self._other_running_valves(valve), now)
+            msg.desc.port_no,
+            msg.reason,
+            msg.desc.state,
+            self._other_running_valves(valve),
+            now,
+        )
         self._send_ofmsgs_by_valve(ofmsgs_by_valve)
 
     def valve_packet_in(self, now, valve, msg):
         """Time a call to Valve packet in handler."""
         self.metrics.of_packet_ins.labels(  # pylint: disable=no-member
-            **valve.dp.base_prom_labels()).inc()
+            **valve.dp.base_prom_labels()
+        ).inc()
         if valve.rate_limit_packet_ins(now):
             return
         pkt_meta = valve.parse_pkt_meta(msg)
         if pkt_meta is None:
             self.metrics.of_unexpected_packet_ins.labels(  # pylint: disable=no-member
-                **valve.dp.base_prom_labels()).inc()
+                **valve.dp.base_prom_labels()
+            ).inc()
             return
         with self.metrics.faucet_packet_in_secs.labels(  # pylint: disable=no-member
-                **valve.dp.base_prom_labels()).time():
-            ofmsgs_by_valve = valve.rcv_packet(now, self._other_running_valves(valve), pkt_meta)
+            **valve.dp.base_prom_labels()
+        ).time():
+            ofmsgs_by_valve = valve.rcv_packet(
+                now, self._other_running_valves(valve), pkt_meta
+            )
         if ofmsgs_by_valve:
             self._send_ofmsgs_by_valve(ofmsgs_by_valve)
             valve.update_metrics(now, pkt_meta.port, rate_limited=True)
 
     def update_config_applied(self, sent=None, reset=False):
         """Update faucet_config_applied from {dpid: sent} dict,
-           defining applied == sent == enqueued via Ryu"""
+        defining applied == sent == enqueued via Ryu"""
         if reset:
             self.config_applied = defaultdict(bool)
         if sent:
             self.config_applied.update(sent)
         count = float(len(self.valves))
-        configured = sum((1 if self.config_applied[dp_id] else 0)
-                         for dp_id in self.valves)
+        configured = sum(
+            (1 if self.config_applied[dp_id] else 0) for dp_id in self.valves
+        )
         fraction = configured / count if count > 0 else 0
         self.metrics.faucet_config_applied.set(fraction)
 
     def datapath_connect(self, now, valve, discovered_up_ports):
         """Handle connection from DP."""
         self.meta_dp_state.dp_last_live_time[valve.dp.name] = now
         self.update_config_applied({valve.dp.dp_id: True})
```

### Comparing `c65faucet-1.0.46/faucet/vlan.py` & `c65faucet-1.0.47/faucet/vlan.py`

 * *Files 8% similar despite different names*

```diff
@@ -33,116 +33,116 @@
         self.name = name
         self.vid = vid
 
 
 class NullVLAN:
     """Placeholder null VLAN."""
 
-    name = 'Null VLAN'
+    name = "Null VLAN"
     vid = valve_of.ofp.OFPVID_NONE
 
 
 class AnyVLAN:
     """Placeholder any tagged VLAN. NOTE: Not used, not well supported by hardware"""
 
-    name = 'Any VLAN'
+    name = "Any VLAN"
     vid = valve_of.ofp.OFPVID_PRESENT
 
 
 class HostCacheEntry:
     """Association of a host with a port."""
 
     __slots__ = [
-        'cache_time',
-        'eth_src',
-        'eth_src_int',
-        'port',
+        "cache_time",
+        "eth_src",
+        "eth_src_int",
+        "port",
     ]
 
     def __init__(self, eth_src, port, cache_time):
         self.eth_src = eth_src
         self.port = port
         self.cache_time = cache_time
-        self.eth_src_int = int(eth_src.replace(':', ''), 16)
+        self.eth_src_int = int(eth_src.replace(":", ""), 16)
 
     def __hash__(self):
         return hash((self.eth_src_int, self.port.number))
 
     def __str__(self):
-        return '%s on %s' % (self.eth_src, self.port)
+        return "%s on %s" % (self.eth_src, self.port)
 
     def __repr__(self):
         return self.__str__()
 
     def __eq__(self, other):
         return self.__hash__() == other.__hash__()
 
     def __lt__(self, other):
         return self.__hash__() < other.__hash__()
 
 
 class VLAN(Conf):
     """Contains state for one VLAN, including its configuration."""
 
-# Note: while vlans are configured once for each datapath, there will be a
-# separate vlan object created for each datapath that the vlan appears on
+    # Note: while vlans are configured once for each datapath, there will be a
+    # separate vlan object created for each datapath that the vlan appears on
 
-    mutable_attrs = frozenset(['tagged', 'untagged', 'dot1x_untagged'])
+    mutable_attrs = frozenset(["tagged", "untagged", "dot1x_untagged"])
 
     defaults = {
-        'name': None,
-        'description': None,
-        'acl_in': None,
-        'acls_in': None,
-        'acl_out': None,
-        'acls_out': None,
-        'faucet_vips': None,
-        'faucet_mac': FAUCET_MAC,
+        "name": None,
+        "description": None,
+        "acl_in": None,
+        "acls_in": None,
+        "acl_out": None,
+        "acls_out": None,
+        "faucet_vips": None,
+        "faucet_mac": FAUCET_MAC,
         # set MAC for FAUCET VIPs on this VLAN
-        'unicast_flood': True,
-        'routes': None,
-        'max_hosts': 256,
+        "unicast_flood": True,
+        "routes": None,
+        "max_hosts": 256,
         # Limit number of hosts that can be learned on a VLAN.
-        'vid': None,
-        'proactive_arp_limit': 0,
+        "vid": None,
+        "proactive_arp_limit": 0,
         # Don't proactively ARP for hosts if over this limit (default 2*max_hosts)
-        'proactive_nd_limit': 0,
+        "proactive_nd_limit": 0,
         # Don't proactively ND for hosts if over this limit (default 2*max_hosts)
-        'targeted_gw_resolution': True,
+        "targeted_gw_resolution": True,
         # If True, target the first re-resolution attempt to last known port only.
-        'minimum_ip_size_check': True,
+        "minimum_ip_size_check": True,
         # If False, don't check that IP packets have a payload (OVS trace/tutorial requires False).
-        'reserved_internal_vlan': False,
+        "reserved_internal_vlan": False,
         # If True, forward packets from the VLAN table to the VLAN_ACL table matching the VID
-        'dot1x_assigned': False,
+        "dot1x_assigned": False,
         # If True, this VLAN may be dynamically added withTunnel-Private-Group-ID radius attribute.
-        'edge_learn_stack_root': True,
+        "edge_learn_stack_root": True,
         # If True, this VLAN will learn flows through the stack root, following forwarding path.
     }
 
     defaults_types = {
-        'name': str,
-        'description': str,
-        'acl_in': (int, str),
-        'acls_in': list,
-        'acl_out': (int, str),
-        'acls_out': list,
-        'faucet_vips': list,
-        'faucet_mac': str,
-        'unicast_flood': bool,
-        'routes': list,
-        'max_hosts': int,
-        'vid': int,
-        'proactive_arp_limit': int,
-        'proactive_nd_limit': int,
-        'targeted_gw_resolution': bool,
-        'minimum_ip_size_check': bool,
-        'reserved_internal_vlan': bool,
-        'dot1x_assigned': bool,
-        'edge_learn_stack_root': bool,
+        "name": str,
+        "description": str,
+        "acl_in": (int, str),
+        "acls_in": list,
+        "acl_out": (int, str),
+        "acls_out": list,
+        "faucet_vips": list,
+        "faucet_mac": str,
+        "unicast_flood": bool,
+        "routes": list,
+        "max_hosts": int,
+        "vid": int,
+        "proactive_arp_limit": int,
+        "proactive_nd_limit": int,
+        "targeted_gw_resolution": bool,
+        "minimum_ip_size_check": bool,
+        "reserved_internal_vlan": bool,
+        "dot1x_assigned": bool,
+        "edge_learn_stack_root": bool,
     }
 
     def __init__(self, _id, dp_id, conf=None):
         self.acl_in = None
         self.acls_in = None
         self.acl_out = None
         self.acls_out = None
@@ -185,78 +185,108 @@
         self.dyn_host_gws_by_ipv = collections.defaultdict(set)
         self.dyn_route_gws_by_ipv = collections.defaultdict(set)
         self.reset_caches()
         super().__init__(_id, dp_id, conf)
 
     def set_defaults(self):
         super().set_defaults()
-        self._set_default('vid', self._id)
-        self._set_default('name', str(self._id))
-        self._set_default('faucet_vips', [])
+        self._set_default("vid", self._id)
+        self._set_default("name", str(self._id))
+        self._set_default("faucet_vips", [])
 
     def check_config(self):
         super().check_config()
-        test_config_condition(not self.vid_valid(self.vid), 'invalid VID %s' % self.vid)
-        test_config_condition(not netaddr.valid_mac(self.faucet_mac), (
-            'invalid MAC address %s' % self.faucet_mac))
-        self.faucet_mac = str(netaddr.EUI(
-            self.faucet_mac, dialect=netaddr.strategy.eui48.mac_unix_expanded))
+        test_config_condition(not self.vid_valid(self.vid), "invalid VID %s" % self.vid)
+        test_config_condition(
+            not netaddr.valid_mac(self.faucet_mac),
+            ("invalid MAC address %s" % self.faucet_mac),
+        )
+        self.faucet_mac = str(
+            netaddr.EUI(
+                self.faucet_mac, dialect=netaddr.strategy.eui48.mac_unix_expanded
+            )
+        )
 
         test_config_condition(
-            self.acl_in and self.acls_in, 'found both acl_in and acls_in, use only acls_in')
+            self.acl_in and self.acls_in,
+            "found both acl_in and acls_in, use only acls_in",
+        )
         test_config_condition(
-            self.acl_out and self.acls_out, 'found both acl_out and acls_out, use only acls_out')
+            self.acl_out and self.acls_out,
+            "found both acl_out and acls_out, use only acls_out",
+        )
         if self.acl_in and not isinstance(self.acl_in, list):
-            self.acls_in = [self.acl_in, ]
+            self.acls_in = [
+                self.acl_in,
+            ]
             self.acl_in = None
         if self.acl_out and not isinstance(self.acl_out, list):
-            self.acls_out = [self.acl_out, ]
+            self.acls_out = [
+                self.acl_out,
+            ]
             self.acl_out = None
         all_acls = []
         if self.acls_in:
             all_acls.extend(self.acls_in)
         if self.acls_out:
             all_acls.extend(self.acls_out)
         for acl in all_acls:
             test_config_condition(
-                not isinstance(acl, (int, str)), 'acl names must be int or str')
+                not isinstance(acl, (int, str)), "acl names must be int or str"
+            )
 
         if self.max_hosts:
             if not self.proactive_arp_limit:
                 self.proactive_arp_limit = 2 * self.max_hosts
             if not self.proactive_nd_limit:
                 self.proactive_nd_limit = 2 * self.max_hosts
 
         if self.faucet_vips:
-            self.faucet_vips = frozenset([
-                self._check_ip_str(ip_str, ip_method=ipaddress.ip_interface)
-                for ip_str in self.faucet_vips])
+            self.faucet_vips = frozenset(
+                [
+                    self._check_ip_str(ip_str, ip_method=ipaddress.ip_interface)
+                    for ip_str in self.faucet_vips
+                ]
+            )
             for faucet_vip in self.faucet_vips:
                 test_config_condition(
                     faucet_vip.network.prefixlen == faucet_vip.max_prefixlen,
-                    'VIP cannot be a host address')
+                    "VIP cannot be a host address",
+                )
 
         if self.routes:
-            test_config_condition(not isinstance(self.routes, list), 'invalid VLAN routes format')
+            test_config_condition(
+                not isinstance(self.routes, list), "invalid VLAN routes format"
+            )
             try:
-                self.routes = [route['route'] for route in self.routes]
+                self.routes = [route["route"] for route in self.routes]
             except TypeError as type_error:
-                raise InvalidConfigError('%s is not a valid routes value' %
-                                         self.routes) from type_error
+                raise InvalidConfigError(
+                    "%s is not a valid routes value" % self.routes
+                ) from type_error
             except KeyError:
                 pass
             for route in self.routes:
-                test_config_condition(not isinstance(route, dict), 'invalid VLAN route format')
-                test_config_condition('ip_gw' not in route, 'missing ip_gw in VLAN route')
-                test_config_condition('ip_dst' not in route, 'missing ip_dst in VLAN route')
-                ip_gw = self._check_ip_str(route['ip_gw'])
-                ip_dst = self._check_ip_str(route['ip_dst'], ip_method=ipaddress.ip_network)
+                test_config_condition(
+                    not isinstance(route, dict), "invalid VLAN route format"
+                )
+                test_config_condition(
+                    "ip_gw" not in route, "missing ip_gw in VLAN route"
+                )
+                test_config_condition(
+                    "ip_dst" not in route, "missing ip_dst in VLAN route"
+                )
+                ip_gw = self._check_ip_str(route["ip_gw"])
+                ip_dst = self._check_ip_str(
+                    route["ip_dst"], ip_method=ipaddress.ip_network
+                )
                 test_config_condition(
                     ip_gw.version != ip_dst.version,
-                    'ip_gw version does not match the ip_dst version')
+                    "ip_gw version does not match the ip_dst version",
+                )
                 self.add_route(ip_dst, ip_gw)
 
     @staticmethod
     def vid_valid(vid):
         """Return True if VID valid."""
         return isinstance(vid, int) and valve_of.MIN_VID <= vid <= valve_of.MAX_VID
 
@@ -268,33 +298,42 @@
         self.dyn_neigh_cache_by_ipv = collections.defaultdict(dict)
         self.dyn_unresolved_route_ip_gws = collections.defaultdict(list)
         self.dyn_unresolved_host_ip_gws = collections.defaultdict(list)
 
     def reset_ports(self, ports):
         """Reset tagged and untagged port lists."""
         sorted_ports = sorted(ports, key=lambda i: i.number)
-        self.tagged = tuple([  # pylint: disable=consider-using-generator
-            port for port in sorted_ports
-            if self in port.tagged_vlans])
-        self.untagged = tuple([  # pylint: disable=consider-using-generator
-            port for port in sorted_ports
-            if (self == port.native_vlan
-                and port.dyn_dot1x_native_vlan is None)])
-        self.dot1x_untagged = tuple([  # pylint: disable=consider-using-generator
-            port for port in sorted_ports
-            if self == port.dyn_dot1x_native_vlan])
+        # pylint: disable=consider-using-generator
+        self.tagged = tuple(
+            [
+                port for port in sorted_ports if self in port.tagged_vlans
+            ]
+        )
+        self.untagged = tuple(
+            [
+                port
+                for port in sorted_ports
+                if (self == port.native_vlan and port.dyn_dot1x_native_vlan is None)
+            ]
+        )
+        self.dot1x_untagged = tuple(
+            [
+                port for port in sorted_ports if self == port.dyn_dot1x_native_vlan
+            ]
+        )
 
     def add_cache_host(self, eth_src, port, cache_time):
         """Add/update a host to the cache on a port at at time."""
         existing_entry = self.cached_host(eth_src)
         if existing_entry is None:
             self.dyn_host_cache_stats_stale[port.number] = True
         else:
             self.dyn_host_cache_by_port[existing_entry.port.number].remove(
-                existing_entry)
+                existing_entry
+            )
         entry = HostCacheEntry(eth_src, port, cache_time)
         if port.number not in self.dyn_host_cache_by_port:
             self.dyn_host_cache_by_port[port.number] = set()
         self.dyn_host_cache_by_port[port.number].add(entry)
         self.dyn_host_cache[eth_src] = entry
 
     def expire_cache_host(self, eth_src):
@@ -335,24 +374,30 @@
             self.expire_cache_host(entry.eth_src)
 
     def expire_cache_hosts(self, now, learn_timeout):
         """Expire stale host entries."""
         expired_hosts = []
         min_cache_time = now - learn_timeout
 
-        if self.dyn_oldest_host_time is None or self.dyn_oldest_host_time < min_cache_time:
+        if (
+            self.dyn_oldest_host_time is None
+            or self.dyn_oldest_host_time < min_cache_time
+        ):
             expired_hosts = [
-                entry for entry in self.dyn_host_cache.values()
-                if entry.cache_time < min_cache_time and not entry.port.permanent_learn]
+                entry
+                for entry in self.dyn_host_cache.values()
+                if entry.cache_time < min_cache_time and not entry.port.permanent_learn
+            ]
             for entry in expired_hosts:
                 self.expire_cache_host(entry.eth_src)
             self.dyn_oldest_host_time = now
             if self.dyn_host_cache:
                 self.dyn_oldest_host_time = min(
-                    [entry.cache_time for entry in self.dyn_host_cache.values()])
+                    [entry.cache_time for entry in self.dyn_host_cache.values()]
+                )
         return expired_hosts
 
     def faucet_vips_by_ipv(self, ipv):
         """Return VIPs with specified IP version on this VLAN."""
         return self._by_ipv(self.faucet_vips, ipv)
 
     def link_and_other_vips(self, ipv):
@@ -379,17 +424,19 @@
 
         Args:
             host_ip: (ipaddress.ip_address): potential host FIB route.
         Returns:
             True if a host FIB route (and not used as a gateway).
         """
         ip_dsts = self.ip_dsts_for_ip_gw(host_ip)
-        if (len(ip_dsts) == 1
-                and ip_dsts[0].prefixlen == ip_dsts[0].max_prefixlen
-                and ip_dsts[0].network_address == host_ip):
+        if (
+            len(ip_dsts) == 1
+            and ip_dsts[0].prefixlen == ip_dsts[0].max_prefixlen
+            and ip_dsts[0].network_address == host_ip
+        ):
             return True
         return False
 
     def _update_gw_types(self, ip_gw):
         """Update dyn host/route gw information to a different ip version"""
         if self.is_host_fib_route(ip_gw):
             self.dyn_host_gws_by_ipv[ip_gw.version].add(ip_gw)
@@ -436,62 +483,94 @@
     def hosts_count(self):
         """Return number of hosts learned on this VLAN."""
         return len(self.dyn_host_cache)
 
     def __str__(self):
         str_ports = []
         if self.tagged:
-            str_ports.append('tagged: %s' % ','.join([str(p) for p in self.tagged]))
+            str_ports.append("tagged: %s" % ",".join([str(p) for p in self.tagged]))
         if self.untagged:
-            str_ports.append('untagged: %s' % ','.join([str(p) for p in self.untagged]))
+            str_ports.append("untagged: %s" % ",".join([str(p) for p in self.untagged]))
         if self.dot1x_untagged:
-            str_ports.append('dot1x_untagged: %s' % ','.join([str(p) for p in self.dot1x_untagged]))
-        return 'VLAN %s vid:%s %s' % (self.name, self.vid, ' '.join(str_ports))
+            str_ports.append(
+                "dot1x_untagged: %s" % ",".join([str(p) for p in self.dot1x_untagged])
+            )
+        return "VLAN %s vid:%s %s" % (self.name, self.vid, " ".join(str_ports))
 
     def __repr__(self):
         return self.__str__()
 
     def get_ports(self):
         """Return all ports on this VLAN."""
         return self.tagged + self.untagged + self.dot1x_untagged
 
     def restricted_bcast_arpnd_ports(self):
         """Return all ports with restricted broadcast enabled."""
-        return tuple([  # pylint: disable=consider-using-generator
-            port for port in self.get_ports() if port.restricted_bcast_arpnd])
+        # pylint: disable=consider-using-generator
+        return tuple(
+            [
+                port for port in self.get_ports() if port.restricted_bcast_arpnd
+            ]
+        )
 
     def hairpin_ports(self):
         """Return all ports with hairpin enabled."""
-        return tuple([  # pylint: disable=consider-using-generator
-            port for port in self.get_ports() if port.hairpin])
+        # pylint: disable=consider-using-generator
+        return tuple(
+            [
+                port for port in self.get_ports() if port.hairpin
+            ]
+        )
 
     def mirrored_ports(self):
         """Return ports that are mirrored on this VLAN."""
-        return tuple([  # pylint: disable=consider-using-generator
-            port for port in self.get_ports() if port.mirror])
+        # pylint: disable=consider-using-generator
+        return tuple(
+            [
+                port for port in self.get_ports() if port.mirror
+            ]
+        )
 
     def loop_protect_external_ports(self):
         """Return ports wth external loop protection set."""
-        return tuple([  # pylint: disable=consider-using-generator
-            port for port in self.get_ports() if port.loop_protect_external])
+        # pylint: disable=consider-using-generator
+        return tuple(
+            [
+                port for port in self.get_ports() if port.loop_protect_external
+            ]
+        )
 
     def loop_protect_external_ports_up(self):
         """Return up ports with external loop protection set."""
-        return tuple([  # pylint: disable=consider-using-generator
-            port for port in self.loop_protect_external_ports() if port.dyn_phys_up])
+        # pylint: disable=consider-using-generator
+        return tuple(
+            [
+                port for port in self.loop_protect_external_ports() if port.dyn_phys_up
+            ]
+        )
 
     def lacp_ports(self):
         """Return ports that have LACP on this VLAN."""
-        return tuple([  # pylint: disable=consider-using-generator
-            port for port in self.get_ports() if port.lacp])
+        # pylint: disable=consider-using-generator
+        return tuple(
+            [
+                port for port in self.get_ports() if port.lacp
+            ]
+        )
 
     def lacp_up_selected_ports(self):
         """Return LACP ports that have been SELECTED and are UP"""
-        return tuple([  # pylint: disable=consider-using-generator
-            port for port in self.lacp_ports() if port.is_port_selected() and port.is_actor_up()])
+        # pylint: disable=consider-using-generator
+        return tuple(
+            [
+                port
+                for port in self.lacp_ports()
+                if port.is_port_selected() and port.is_actor_up()
+            ]
+        )
 
     def lags(self):
         """Return dict of LAGs mapped to member ports."""
         lags = collections.defaultdict(list)
         for port in self.lacp_ports():
             lags[port.lacp].append(port)
         return lags
@@ -540,16 +619,17 @@
 
     def tagged_flood_ports(self, exclude_unicast):
         return self.flood_ports(self.tagged, exclude_unicast)
 
     def untagged_flood_ports(self, exclude_unicast):
         return self.flood_ports(self.untagged + self.dot1x_untagged, exclude_unicast)
 
-    def output_port(self, port, hairpin=False, output_table=None,
-                    external_forwarding_requested=None):
+    def output_port(
+        self, port, hairpin=False, output_table=None, external_forwarding_requested=None
+    ):
         actions = []
         if self.port_is_untagged(port):
             actions.append(valve_of.pop_vlan())
             # Packet is mirrored, as the receiving host sees it (without a tag).
             actions.extend(port.mirror_actions())
         else:
             actions.extend(port.mirror_actions())
@@ -572,28 +652,36 @@
         pkt = packet_builder(vid, *args)
         return valve_of.packetout(port.number, bytes(pkt.data))
 
     def flood_pkt(self, packet_builder, multi_out, *args):
         """Return Packet-out actions via flooding"""
         ofmsgs = []
         for vid, ports in (
-                (self.vid, self.tagged_flood_ports(False)),
-                (None, self.untagged_flood_ports(False))):
+            (self.vid, self.tagged_flood_ports(False)),
+            (None, self.untagged_flood_ports(False)),
+        ):
             if ports:
                 pkt = packet_builder(vid, *args)
                 exclude_ports = self.excluded_lag_ports()
                 running_port_nos = [
-                    port.number for port in ports if port.running() and port not in exclude_ports]
+                    port.number
+                    for port in ports
+                    if port.running() and port not in exclude_ports
+                ]
                 if running_port_nos:
                     random.shuffle(running_port_nos)
                     if multi_out:
                         ofmsgs.append(valve_of.packetouts(running_port_nos, pkt.data))
                     else:
                         ofmsgs.extend(
-                            [valve_of.packetout(port_no, pkt.data) for port_no in running_port_nos])
+                            [
+                                valve_of.packetout(port_no, pkt.data)
+                                for port_no in running_port_nos
+                            ]
+                        )
         return ofmsgs
 
     def port_is_tagged(self, port):
         """Return True if port number is an tagged port on this VLAN."""
         return port in self.tagged
 
     def port_is_untagged(self, port):
@@ -615,16 +703,17 @@
 
     def ip_in_vip_subnet(self, ipa, faucet_vip=None):
         """Return faucet_vip if IP in same IP network as a VIP on this VLAN."""
         if faucet_vip is None:
             faucet_vip = self.vip_map(ipa)
         if faucet_vip:
             if ipa not in (
-                    faucet_vip.network.network_address,
-                    faucet_vip.network.broadcast_address):
+                faucet_vip.network.network_address,
+                faucet_vip.network.broadcast_address,
+            ):
                 return faucet_vip
         return None
 
     def from_connected_to_vip(self, src_ip, dst_ip):
         """Return True if src_ip in connected network and dst_ip is a VIP.
 
         Args:
```

### Comparing `c65faucet-1.0.46/faucet/watcher.py` & `c65faucet-1.0.47/faucet/watcher.py`

 * *Files 5% similar despite different names*

```diff
@@ -76,26 +76,26 @@
 class GaugePortStateLogger(GaugePortStatePoller):
     """Abstraction for port state logger."""
 
     def _update(self, rcv_time, msg):
         rcv_time_str = self._rcv_time(rcv_time)
         reason = msg.reason
         port_no = msg.desc.port_no
-        log_msg = f'port {port_no} unknown state {reason}'
+        log_msg = 'port %s unknown state %s' % (port_no, reason)
         if reason == ofp.OFPPR_ADD:
-            log_msg = f'port {port_no} added'
+            log_msg = 'port %s added' % port_no
         elif reason == ofp.OFPPR_DELETE:
-            log_msg = f'port {port_no} deleted'
+            log_msg = 'port %s deleted' % port_no
         elif reason == ofp.OFPPR_MODIFY:
             link_down = (msg.desc.state & ofp.OFPPS_LINK_DOWN)
             if link_down:
-                log_msg = f'port {port_no} down'
+                log_msg = 'port %s down' % port_no
             else:
-                log_msg = f'port {port_no} up'
-        log_msg = f'{dpid_log(self.dp.dp_id)} {log_msg}'
+                log_msg = 'port %s up' % port_no
+        log_msg = '%s %s' % (dpid_log(self.dp.dp_id), log_msg)
         self.logger.info(log_msg)
         if self.conf.file:
             with open(self.conf.file, 'a', encoding='utf-8') as logfile:
                 logfile.write('\t'.join((rcv_time_str, log_msg)) + '\n')
 
     def send_req(self):
         """Send a stats request to a datapath."""
@@ -148,22 +148,23 @@
 
     def _update(self, rcv_time, msg):
         rcv_time_str = self._rcv_time(rcv_time)
         path = self.conf.path
         # Double Hyphen to avoid confusion with ISO8601 times
         filename = os.path.join(
             path,
-            f"{self.dp.name}--flowtable--{rcv_time_str}.json"
+            "{}--flowtable--{}.json".format(self.dp.name, rcv_time_str)
         )
         if os.path.isfile(filename):
             # If this filename already exists, add an increment to the filename
             # (for dealing with parts of a multipart message arriving at the same time)
             inc = 1
             while os.path.isfile(filename):
-                filename = os.path.join(path, f"{self.dp.name}--flowtable--{rcv_time_str}--{inc}.json")
+                filename = os.path.join(path, "{}--flowtable--{}--{}.json".format(
+                    self.dp.name, rcv_time_str, inc))
                 inc += 1
 
         if self.conf.compress:
             with gzip.open(filename, 'wt') as outfile:
                 outfile.write(json.dumps(msg.to_jsondict()))
         else:
             with open(filename, 'w', encoding='utf-8') as outfile:
```

### Comparing `c65faucet-1.0.46/faucet/watcher_conf.py` & `c65faucet-1.0.47/faucet/watcher_conf.py`

 * *Files 3% similar despite different names*

```diff
@@ -178,18 +178,18 @@
         db_conf = deepcopy(db_conf)
         db_type = db_conf.pop('type')
         db_conf['db_type'] = db_type
         self.update(db_conf)
         test_config_condition(
             self.file is not None and not
             (os.path.dirname(self.file) and os.access(os.path.dirname(self.file), os.W_OK)),
-            f'{self.file} is not writable')
+            '%s is not writable' % self.file)
         test_config_condition(
             self.path is not None and not os.access(self.path, os.W_OK),
-            f'{self.file} is not writable')
+            '%s is not writable' % self.file)
 
     def add_dp(self, dp):
         """Add a datapath to this watcher."""
         self.dp = dp
 
     def check_config(self):
         super().check_config()
@@ -197,8 +197,8 @@
             self.all_dps and self.dps is not None,
             'all_dps and dps cannot be set together')
         test_config_condition(
             not self.type, 'type must be set')
         valid_types = {'flow_table', 'port_stats', 'port_state', 'meter_stats'}
         test_config_condition(
             self.type not in valid_types,
-            f'type {self.type} not one of {valid_types}')
+            'type %s not one of %s' % (self.type, valid_types))
```

### Comparing `c65faucet-1.0.46/hw_switch_config.yaml` & `c65faucet-1.0.47/hw_switch_config.yaml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/ofctl_rest/ofctl_rest.py` & `c65faucet-1.0.47/ofctl_rest/ofctl_rest.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/ofctl_rest/wsgi.py` & `c65faucet-1.0.47/ofctl_rest/wsgi.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/setup.cfg` & `c65faucet-1.0.47/setup.cfg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/setup.py` & `c65faucet-1.0.47/setup.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/codecheck/src_files.sh` & `c65faucet-1.0.47/tests/codecheck/src_files.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/fuzzer/config/fuzz_config.py` & `c65faucet-1.0.47/tests/generative/fuzzer/config/fuzz_config.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/fuzzer/config/generate_dict.py` & `c65faucet-1.0.47/tests/generative/fuzzer/config/generate_dict.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/fuzzer/packet/display_packet_crash.py` & `c65faucet-1.0.47/tests/generative/fuzzer/packet/display_packet_crash.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/http.ex2` & `c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/http.ex2`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/icmp.ex1` & `c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/icmp.ex1`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/icmp.ex2` & `c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/icmp.ex2`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/ipv4.ex1` & `c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/ipv4.ex1`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/fuzzer/packet/examples/msger.ex1` & `c65faucet-1.0.47/tests/generative/fuzzer/packet/examples/msger.ex1`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/fuzzer/packet/fake_packet.py` & `c65faucet-1.0.47/tests/generative/fuzzer/packet/fake_packet.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/fuzzer/packet/fuzz_packet.py` & `c65faucet-1.0.47/tests/generative/fuzzer/packet/fuzz_packet.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/integration/fault_tolerance_main.py` & `c65faucet-1.0.47/tests/generative/integration/fault_tolerance_main.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/integration/fault_tolerance_tests.py` & `c65faucet-1.0.47/tests/generative/integration/fault_tolerance_tests.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/generative/unit/test_topology.py` & `c65faucet-1.0.47/tests/generative/unit/test_topology.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/integration/mininet_main.py` & `c65faucet-1.0.47/tests/integration/mininet_main.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/integration/mininet_multidp_tests.py` & `c65faucet-1.0.47/tests/integration/mininet_multidp_tests.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/integration/mininet_tests.py` & `c65faucet-1.0.47/tests/integration/mininet_tests.py`

 * *Files 3% similar despite different names*

```diff
@@ -256,15 +256,15 @@
     nfv_intf = None
     nfv_portno = None
 
     @staticmethod
     def _priv_mac(host_id):
         two_byte_port_num = '%04x' % host_id
         two_byte_port_num_formatted = ':'.join((two_byte_port_num[:2], two_byte_port_num[2:]))
-        return f'00:00:00:00:{two_byte_port_num_formatted}'
+        return '00:00:00:00:%s' % two_byte_port_num_formatted
 
     def pre_start_net(self):
         self.eapol1_host, self.eapol2_host, self.ping_host, self.nfv_host = self.hosts_name_ordered()
         switch = self.first_switch()
         last_host_switch_link = switch.connectionsTo(self.nfv_host)[0]
         nfv_intf = [
             intf for intf in last_host_switch_link if intf in switch.intfList()][0]
@@ -309,15 +309,15 @@
         self.nfv_pids.append(int(self.nfv_host.lastPid))
         self.radius_log_path = self.start_freeradius()
         self.nfv_pids.append(int(self.nfv_host.lastPid))
         self._enable_event_log(300)
 
     def tearDown(self, ignore_oferrors=False):
         for pid in self.nfv_pids:
-            self.nfv_host.cmd(f'kill {pid}')
+            self.nfv_host.cmd('kill %u' % pid)
         super().tearDown(ignore_oferrors=ignore_oferrors)
 
     def post_test_checks(self):
         self.assertGreater(os.path.getsize(self.event_log), 0)
         self.verify_dot1x_events_log()
 
     def verify_dot1x_events_log(self):
@@ -354,26 +354,26 @@
                 if 'DOT1X' not in event_log_line:
                     continue
                 event = json.loads(event_log_line.strip())
                 events_that_happened.append(event['DOT1X'])
 
             for expected_event in dot1x_expected_events:
                 self.assertTrue(expected_event in events_that_happened,
-                                msg=f'expected event: {expected_event} not in '
-                                    f'events_that_happened {events_that_happened}')
+                                msg='expected event: {} not in events_that_happened {}'.format(
+                                    expected_event, events_that_happened))
 
     @staticmethod
     def _eapol_filter(fields):
         return '(' + ' and '.join(('ether proto 0x888e',) + fields) + ')'
 
     def _success_eapol_filter(self, expect_success):
         eap_code = '0x04'
         if expect_success:
             eap_code = '0x03'
-        return self._eapol_filter(('ether[14:4] == 0x01000004', f'ether[18] == {eap_code}'))
+        return self._eapol_filter(('ether[14:4] == 0x01000004', 'ether[18] == %s' % eap_code))
 
     def _logoff_eapol_filter(self):
         return self._eapol_filter(('ether[14:4] == 0x01020000',))
 
     def try_8021x(self, host, port_num, conf, and_logoff=False, terminate_wpasupplicant=False,
                   wpasup_timeout=180, tcpdump_timeout=30, expect_success=True):
         if expect_success:
@@ -445,17 +445,17 @@
             if self.try_8021x(host, port_num, conf, and_logoff, expect_success=expect_success):
                 return True
             time.sleep(1)
         return False
 
     def wait_8021x_flows(self, port_no):
         port_actions = [
-            'SET_FIELD: {eth_dst:%s}' % self._priv_mac(port_no), f'OUTPUT:{self.nfv_portno}']
+            'SET_FIELD: {eth_dst:%s}' % self._priv_mac(port_no), 'OUTPUT:%u' % self.nfv_portno]
         from_nfv_actions = [
-            'SET_FIELD: {eth_src:01:80:c2:00:00:03}', f'OUTPUT:{port_no}']
+            'SET_FIELD: {eth_src:01:80:c2:00:00:03}', 'OUTPUT:%d' % port_no]
         from_nfv_match = {
             'in_port': self.nfv_portno, 'dl_src': self._priv_mac(port_no), 'dl_type': 0x888e}
         self.wait_until_matching_flow(None, table_id=0, actions=port_actions)
         self.wait_until_matching_flow(from_nfv_match, table_id=0, actions=from_nfv_actions)
 
     def wait_8021x_success_flows(self, host, port_no):
         from_host_actions = [
@@ -473,15 +473,15 @@
         self.one_ipv4_ping(
             self.eapol1_host, self.ping_host.IP(), require_host_learned=False, expected_result=True)
 
     def wpa_supplicant_callback(self, host, port_num, conf, and_logoff, timeout=10, terminate_wpasupplicant=False):
         wpa_ctrl_path = self.get_wpa_ctrl_path(host)
         if os.path.exists(wpa_ctrl_path):
             self.terminate_wpasupplicant(host)
-            for pid in host.cmd(f'lsof -t {wpa_ctrl_path}').splitlines():
+            for pid in host.cmd('lsof -t %s' % wpa_ctrl_path).splitlines():
                 try:
                     os.kill(int(pid), 15)
                 except (ValueError, ProcessLookupError):
                     pass
             try:
                 shutil.rmtree(wpa_ctrl_path)
             except FileNotFoundError:
@@ -492,55 +492,55 @@
             wpa_ctrl_socket_path=wpa_ctrl_path, log_prefix=log_prefix)
         if and_logoff:
             self.wait_for_eap_success(host, wpa_ctrl_path)
             self.wait_until_matching_flow(
                 {'eth_src': host.MAC(), 'in_port': port_num}, table_id=0)
             self.one_ipv4_ping(
                 host, self.ping_host.IP(), require_host_learned=False)
-            host.cmd(f'wpa_cli -p {wpa_ctrl_path} logoff')
+            host.cmd('wpa_cli -p %s logoff' % wpa_ctrl_path)
             self.wait_until_no_matching_flow(
                 {'eth_src': host.MAC(), 'in_port': port_num}, table_id=0)
             self.one_ipv4_ping(
                 host, self.ping_host.IP(),
                 require_host_learned=False, expected_result=False)
 
         if terminate_wpasupplicant:
             self.terminate_wpasupplicant(host)
 
     def terminate_wpasupplicant(self, host):
         wpa_ctrl_path = self.get_wpa_ctrl_path(host)
-        host.cmd(f'wpa_cli -p {wpa_ctrl_path} terminate')
+        host.cmd('wpa_cli -p %s terminate' % wpa_ctrl_path)
 
     def get_wpa_ctrl_path(self, host):
         wpa_ctrl_path = os.path.join(
-            self.tmpdir, f'{self.tmpdir}/{host.name}-wpasupplicant')
+            self.tmpdir, '%s/%s-wpasupplicant' % (self.tmpdir, host.name))
         return wpa_ctrl_path
 
     @staticmethod
     def get_wpa_status(host, wpa_ctrl_path):
-        status = host.cmdPrint(f'wpa_cli -p {wpa_ctrl_path} status')
+        status = host.cmdPrint('wpa_cli -p %s status' % wpa_ctrl_path)
         for line in status.splitlines():
             if line.startswith('EAP state'):
                 return line.split('=')[1].strip()
         return None
 
     def wait_for_eap_success(self, host, wpa_ctrl_path, timeout=5):
         for _ in range(timeout):
             eap_state = self.get_wpa_status(host, wpa_ctrl_path)
             if eap_state == 'SUCCESS':
                 return
             time.sleep(1)
-        self.fail(f'did not get EAP success: {eap_state}')
+        self.fail('did not get EAP success: %s' % eap_state)
 
     def wait_for_radius(self, radius_log_path):
         self.wait_until_matching_lines_from_file(
             r'.*Ready to process requests', radius_log_path)
 
     def start_freeradius(self):
-        radius_log_path = f'{self.tmpdir}/radius.log'
+        radius_log_path = '%s/radius.log' % self.tmpdir
 
         listen_match = r'(listen {[^}]*(limit {[^}]*})[^}]*})|(listen {[^}]*})'
         listen_config = """listen {
         type = auth
         ipaddr = *
         port = %s
 }
@@ -548,66 +548,66 @@
         type = acct
         ipaddr = *
         port = %d
 }""" % (self.RADIUS_PORT, self.RADIUS_PORT + 1)
 
         if os.path.isfile('/etc/freeradius/users'):
             # Assume we are dealing with freeradius 2 configuration
-            shutil.copytree('/etc/freeradius/', f'{self.tmpdir}/freeradius')
-            users_path = f'{self.tmpdir}/freeradius/users'
+            shutil.copytree('/etc/freeradius/', '%s/freeradius' % self.tmpdir)
+            users_path = '%s/freeradius/users' % self.tmpdir
 
-            with open(f'{self.tmpdir}/freeradius/radiusd.conf', 'r+', encoding='utf-8') as default_site:
+            with open('%s/freeradius/radiusd.conf' % self.tmpdir, 'r+', encoding='utf-8') as default_site:
                 default_config = default_site.read()
                 default_config = re.sub(listen_match, '', default_config)
                 default_site.seek(0)
                 default_site.write(default_config)
                 default_site.write(listen_config)
                 default_site.truncate()
         else:
             # Assume we are dealing with freeradius >=3 configuration
             freerad_version = os.popen(
                 r'freeradius -v | egrep -o -m 1 "Version ([0-9]\.[0.9])"').read().rstrip()
             freerad_major_version = freerad_version.split(' ')[1]
-            shutil.copytree(f'/etc/freeradius/{freerad_major_version}/',
-                            f'{self.tmpdir}/freeradius')
-            users_path = f'{self.tmpdir}/freeradius/mods-config/files/authorize'
+            shutil.copytree('/etc/freeradius/%s/' % freerad_major_version,
+                            '%s/freeradius' % self.tmpdir)
+            users_path = '%s/freeradius/mods-config/files/authorize' % self.tmpdir
 
-            with open(f'{self.tmpdir}/freeradius/sites-enabled/default', 'r+', encoding='utf-8') as default_site:
+            with open('%s/freeradius/sites-enabled/default' % self.tmpdir, 'r+', encoding='utf-8') as default_site:
                 default_config = default_site.read()
                 default_config = re.sub(
                     listen_match, '', default_config)
                 default_config = re.sub(
                     r'server default {', 'server default {\n' + listen_config, default_config)
                 default_site.seek(0)
                 default_site.write(default_config)
                 default_site.truncate()
 
         with open(users_path, 'w', encoding='utf-8') as users_file:
             users_file.write(self.freeradius_user_conf.format(self.SESSION_TIMEOUT))
 
-        with open(f'{self.tmpdir}/freeradius/clients.conf', 'w', encoding='utf-8') as clients:
+        with open('%s/freeradius/clients.conf' % self.tmpdir, 'w', encoding='utf-8') as clients:
             clients.write("""client localhost {
     ipaddr = 127.0.0.1
     secret = SECRET
 }""")
 
-        with open(f'{self.tmpdir}/freeradius/sites-enabled/inner-tunnel', 'r+', encoding='utf-8') as innertunnel_site:
+        with open('%s/freeradius/sites-enabled/inner-tunnel' % self.tmpdir, 'r+', encoding='utf-8') as innertunnel_site:
             tunnel_config = innertunnel_site.read()
             listen_config = """listen {
        ipaddr = 127.0.0.1
        port = %d
        type = auth
 }""" % (self.RADIUS_PORT + 2)
             tunnel_config = re.sub(listen_match, listen_config, tunnel_config)
             innertunnel_site.seek(0)
             innertunnel_site.write(tunnel_config)
             innertunnel_site.truncate()
 
-        os.system(f'chmod o+rx {self.root_tmpdir}')
-        os.system(f'chown -R root:freerad {self.tmpdir}/freeradius/')
+        os.system('chmod o+rx %s' % self.root_tmpdir)
+        os.system('chown -R root:freerad %s/freeradius/' % self.tmpdir)
 
         self.nfv_host.cmd(
             mininet_test_util.timeout_cmd(
                 'freeradius -X -l %s -d %s/freeradius &' % (radius_log_path, self.tmpdir),
                 300))
 
         self.wait_for_radius(radius_log_path)
@@ -765,24 +765,24 @@
             self.set_port_up(port)
             self.wait_8021x_flows(port)
 
         username = 'user'
         username_bytes = ''.join(('%2x' % ord(c) for c in username))
         tcpdump_filter = ' or '.join((
             self._success_eapol_filter(True),
-            self._eapol_filter((f'ether[23:4] == 0x{username_bytes}',))))
+            self._eapol_filter(('ether[23:4] == 0x%s' % username_bytes,))))
         tcpdump_txt = self.tcpdump_helper(
             self.eapol1_host, tcpdump_filter, [
                 lambda: port_up(port_no1)],
             timeout=30, vflags='-vvv', packets=2)
         for req_str in (
-                f'Identity: {username}',  # supplicant replies with username
+                'Identity: %s' % username,  # supplicant replies with username
                 'Success',  # supplicant success
         ):
-            self.assertTrue(req_str in tcpdump_txt, msg=f'{req_str} not in {tcpdump_txt}')
+            self.assertTrue(req_str in tcpdump_txt, msg='%s not in %s' % (req_str, tcpdump_txt))
 
         self.one_ipv4_ping(
             self.eapol1_host, self.ping_host.IP(),
             require_host_learned=False, expected_result=True, retries=10)
 
         self.post_test_checks()
 
@@ -1191,23 +1191,23 @@
         self.wait_until_matching_flow(
             {'in_port': port_no1},
             table_id=self._VLAN_TABLE,
             actions=['SET_FIELD: {vlan_vid:%u}' % radius_vid1])
         self.wait_until_matching_flow(
             {'vlan_vid': radius_vid1},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', f'OUTPUT:{port_no1}', f'OUTPUT:{port_no3}'])
+            actions=['POP_VLAN', 'OUTPUT:%s' % port_no1, 'OUTPUT:%s' % port_no3])
         self.wait_until_matching_flow(
             {'vlan_vid': vid},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', f'OUTPUT:{port_no2}'])
+            actions=['POP_VLAN', 'OUTPUT:%s' % port_no2])
         self.wait_until_no_matching_flow(
             {'vlan_vid': radius_vid2},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', f'OUTPUT:{port_no1}', f'OUTPUT:{port_no2}'])
+            actions=['POP_VLAN', 'OUTPUT:%s' % port_no1, 'OUTPUT:%s' % port_no2])
 
         self.one_ipv4_ping(
             self.eapol1_host, self.ping_host.IP(),
             require_host_learned=False, expected_result=True)
         self.assertTrue(self.try_8021x(
             self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=True))
         self.one_ipv4_ping(
@@ -1224,19 +1224,19 @@
             table_id=self._VLAN_TABLE,
             actions=['SET_FIELD: {vlan_vid:%u}' % vid])
 
         # check flood ports are in the right vlans
         self.wait_until_no_matching_flow(
             {'vlan_vid': radius_vid1},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', f'OUTPUT:{port_no1}', f'OUTPUT:{port_no2}'])
+            actions=['POP_VLAN', 'OUTPUT:%s' % port_no1, 'OUTPUT:%s' % port_no3])
         self.wait_until_matching_flow(
             {'vlan_vid': vid},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', f'OUTPUT:{port_no1}', f'OUTPUT:{port_no2}'])
+            actions=['POP_VLAN', 'OUTPUT:%s' % port_no1, 'OUTPUT:%s' % port_no2])
 
         # check two 1x hosts play nicely. (same dyn vlan)
         self.assertTrue(self.try_8021x(
             self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False))
         self.one_ipv4_ping(
             self.eapol1_host, self.ping_host.IP(),
             require_host_learned=False, expected_result=True)
@@ -1297,30 +1297,30 @@
         self.wait_until_matching_flow(
             {'in_port': port_no2},
             table_id=self._VLAN_TABLE,
             actions=['SET_FIELD: {vlan_vid:%u}' % vid])
         self.wait_until_matching_flow(
             {'vlan_vid': vid},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', f'OUTPUT:{port_no2}'])
+            actions=['POP_VLAN', 'OUTPUT:%s' % port_no2])
         self.wait_until_no_matching_flow(
             {'in_port': port_no2},
             table_id=self._VLAN_TABLE,
             actions=['SET_FIELD: {vlan_vid:%u}' % radius_vid2])
         self.wait_until_no_matching_flow(
             {'vlan_vid': radius_vid2},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', f'OUTPUT:{port_no1}', f'OUTPUT:{port_no2}'])
+            actions=['POP_VLAN', 'OUTPUT:%s' % port_no1, 'OUTPUT:%s' % port_no2])
         self.wait_until_no_matching_flow(
             {'eth_src': self.eapol2_host.MAC()},
             table_id=self._ETH_SRC_TABLE)
         self.wait_until_no_matching_flow(
             {'eth_dst': self.eapol2_host.MAC(), 'vlan_vid': radius_vid1},
             table_id=self._ETH_DST_TABLE,
-            actions=['POP_VLAN', f'OUTPUT:{port_no2}'])
+            actions=['POP_VLAN', 'OUTPUT:%s' % port_no2])
 
         self.post_test_checks()
 
 
 class FaucetUntaggedRandomVidTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
@@ -1370,15 +1370,15 @@
         switch = self.first_switch()
         last_host_switch_link = switch.connectionsTo(last_host)[0]
         last_host_switch_intf = [intf for intf in last_host_switch_link if intf in switch.intfList()][0]
 
         super().test_untagged()
 
         # Confirm controller can see switch interface with traffic.
-        ifconfig_output = self.net.controllers[0].cmd(f'ifconfig {last_host_switch_intf}')
+        ifconfig_output = self.net.controllers[0].cmd('ifconfig %s' % last_host_switch_intf)
         self.assertTrue(
             re.search('(R|T)X packets[: ][1-9]', ifconfig_output),
             msg=ifconfig_output)
 
 
 class FaucetUntaggedBroadcastTest(FaucetUntaggedTest):
 
@@ -1483,15 +1483,15 @@
 
     def test_untagged(self):
         first_host = self.hosts_name_ordered()[0]
         tcpdump_filter = 'ether proto 0x88cc'
         timeout = 5 * 3
         tcpdump_txt = self.tcpdump_helper(
             first_host, tcpdump_filter, [
-                lambda: first_host.cmd(f'sleep {timeout}')],
+                lambda: first_host.cmd('sleep %u' % timeout)],
             timeout=timeout, vflags='-vv', packets=1)
         oui_prefix = ''.join(self.FAUCET_MAC.split(':')[:3])
         faucet_lldp_dp_id_attr = '%2.2x' % 1
         expected_lldp_dp_id = ''.join((
             oui_prefix,
             faucet_lldp_dp_id_attr,
             binascii.hexlify(str(self.dpid).encode('UTF-8')).decode()))
@@ -1534,15 +1534,15 @@
     def test_untagged(self):
         first_host = self.hosts_name_ordered()[0]
         tcpdump_filter = 'ether proto 0x88cc'
         interval = 10
         timeout = interval * 3
         tcpdump_txt = self.tcpdump_helper(
             first_host, tcpdump_filter, [
-                lambda: first_host.cmd(f'sleep {timeout}')],
+                lambda: first_host.cmd('sleep %u' % timeout)],
             # output epoch secs
             timeout=timeout, vflags='-tt', packets=2)
         timestamps = re.findall(r'(\d+)\.\d+ [0-9a-f:]+ \> [0-9a-f:]+', tcpdump_txt)
         timestamps = [int(timestamp) for timestamp in timestamps]
         self.assertTrue(timestamps[1] - timestamps[0] >= interval, msg=tcpdump_txt)
 
 
@@ -1563,24 +1563,24 @@
 
     def test_untagged(self):
         first_host = self.hosts_name_ordered()[0]
         tcpdump_filter = 'ether proto 0x88cc'
         timeout = 5 * 3
         tcpdump_txt = self.tcpdump_helper(
             first_host, tcpdump_filter, [
-                lambda: first_host.cmd(f'sleep {timeout}')],
+                lambda: first_host.cmd('sleep %u' % timeout)],
             timeout=timeout, vflags='-vv', packets=1)
         for lldp_required in (
                 r'%s > 01:80:c2:00:00:0e, ethertype LLDP' % self.FAUCET_MAC,
                 r'Application type \[voice\] \(0x01\), Flags \[Tagged\]Vlan id 50',
                 r'System Name TLV \(5\), length 8: faucet-1',
                 r'Port Description TLV \(4\), length [1-9]: b%u' % self.port_map['port_1']):
             self.assertTrue(
                 re.search(lldp_required, tcpdump_txt),
-                msg=f'{lldp_required}: {tcpdump_txt}')
+                msg='%s: %s' % (lldp_required, tcpdump_txt))
 
 
 class FaucetUntaggedMeterParseTest(FaucetUntaggedTest):
 
     REQUIRES_METERS = True
     OVS_TYPE = 'user'
     CONFIG_GLOBAL = """
@@ -1800,19 +1800,20 @@
         macvlan2_ipv4 = '10.0.0.101'
         self.add_macvlan(first_host, macvlan1_intf, ipa=macvlan1_ipv4, mode='vepa')
         self.add_macvlan(first_host, macvlan2_intf, mode='vepa')
         macvlan2_mac = self.get_host_intf_mac(first_host, macvlan2_intf)
         netns = self.hostns(first_host)
         setup_cmds = []
         setup_cmds.extend(
-            [f'ip link set {macvlan2_intf} netns {netns}'])
+            ['ip link set %s netns %s' % (macvlan2_intf, netns)])
         for exec_cmd in (
-                (f'ip address add {macvlan2_ipv4}/24 brd + dev {macvlan2_intf}',
-                 f'ip link set {macvlan2_intf} up')):
-            setup_cmds.append(f'ip netns exec {netns} {exec_cmd}')
+                ('ip address add %s/24 brd + dev %s' % (
+                    macvlan2_ipv4, macvlan2_intf),
+                 'ip link set %s up' % macvlan2_intf)):
+            setup_cmds.append('ip netns exec %s %s' % (netns, exec_cmd))
         self.quiet_commands(first_host, setup_cmds)
         self.one_ipv4_ping(first_host, macvlan2_ipv4, intf=macvlan1_intf)
         self.one_ipv4_ping(first_host, second_host.IP())
         # Verify OUTPUT:IN_PORT flood rules are exercised.
         self.wait_nonzero_packet_count_flow(
             {'in_port': self.port_map['port_1'],
              'dl_dst': 'ff:ff:ff:ff:ff:ff'},
@@ -1882,15 +1883,15 @@
         #  so black-list that version with a test
         # https://github.com/secdev/scapy/issues/3306
         # TODO: fix expected in next scapy release, > 2.4.5.
         exception = False
         try:
             scapy.all.send(scapy.all.fuzz(scapy.all.Ether()))  # pylint: disable=no-member
         except Exception as e:  # pylint: disable=broad-except
-            error(f'{self._test_name()}:', e)
+            error('%s:' % self._test_name(), e)
             exception = True
         self.assertFalse(exception, 'Scapy threw an exception in send(fuzz())')
 
     def test_ryu_config(self):
         varstr = ', '.join(self.scrape_prometheus(var='ryu_config'))
         self.assertTrue('echo_request_interval"} 10.0' in varstr)
         self.assertTrue('maximum_unreplied_echo_requests"} 5.0' in varstr)
@@ -1898,44 +1899,49 @@
     def verify_dp_port_healthy(self, dp_port, retries=5, min_mbps=MIN_MBPS):
         for _ in range(retries):
             port_desc = self.get_port_desc_from_dpid(self.dpid, dp_port)
             port_name = port_desc['name']
             port_state = port_desc['state']
             port_config = port_desc['config']
             port_speed_mbps = (port_desc['curr_speed'] * 1e3) / 1e6
-            error(f'DP {dp_port} is {port_name}, at {port_speed_mbps} mbps\n')
+            error('DP %u is %s, at %u mbps\n' % (dp_port, port_name, port_speed_mbps))
             if port_speed_mbps < min_mbps:
-                error(f'port speed {port_speed_mbps} below minimum {min_mbps} mbps\n')
+                error('port speed %u below minimum %u mbps\n' % (
+                    port_speed_mbps, min_mbps))
             elif port_config != 0:
-                error(f'port config {port_config} must be 0 (all clear)')
+                error('port config %u must be 0 (all clear)' % port_config)
             elif port_state not in (0, 4):
-                error(f'state {port_state} must be 0 (all flags clear or live)\n')
+                error('state %u must be 0 (all flags clear or live)\n' % (
+                    port_state))
             else:
                 return
             time.sleep(1)
-        self.fail(f'DP port {dp_port} not healthy ({port_desc})')
+        self.fail('DP port %u not healthy (%s)' % (dp_port, port_desc))
 
     def test_portmap(self):
         prom_desc = self.scrape_prometheus(var='of_dp_desc_stats')
         self.assertIsNotNone(prom_desc, msg='Cannot scrape of_dp_desc_stats')
-        error(f'DP: {prom_desc[0]}\n')
-        error(f'port_map: {self.port_map}\n')
+        error('DP: %s\n' % prom_desc[0])
+        error('port_map: %s\n' % self.port_map)
         for i, host in enumerate(self.hosts_name_ordered(), start=1):
-            in_port = f'port_{i}'
+            in_port = 'port_%u' % i
             dp_port = self.port_map[in_port]
             if dp_port in self.switch_map:
-                error(f'verifying cabling for {in_port}: host {self.switch_map[dp_port]} -> dp {dp_port}\n')
+                error('verifying cabling for %s: host %s -> dp %u\n' % (
+                    in_port, self.switch_map[dp_port], dp_port))
             else:
-                error(f'verifying host {in_port} -> dp {dp_port}\n')
+                error('verifying host %s -> dp %s\n' % (
+                    in_port, dp_port))
             self.verify_dp_port_healthy(dp_port)
             self.require_host_learned(host, in_port=dp_port)
         learned = self.prom_macs_learned()
         self.assertEqual(
             len(self.hosts_name_ordered()), len(learned),
-            msg=f'test requires exactly {len(self.hosts_name_ordered())} hosts learned (got {learned})')
+            msg='test requires exactly %u hosts learned (got %s)' % (
+                len(self.hosts_name_ordered()), learned))
 
     def test_listening(self):
         msg_template = (
             'Processes listening on test, or all interfaces may interfere with tests. '
             'Please deconfigure them (e.g. configure interface as "unmanaged"):\n\n%s')
         controller = self._get_controller()
         ss_out = controller.cmd('ss -lnep').splitlines()
@@ -1958,15 +1964,15 @@
         for host in self.hosts_name_ordered():
             tcpdump_filter = ''
             tcpdump_txt = self.tcpdump_helper(
                 host, tcpdump_filter, [], timeout=10, vflags='-vv', packets=1)
             self.tcpdump_rx_packets(tcpdump_txt, 0)
             self.assertTrue(
                 self.tcpdump_rx_packets(tcpdump_txt, 0),
-                msg=f'got unexpected packet from test switch: {tcpdump_txt}')
+                msg='got unexpected packet from test switch: %s' % tcpdump_txt)
 
 
 class FaucetUntaggedPrometheusGaugeTest(FaucetUntaggedTest):
     """Testing Gauge Prometheus"""
 
     GAUGE_CONFIG_DBS = """
     prometheus:
@@ -2152,15 +2158,15 @@
             self.server.timeout = self.DB_TIMEOUT
             self.server_thread = threading.Thread(
                 target=self.server.serve_forever)
             self.server_thread.daemon = True
             self.server_thread.start()
             return None
         except socket.error as err:
-            return f'cannot start Influx test server: {err}'
+            return 'cannot start Influx test server: %s' % err
 
     def test_untagged(self):
         self.ping_all_when_learned()
         self.hup_controller(self.gauge_controller.name)
         self.flap_all_switch_ports()
         self._wait_influx_log()
         self._verify_influx_log()
@@ -2726,16 +2732,16 @@
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
         self.ping_all_when_learned()
         for i in range(10, 10 + (self.MAX_HOSTS * 2)):
-            mac_intf = f'mac{i}'
-            mac_ipv4 = f'10.0.0.{i}'
+            mac_intf = 'mac%u' % i
+            mac_ipv4 = '10.0.0.%u' % i
             self.add_macvlan(second_host, mac_intf, ipa=mac_ipv4)
             ping_cmd = mininet_test_util.timeout_cmd(
                 'fping %s -c1 -t1 -I%s %s > /dev/null 2> /dev/null' % (
                     self.FPING_ARGS_SHORT, mac_intf, first_host.IP()),
                 2)
             second_host.cmd(ping_cmd)
         flows = self.get_matching_flows_on_dpid(
@@ -2786,47 +2792,47 @@
         mac_ipv4s = [mac_ipv4 for mac_ipv4, _ in mac_ips]
         fping_cmd = mininet_test_util.timeout_cmd(
             'fping %s -c%u %s' % (
                 self.FPING_ARGS_SHORT, int(self.TIMEOUT / 3), ' '.join(mac_ipv4s)),
             self.TIMEOUT / 2)
         for _ in range(3):
             fping_out = first_host.cmd(fping_cmd)
-            self.assertTrue(fping_out, msg=f'fping did not complete: {fping_cmd}')
+            self.assertTrue(fping_out, msg='fping did not complete: %s' % fping_cmd)
             macs_learned = self.hosts_learned(hosts)
             if len(macs_learned) == len(hosts):
                 return
             time.sleep(1)
         first_host_diag = first_host.cmd('ifconfig -a ; arp -an')
         second_host_diag = second_host.cmd('ifconfig -a ; arp -an')
-        self.fail(f'{mac_ips} cannot be learned ({macs_learned} != {fping_out})\n'
-                  f'first host {first_host_diag}\nsecond host {second_host_diag}\n')
+        self.fail('%s cannot be learned (%s != %s)\nfirst host %s\nsecond host %s\n' % (
+            mac_ips, macs_learned, fping_out, first_host_diag, second_host_diag))
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
         all_learned_mac_ports = {}
 
         # learn batches of hosts, then down them
         for base in (10, 20, 30):
             def add_macvlans(base, count):
                 mac_intfs = []
                 mac_ips = []
                 learned_mac_ports = {}
                 for i in range(base, base + count):
-                    mac_intf = f'mac{i}'
+                    mac_intf = 'mac%u' % i
                     mac_intfs.append(mac_intf)
-                    mac_ipv4 = f'10.0.0.{i}'
+                    mac_ipv4 = '10.0.0.%u' % i
                     self.add_macvlan(second_host, mac_intf, ipa=mac_ipv4)
                     macvlan_mac = self.get_mac_of_intf(mac_intf, second_host)
                     learned_mac_ports[macvlan_mac] = self.port_map['port_2']
                     mac_ips.append((mac_ipv4, macvlan_mac))
                 return (mac_intfs, mac_ips, learned_mac_ports)
 
             def down_macvlans(macvlans):
                 for macvlan in macvlans:
-                    second_host.cmd(f'ip link set dev {macvlan} down')
+                    second_host.cmd('ip link set dev %s down' % macvlan)
 
             def learn_then_down_hosts(base, count):
                 mac_intfs, mac_ips, learned_mac_ports = add_macvlans(base, count)
                 self.verify_hosts_learned(first_host, second_host, mac_ips, learned_mac_ports)
                 down_macvlans(mac_intfs)
                 return learned_mac_ports
 
@@ -2843,15 +2849,15 @@
             learned_macs = self.hosts_learned(all_learned_mac_ports)
             self.verify_learn_counters(
                 100, list(range(1, len(self.hosts_name_ordered()) + 1)))
             if not learned_macs:
                 break
             time.sleep(1)
 
-        self.assertFalse(learned_macs, msg=f'MACs did not expire: {learned_macs}')
+        self.assertFalse(learned_macs, msg='MACs did not expire: %s' % learned_macs)
 
         self.assertTrue(before_expiry_learned_macs)
         for mac in before_expiry_learned_macs:
             self.wait_until_no_matching_flow({'eth_dst': mac}, table_id=self._ETH_DST_TABLE)
 
 
 class FaucetSingleHostsNoIdleTimeoutPrometheusTest(FaucetSingleHostsTimeoutPrometheusTest):
@@ -2871,37 +2877,34 @@
 
 class FaucetSingleL3LearnMACsOnPortTest(FaucetUntaggedTest):
 
     # TODO: currently set to accommodate least hardware
     def _max_hosts():  # pylint: disable=no-method-argument
         return 512
 
-    MAX_HOSTS = _max_hosts()  # pylint: disable=too-many-function-args
+    MAX_HOSTS = _max_hosts()
     TEST_IPV4_NET = '10.0.0.0'
     TEST_IPV4_PREFIX = 16  # must hold more than MAX_HOSTS + 4
     LEARN_IPV4 = '10.0.254.254'
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         max_hosts: %u
         faucet_vips: ["10.0.254.254/16"]
-""" % (_max_hosts() + 4)  # pylint: disable=too-many-function-args
+""" % (_max_hosts() + 4)
 
     CONFIG = ("""
         ignore_learn_ins: 0
         metrics_rate_limit_sec: 3
         table_sizes:
             eth_src: %u
             eth_dst: %u
             ipv4_fib: %u
-""" % (_max_hosts() + 64,  # pylint: disable=too-many-function-args
-           _max_hosts() + 64,  # pylint: disable=too-many-function-args
-           _max_hosts() + 64   # pylint: disable=too-many-function-args
-           ) + """
+""" % (_max_hosts() + 64, _max_hosts() + 64, _max_hosts() + 64) + """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 max_hosts: 4096
             %(port_2)d:
                 native_vlan: 100
                 max_hosts: 4096
@@ -2911,45 +2914,43 @@
             %(port_4)d:
                 native_vlan: 100
                 max_hosts: 4096
 """)
 
     def test_untagged(self):
         test_net = ipaddress.IPv4Network(
-            f'{self.TEST_IPV4_NET}/{self.TEST_IPV4_PREFIX}')
+            '%s/%s' % (self.TEST_IPV4_NET, self.TEST_IPV4_PREFIX))
         learn_ip = ipaddress.IPv4Address(self.LEARN_IPV4)
         self.verify_learning(test_net, learn_ip, 64, self.MAX_HOSTS)
 
 
 class FaucetSingleL2LearnMACsOnPortTest(FaucetUntaggedTest):
 
     # TODO: currently set to accommodate least hardware
     def _max_hosts():  # pylint: disable=no-method-argument
         return 1024
 
-    MAX_HOSTS = _max_hosts()  # pylint: disable=too-many-function-args
+    MAX_HOSTS = _max_hosts()
     TEST_IPV4_NET = '10.0.0.0'
     TEST_IPV4_PREFIX = 16  # must hold more than MAX_HOSTS + 4
     LEARN_IPV4 = '10.0.0.1'
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         max_hosts: %u
-""" % (_max_hosts() + 4)  # pylint: disable=too-many-function-args
+""" % (_max_hosts() + 4)
 
     CONFIG = ("""
         ignore_learn_ins: 0
         metrics_rate_limit_sec: 3
         table_sizes:
             eth_src: %u
             eth_dst: %u
-""" % (_max_hosts() + 64,  # pylint: disable=too-many-function-args
-           _max_hosts() + 64  # pylint: disable=too-many-function-args
-           ) + """
+""" % (_max_hosts() + 64, _max_hosts() + 64) + """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 max_hosts: 4096
             %(port_2)d:
                 native_vlan: 100
                 max_hosts: 4096
@@ -2959,15 +2960,15 @@
             %(port_4)d:
                 native_vlan: 100
                 max_hosts: 4096
 """)
 
     def test_untagged(self):
         test_net = ipaddress.IPv4Network(
-            f'{self.TEST_IPV4_NET}/{self.TEST_IPV4_PREFIX}')
+            '%s/%s' % (self.TEST_IPV4_NET, self.TEST_IPV4_PREFIX))
         learn_ip = ipaddress.IPv4Address(self.LEARN_IPV4)
         self.verify_learning(test_net, learn_ip, 64, self.MAX_HOSTS)
 
 
 class FaucetUntaggedHUPTest(FaucetUntaggedTest):
     """Test handling HUP signal without config change."""
 
@@ -2980,15 +2981,15 @@
                 count = self.get_configure_count(controller=controller.name)
                 counts.append(count)
             if counts == expected:
                 break
             time.sleep(1)
         self.assertEqual(
             counts, expected,
-            f'Controller configure counts {counts} != expected counts {expected}')
+            'Controller configure counts %s != expected counts %s' % (counts, expected))
 
     def test_untagged(self):
         """Test that FAUCET receives HUP signal and keeps switching."""
         init_config_count = self.get_configure_count()
         reload_type_vars = (
             'faucet_config_reload_cold',
             'faucet_config_reload_warm')
@@ -3129,15 +3130,15 @@
         tcp_src: 65535
 """
 
     def setUp(self):
         super().setUp()
         self.acl_config_file = os.path.join(self.tmpdir, 'acl.txt')
         self.CONFIG = '\n'.join(
-            (self.CONFIG, f'include:\n     - {self.acl_config_file}'))
+            (self.CONFIG, 'include:\n     - %s' % self.acl_config_file))
         with open(self.acl_config_file, 'w', encoding='utf-8') as acf:
             acf.write(self.START_ACL_CONFIG)
         self.topo = self.topo_class(
             self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
             n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
             links_per_host=self.LINKS_PER_HOST, hw_dpid=self.hw_dpid)
         self.start_net()
@@ -3152,26 +3153,26 @@
                 port = (rule + 1) % 2**16
                 ip_match = str(host_ip)
                 rule_yaml = {
                     'eth_type': eth_type,
                     'ip_proto': 6,
                     'tcp_src': port,
                     'tcp_dst': port,
-                    f'ipv{host_ip.version}_src': ip_match,
-                    f'ipv{host_ip.version}_dst': ip_match,
+                    'ipv%u_src' % host_ip.version: ip_match,
+                    'ipv%u_dst' % host_ip.version: ip_match,
                     'actions': {'allow': 1},
                 }
                 rules_yaml.append({'rule': rule_yaml})
             yaml_acl_conf = {'acls': {1: {'exact_match': True, 'rules': rules_yaml}}}
-            tuple_txt = f'{len(rules_yaml)} IPv{host_ip.version} tuples\n'
-            error(f'pushing {tuple_txt}')
+            tuple_txt = '%u IPv%u tuples\n' % (len(rules_yaml), host_ip.version)
+            error('pushing %s' % tuple_txt)
             self.reload_conf(
                 yaml_acl_conf, self.acl_config_file,  # pytype: disable=attribute-error
                 restart=True, cold_start=False)
-            error(f'pushed {tuple_txt}')
+            error('pushed %s' % tuple_txt)
             self.wait_until_matching_flow(
                 {'tp_src': port, 'ip_proto': 6, 'dl_type': eth_type}, table_id=0)
             rules *= 2
 
     def test_tuples(self):
         host_ips = list(itertools.islice(self.NET_BASE.hosts(), self.MAX_RULES))
         self._push_tuples(self.ETH_TYPE, host_ips)
@@ -3316,19 +3317,19 @@
 """
     ACL_COOKIE = None
 
     def setUp(self):
         super().setUp()
         self.ACL_COOKIE = random.randint(1, 2**16 - 1)
         self.ACL = self.ACL.replace('COOKIE', str(self.ACL_COOKIE))
-        self.acl_config_file = f'{self.tmpdir}/acl.yaml'
+        self.acl_config_file = '%s/acl.yaml' % self.tmpdir
         with open(self.acl_config_file, 'w', encoding='utf-8') as config_file:
             config_file.write(self.ACL)
         self.CONFIG = '\n'.join(
-            (self.CONFIG, f'include:\n     - {self.acl_config_file}'))
+            (self.CONFIG, 'include:\n     - %s' % self.acl_config_file))
         self.topo = self.topo_class(
             self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
             n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
             links_per_host=self.LINKS_PER_HOST, hw_dpid=self.hw_dpid)
         self.start_net()
 
 
@@ -3408,15 +3409,15 @@
         third_host, fourth_host = self.hosts_name_ordered()[2:]
         self.ping_all_when_learned()
         self.change_port_config(
             self.port_map['port_1'], 'native_vlan', 200,
             restart=False, cold_start=False)
         self.wait_until_matching_flow(
             {'vlan_vid': 200}, table_id=self._ETH_SRC_TABLE,
-            actions=['OUTPUT:CONTROLLER', f'GOTO_TABLE:{self._ETH_DST_TABLE}'])
+            actions=['OUTPUT:CONTROLLER', 'GOTO_TABLE:%u' % self._ETH_DST_TABLE])
         self.change_port_config(
             self.port_map['port_2'], 'native_vlan', 200,
             restart=True, cold_start=False)
         for port_name in ('port_1', 'port_2'):
             self.wait_until_matching_flow(
                 {'in_port': int(self.port_map[port_name])},
                 table_id=self._VLAN_TABLE,
@@ -3435,15 +3436,15 @@
             cold_start=False)
         self.wait_until_matching_flow(
             {'in_port': int(self.port_map['port_1']),
              'eth_type': IPV4_ETH, 'tcp_dst': 5001, 'ip_proto': 6},
             table_id=self._PORT_ACL_TABLE, cookie=self.ACL_COOKIE)
         self.wait_until_matching_flow(
             {'vlan_vid': 100}, table_id=self._ETH_SRC_TABLE,
-            actions=['OUTPUT:CONTROLLER', f'GOTO_TABLE:{self._ETH_DST_TABLE}'])
+            actions=['OUTPUT:CONTROLLER', 'GOTO_TABLE:%u' % self._ETH_DST_TABLE])
         self.verify_tp_dst_blocked(5001, first_host, second_host)
         self.verify_tp_dst_notblocked(5002, first_host, second_host)
         self.reload_conf(
             orig_conf, self.faucet_config_path,
             restart=True, cold_start=False, host_cache=100)
         self.verify_tp_dst_notblocked(
             5001, first_host, second_host, table_id=None)
@@ -3552,15 +3553,15 @@
         self.ping_all_when_learned()
         self.assertEqual(4, len(self.scrape_prometheus(var='learned_l2_port')))
         self.change_port_config(
             self.port_map['port_1'], 'native_vlan', 200,
             restart=False, cold_start=False)
         self.wait_until_matching_flow(
             {'vlan_vid': 200}, table_id=self._ETH_SRC_TABLE,
-            actions=['OUTPUT:CONTROLLER', f'GOTO_TABLE:{self._ETH_DST_TABLE}'])
+            actions=['OUTPUT:CONTROLLER', 'GOTO_TABLE:%u' % self._ETH_DST_TABLE])
         self.change_port_config(
             self.port_map['port_2'], 'native_vlan', 200,
             restart=True, cold_start=False)
         self.wait_until_matching_flow(
             {'in_port': int(self.port_map['port_2'])},
             table_id=self._VLAN_TABLE,
             actions=['SET_FIELD: {vlan_vid:4296}'])
@@ -3896,15 +3897,15 @@
             time.sleep(1)
         self.assertTrue(ofmsg, msg=match)
         self.set_port_down(self.port_map['port_1'])
         for _ in range(5):
             if not self.get_matching_flow(match, table_id=table):
                 return
             time.sleep(1)
-        self.fail(f'host route {match} still present')
+        self.fail('host route %s still present' % match)
 
 
 class FaucetUntaggedRestBcastIPv4RouteTest(FaucetUntaggedIPv4RouteTest):
 
     CONFIG = """
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
@@ -4085,16 +4086,16 @@
 """
 
     def test_untagged(self):
         # Inject packet into pipeline using coprocessor.
         coprocessor_host, first_host, second_host, _ = self.hosts_name_ordered()
         self.one_ipv4_ping(first_host, second_host.IP())
         tcpdump_filter = ' and '.join((
-            f'ether dst {first_host.MAC()}',
-            f'ether src {coprocessor_host.MAC()}',
+            'ether dst %s' % first_host.MAC(),
+            'ether src %s' % coprocessor_host.MAC(),
             'icmp'))
         cmds = [
             lambda: coprocessor_host.cmd(
                 'arp -s %s %s' % (first_host.IP(), first_host.MAC())),
             lambda: coprocessor_host.cmd(
                 'fping %s -c3 %s' % (self.FPING_ARGS_SHORT, first_host.IP())),
         ]
@@ -4137,15 +4138,15 @@
             n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
             links_per_host=self.LINKS_PER_HOST, hw_dpid=self.hw_dpid)
         self.start_net()
 
     def total_port_bans(self):
         total_bans = 0
         for i in range(self.LINKS_PER_HOST * self.N_UNTAGGED):
-            port_labels = self.port_labels(self.port_map[f'port_{i + 1}'])
+            port_labels = self.port_labels(self.port_map['port_%u' % (i + 1)])
             total_bans += self.scrape_prometheus_var(
                 'port_learn_bans', port_labels, dpid=True, default=0)
         return total_bans
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()
         # Normal learning works
@@ -4163,21 +4164,21 @@
             'tc qdisc add dev veth-loop1 root tbf rate 1000kbps latency 10ms burst 1000',
             'tc qdisc add dev veth-loop2 root tbf rate 1000kbps latency 10ms burst 1000',
             # Connect one leg of veth pair to first host interface.
             'brctl addbr br-loop1',
             'brctl setfd br-loop1 0',
             'ip link set br-loop1 up',
             'brctl addif br-loop1 veth-loop1',
-            f'brctl addif br-loop1 {second_host.name}-eth0',
+            'brctl addif br-loop1 %s-eth0' % second_host.name,
             # Connect other leg of veth pair.
             'brctl addbr br-loop2',
             'brctl setfd br-loop2 0',
             'ip link set br-loop2 up',
             'brctl addif br-loop2 veth-loop2',
-            f'brctl addif br-loop2 {second_host.name}-eth1'))
+            'brctl addif br-loop2 %s-eth1' % second_host.name))
 
         # Flood some traffic into the loop
         for _ in range(3):
             first_host.cmd('fping %s -c3 10.0.0.254' % self.FPING_ARGS_SHORT)
             end_bans = self.total_port_bans()
             if end_bans > start_bans:
                 return
@@ -4317,22 +4318,22 @@
 details partner lacp pdu:
     system priority: 65535
     system mac address: 0e:00:00:00:00:01
     oper key: 1
     port priority: 2
     port number: %d
     port state: 62
-""".strip() % tuple(get_lacp_port_id(self.port_map[f'port_{i}']) for i in lag_ports)
+""".strip() % tuple(get_lacp_port_id(self.port_map['port_%u' % i]) for i in lag_ports)
 
         lacp_timeout = 5
 
         def prom_lacp_up_ports():
             lacp_up_ports = 0
             for lacp_port in lag_ports:
-                port_labels = self.port_labels(self.port_map[f'port_{lacp_port}'])
+                port_labels = self.port_labels(self.port_map['port_%u' % lacp_port])
                 lacp_state = self.scrape_prometheus_var('port_lacp_state', port_labels, default=0)
                 lacp_up_ports += 1 if lacp_state == 3 else 0
             return lacp_up_ports
 
         def require_lag_up_ports(expected_up_ports):
             for _ in range(lacp_timeout * 10):
                 if prom_lacp_up_ports() == expected_up_ports:
@@ -4347,84 +4348,86 @@
                 with open(os.path.join(self.tmpdir, 'bonding-state.txt'), 'w', encoding='utf-8') as state_file:
                     state_file.write(result)
                 if re.search(synced_state_txt, result):
                     break
                 time.sleep(1)
             self.assertTrue(
                 re.search(synced_state_txt, result),
-                msg=f'LACP did not synchronize: {result}\n\nexpected:\n\n{synced_state_txt}')
+                msg='LACP did not synchronize: %s\n\nexpected:\n\n%s' % (
+                    result, synced_state_txt))
 
         # Start with ports down.
         for port in lag_ports:
-            self.set_port_down(self.port_map[f'port_{port}'])
+            self.set_port_down(self.port_map['port_%u' % port])
         require_lag_up_ports(0)
         orig_ip = first_host.IP()
         switch = self.first_switch()
         bond_members = [pair[0].name for pair in first_host.connectionsTo(switch)]
         # Deconfigure bond members
         for bond_member in bond_members:
             self.quiet_commands(first_host, (
-                f'ip link set {bond_member} down',
-                f'ip address flush dev {bond_member}'))
+                'ip link set %s down' % bond_member,
+                'ip address flush dev %s' % bond_member))
         # Configure bond interface
         self.quiet_commands(first_host, (
-            f'ip link add {bond} address 0e:00:00:00:00:99 type bond mode 802.3ad lacp_rate fast miimon 100',
-            f'ip add add {orig_ip}/24 dev {bond}',
-            f'ip link set {bond} up'))
+            ('ip link add %s address 0e:00:00:00:00:99 '
+             'type bond mode 802.3ad lacp_rate fast miimon 100') % bond,
+            'ip add add %s/24 dev %s' % (orig_ip, bond),
+            'ip link set %s up' % bond))
         # Add bond members
         for bond_member in bond_members:
             self.quiet_commands(first_host, (
-                f'ip link set dev {bond_member} master {bond}',))
+                'ip link set dev %s master %s' % (bond_member, bond),))
 
         for _flaps in range(2):
             # All ports down.
             for port in lag_ports:
-                self.set_port_down(self.port_map[f'port_{port}'])
+                self.set_port_down(self.port_map['port_%u' % port])
             require_lag_up_ports(0)
             # Pick a random port to come up.
             up_port = random.choice(lag_ports)
-            self.set_port_up(self.port_map[f'port_{up_port}'])
+            self.set_port_up(self.port_map['port_%u' % up_port])
             require_lag_up_ports(1)
             # We have connectivity with only one port.
             self.one_ipv4_ping(
                 first_host, self.FAUCET_VIPV4.ip, require_host_learned=False, intf=bond, retries=5)
             for port in lag_ports:
-                self.set_port_up(self.port_map[f'port_{port}'])
+                self.set_port_up(self.port_map['port_%u' % port])
             # We have connectivity with two ports.
             require_lag_up_ports(2)
             require_linux_bond_up()
             self.one_ipv4_ping(
                 first_host, self.FAUCET_VIPV4.ip, require_host_learned=False, intf=bond, retries=5)
             # We have connectivity if that random port goes down.
-            self.set_port_down(self.port_map[f'port_{up_port}'])
+            self.set_port_down(self.port_map['port_%u' % up_port])
             require_lag_up_ports(1)
             self.one_ipv4_ping(
                 first_host, self.FAUCET_VIPV4.ip, require_host_learned=False, intf=bond, retries=5)
             for port in lag_ports:
-                self.set_port_up(self.port_map[f'port_{port}'])
+                self.set_port_up(self.port_map['port_%u' % port])
 
 
 class FaucetUntaggedIPv4LACPMismatchTest(FaucetUntaggedIPv4LACPTest):
     """Ensure remote LACP system ID mismatch is logged."""
 
     def test_untagged(self):
         first_host = self.hosts_name_ordered()[0]
         orig_ip = first_host.IP()
         switch = self.first_switch()
         bond_members = [pair[0].name for pair in first_host.connectionsTo(switch)]
         for i, bond_member in enumerate(bond_members):
-            bond = f'bond{i}'
+            bond = 'bond%u' % i
             self.quiet_commands(first_host, (
-                f'ip link set {bond_member} down',
-                f'ip address flush dev {bond_member}',
+                'ip link set %s down' % bond_member,
+                'ip address flush dev %s' % bond_member,
                 ('ip link add %s address 0e:00:00:00:00:%2.2x '
                  'type bond mode 802.3ad lacp_rate fast miimon 100') % (bond, i * 2 + i),
-                f'ip add add {orig_ip}/24 dev {bond}',
-                f'ip link set {bond} up',
-                f'ip link set dev {bond_member} master {bond}'))
+                'ip add add %s/24 dev %s' % (orig_ip, bond),
+                'ip link set %s up' % bond,
+                'ip link set dev %s master %s' % (bond_member, bond)))
         self.wait_until_matching_lines_from_faucet_log_files(r'.+actor system mismatch.+')
 
 
 class FaucetUntaggedIPv4ControlPlaneFuzzTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
 vlans:
@@ -4449,15 +4452,16 @@
         fuzz_template = 'python3 -c \"from scapy.all import * ; scapy.all.send(%s, count=%u)\"'
         for fuzz_cmd in (
                 fuzz_template % ('IP(dst=\'%s\')/fuzz(%s(type=0))' % (self.FAUCET_VIPV4.ip, 'ICMP'), packets),
                 fuzz_template % ('IP(dst=\'%s\')/fuzz(%s(type=8))' % (self.FAUCET_VIPV4.ip, 'ICMP'), packets),
                 fuzz_template % ('fuzz(%s(pdst=\'%s\'))' % ('ARP', self.FAUCET_VIPV4.ip), packets)):
             fuzz_out = first_host.cmd(mininet_test_util.timeout_cmd(fuzz_cmd, 180))
             self.assertTrue(
-                re.search(f'Sent {packets} packets', fuzz_out), msg=f'{fuzz_cmd}: {fuzz_out}')
+                re.search('Sent %u packets' % packets, fuzz_out), msg='%s: %s' % (
+                    fuzz_cmd, fuzz_out))
         self.one_ipv4_controller_ping(first_host)
 
     def test_flap_ping_controller(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         for _ in range(5):
             self.one_ipv4_ping(first_host, second_host.IP())
             for host in first_host, second_host:
@@ -4504,65 +4508,65 @@
 """ + CONFIG_BOILER_UNTAGGED
 
     def test_ndisc6(self):
         first_host = self.hosts_name_ordered()[0]
         for vip in ('fe80::1:254', 'fc00::1:254', 'fc00::2:254'):
             self.assertEqual(
                 self.FAUCET_MAC.upper(),
-                first_host.cmd(f'ndisc6 -q {vip} {first_host.defaultIntf()}').strip())
+                first_host.cmd('ndisc6 -q %s %s' % (vip, first_host.defaultIntf())).strip())
 
     def test_rdisc6(self):
         first_host = self.hosts_name_ordered()[0]
         rdisc6_results = sorted(list(set(first_host.cmd(
-            f'rdisc6 -q {first_host.defaultIntf()}').splitlines())))
+            'rdisc6 -q %s' % first_host.defaultIntf()).splitlines())))
         self.assertEqual(
             ['fc00::1:0/112', 'fc00::2:0/112'],
             rdisc6_results)
 
     def test_ra_advertise(self):
         first_host = self.hosts_name_ordered()[0]
         tcpdump_filter = ' and '.join((
             'ether dst 33:33:00:00:00:01',
-            f'ether src {self.FAUCET_MAC}',
+            'ether src %s' % self.FAUCET_MAC,
             'icmp6',
             'ip6[40] == 134',
             'ip6 host fe80::1:254'))
         tcpdump_txt = self.tcpdump_helper(
             first_host, tcpdump_filter, [], timeout=30, vflags='-vv', packets=1)
         for ra_required in (
                 r'ethertype IPv6 \(0x86dd\), length 142',
                 r'fe80::1:254 > ff02::1:.+ICMP6, router advertisement',
                 r'fc00::1:0/112, Flags \[onlink, auto\]',
                 r'fc00::2:0/112, Flags \[onlink, auto\]',
                 r'source link-address option \(1\), length 8 \(1\): %s' % self.FAUCET_MAC):
             self.assertTrue(
                 re.search(ra_required, tcpdump_txt),
-                msg=f'{ra_required}: {tcpdump_txt}')
+                msg='%s: %s' % (ra_required, tcpdump_txt))
 
     def test_rs_reply(self):
         first_host = self.hosts_name_ordered()[0]
         tcpdump_filter = ' and '.join((
-            f'ether src {self.FAUCET_MAC}',
-            f'ether dst {first_host.MAC()}',
+            'ether src %s' % self.FAUCET_MAC,
+            'ether dst %s' % first_host.MAC(),
             'icmp6',
             'ip6[40] == 134',
             'ip6 host fe80::1:254'))
         tcpdump_txt = self.tcpdump_helper(
             first_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'rdisc6 -1 {first_host.defaultIntf()}')],
+                    'rdisc6 -1 %s' % first_host.defaultIntf())],
             timeout=30, vflags='-vv', packets=1)
         for ra_required in (
                 r'fe80::1:254 > fe80::.+ICMP6, router advertisement',
                 r'fc00::1:0/112, Flags \[onlink, auto\]',
                 r'fc00::2:0/112, Flags \[onlink, auto\]',
                 r'source link-address option \(1\), length 8 \(1\): %s' % self.FAUCET_MAC):
             self.assertTrue(
                 re.search(ra_required, tcpdump_txt),
-                msg=f'{ra_required}: {tcpdump_txt} ({tcpdump_filter})')
+                msg='%s: %s (%s)' % (ra_required, tcpdump_txt, tcpdump_filter))
 
 
 class FaucetUntaggedIPv6ControlPlaneFuzzTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
 vlans:
     100:
@@ -4590,15 +4594,15 @@
         self.one_ipv6_controller_ping(first_host)
         fuzz_success = False
         packets = 1000
         count = 0
         abort = False
 
         def note(*args):
-            error(f'{self._test_name()}:', *args + tuple('\n'))
+            error('%s:' % self._test_name(), *args + tuple('\n'))
 
         # Some of these tests have been slowing down and timing out,
         # So this code is intended to allow some debugging and analysis
         for fuzz_class in dir(scapy.all):
             if fuzz_class.startswith('ICMPv6'):
                 fuzz_cmd = ("from scapy.all import * ;"
                             "scapy.all.send(IPv6(dst='%s')/fuzz(%s()),count=%u)" %
@@ -4610,15 +4614,15 @@
                     if time.time() - start > too_long:
                         note('stopping', fuzz_class, 'after >', too_long, 'seconds')
                         note('output was:', out)
                         popen.terminate()
                         abort = True
                         break
                 popen.wait()
-                if f'Sent {packets} packets' in out:
+                if 'Sent %u packets' % packets in out:
                     count += packets
                     elapsed = time.time() - start
                     note('sent', packets, fuzz_class, 'packets in %.2fs' % elapsed)
                     fuzz_success = True
                 if abort:
                     break
         note('successfully sent', count, 'packets')
@@ -5170,27 +5174,27 @@
     def test_untagged(self):
         first_host, second_host, third_host, fourth_host = self.hosts_name_ordered()[0:4]
         tcpdump_filter = ('icmp')
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
         tcpdump_txt = self.tcpdump_helper(
             third_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {third_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (third_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, third_host.IP())))])
         self.assertTrue(re.search(
-            f'{third_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % third_host.IP(), tcpdump_txt))
         tcpdump_txt = self.tcpdump_helper(
             fourth_host, tcpdump_filter, [
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, fourth_host.IP())))])
         self.assertFalse(re.search(
-            f'{fourth_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % fourth_host.IP(), tcpdump_txt))
 
 
 class FaucetMultiOrderedOutputTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
 vlans:
     100:
@@ -5219,27 +5223,27 @@
     def test_untagged(self):
         first_host, second_host, third_host, fourth_host = self.hosts_name_ordered()[0:4]
         tcpdump_filter = ('icmp')
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
         tcpdump_txt = self.tcpdump_helper(
             third_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {third_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (third_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, third_host.IP())))])
         self.assertTrue(re.search(
-            f'{third_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % third_host.IP(), tcpdump_txt))
         tcpdump_txt = self.tcpdump_helper(
             fourth_host, tcpdump_filter, [
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, fourth_host.IP())))])
         self.assertFalse(re.search(
-            f'{fourth_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % fourth_host.IP(), tcpdump_txt))
 
 
 class FaucetUntaggedOutputTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
 vlans:
     100:
@@ -5273,18 +5277,18 @@
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
         tcpdump_filter = ('icmp and ether dst 06:06:06:06:06:06')
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
         self.assertTrue(re.search(
             'vlan 123', tcpdump_txt))
 
 
 class FaucetUntaggedOrderedOutputTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
@@ -5320,18 +5324,18 @@
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
         tcpdump_filter = ('icmp and ether dst 06:06:06:06:06:06')
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
         self.assertTrue(re.search(
             'vlan 123', tcpdump_txt))
 
 
 class FaucetUntaggedMultiVlansOutputTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
@@ -5367,18 +5371,18 @@
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
         tcpdump_filter = 'vlan'
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
         self.assertTrue(re.search(
             'vlan 456.+vlan 123', tcpdump_txt))
 
 
 class FaucetUntaggedMultiVlansOrderedOutputTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
@@ -5414,18 +5418,18 @@
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
         tcpdump_filter = 'vlan'
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
         self.assertTrue(re.search(
             'vlan 456.+vlan 123', tcpdump_txt))
 
 
 class FaucetUntaggedMultiConfVlansOutputTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
@@ -5461,19 +5465,19 @@
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
         tcpdump_filter = 'ether proto 0x88a8'
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
             packets=1)
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt), msg=tcpdump_txt)
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt), msg=tcpdump_txt)
         self.assertTrue(re.search(
             r'vlan 456.+ethertype 802\.1Q-QinQ \(0x88a8\), vlan 123', tcpdump_txt), msg=tcpdump_txt)
 
 
 class FaucetUntaggedMultiConfVlansOrderedOutputTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
@@ -5509,19 +5513,19 @@
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
         tcpdump_filter = 'ether proto 0x88a8'
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
             packets=1)
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt), msg=tcpdump_txt)
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt), msg=tcpdump_txt)
         self.assertTrue(re.search(
             r'vlan 456.+ethertype 802\.1Q-QinQ \(0x88a8\), vlan 123', tcpdump_txt), msg=tcpdump_txt)
 
 
 class FaucetUntaggedMirrorTest(FaucetUntaggedTest):
 
     CONFIG_GLOBAL = """
@@ -5711,23 +5715,23 @@
             MIN_MBPS, first_host_ip, second_host_ip,
             sync_counters_func=lambda: self.one_ipv4_ping(first_host, second_host_ip))
         tagged_ports = (self.port_map['port_1'], self.port_map['port_2'], self.port_map['port_4'])
         for port in tagged_ports:
             self.wait_until_matching_flow(
                 {'vlan_vid': 100, 'in_port': port},
                 table_id=self._VLAN_TABLE,
-                actions=[f'GOTO_TABLE:{self._ETH_SRC_TABLE}'])
+                actions=['GOTO_TABLE:%u' % self._ETH_SRC_TABLE])
         self.change_port_config(
             self.port_map['port_3'], 'mirror', None,
             restart=True, cold_start=False)
         for port in tagged_ports:
             self.wait_until_matching_flow(
                 {'vlan_vid': 100, 'in_port': port},
                 table_id=self._VLAN_TABLE,
-                actions=[f'GOTO_TABLE:{self._ETH_SRC_TABLE}'])
+                actions=['GOTO_TABLE:%u' % self._ETH_SRC_TABLE])
 
 
 class FaucetTaggedVLANPCPTest(FaucetTaggedTest):
 
     CONFIG_GLOBAL = """
 vlans:
     100:
@@ -5759,24 +5763,24 @@
                 tagged_vlans: [100]
 """
 
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
         self.quiet_commands(
             first_host,
-            [f'ip link set {first_host.defaultIntf()} type vlan egress {i}:1'
-                for i in range(0, 8)])
+            ['ip link set %s type vlan egress %u:1' % (
+                first_host.defaultIntf(), i) for i in range(0, 8)])
         self.one_ipv4_ping(first_host, second_host.IP())
         self.wait_nonzero_packet_count_flow(
             {'vlan_vid': 100, 'vlan_pcp': 1}, table_id=self._PORT_ACL_TABLE)
-        tcpdump_filter = f'ether dst {second_host.MAC()}'
+        tcpdump_filter = 'ether dst %s' % second_host.MAC()
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'ping -c3 {second_host.IP()}')], root_intf=True, packets=1)
+                    'ping -c3 %s' % second_host.IP())], root_intf=True, packets=1)
         self.assertTrue(re.search('vlan 100, p 2,', tcpdump_txt))
 
 
 class FaucetTaggedVLANPCPOrderedTest(FaucetTaggedTest):
 
     CONFIG_GLOBAL = """
 vlans:
@@ -5809,24 +5813,24 @@
                 tagged_vlans: [100]
 """
 
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
         self.quiet_commands(
             first_host,
-            [f'ip link set {first_host.defaultIntf()} type vlan egress {i}:1'
-                for i in range(0, 8)])
+            ['ip link set %s type vlan egress %u:1' % (
+                first_host.defaultIntf(), i) for i in range(0, 8)])
         self.one_ipv4_ping(first_host, second_host.IP())
         self.wait_nonzero_packet_count_flow(
             {'vlan_vid': 100, 'vlan_pcp': 1}, table_id=self._PORT_ACL_TABLE)
-        tcpdump_filter = f'ether dst {second_host.MAC()}'
+        tcpdump_filter = 'ether dst %s' % second_host.MAC()
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'ping -c3 {second_host.IP()}')], root_intf=True, packets=1)
+                    'ping -c3 %s' % second_host.IP())], root_intf=True, packets=1)
         self.assertTrue(re.search('vlan 100, p 2,', tcpdump_txt))
 
 
 class FaucetTaggedGlobalIPv4RouteTest(FaucetTaggedTest):
 
     def _vids():  # pylint: disable=no-method-argument
         return list(range(100, 148))
@@ -5834,35 +5838,35 @@
     def global_vid():  # pylint: disable=no-method-argument
         return 2047
 
     IPV = 4
     NETPREFIX = 24
     ETH_TYPE = IPV4_ETH
     NETNS = True
-    VIDS = _vids()  # pylint: disable=too-many-function-args
-    GLOBAL_VID = global_vid()  # pylint: disable=too-many-function-args
-    STR_VIDS = [str(i) for i in _vids()]  # pylint: disable=too-many-function-args
+    VIDS = _vids()
+    GLOBAL_VID = global_vid()
+    STR_VIDS = [str(i) for i in _vids()]
     NEW_VIDS = VIDS[1:]
 
     @staticmethod
     def netbase(vid, host):
-        return ipaddress.ip_interface(f'192.168.{vid}.{host}')
+        return ipaddress.ip_interface('192.168.%u.%u' % (vid, host))
 
     def fping(self, macvlan_int, ipg):
         return 'fping %s -c1 -t1 -I%s %s > /dev/null 2> /dev/null' % (
             self.FPING_ARGS_SHORT, macvlan_int, ipg)
 
     def fib_table(self):
         return self._IPV4_FIB_TABLE
 
     def macvlan_ping(self, host, ipa, macvlan_int):
         return self.one_ipv4_ping(host, ipa, intf=macvlan_int)
 
     def run_ip(self, args):
-        return f'ip -{self.IPV} {args}'
+        return 'ip -%u %s' % (self.IPV, args)
 
     CONFIG_GLOBAL = """
 routers:
     global:
         vlans: [%s]
 vlans:
 %s
@@ -5887,53 +5891,54 @@
                 native_vlan: 99
                 tagged_vlans: [%s]
                 hairpin_unicast: True
             %s:
                 native_vlan: 99
                 tagged_vlans: [%s]
                 hairpin_unicast: True
-""" % (global_vid(),  # pylint: disable=too-many-function-args
+""" % (global_vid(),
        len(STR_VIDS) * 3,   # VLAN
        len(STR_VIDS) * 2,   # VIP
        len(STR_VIDS) * 12,  # Flood
        '%(port_3)d', '%(port_1)d', '%(port_1)d',
        ','.join(STR_VIDS), '%(port_2)d', ','.join(STR_VIDS))
 
     def configure_mesh(self, first_host, second_host):
         hosts = (first_host, second_host)
         required_ipds = set()
         ipd_to_macvlan = {}
 
         for i, host in enumerate(hosts, start=1):
             setup_commands = []
             for vid in self.NEW_VIDS:
-                vlan_int = f'{host.intf_root_name}.{vid}'
-                macvlan_int = f'macvlan{vid}'
+                vlan_int = '%s.%u' % (host.intf_root_name, vid)
+                macvlan_int = 'macvlan%u' % vid
                 ipa = self.netbase(vid, i)
                 ipg = self.netbase(vid, 254)
                 ipd = self.netbase(vid, 253)
                 required_ipds.add(str(ipd.ip))
                 ipd_to_macvlan[str(ipd.ip)] = (macvlan_int, host)
                 setup_commands.extend([
-                    self.run_ip(f'link add link {host.intf_root_name} name {vlan_int} type vlan id {vid}'),
-                    self.run_ip(f'link set dev {vlan_int} up'),
-                    self.run_ip(f'link add {macvlan_int} link {vlan_int} type macvlan mode vepa'),
-                    self.run_ip(f'link set dev {macvlan_int} up'),
-                    self.run_ip(f'address add {ipa.ip}/{self.NETPREFIX} dev {macvlan_int}'),
-                    self.run_ip(f'route add default via {ipg.ip} table {vid}'),
-                    self.run_ip(f'rule add from {ipa} table {vid} priority 100'),
+                    self.run_ip('link add link %s name %s type vlan id %u' % (
+                        host.intf_root_name, vlan_int, vid)),
+                    self.run_ip('link set dev %s up' % vlan_int),
+                    self.run_ip('link add %s link %s type macvlan mode vepa' % (macvlan_int, vlan_int)),
+                    self.run_ip('link set dev %s up' % macvlan_int),
+                    self.run_ip('address add %s/%u dev %s' % (ipa.ip, self.NETPREFIX, macvlan_int)),
+                    self.run_ip('route add default via %s table %u' % (ipg.ip, vid)),
+                    self.run_ip('rule add from %s table %u priority 100' % (ipa, vid)),
                     # stimulate learning attempts for down host.
-                    self.run_ip(f'neigh add {ipd.ip} lladdr {self.FAUCET_MAC} dev {macvlan_int}')])
+                    self.run_ip('neigh add %s lladdr %s dev %s' % (ipd.ip, self.FAUCET_MAC, macvlan_int))])
                 # next host routes via FAUCET for other host in same connected subnet
                 # to cause routing to be exercised.
                 for j, _ in enumerate(hosts, start=1):
                     if j != i:
                         other_ip = self.netbase(vid, j)
                         setup_commands.append(
-                            self.run_ip(f'route add {other_ip} via {ipg.ip} table {vid}'))
+                            self.run_ip('route add %s via %s table %u' % (other_ip, ipg.ip, vid)))
                 for ipa in (ipg.ip, ipd.ip):
                     setup_commands.append(self.fping(macvlan_int, ipa))
 
             self.quiet_commands(host, setup_commands)
         return required_ipds, ipd_to_macvlan
 
     def verify_drop_rules(self, required_ipds, ipd_to_macvlan):
@@ -5952,52 +5957,52 @@
                     ipd = list(match.values())[0].split('/')[0]
                     if ipd in required_ipds:
                         required_ipds.remove(ipd)
             for ipd in required_ipds:
                 macvlan_int, host = ipd_to_macvlan[ipd]
                 host.cmd(self.fping(macvlan_int, ipd))
             time.sleep(1)
-        self.assertFalse(required_ipds, msg=f'no drop rules for {required_ipds}')
+        self.assertFalse(required_ipds, msg='no drop rules for %s' % required_ipds)
 
     def verify_routing_performance(self, first_host, second_host):
         for first_host_ip, second_host_ip in (
                 (self.netbase(self.NEW_VIDS[0], 1), self.netbase(self.NEW_VIDS[0], 2)),
                 (self.netbase(self.NEW_VIDS[0], 1), self.netbase(self.NEW_VIDS[-1], 2)),
                 (self.netbase(self.NEW_VIDS[-1], 1), self.netbase(self.NEW_VIDS[0], 2))):
             self.verify_iperf_min(
                 ((first_host, self.port_map['port_1']),
                  (second_host, self.port_map['port_2'])),
                 MIN_MBPS, first_host_ip.ip, second_host_ip.ip,
                 sync_counters_func=lambda: self.scapy_bcast(first_host))
 
     def verify_l3_mesh(self, first_host, second_host):
         for vid in self.NEW_VIDS:
-            macvlan_int = f'macvlan{vid}'
+            macvlan_int = 'macvlan%u' % vid
             first_host_ip = self.netbase(vid, 1)
             second_host_ip = self.netbase(vid, 2)
             self.macvlan_ping(first_host, second_host_ip.ip, macvlan_int)
             self.macvlan_ping(second_host, first_host_ip.ip, macvlan_int)
 
     def verify_l3_hairpin(self, first_host):
-        macvlan1_int = f'macvlan{self.NEW_VIDS[0]}'
-        macvlan2_int = f'macvlan{self.NEW_VIDS[1]}'
+        macvlan1_int = 'macvlan%u' % self.NEW_VIDS[0]
+        macvlan2_int = 'macvlan%u' % self.NEW_VIDS[1]
         macvlan2_ip = self.netbase(self.NEW_VIDS[1], 1)
         macvlan1_gw = self.netbase(self.NEW_VIDS[0], 254)
         macvlan2_gw = self.netbase(self.NEW_VIDS[1], 254)
         netns = self.hostns(first_host)
         setup_cmds = []
         setup_cmds.extend(
-            [self.run_ip(f'link set {macvlan2_int} netns {netns}')])
+            [self.run_ip('link set %s netns %s' % (macvlan2_int, netns))])
         for exec_cmd in (
-                (self.run_ip(f'address add {macvlan2_ip.ip}/{self.NETPREFIX} dev {macvlan2_int}'),
-                 self.run_ip(f'link set {macvlan2_int} up'),
-                 self.run_ip(f'route add default via {macvlan2_gw.ip}'))):
-            setup_cmds.append(f'ip netns exec {netns} {exec_cmd}')
+                (self.run_ip('address add %s/%u dev %s' % (macvlan2_ip.ip, self.NETPREFIX, macvlan2_int)),
+                 self.run_ip('link set %s up' % macvlan2_int),
+                 self.run_ip('route add default via %s' % macvlan2_gw.ip))):
+            setup_cmds.append('ip netns exec %s %s' % (netns, exec_cmd))
         setup_cmds.append(
-            self.run_ip(f'route add {macvlan2_ip} via {macvlan1_gw.ip}'))
+            self.run_ip('route add %s via %s' % (macvlan2_ip, macvlan1_gw.ip)))
         self.quiet_commands(first_host, setup_cmds)
         self.macvlan_ping(first_host, macvlan2_ip.ip, macvlan1_int)
 
     def test_tagged(self):
         first_host, second_host, mirror_host = self.hosts_name_ordered()[:3]
         required_ipds, ipd_to_macvlan = self.configure_mesh(first_host, second_host)
         self.verify_drop_rules(required_ipds, ipd_to_macvlan)
@@ -6016,34 +6021,34 @@
 
     def _vids():  # pylint: disable=no-method-argument
         return list(range(100, 103))
 
     def global_vid():  # pylint: disable=no-method-argument
         return 2047
 
-    VIDS = _vids()  # pylint: disable=too-many-function-args
-    GLOBAL_VID = global_vid()  # pylint: disable=too-many-function-args
-    STR_VIDS = [str(i) for i in _vids()]  # pylint: disable=too-many-function-args
+    VIDS = _vids()
+    GLOBAL_VID = global_vid()
+    STR_VIDS = [str(i) for i in _vids()]
     NEW_VIDS = VIDS[1:]
 
     def netbase(self, vid, host):
-        return ipaddress.ip_interface(f'fc00::{vid}:{host}')
+        return ipaddress.ip_interface('fc00::%u:%u' % (vid, host))
 
     def fib_table(self):
         return self._IPV6_FIB_TABLE
 
     def fping(self, macvlan_int, ipg):
         return 'fping6 %s -c1 -t1 -I%s %s > /dev/null 2> /dev/null' % (
             self.FPING_ARGS_SHORT, macvlan_int, ipg)
 
     def macvlan_ping(self, host, ipa, macvlan_int):
         return self.one_ipv6_ping(host, ipa, intf=macvlan_int)
 
     def run_ip(self, args):
-        return f'ip -{self.IPV} {args}'
+        return 'ip -%u %s' % (self.IPV, args)
 
     CONFIG_GLOBAL = """
 routers:
     global:
         vlans: [%s]
 vlans:
 %s
@@ -6068,25 +6073,25 @@
                 native_vlan: 99
                 tagged_vlans: [%s]
                 hairpin_unicast: True
             %s:
                 native_vlan: 99
                 tagged_vlans: [%s]
                 hairpin_unicast: True
-""" % (global_vid(), '%(port_3)d', '%(port_1)d', '%(port_1)d',  # pylint: disable=too-many-function-args
+""" % (global_vid(), '%(port_3)d', '%(port_1)d', '%(port_1)d',
        ','.join(STR_VIDS), '%(port_2)d', ','.join(STR_VIDS))
 
 
 class FaucetTaggedScaleTest(FaucetTaggedTest):
 
     def _vids():  # pylint: disable=no-method-argument
         return list(range(100, 148))
 
-    VIDS = _vids()  # pylint: disable=too-many-function-args
-    STR_VIDS = [str(i) for i in _vids()]  # pylint: disable=too-many-function-args
+    VIDS = _vids()
+    STR_VIDS = [str(i) for i in _vids()]
     NEW_VIDS = VIDS[1:]
 
     CONFIG_GLOBAL = """
 vlans:
 """ + '\n'.join(['\n'.join(
         ('    %u:',
          '        description: "tagged"')) % i for i in VIDS])
@@ -6106,41 +6111,42 @@
        '%(port_4)d', ','.join(STR_VIDS))
 
     def test_tagged(self):
         self.ping_all_when_learned()
         for host in self.hosts_name_ordered():
             setup_commands = []
             for vid in self.NEW_VIDS:
-                vlan_int = f'{host.intf_root_name}.{vid}'
+                vlan_int = '%s.%u' % (host.intf_root_name, vid)
                 setup_commands.extend([
-                    f'ip link add link {host.intf_root_name} name {vlan_int} type vlan id {vid}',
-                    f'ip link set dev {vlan_int} up'])
+                    'ip link add link %s name %s type vlan id %u' % (
+                        host.intf_root_name, vlan_int, vid),
+                    'ip link set dev %s up' % vlan_int])
             self.quiet_commands(host, setup_commands)
         for host in self.hosts_name_ordered():
             rdisc6_commands = []
             for vid in self.NEW_VIDS:
-                vlan_int = f'{host.intf_root_name}.{vid}'
+                vlan_int = '%s.%u' % (host.intf_root_name, vid)
                 rdisc6_commands.append(
                     'rdisc6 -r2 -w1 -q %s 2> /dev/null' % vlan_int)
             self.quiet_commands(host, rdisc6_commands)
         for vlan in self.NEW_VIDS:
-            vlan_int = f'{host.intf_root_name}.{vid}'
+            vlan_int = '%s.%u' % (host.intf_root_name, vid)
             for _ in range(3):
                 for host in self.hosts_name_ordered():
                     self.quiet_commands(
                         host,
                         ['rdisc6 -r2 -w1 -q %s 2> /dev/null' % vlan_int])
                 vlan_hosts_learned = self.scrape_prometheus_var(
                     'vlan_hosts_learned', {'vlan': str(vlan)})
                 if vlan_hosts_learned == len(self.hosts_name_ordered()):
                     break
                 time.sleep(1)
             self.assertGreater(
                 vlan_hosts_learned, 1,
-                msg=f'not all VLAN {vlan} hosts learned ({vlan_hosts_learned})')
+                msg='not all VLAN %u hosts learned (%u)' % (vlan, vlan_hosts_learned))
 
 
 class FaucetTaggedBroadcastTest(FaucetTaggedTest):
 
     def test_tagged(self):
         super().test_tagged()
         self.verify_broadcast()
@@ -6198,15 +6204,15 @@
                 native_vlan: 200
                 tagged_vlans: [100]
 """
 
     def test_tagged(self):
         self.ping_all_when_learned()
         native_ips = [
-            ipaddress.ip_interface(f'10.99.99.{i + 1}/24') for i in range(len(self.hosts_name_ordered()))]
+            ipaddress.ip_interface('10.99.99.%u/24' % (i + 1)) for i in range(len(self.hosts_name_ordered()))]
         for native_ip, host in zip(native_ips, self.hosts_name_ordered()):
             self.host_ipv4_alias(host, native_ip, intf=host.intf_root_name)
         for own_native_ip, host in zip(native_ips, self.hosts_name_ordered()):
             for native_ip in native_ips:
                 if native_ip != own_native_ip:
                     self.one_ipv4_ping(host, native_ip.ip, intf=host.intf_root_name)
 
@@ -6247,19 +6253,19 @@
     def test_tagged(self):
         first_host, second_host, third_host = self.hosts_name_ordered()[:3]
 
         def test_acl(tcpdump_host, tcpdump_filter):
             tcpdump_txt = self.tcpdump_helper(
                 tcpdump_host, tcpdump_filter, [
                     lambda: first_host.cmd(
-                        f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                        'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                     lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
                 root_intf=True)
             self.assertTrue(re.search(
-                f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+                '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
             self.assertTrue(re.search(
                 tcpdump_filter, tcpdump_txt))
 
         # Saw swapped VID on second host
         test_acl(second_host, 'vlan 101')
         # Saw original VID on mirror host
         test_acl(third_host, 'vlan 100')
@@ -6301,19 +6307,19 @@
     def test_tagged(self):
         first_host, second_host, third_host = self.hosts_name_ordered()[:3]
 
         def test_acl(tcpdump_host, tcpdump_filter):
             tcpdump_txt = self.tcpdump_helper(
                 tcpdump_host, tcpdump_filter, [
                     lambda: first_host.cmd(
-                        f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                        'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                     lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
                 root_intf=True)
             self.assertTrue(re.search(
-                f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+                '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
             self.assertTrue(re.search(
                 tcpdump_filter, tcpdump_txt))
 
         # Saw swapped VID on second host
         test_acl(second_host, 'vlan 101')
         # Saw original VID on mirror host
         test_acl(third_host, 'vlan 100')
@@ -6355,19 +6361,19 @@
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the swapped VLAN VID
         tcpdump_filter = 'vlan 101'
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
             root_intf=True)
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
         self.assertTrue(re.search(
             'vlan 101', tcpdump_txt))
 
 
 class FaucetTaggedSwapVidOrderedOutputTest(FaucetTaggedTest):
 
     CONFIG_GLOBAL = """
@@ -6404,19 +6410,19 @@
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the swapped VLAN VID
         tcpdump_filter = 'vlan 101'
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
             root_intf=True)
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
         self.assertTrue(re.search(
             'vlan 101', tcpdump_txt))
 
 
 class FaucetTaggedPopVlansOutputTest(FaucetTaggedTest):
 
     CONFIG_GLOBAL = """
@@ -6452,20 +6458,20 @@
 
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         tcpdump_filter = 'icmp and ether dst 06:06:06:06:06:06 and not vlan'
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(
                     ' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
             packets=10, root_intf=True)
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
 
 
 class FaucetTaggedPopVlansOrderedOutputTest(FaucetTaggedTest):
 
     CONFIG_GLOBAL = """
 vlans:
     100:
@@ -6499,20 +6505,20 @@
 
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         tcpdump_filter = 'icmp and ether dst 06:06:06:06:06:06 and not vlan'
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} 01:02:03:04:05:06'),
+                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
                 lambda: first_host.cmd(
                     ' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
             packets=10, root_intf=True)
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
 
 
 class FaucetTaggedIPv4ControlPlaneTest(FaucetTaggedTest):
 
     CONFIG_GLOBAL = """
 vlans:
     100:
@@ -7029,18 +7035,18 @@
         first_host, second_host = self.hosts_name_ordered()[:2]
         first_host.setIP(str(first_host_ip.ip), prefixLen=24)
         second_host.setIP(str(second_host_ip.ip), prefixLen=24)
         self.add_host_route(first_host, second_host_ip, first_faucet_vip.ip)
         self.add_host_route(second_host, first_host_ip, second_faucet_vip.ip)
         self.one_ipv4_ping(first_host, second_host_ip.ip)
         self.one_ipv4_ping(second_host, first_host_ip.ip)
-        second_host.cmd(f'ifconfig {second_host.defaultIntf().name} down')
+        second_host.cmd('ifconfig %s down' % second_host.defaultIntf().name)
         expired_re = r'.+expiring dead route %s.+' % second_host_ip.ip
         self.wait_until_matching_lines_from_faucet_log_files(expired_re)
-        second_host.cmd(f'ifconfig {second_host.defaultIntf().name} up')
+        second_host.cmd('ifconfig %s up' % second_host.defaultIntf().name)
         self.add_host_route(second_host, first_host_ip, second_faucet_vip.ip)
         self.one_ipv4_ping(second_host, first_host_ip.ip)
         self.one_ipv4_ping(first_host, second_host_ip.ip)
 
 
 class FaucetUntaggedIPv6InterVLANRouteTest(FaucetUntaggedTest):
 
@@ -7706,20 +7712,20 @@
 
 
 class FaucetDestRewriteTest(FaucetUntaggedTest):
 
     def override_mac():  # pylint: disable=no-method-argument
         return '0e:00:00:00:00:02'
 
-    OVERRIDE_MAC = override_mac()  # pylint: disable=too-many-function-args
+    OVERRIDE_MAC = override_mac()
 
     def rewrite_mac():  # pylint: disable=no-method-argument
         return '0e:00:00:00:00:03'
 
-    REWRITE_MAC = rewrite_mac()  # pylint: disable=too-many-function-args
+    REWRITE_MAC = rewrite_mac()
 
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 
 acls:
@@ -7730,15 +7736,15 @@
                 allow: 1
                 output:
                     set_fields:
                         - eth_dst: "%s"
         - rule:
             actions:
                 allow: 1
-""" % (override_mac(), rewrite_mac())  # pylint: disable=too-many-function-args
+""" % (override_mac(), rewrite_mac())
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 acl_in: 1
             %(port_2)d:
                 native_vlan: 100
@@ -7747,46 +7753,47 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expect to see the rewritten mac address.
-        tcpdump_filter = (f'icmp and ether dst {self.REWRITE_MAC}')
+        tcpdump_filter = ('icmp and ether dst %s' % self.REWRITE_MAC)
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} {self.OVERRIDE_MAC}'),
+                    'arp -s %s %s' % (second_host.IP(), self.OVERRIDE_MAC)),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
             timeout=5, packets=1)
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
 
     def verify_dest_rewrite(self, source_host, overridden_host, rewrite_host, tcpdump_host):
         overridden_host.setMAC(self.OVERRIDE_MAC)
         rewrite_host.setMAC(self.REWRITE_MAC)
-        rewrite_host.cmd(f'arp -s {overridden_host.IP()} {overridden_host.MAC()}')
+        rewrite_host.cmd('arp -s %s %s' % (overridden_host.IP(), overridden_host.MAC()))
         rewrite_host.cmd(' '.join((self.FPINGS_ARGS_ONE, overridden_host.IP())))
         self.wait_until_matching_flow(
             {'dl_dst': self.REWRITE_MAC},
             table_id=self._ETH_DST_TABLE,
-            actions=[f'OUTPUT:{self.port_map["port_3"]}'])
-        tcpdump_filter = f'icmp and ether src {source_host.MAC()} and ether dst {rewrite_host.MAC()}'
+            actions=['OUTPUT:%u' % self.port_map['port_3']])
+        tcpdump_filter = ('icmp and ether src %s and ether dst %s' % (
+            source_host.MAC(), rewrite_host.MAC()))
         tcpdump_txt = self.tcpdump_helper(
             tcpdump_host, tcpdump_filter, [
                 lambda: source_host.cmd(
-                    f'arp -s {rewrite_host.IP()} {overridden_host.MAC()}'),
+                    'arp -s %s %s' % (rewrite_host.IP(), overridden_host.MAC())),
                 # this will fail if no reply
                 lambda: self.one_ipv4_ping(
                     source_host, rewrite_host.IP(), require_host_learned=False)],
             timeout=3, packets=1)
         # ping from h1 to h2.mac should appear in third host, and not second host, as
         # the acl should rewrite the dst mac.
         self.assertFalse(re.search(
-            f'{rewrite_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % rewrite_host.IP(), tcpdump_txt))
 
     def test_switching(self):
         """Tests that a acl can rewrite the destination mac address,
            and the packet will only go out the port of the new mac.
            (Continues through faucet pipeline)
         """
         source_host, overridden_host, rewrite_host = self.hosts_name_ordered()[0:3]
@@ -7795,20 +7802,20 @@
 
 
 class FaucetDestRewriteOrderedTest(FaucetUntaggedTest):
 
     def override_mac():  # pylint: disable=no-method-argument
         return '0e:00:00:00:00:02'
 
-    OVERRIDE_MAC = override_mac()  # pylint: disable=too-many-function-args
+    OVERRIDE_MAC = override_mac()
 
     def rewrite_mac():  # pylint: disable=no-method-argument
         return '0e:00:00:00:00:03'
 
-    REWRITE_MAC = rewrite_mac()  # pylint: disable=too-many-function-args
+    REWRITE_MAC = rewrite_mac()
 
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 
 acls:
@@ -7819,15 +7826,15 @@
                 allow: 1
                 output:
                     - set_fields:
                         - eth_dst: "%s"
         - rule:
             actions:
                 allow: 1
-""" % (override_mac(), rewrite_mac())  # pylint: disable=too-many-function-args
+""" % (override_mac(), rewrite_mac())
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 acl_in: 1
             %(port_2)d:
                 native_vlan: 100
@@ -7836,46 +7843,47 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expect to see the rewritten mac address.
-        tcpdump_filter = (f'icmp and ether dst {self.REWRITE_MAC}')
+        tcpdump_filter = ('icmp and ether dst %s' % self.REWRITE_MAC)
         tcpdump_txt = self.tcpdump_helper(
             second_host, tcpdump_filter, [
                 lambda: first_host.cmd(
-                    f'arp -s {second_host.IP()} {self.OVERRIDE_MAC}'),
+                    'arp -s %s %s' % (second_host.IP(), self.OVERRIDE_MAC)),
                 lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
             timeout=5, packets=1)
         self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
 
     def verify_dest_rewrite(self, source_host, overridden_host, rewrite_host, tcpdump_host):
         overridden_host.setMAC(self.OVERRIDE_MAC)
         rewrite_host.setMAC(self.REWRITE_MAC)
-        rewrite_host.cmd(f'arp -s {overridden_host.IP()} {overridden_host.MAC()}')
+        rewrite_host.cmd('arp -s %s %s' % (overridden_host.IP(), overridden_host.MAC()))
         rewrite_host.cmd(' '.join((self.FPINGS_ARGS_ONE, overridden_host.IP())))
         self.wait_until_matching_flow(
             {'dl_dst': self.REWRITE_MAC},
             table_id=self._ETH_DST_TABLE,
-            actions=[f'OUTPUT:{self.port_map["port_3"]}'])
-        tcpdump_filter = f'icmp and ether src {source_host.MAC()} and ether dst {rewrite_host.MAC()}'
+            actions=['OUTPUT:%u' % self.port_map['port_3']])
+        tcpdump_filter = ('icmp and ether src %s and ether dst %s' % (
+            source_host.MAC(), rewrite_host.MAC()))
         tcpdump_txt = self.tcpdump_helper(
             tcpdump_host, tcpdump_filter, [
                 lambda: source_host.cmd(
-                    f'arp -s {rewrite_host.IP()} {overridden_host.MAC()}'),
+                    'arp -s %s %s' % (rewrite_host.IP(), overridden_host.MAC())),
                 # this will fail if no reply
                 lambda: self.one_ipv4_ping(
                     source_host, rewrite_host.IP(), require_host_learned=False)],
             timeout=3, packets=1)
         # ping from h1 to h2.mac should appear in third host, and not second host, as
         # the acl should rewrite the dst mac.
         self.assertFalse(re.search(
-            f'{rewrite_host.IP()}: ICMP echo request', tcpdump_txt))
+            '%s: ICMP echo request' % rewrite_host.IP(), tcpdump_txt))
 
     def test_switching(self):
         """Tests that a acl can rewrite the destination mac address,
            and the packet will only go out the port of the new mac.
            (Continues through faucet pipeline)
         """
         source_host, overridden_host, rewrite_host = self.hosts_name_ordered()[0:3]
@@ -7945,40 +7953,40 @@
 
         # scapy command to create and send a UDP packet
         scapy_pkt = self.scapy_base_udp(
             self.SRC_MAC, source_host.defaultIntf(), source_host.IP(),
             dest_host.IP(), self.UDP_DST_PORT, self.UDP_SRC_PORT,
             dst=self.OUTPUT_MAC)
 
-        tcpdump_filter = f"ether dst {self.OUTPUT_MAC}"
+        tcpdump_filter = "ether dst %s" % self.OUTPUT_MAC
         tcpdump_txt = self.tcpdump_helper(
             dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
             root_intf=True, packets=1)
 
         # verify that the packet we've received on the dest_host has the
         # overwritten values
         self.assertTrue(
             re.search("%s.%s > %s.%s" % (self.IPV4_SRC_VAL, self.UDP_SRC_PORT,
                                          self.IPV4_DST_VAL, self.UDP_DST_PORT),
                       tcpdump_txt))
         # check the packet's converted dscp value
-        self.assertTrue(re.search(f"tos {hex(self.NW_TOS_VAL)}", tcpdump_txt))
+        self.assertTrue(re.search("tos %s" % hex(self.NW_TOS_VAL), tcpdump_txt))
 
     def test_set_fields_icmp(self):
         # Send a basic ICMP packet through the faucet pipeline and verify that
         # the expected fields were updated via tcpdump output
         source_host, dest_host = self.hosts_name_ordered()[0:2]
         dest_host.setMAC(self.OUTPUT_MAC)
 
         # scapy command to create and send an ICMP packet
         scapy_pkt = self.scapy_icmp(
             self.SRC_MAC, source_host.defaultIntf(), source_host.IP(),
             dest_host.IP(), dst=self.OUTPUT_MAC)
 
-        tcpdump_filter = f"ether dst {self.OUTPUT_MAC}"
+        tcpdump_filter = "ether dst %s" % self.OUTPUT_MAC
         tcpdump_txt = self.tcpdump_helper(
             dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
             root_intf=True, packets=1)
 
         # verify that the packet we've received on the dest_host has been
         # overwritten to be an ICMP echo request
         self.assertTrue(re.search("ICMP echo request", tcpdump_txt))
@@ -8049,40 +8057,40 @@
 
         # scapy command to create and send a UDP packet
         scapy_pkt = self.scapy_base_udp(
             self.SRC_MAC, source_host.defaultIntf(), source_host.IP(),
             dest_host.IP(), self.UDP_DST_PORT, self.UDP_SRC_PORT,
             dst=self.OUTPUT_MAC)
 
-        tcpdump_filter = f"ether dst {self.OUTPUT_MAC}"
+        tcpdump_filter = "ether dst %s" % self.OUTPUT_MAC
         tcpdump_txt = self.tcpdump_helper(
             dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
             root_intf=True, packets=1)
 
         # verify that the packet we've received on the dest_host has the
         # overwritten values
         self.assertTrue(
             re.search("%s.%s > %s.%s" % (self.IPV4_SRC_VAL, self.UDP_SRC_PORT,
                                          self.IPV4_DST_VAL, self.UDP_DST_PORT),
                       tcpdump_txt))
         # check the packet's converted dscp value
-        self.assertTrue(re.search(f"tos {hex(self.NW_TOS_VAL)}", tcpdump_txt))
+        self.assertTrue(re.search("tos %s" % hex(self.NW_TOS_VAL), tcpdump_txt))
 
     def test_set_fields_icmp(self):
         # Send a basic ICMP packet through the faucet pipeline and verify that
         # the expected fields were updated via tcpdump output
         source_host, dest_host = self.hosts_name_ordered()[0:2]
         dest_host.setMAC(self.OUTPUT_MAC)
 
         # scapy command to create and send an ICMP packet
         scapy_pkt = self.scapy_icmp(
             self.SRC_MAC, source_host.defaultIntf(), source_host.IP(),
             dest_host.IP(), dst=self.OUTPUT_MAC)
 
-        tcpdump_filter = f"ether dst {self.OUTPUT_MAC}"
+        tcpdump_filter = "ether dst %s" % self.OUTPUT_MAC
         tcpdump_txt = self.tcpdump_helper(
             dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
             root_intf=True, packets=1)
 
         # verify that the packet we've received on the dest_host has been
         # overwritten to be an ICMP echo request
         self.assertTrue(re.search("ICMP echo request", tcpdump_txt))
@@ -8387,15 +8395,15 @@
              'eth_type': self.ETH_TYPE},
             table_id=self._PORT_ACL_TABLE)
 
         # scapy command to create and send a packet with the specified fields
         scapy_pkt = self.scapy_dscp(self.SRC_MAC, self.DST_MAC, 184,
                                     source_host.defaultIntf())
 
-        tcpdump_filter = f"ether dst {self.REWRITE_MAC}"
+        tcpdump_filter = "ether dst %s" % self.REWRITE_MAC
         tcpdump_txt = self.tcpdump_helper(
             dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
             root_intf=True, packets=1)
         # verify that the packet we've received on the dest_host is from the
         # source MAC address
         self.assertTrue(re.search("%s > %s" % (self.SRC_MAC, self.REWRITE_MAC),
                                   tcpdump_txt))
@@ -8457,15 +8465,15 @@
              'eth_type': self.ETH_TYPE},
             table_id=self._PORT_ACL_TABLE)
 
         # scapy command to create and send a packet with the specified fields
         scapy_pkt = self.scapy_dscp(self.SRC_MAC, self.DST_MAC, 184,
                                     source_host.defaultIntf())
 
-        tcpdump_filter = f"ether dst {self.REWRITE_MAC}"
+        tcpdump_filter = "ether dst %s" % self.REWRITE_MAC
         tcpdump_txt = self.tcpdump_helper(
             dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
             root_intf=True, packets=1)
         # verify that the packet we've received on the dest_host is from the
         # source MAC address
         self.assertTrue(re.search("%s > %s" % (self.SRC_MAC, self.REWRITE_MAC),
                                   tcpdump_txt))
@@ -8483,33 +8491,33 @@
         use_idle_timeout: True
 """ + CONFIG_BOILER_UNTAGGED
 
     def wait_for_host_removed(self, host, in_port, timeout=5):
         for _ in range(timeout):
             if not self.host_learned(host, in_port=in_port, timeout=1):
                 return
-        self.fail(f'host {host} still learned')
+        self.fail('host %s still learned' % host)
 
     def wait_for_flowremoved_msg(self, src_mac=None, dst_mac=None, timeout=30):
         pattern = "OFPFlowRemoved"
         mac = None
         if src_mac:
-            pattern = f"OFPFlowRemoved(.*)'eth_src': '{src_mac}'"
+            pattern = "OFPFlowRemoved(.*)'eth_src': '%s'" % src_mac
             mac = src_mac
         if dst_mac:
-            pattern = f"OFPFlowRemoved(.*)'eth_dst': '{dst_mac}'"
+            pattern = "OFPFlowRemoved(.*)'eth_dst': '%s'" % dst_mac
             mac = dst_mac
         for _ in range(timeout):
             for _, debug_log_name in self._get_ofchannel_logs():
                 with open(debug_log_name, encoding='utf-8') as debug_log:
                     debug = debug_log.read()
                 if re.search(pattern, debug):
                     return
             time.sleep(1)
-        self.fail(f'Not received OFPFlowRemoved for host {mac}')
+        self.fail('Not received OFPFlowRemoved for host %s' % mac)
 
     def wait_for_host_log_msg(self, host_mac, msg):
         host_log_re = r'.*%s %s.*' % (msg, host_mac)
         self.wait_until_matching_lines_from_faucet_log_files(host_log_re)
 
     def test_untagged(self):
         self.ping_all_when_learned()
@@ -8528,15 +8536,15 @@
     def test_untagged(self):
         """Host that is actively sending should have its dst rule renewed as the
         rule expires. Host that is not sending expires as usual.
         """
         self.ping_all_when_learned()
         first_host, second_host, third_host, fourth_host = self.hosts_name_ordered()
         self.host_ipv4_alias(first_host, ipaddress.ip_interface('10.99.99.1/24'))
-        first_host.cmd(f'arp -s {second_host.IP()} {second_host.MAC()}')
+        first_host.cmd('arp -s %s %s' % (second_host.IP(), second_host.MAC()))
         first_host.cmd('timeout 120s ping -I 10.99.99.1 %s &' % second_host.IP())
         for host in (second_host, third_host, fourth_host):
             self.host_drop_all_ips(host)
         self.wait_for_host_log_msg(first_host.MAC(), 'refreshing host')
         self.assertTrue(self.host_learned(
             first_host, in_port=int(self.port_map['port_1'])))
         for host, port in (
@@ -8651,30 +8659,30 @@
     def bad_flow_mod(self):
         """Return a flow mod with some bad parameters"""
         flow_mod = self.base_flow_mod()
         # Add two or more bad options
         options = random.sample(self.bad_options,
                                 random.randint(2, len(self.bad_options)))
         for option in options:
-            param = getattr(self, f'bad_{option}')()
+            param = getattr(self, 'bad_%s' % option)()
             flow_mod.update(param)
         return flow_mod
 
     def send_flow_mod(self, flow_mod, timeout=5):
         """Send flow_mod to switch via ofctl"""
         int_dpid = mininet_test_util.str_int_dpid(self.dpid)
         return self._ofctl_post(int_dpid, 'stats/flowentry/modify',
                                 timeout=timeout, params=flow_mod)
 
     def tearDown(self, ignore_oferrors=True):
         """Ignore OF errors on teardown"""
         oferrors = super().tearDown(ignore_oferrors)
         oferrors = re.findall(r'type: (\w+)', oferrors)
         counter = collections.Counter(oferrors)
-        error(f'Ignored OF error count: {dict(counter)}\n')
+        error('Ignored OF error count: %s\n' % dict(counter))
         # TODO: ensure at least one error is always generated.
 
     # pylint: disable=arguments-differ
     def test_untagged(self, count=10):
         """Send a bunch of bad flow mods, then verify connectivity"""
         for _ in range(count):
             flow_mod = self.bad_flow_mod()
@@ -8706,15 +8714,15 @@
         super()._init_faucet_config()
 
     def setUp(self):
         """Make sure N_UNTAGGED doesn't exceed hw port count"""
         if self.config and self.config.get('hw_switch', False):
             self.N_UNTAGGED = min(len(self.config['dp_ports']),
                                   self.N_UNTAGGED)
-        error(f'({self.N_UNTAGGED} ports) ')
+        error('(%d ports) ' % self.N_UNTAGGED)
         super().setUp()
 
 
 class FaucetSingleUntagged32PortTest(FaucetUntaggedMorePortsBase):
     """Untagged test with up to 32 ports"""
 
     # pylint: disable=invalid-name
```

### Comparing `c65faucet-1.0.46/tests/run_unit_tests.sh` & `c65faucet-1.0.47/tests/run_unit_tests.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/unit/clib/test_topo.py` & `c65faucet-1.0.47/tests/unit/clib/test_topo.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_check_config.py` & `c65faucet-1.0.47/tests/unit/faucet/test_check_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -39,27 +39,26 @@
     def tearDown(self):
         shutil.rmtree(self.tmpdir)
 
     def run_check_config(self, config, expected_ok):
         """Try to parse config and return True if result fails or succeeds as expected."""
         conf_files = []
         if config is not None:
-            conf_file_name = os.path.join(self.tmpdir, 'faucet.yaml')
-            with open(conf_file_name, 'w', encoding='utf-8') as conf_file:
+            conf_file_name = os.path.join(self.tmpdir, "faucet.yaml")
+            with open(conf_file_name, "w", encoding="utf-8") as conf_file:
                 conf_file.write(config)
             conf_files = [conf_file_name]
-        with open(os.devnull, 'w', encoding='utf-8') as check_output_file:
-            result_ok = check_config(
-                conf_files, logging.FATAL, check_output_file)
+        with open(os.devnull, "w", encoding="utf-8") as check_output_file:
+            result_ok = check_config(conf_files, logging.FATAL, check_output_file)
         return expected_ok == result_ok
 
     def _deprecated_acl_check(self, config, success):
         # TODO: Check acls_in work now acl_in is deprecated, remove in future
-        if config and 'acl_in' in config and 'acls_in' not in config:
-            acls_cfg = re.sub('(acl_in: )(.*)', 'acls_in: [\\2]', config)
+        if config and "acl_in" in config and "acls_in" not in config:
+            acls_cfg = re.sub("(acl_in: )(.*)", "acls_in: [\\2]", config)
             self.assertTrue(self.run_check_config(acls_cfg, success))
 
     def check_config_success(self, config):
         """Try to parse config and expect success."""
         self.assertTrue(self.run_check_config(config, True))
         self._deprecated_acl_check(config, True)
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_config.py` & `c65faucet-1.0.47/tests/unit/faucet/test_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 import shutil
 import tempfile
 import os
 import unittest
 
 from faucet import config_parser as cp
 
-LOGNAME = '/dev/null'
+LOGNAME = "/dev/null"
 
 
 class TestConfig(unittest.TestCase):  # pytype: disable=module-attr
     """Test config parsing raises correct exception."""
 
     tmpdir = None
 
@@ -28,50 +28,52 @@
 
     def tearDown(self):
         logging.disable(logging.NOTSET)
         shutil.rmtree(self.tmpdir)
 
     def conf_file_name(self):
         """Return path to test config file in test directory."""
-        return os.path.join(self.tmpdir, 'faucet.yaml')
+        return os.path.join(self.tmpdir, "faucet.yaml")
 
     def create_config_file(self, config):
         """Returns file path to file containing the config parameter."""
         conf_file_name = self.conf_file_name()
-        with open(conf_file_name, 'wb', encoding=None) as conf_file:
+        with open(conf_file_name, "wb", encoding=None) as conf_file:
             if isinstance(config, bytes):
                 conf_file.write(config)
             else:
-                conf_file.write(config.encode('utf-8'))
+                conf_file.write(config.encode("utf-8"))
         return conf_file_name
 
     def run_function_with_config(self, config, function, before_function=None):
         """Return False with error if provided function raises InvalidConfigError."""
         # TODO: Check acls_in work now acl_in is deprecated
-        if isinstance(config, str) and 'acl_in' in config and 'acls_in' not in config:
-            config = re.sub('(acl_in: )(.*)', 'acls_in: [\\2]', config)
+        if isinstance(config, str) and "acl_in" in config and "acls_in" not in config:
+            config = re.sub("(acl_in: )(.*)", "acls_in: [\\2]", config)
         conf_file = self.create_config_file(config)
         if before_function:
             before_function()
         try:
             function(conf_file, LOGNAME)
         except cp.InvalidConfigError as err:
             return (False, err)
         return (True, None)
 
     def check_config_failure(self, config, function, before_function=None):
         """Ensure config parsing reported as failed."""
         config_success, config_err = self.run_function_with_config(
-            config, function, before_function)
+            config, function, before_function
+        )
         self.assertEqual(config_success, False, config_err)
 
     def check_config_success(self, config, function, before_function=None):
         """Ensure config parsing reported succeeded."""
         config_success, config_err = self.run_function_with_config(
-            config, function, before_function)
+            config, function, before_function
+        )
         self.assertEqual(config_success, True, config_err)
 
     def _get_dps_as_dict(self, config):
         _, _, dps, _ = cp.dp_parser(self.create_config_file(config), LOGNAME)
         return {dp.dp_id: dp for dp in dps}
 
     def test_one_port_dp(self):
@@ -86,43 +88,31 @@
         interfaces:
             testing:
                 number: 1
                 native_vlan: office
 """
         self.check_config_success(config, cp.dp_parser)
         dps = self._get_dps_as_dict(config)
-        self.assertTrue(1 in dps, 'datapath configured with incorrect dp_id')
+        self.assertTrue(1 in dps, "datapath configured with incorrect dp_id")
         dp = dps[1]
+        self.assertEqual(dp.name, "sw1", "datapath configured with incorrect name")
+        self.assertTrue(1 in dp.ports, "interface not configured in datapath")
         self.assertEqual(
-            dp.name, 'sw1', 'datapath configured with incorrect name')
-        self.assertTrue(
-            1 in dp.ports, 'interface not configured in datapath')
-        self.assertEqual(
-            len(dp.ports),
-            1,
-            'unexpected interface configured in datapath'
-        )
-        self.assertTrue(100 in dp.vlans, 'vlan not configured in datapath')
-        self.assertEqual(
-            len(dp.vlans),
-            1,
-            'unexpected vlan configured in datapath'
+            len(dp.ports), 1, "unexpected interface configured in datapath"
         )
+        self.assertTrue(100 in dp.vlans, "vlan not configured in datapath")
+        self.assertEqual(len(dp.vlans), 1, "unexpected vlan configured in datapath")
         port = dp.ports[1]
-        self.assertEqual(port.number, 1, 'port number configured incorrectly')
-        self.assertEqual(
-            port.name, 'testing', 'port name configured incorrectly')
+        self.assertEqual(port.number, 1, "port number configured incorrectly")
+        self.assertEqual(port.name, "testing", "port name configured incorrectly")
         vlan = dp.vlans[100]
-        self.assertEqual(vlan.vid, 100, 'vlan vid configured incorrectly')
-        self.assertEqual(
-            vlan.name, 'office', 'vlan name configured incorrectly')
+        self.assertEqual(vlan.vid, 100, "vlan vid configured incorrectly")
+        self.assertEqual(vlan.name, "office", "vlan name configured incorrectly")
         self.assertEqual(
-            port.native_vlan,
-            vlan,
-            'native vlan configured incorrectly in port'
+            port.native_vlan, vlan, "native vlan configured incorrectly in port"
         )
 
     def test_config_stack(self):
         """Test valid stacking config."""
         config = """
 vlans:
     office:
@@ -173,54 +163,60 @@
             3:
                 native_vlan: office
                 loop_protect_external: False
 """
         self.check_config_success(config, cp.dp_parser)
         dps = self._get_dps_as_dict(config)
         for dp in dps.values():
-            self.assertTrue(
-                dp.stack is not None, 'stack not configured for DP')
+            self.assertTrue(dp.stack is not None, "stack not configured for DP")
             self.assertEqual(
-                dp.stack.root_name, 't1-1', 'root_dp configured incorrectly')
+                dp.stack.root_name, "t1-1", "root_dp configured incorrectly"
+            )
             self.assertEqual(
-                dp.stack.roots_names, ('t1-1', 't1-2'), 'root_dps configured incorrectly')
+                dp.stack.roots_names,
+                ("t1-1", "t1-2"),
+                "root_dps configured incorrectly",
+            )
             self.assertEqual(
-                len(dp.stack.graph.nodes),
-                3,
-                'stack graph has incorrect nodes'
+                len(dp.stack.graph.nodes), 3, "stack graph has incorrect nodes"
             )
             self.assertTrue(
-                dp.has_externals,
-                'All DPs must have external flag set if one DP has it')
+                dp.has_externals, "All DPs must have external flag set if one DP has it"
+            )
 
         t2_dpid = 0x3
         for root_dpid in (1, 2):
             root_stack_port = dps[root_dpid].stack_ports()[0]
             t2_stack_port = dps[t2_dpid].stack_ports()[root_dpid - 1]
             stack_link_a = (root_dpid, root_stack_port)
             stack_link_b = (t2_dpid, t2_stack_port)
             for dpid_a, port_a, dpid_b, port_b in (
-                    (stack_link_a + stack_link_b),
-                    (stack_link_b + stack_link_a)):
+                (stack_link_a + stack_link_b),
+                (stack_link_b + stack_link_a),
+            ):
                 self.assertEqual(
-                    port_a.stack['dp'].dp_id,  # pytype: disable=attribute-error
+                    port_a.stack["dp"].dp_id,  # pytype: disable=attribute-error
                     dpid_b,
-                    'remote stack dp configured incorrectly')
+                    "remote stack dp configured incorrectly",
+                )
                 self.assertEqual(
-                    port_b.stack['dp'].dp_id,  # pytype: disable=attribute-error
+                    port_b.stack["dp"].dp_id,  # pytype: disable=attribute-error
                     dpid_a,
-                    'remote stack dp configured incorrectly')
+                    "remote stack dp configured incorrectly",
+                )
                 self.assertEqual(
-                    port_a.stack['port'].number,  # pytype: disable=attribute-error
+                    port_a.stack["port"].number,  # pytype: disable=attribute-error
                     port_b.number,  # pytype: disable=attribute-error
-                    'remote stack dp configured incorrectly')
+                    "remote stack dp configured incorrectly",
+                )
                 self.assertEqual(
-                    port_b.stack['port'].number,  # pytype: disable=attribute-error
+                    port_b.stack["port"].number,  # pytype: disable=attribute-error
                     port_a.number,  # pytype: disable=attribute-error
-                    'remote stack dp configured incorrectly')
+                    "remote stack dp configured incorrectly",
+                )
 
     def test_config_route_learning_override(self):
         """Test DP stack class, route_learning configuration overwrite when routing"""
         config = """
 vlans:
     vlan100:
         vid: 100
@@ -512,16 +508,16 @@
             1:
                 native_vlan: office
 """
         self.check_config_success(config, cp.dp_parser)
         dp = self._get_dps_as_dict(config)[0x1]
         self.assertEqual(
             dp.vlans[100].faucet_mac,
-            '11:22:33:44:55:66',
-            'faucet mac configured incorrectly'
+            "11:22:33:44:55:66",
+            "faucet mac configured incorrectly",
         )
 
     def test_novlans(self):
         """Test DP with no VLANs."""
         config = """
 dps:
     sw1:
@@ -546,21 +542,18 @@
                 number: 1
                 native_vlan: office
             2:
                 mirror: mirrored_port
 """
         self.check_config_success(config, cp.dp_parser)
         sw1 = self._get_dps_as_dict(config)[0x1]
-        self.assertTrue(
-            sw1.ports[2].output_only,
-            'mirror port not set to output only'
-        )
+        self.assertTrue(sw1.ports[2].output_only, "mirror port not set to output only")
         self.assertTrue(
             sw1.ports[1].mirror_actions() is not None,
-            'mirror port has no mirror actions'
+            "mirror port has no mirror actions",
         )
 
     def test_acl_dictionary_valid(self):
         """test acl config is valid when not using 'rule' key"""
         config = """
 acls:
     office-vlan-protect:
@@ -1250,27 +1243,24 @@
                 native_vlan: 'v100'
             3:
                 name: 'target'
                 native_vlan: 'v100'
 """
         conf_file = self.create_config_file(config)
         _, _, dps, _ = cp.dp_parser(conf_file, LOGNAME)
-        outputs = {
-            's1': 2,
-            's2': 3
-        }
+        outputs = {"s1": 2, "s2": 3}
         for dp in dps:
             v100 = dp.vlans[100]
             for acl in v100.acls_in:
                 for rule in acl.rules:
-                    port = rule['actions']['output']['port']
+                    port = rule["actions"]["output"]["port"]
                     self.assertEqual(
                         outputs[dp.name],
                         port,
-                        msg='acl output port resolved incorrectly'
+                        msg="acl output port resolved incorrectly",
                     )
 
     def test_acl_multi_dp_output_rule_ordered(self):
         """Verify that an acl can output to different ports with the same name
         on different DPs'
         """
         config = """
@@ -1303,27 +1293,24 @@
                 native_vlan: 'v100'
             3:
                 name: 'target'
                 native_vlan: 'v100'
 """
         conf_file = self.create_config_file(config)
         _, _, dps, _ = cp.dp_parser(conf_file, LOGNAME)
-        outputs = {
-            's1': 2,
-            's2': 3
-        }
+        outputs = {"s1": 2, "s2": 3}
         for dp in dps:
             v100 = dp.vlans[100]
             for acl in v100.acls_in:
                 for rule in acl.rules:
-                    port = rule['actions']['output'][0]['port']
+                    port = rule["actions"]["output"][0]["port"]
                     self.assertEqual(
                         outputs[dp.name],
                         port,
-                        msg='acl output port resolved incorrectly'
+                        msg="acl output port resolved incorrectly",
                     )
 
     def test_port_range_valid_config(self):
         """Test if port range config applied correctly"""
         config = """
 vlans:
     office:
@@ -1346,16 +1333,20 @@
                 max_hosts: 4
                 description: "video conf"
 """
         conf_file = self.create_config_file(config)
         _, _, dps, _ = cp.dp_parser(conf_file, LOGNAME)
         dp = dps[0]
         self.assertEqual(len(dp.ports), 8)
-        self.assertTrue(all(p.permanent_learn for p in dp.ports.values() if p.number < 9))
-        self.assertTrue(all(p.max_hosts == 2 for p in dp.ports.values() if p.number > 1))
+        self.assertTrue(
+            all(p.permanent_learn for p in dp.ports.values() if p.number < 9)
+        )
+        self.assertTrue(
+            all(p.max_hosts == 2 for p in dp.ports.values() if p.number > 1)
+        )
         self.assertTrue(dp.ports[1].max_hosts == 4)
         self.assertEqual(dp.ports[1].description, "video conf")
 
     def test_range_description(self):
         """Test that ranges can have a description."""
         config = """
 vlans:
@@ -1404,34 +1395,28 @@
         conf_file = self.create_config_file(config)
         _, _, dps, _ = cp.dp_parser(conf_file, LOGNAME)
         dp = dps[0]
         self.assertEqual(len(dp.ports), 1)
 
     def _check_table_names_numbers(self, dp, tables):
         for table_name, table in dp.tables.items():
-            self.assertTrue(
-                table_name in tables,
-                'Incorrect table configured in dp'
-            )
+            self.assertTrue(table_name in tables, "Incorrect table configured in dp")
             self.assertEqual(
                 tables[table_name],
                 table.table_id,
-                'Table configured with wrong table_id'
+                "Table configured with wrong table_id",
             )
         for table_name in tables:
-            self.assertTrue(
-                table_name in dp.tables,
-                'Table not configured in dp'
-            )
+            self.assertTrue(table_name in dp.tables, "Table not configured in dp")
 
     def _check_next_tables(self, table, next_tables):
         for next_table in table.next_tables:
-            self.assertIn(next_table, next_tables, 'incorrect next table configured')
+            self.assertIn(next_table, next_tables, "incorrect next table configured")
         for next_table in next_tables:
-            self.assertIn(next_table, table.next_tables, 'missing next table')
+            self.assertIn(next_table, table.next_tables, "missing next table")
 
     def test_pipeline_config_no_acl(self):
         """Test pipelines are generated correctly with different configs"""
         config = """
 vlans:
     office:
         vid: 100
@@ -1440,25 +1425,20 @@
         dp_id: 0x1
         interfaces:
             1:
                 native_vlan: office
 """
         self.check_config_success(config, cp.dp_parser)
         dp = self._get_dps_as_dict(config)[0x1]
-        tables = {
-            'vlan': 0,
-            'eth_src': 1,
-            'eth_dst': 2,
-            'flood': 3
-        }
+        tables = {"vlan": 0, "eth_src": 1, "eth_dst": 2, "flood": 3}
         self._check_table_names_numbers(dp, tables)
-        self._check_next_tables(dp.tables['vlan'], [1])
-        self._check_next_tables(dp.tables['eth_src'], [2, 3])
-        self._check_next_tables(dp.tables['eth_dst'], [])
-        self._check_next_tables(dp.tables['flood'], [])
+        self._check_next_tables(dp.tables["vlan"], [1])
+        self._check_next_tables(dp.tables["eth_src"], [2, 3])
+        self._check_next_tables(dp.tables["eth_dst"], [])
+        self._check_next_tables(dp.tables["flood"], [])
 
     def test_pipeline_config_no_acl_static_ids(self):
         """Test pipelines are generated correctly with different configs"""
         config = """
 vlans:
     office:
         vid: 100
@@ -1468,21 +1448,15 @@
         hardware: NoviFlow
         interfaces:
             1:
                 native_vlan: office
 """
         self.check_config_success(config, cp.dp_parser)
         dp = self._get_dps_as_dict(config)[0x1]
-        tables = {
-            'port_acl': 0,
-            'vlan': 1,
-            'eth_src': 4,
-            'eth_dst': 9,
-            'flood': 12
-        }
+        tables = {"port_acl": 0, "vlan": 1, "eth_src": 4, "eth_dst": 9, "flood": 12}
         self._check_table_names_numbers(dp, tables)
 
     def test_pipeline_config_ipv4_no_acl(self):
         """Test pipelines are generated correctly with different configs"""
         config = """
 vlans:
     office:
@@ -1494,20 +1468,20 @@
         interfaces:
             1:
                 native_vlan: office
 """
         self.check_config_success(config, cp.dp_parser)
         dp = self._get_dps_as_dict(config)[0x1]
         tables = {
-            'vlan': 0,
-            'eth_src': 1,
-            'ipv4_fib': 2,
-            'vip': 3,
-            'eth_dst': 4,
-            'flood': 5
+            "vlan": 0,
+            "eth_src": 1,
+            "ipv4_fib": 2,
+            "vip": 3,
+            "eth_dst": 4,
+            "flood": 5,
         }
         self._check_table_names_numbers(dp, tables)
 
     def test_pipeline_config_ipv6_4_no_acl(self):
         """Test pipelines are generated correctly with different configs"""
         config = """
 vlans:
@@ -1520,21 +1494,21 @@
         interfaces:
             1:
                 native_vlan: office
 """
         self.check_config_success(config, cp.dp_parser)
         dp = self._get_dps_as_dict(config)[0x1]
         tables = {
-            'vlan': 0,
-            'eth_src': 1,
-            'ipv4_fib': 2,
-            'ipv6_fib': 3,
-            'vip': 4,
-            'eth_dst': 5,
-            'flood': 6
+            "vlan": 0,
+            "eth_src": 1,
+            "ipv4_fib": 2,
+            "ipv6_fib": 3,
+            "vip": 4,
+            "eth_dst": 5,
+            "flood": 6,
         }
         self._check_table_names_numbers(dp, tables)
 
     def test_pipeline_config_ipv6_4_vlan_acl(self):
         """Test pipelines are generated correctly with different configs"""
         config = """
 vlans:
@@ -1554,22 +1528,22 @@
         interfaces:
             1:
                 native_vlan: office
 """
         self.check_config_success(config, cp.dp_parser)
         dp = self._get_dps_as_dict(config)[0x1]
         tables = {
-            'vlan': 0,
-            'vlan_acl': 1,
-            'eth_src': 2,
-            'ipv4_fib': 3,
-            'ipv6_fib': 4,
-            'vip': 5,
-            'eth_dst': 6,
-            'flood': 7
+            "vlan": 0,
+            "vlan_acl": 1,
+            "eth_src": 2,
+            "ipv4_fib": 3,
+            "ipv6_fib": 4,
+            "vip": 5,
+            "eth_dst": 6,
+            "flood": 7,
         }
         self._check_table_names_numbers(dp, tables)
 
     def test_pipeline_full(self):
         """Test pipelines are generated correctly with different configs"""
         config = """
 vlans:
@@ -1592,38 +1566,38 @@
             1:
                 native_vlan: office
                 acls_in: [test]
 """
         self.check_config_success(config, cp.dp_parser)
         dp = self._get_dps_as_dict(config)[0x1]
         tables = {
-            'port_acl': 0,
-            'vlan': 1,
-            'vlan_acl': 2,
-            'classification': 3,
-            'eth_src': 4,
-            'ipv4_fib': 5,
-            'ipv6_fib': 6,
-            'vip': 7,
-            'eth_dst': 8,
-            'egress': 9,
-            'flood': 10,
+            "port_acl": 0,
+            "vlan": 1,
+            "vlan_acl": 2,
+            "classification": 3,
+            "eth_src": 4,
+            "ipv4_fib": 5,
+            "ipv6_fib": 6,
+            "vip": 7,
+            "eth_dst": 8,
+            "egress": 9,
+            "flood": 10,
         }
         self._check_table_names_numbers(dp, tables)
-        self._check_next_tables(dp.tables['port_acl'], [1, 7, 8, 10])
-        self._check_next_tables(dp.tables['vlan'], [2, 3, 4])
-        self._check_next_tables(dp.tables['vlan_acl'], [3, 4, 8, 10])
-        self._check_next_tables(dp.tables['classification'], [4, 5, 6, 7, 8, 10])
-        self._check_next_tables(dp.tables['eth_src'], [5, 6, 7, 8, 10])
-        self._check_next_tables(dp.tables['ipv4_fib'], [7, 8, 10])
-        self._check_next_tables(dp.tables['ipv6_fib'], [7, 8, 10])
-        self._check_next_tables(dp.tables['vip'], [8, 10])
-        self._check_next_tables(dp.tables['eth_dst'], [9])
-        self._check_next_tables(dp.tables['egress'], [10])
-        self._check_next_tables(dp.tables['flood'], [])
+        self._check_next_tables(dp.tables["port_acl"], [1, 7, 8, 10])
+        self._check_next_tables(dp.tables["vlan"], [2, 3, 4])
+        self._check_next_tables(dp.tables["vlan_acl"], [3, 4, 8, 10])
+        self._check_next_tables(dp.tables["classification"], [4, 5, 6, 7, 8, 10])
+        self._check_next_tables(dp.tables["eth_src"], [5, 6, 7, 8, 10])
+        self._check_next_tables(dp.tables["ipv4_fib"], [7, 8, 10])
+        self._check_next_tables(dp.tables["ipv6_fib"], [7, 8, 10])
+        self._check_next_tables(dp.tables["vip"], [8, 10])
+        self._check_next_tables(dp.tables["eth_dst"], [9])
+        self._check_next_tables(dp.tables["egress"], [10])
+        self._check_next_tables(dp.tables["flood"], [])
 
     def test_pipeline_config_egress(self):
         """Test pipelines are generated correctly with different configs"""
         config = """
 vlans:
     office:
         vid: 100
@@ -1634,19 +1608,19 @@
         interfaces:
             1:
                 native_vlan: office
 """
         self.check_config_success(config, cp.dp_parser)
         dp = self._get_dps_as_dict(config)[0x1]
         tables = {
-            'vlan': 0,
-            'eth_src': 1,
-            'eth_dst': 2,
-            'egress': 3,
-            'flood': 4,
+            "vlan": 0,
+            "eth_src": 1,
+            "eth_dst": 2,
+            "egress": 3,
+            "flood": 4,
         }
         self._check_table_names_numbers(dp, tables)
 
     def test_pipeline_config_egress_acl(self):
         """test acl config is valid when not using 'rule' key"""
         config = """
 acls:
@@ -1665,20 +1639,20 @@
         interfaces:
             1:
                 native_vlan: office
 """
         self.check_config_success(config, cp.dp_parser)
         dp = self._get_dps_as_dict(config)[0x1]
         tables = {
-            'vlan': 0,
-            'eth_src': 1,
-            'eth_dst': 2,
-            'egress_acl': 3,
-            'egress': 4,
-            'flood': 5,
+            "vlan": 0,
+            "eth_src": 1,
+            "eth_dst": 2,
+            "egress_acl": 3,
+            "egress": 4,
+            "flood": 5,
         }
         self._check_table_names_numbers(dp, tables)
 
     def test_tunnel_dp_acl_accepted(self):
         """Test config is accepted when a tunnel is configured as a DP acl"""
         config = """
 acls:
@@ -2381,31 +2355,37 @@
                 native_vlan: vlan100
                 acls_in: [reverse_tunnel]
             2:
                 stack: {dp: s1, port: 2}
 """
         self.check_config_success(config, cp.dp_parser)
         sw1, sw2 = self._get_dps_as_dict(config).values()
-        self.assertTrue(sw1.tunnel_acls, 'Did not generate tunnel ACL')
+        self.assertTrue(sw1.tunnel_acls, "Did not generate tunnel ACL")
         self.assertEqual(
-            len(sw1.tunnel_acls), 2,
-            'Did not generate the correct number of tunnel ACLs')
-        self.assertTrue(sw2.tunnel_acls, 'Did not generate tunnel ACL')
+            len(sw1.tunnel_acls),
+            2,
+            "Did not generate the correct number of tunnel ACLs",
+        )
+        self.assertTrue(sw2.tunnel_acls, "Did not generate tunnel ACL")
         self.assertEqual(
-            len(sw2.tunnel_acls), 2,
-            'Did not generate the correct number of tunnel ACLs')
+            len(sw2.tunnel_acls),
+            2,
+            "Did not generate the correct number of tunnel ACLs",
+        )
         sw1_ids = {}
         sw2_ids = {}
         for acl in sw1.tunnel_acls:
             sw1_ids[acl._id] = list(acl.tunnel_dests.keys())[0]
         for acl in sw2.tunnel_acls:
             sw2_ids[acl._id] = list(acl.tunnel_dests.keys())[0]
         self.assertEqual(
-            sw1_ids, sw2_ids,
-            'Did not generate the same ID for same tunnels on different DPs')
+            sw1_ids,
+            sw2_ids,
+            "Did not generate the same ID for same tunnels on different DPs",
+        )
 
     def test_dynamic_vlan_tunnel_ordered(self):
         """Test tunnel ACL correctly generates the tunnel ID"""
         config = """
 acls:
     forward_tunnel:
         - rule:
@@ -2438,31 +2418,37 @@
                 native_vlan: vlan100
                 acls_in: [reverse_tunnel]
             2:
                 stack: {dp: s1, port: 2}
 """
         self.check_config_success(config, cp.dp_parser)
         sw1, sw2 = self._get_dps_as_dict(config).values()
-        self.assertTrue(sw1.tunnel_acls, 'Did not generate tunnel ACL')
+        self.assertTrue(sw1.tunnel_acls, "Did not generate tunnel ACL")
         self.assertEqual(
-            len(sw1.tunnel_acls), 2,
-            'Did not generate the correct number of tunnel ACLs')
-        self.assertTrue(sw2.tunnel_acls, 'Did not generate tunnel ACL')
+            len(sw1.tunnel_acls),
+            2,
+            "Did not generate the correct number of tunnel ACLs",
+        )
+        self.assertTrue(sw2.tunnel_acls, "Did not generate tunnel ACL")
         self.assertEqual(
-            len(sw2.tunnel_acls), 2,
-            'Did not generate the correct number of tunnel ACLs')
+            len(sw2.tunnel_acls),
+            2,
+            "Did not generate the correct number of tunnel ACLs",
+        )
         sw1_ids = {}
         sw2_ids = {}
         for acl in sw1.tunnel_acls:
             sw1_ids[acl._id] = list(acl.tunnel_dests.keys())[0]
         for acl in sw2.tunnel_acls:
             sw2_ids[acl._id] = list(acl.tunnel_dests.keys())[0]
         self.assertEqual(
-            sw1_ids, sw2_ids,
-            'Did not generate the same ID for same tunnels on different DPs')
+            sw1_ids,
+            sw2_ids,
+            "Did not generate the same ID for same tunnels on different DPs",
+        )
 
     def test_dynamic_specified_vlan_tunnel(self):
         """Test tunnel ACL can generate without clashing with a specified tunnel ACL"""
         config = """
 acls:
     forward_tunnel:
         - rule:
@@ -2498,31 +2484,37 @@
                 stack: {dp: s1, port: 2}
             3:
                 native_vlan: vlan100
                 acls_in: [reverse_tunnel]
 """
         self.check_config_success(config, cp.dp_parser)
         sw1, sw2 = self._get_dps_as_dict(config).values()
-        self.assertTrue(sw1.tunnel_acls, 'Did not generate tunnel ACL')
+        self.assertTrue(sw1.tunnel_acls, "Did not generate tunnel ACL")
         self.assertEqual(
-            len(sw1.tunnel_acls), 2,
-            'Did not generate the correct number of tunnel ACLs')
-        self.assertTrue(sw2.tunnel_acls, 'Did not generate tunnel ACL')
+            len(sw1.tunnel_acls),
+            2,
+            "Did not generate the correct number of tunnel ACLs",
+        )
+        self.assertTrue(sw2.tunnel_acls, "Did not generate tunnel ACL")
         self.assertEqual(
-            len(sw2.tunnel_acls), 2,
-            'Did not generate the correct number of tunnel ACLs')
+            len(sw2.tunnel_acls),
+            2,
+            "Did not generate the correct number of tunnel ACLs",
+        )
         sw1_ids = {}
         sw2_ids = {}
         for acl in sw1.tunnel_acls:
             sw1_ids[acl._id] = list(acl.tunnel_dests.keys())[0]
         for acl in sw2.tunnel_acls:
             sw2_ids[acl._id] = list(acl.tunnel_dests.keys())[0]
         self.assertEqual(
-            sw1_ids, sw2_ids,
-            'Did not generate the same ID for same tunnels on different DPs')
+            sw1_ids,
+            sw2_ids,
+            "Did not generate the same ID for same tunnels on different DPs",
+        )
 
     def test_dynamic_specified_vlan_tunnel_ordered(self):
         """Test tunnel ACL can generate without clashing with a specified tunnel ACL"""
         config = """
 acls:
     forward_tunnel:
         - rule:
@@ -2558,31 +2550,37 @@
                 stack: {dp: s1, port: 2}
             3:
                 native_vlan: vlan100
                 acls_in: [reverse_tunnel]
 """
         self.check_config_success(config, cp.dp_parser)
         sw1, sw2 = self._get_dps_as_dict(config).values()
-        self.assertTrue(sw1.tunnel_acls, 'Did not generate tunnel ACL')
+        self.assertTrue(sw1.tunnel_acls, "Did not generate tunnel ACL")
         self.assertEqual(
-            len(sw1.tunnel_acls), 2,
-            'Did not generate the correct number of tunnel ACLs')
-        self.assertTrue(sw2.tunnel_acls, 'Did not generate tunnel ACL')
+            len(sw1.tunnel_acls),
+            2,
+            "Did not generate the correct number of tunnel ACLs",
+        )
+        self.assertTrue(sw2.tunnel_acls, "Did not generate tunnel ACL")
         self.assertEqual(
-            len(sw2.tunnel_acls), 2,
-            'Did not generate the correct number of tunnel ACLs')
+            len(sw2.tunnel_acls),
+            2,
+            "Did not generate the correct number of tunnel ACLs",
+        )
         sw1_ids = {}
         sw2_ids = {}
         for acl in sw1.tunnel_acls:
             sw1_ids[acl._id] = list(acl.tunnel_dests.keys())[0]
         for acl in sw2.tunnel_acls:
             sw2_ids[acl._id] = list(acl.tunnel_dests.keys())[0]
         self.assertEqual(
-            sw1_ids, sw2_ids,
-            'Did not generate the same ID for same tunnels on different DPs')
+            sw1_ids,
+            sw2_ids,
+            "Did not generate the same ID for same tunnels on different DPs",
+        )
 
     def test_tunnel_two_ports(self):
         """Test tunnel ACL does not try to generate different VIDs for the same tunnel"""
         config = """
 acls:
     forward_tunnel:
         - rule:
@@ -2613,15 +2611,17 @@
                 native_vlan: vlan100
                 acls_in: [forward_tunnel]
             2:
                 stack: {dp: s1, port: 2}
 """
         self.check_config_success(config, cp.dp_parser)
         sw1, sw2 = self._get_dps_as_dict(config).values()
-        self.assertEqual(sw1.vlans.keys(), sw2.vlans.keys(), 'Did not generate the same VLANs')
+        self.assertEqual(
+            sw1.vlans.keys(), sw2.vlans.keys(), "Did not generate the same VLANs"
+        )
 
     def test_tunnel_two_ports_ordered(self):
         """Test tunnel ACL does not try to generate different VIDs for the same tunnel"""
         config = """
 acls:
     forward_tunnel:
         - rule:
@@ -2652,15 +2652,17 @@
                 native_vlan: vlan100
                 acls_in: [forward_tunnel]
             2:
                 stack: {dp: s1, port: 2}
 """
         self.check_config_success(config, cp.dp_parser)
         sw1, sw2 = self._get_dps_as_dict(config).values()
-        self.assertEqual(sw1.vlans.keys(), sw2.vlans.keys(), 'Did not generate the same VLANs')
+        self.assertEqual(
+            sw1.vlans.keys(), sw2.vlans.keys(), "Did not generate the same VLANs"
+        )
 
     def test_two_tunnel_acl(self):
         """Test tunnel ACL correctly allocates VLANs for an ACL with two tunnel rules"""
         config = """
 acls:
     tunnel_acl:
         - rule:
@@ -2697,15 +2699,17 @@
             2:
                 native_vlan: vlan200
             3:
                 stack: {dp: s1, port: 2}
 """
         self.check_config_success(config, cp.dp_parser)
         sw1, sw2 = self._get_dps_as_dict(config).values()
-        self.assertEqual(sw1.vlans.keys(), sw2.vlans.keys(), 'Did not generate the same VLANs')
+        self.assertEqual(
+            sw1.vlans.keys(), sw2.vlans.keys(), "Did not generate the same VLANs"
+        )
 
     def test_two_tunnel_acl_ordered(self):
         """Test tunnel ACL correctly allocates VLANs for an ACL with two tunnel rules"""
         config = """
 acls:
     tunnel_acl:
         - rule:
@@ -2742,15 +2746,17 @@
             2:
                 native_vlan: vlan200
             3:
                 stack: {dp: s1, port: 2}
 """
         self.check_config_success(config, cp.dp_parser)
         sw1, sw2 = self._get_dps_as_dict(config).values()
-        self.assertEqual(sw1.vlans.keys(), sw2.vlans.keys(), 'Did not generate the same VLANs')
+        self.assertEqual(
+            sw1.vlans.keys(), sw2.vlans.keys(), "Did not generate the same VLANs"
+        )
 
     def test_lacp_port_options(self):
         """Test LACP port selection options pass config checking"""
         config = """
 vlans:
     vlan100:
         vid: 100
@@ -3846,25 +3852,25 @@
                 acl_in: office-vlan-protect
                 acls_in: [access-port-protect]
 """
         self.check_config_failure(config, cp.dp_parser)
 
     def test_invalid_char(self):
         """Test config file with invalid characters."""
-        config = b'\x63\xe1'
+        config = b"\x63\xe1"
         self.check_config_failure(config, cp.dp_parser)
 
     def test_perm_denied(self):
         """Test config file has no read permission."""
 
         def unreadable():
             """Make config unreadable."""
             os.chmod(self.conf_file_name(), 0)
 
-        config = ''
+        config = ""
         self.check_config_failure(config, cp.dp_parser, before_function=unreadable)
 
     def test_missing_route_config(self):
         """Test missing IP gateway for route."""
         config = """
 vlans:
     office:
@@ -4648,15 +4654,15 @@
         interfaces:
             1:
                 native_vlan: office
 """
         self.check_config_success(config, cp.dp_parser)
         dp = self._get_dps_as_dict(config)[0x1]
         vlan = dp.vlans[100]
-        self.assertEqual('0e:00:00:0f:02:03', vlan.faucet_mac)
+        self.assertEqual("0e:00:00:0f:02:03", vlan.faucet_mac)
 
     def test_dupe_dpid(self):
         """Test duplicate DPID."""
         config = """
 dps:
     sw1:
         dp_id: 0x1
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_fctl.py` & `c65faucet-1.0.47/tests/unit/faucet/test_fctl.py`

 * *Files 15% similar despite different names*

```diff
@@ -28,44 +28,46 @@
 from faucet import fctl
 
 
 class FctlTestCaseBase(unittest.TestCase):  # pytype: disable=module-attr
     """Base class for fctl tests."""
 
     DEFAULT_VALUES = {
-        'dp_id': '0xb827eb608918',
-        'mac_addr': 'a4:5e:60:c5:5c:ed',
-        'metrics': 'learned_macs',
-        'n': 3,
-        'port': '17',
-        'vlan': '2004',
-        'value': 180725257428205.0
+        "dp_id": "0xb827eb608918",
+        "mac_addr": "a4:5e:60:c5:5c:ed",
+        "metrics": "learned_macs",
+        "n": 3,
+        "port": "17",
+        "vlan": "2004",
+        "value": 180725257428205.0,
     }
 
-    SRC_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../../faucet')
+    SRC_DIR = os.path.join(
+        os.path.dirname(os.path.realpath(__file__)), "../../../faucet"
+    )
 
     FCTL_BASE_ARGS = [
-        '--metrics={metrics}'.format(**DEFAULT_VALUES),
-        '--labels=dp_id:{dp_id}'.format(**DEFAULT_VALUES)
+        "--metrics={metrics}".format(**DEFAULT_VALUES),
+        "--labels=dp_id:{dp_id}".format(**DEFAULT_VALUES),
     ]
-    FCTL = os.path.join(SRC_DIR, 'fctl.py')
+    FCTL = os.path.join(SRC_DIR, "fctl.py")
     tmpdir = None
     prom_input_file_name = None
 
     def setUp(self):
         self.tmpdir = tempfile.mkdtemp()
-        self.prom_input_file_name = os.path.join(self.tmpdir, 'prom_input.txt')
+        self.prom_input_file_name = os.path.join(self.tmpdir, "prom_input.txt")
 
     def tearDown(self):
         shutil.rmtree(self.tmpdir)
 
     def fctl_args(self, extra_args=None):
         """generate argument list for fctl"""
         result = copy.copy(self.FCTL_BASE_ARGS)
-        result += [f'--endpoints=file:{self.prom_input_file_name}']
+        result += ["--endpoints=file:%s" % self.prom_input_file_name]
         if extra_args is not None:
             result += extra_args
         return result
 
     def learned_macs_prom(self, overwrite_labels=None):
         """generate prometheus formated data"""
         labels = copy.copy(self.DEFAULT_VALUES)
@@ -87,94 +89,102 @@
 
 
 class FctlTestCase(FctlTestCaseBase):
     """Drive fctl from shell."""
 
     def run_fctl(self, prom_input, expected_output, extra_args=None):
         """Ensure fctl succeeds and returns expected output."""
-        with open(self.prom_input_file_name, 'w', encoding='utf-8') as prom_input_file:
+        with open(self.prom_input_file_name, "w", encoding="utf-8") as prom_input_file:
             prom_input_file.write(prom_input)
-        fctl_cli = ' '.join(
-            ['python3', self.FCTL] + self.fctl_args(extra_args))
-        retcode, output = subprocess.getstatusoutput(fctl_cli)  # pytype: disable=module-attr
-        self.assertEqual(0, retcode, msg=f'{fctl_cli} returned {retcode}')
+        fctl_cli = " ".join(["python3", self.FCTL] + self.fctl_args(extra_args))
+        retcode, output = subprocess.getstatusoutput(
+            fctl_cli
+        )  # pytype: disable=module-attr
+        self.assertEqual(0, retcode, msg="%s returned %d" % (fctl_cli, retcode))
         output = output.strip()
         self.assertEqual(output, expected_output)
 
     def test_macs(self):
         """Test can parse learned MACs from Prometheus data."""
         self.run_fctl(self.learned_macs_prom(), self.learned_macs_result())
 
     def test_display_labels(self):
         """Test can filter by display labels."""
         expected_output = """
 learned_macs\t[('dp_id', '{dp_id}')]\t{mac_addr}
-""".format(**self.DEFAULT_VALUES).strip()
+""".format(
+            **self.DEFAULT_VALUES
+        ).strip()
 
         self.run_fctl(
             self.learned_macs_prom(),
             expected_output,
-            extra_args=['--display-labels=dp_id'])
+            extra_args=["--display-labels=dp_id"],
+        )
 
 
 class FctlClassTestCase(FctlTestCaseBase):
     """Test fctl internal methods."""
 
     def test_http_fail(self):
         """Test HTTP scrape handled."""
-        with open(os.devnull, 'w', encoding='utf-8') as err_output_file:
+        with open(os.devnull, "w", encoding="utf-8") as err_output_file:
             self.assertEqual(
                 None,
                 fctl.scrape_prometheus(
-                    ['http://127.0.0.1:23'], err_output_file=err_output_file))
+                    ["http://127.0.0.1:23"], err_output_file=err_output_file
+                ),
+            )
 
     def test_bad_url(self):
         """Test unparseable URL."""
-        with open(os.devnull, 'w', encoding='utf-8') as err_output_file:
+        with open(os.devnull, "w", encoding="utf-8") as err_output_file:
             self.assertEqual(
                 None,
                 fctl.scrape_prometheus(
-                    ['not/a$#@/valid_URL'], err_output_file=err_output_file))
+                    ["not/a$#@/valid_URL"], err_output_file=err_output_file
+                ),
+            )
 
     def test_bad_content(self):
         """Test bad content."""
-        bad_input_file_name = os.path.join(self.tmpdir, 'bad_content.txt')
-        with open(bad_input_file_name, 'w', encoding='utf-8') as bad_input_file:
-            bad_input_file.write('NOT/_prometheus_data')
-        with open(os.devnull, 'w', encoding='utf-8') as err_output_file:
+        bad_input_file_name = os.path.join(self.tmpdir, "bad_content.txt")
+        with open(bad_input_file_name, "w", encoding="utf-8") as bad_input_file:
+            bad_input_file.write("NOT/_prometheus_data")
+        with open(os.devnull, "w", encoding="utf-8") as err_output_file:
             self.assertEqual(
                 None,
                 fctl.scrape_prometheus(
-                    [f'file://{bad_input_file_name}'], err_output_file=err_output_file))
+                    ["file://%s" % bad_input_file_name], err_output_file=err_output_file
+                ),
+            )
 
     def write_prom_input_file(self, input_data):
-        with open(self.prom_input_file_name, 'w', encoding='utf-8') as prom_input_file:
+        with open(self.prom_input_file_name, "w", encoding="utf-8") as prom_input_file:
             prom_input_file.write(input_data)
 
     def test_macs(self):
         """Test reporting of learned MACs."""
         self.write_prom_input_file(self.learned_macs_prom())
-        (
-            endpoints,
-            report_metrics,
-            label_matches,
-            nonzero_only,
-            _
-        ) = fctl.parse_args(self.fctl_args())
+        (endpoints, report_metrics, label_matches, nonzero_only, _) = fctl.parse_args(
+            self.fctl_args()
+        )
         metrics = fctl.scrape_prometheus(endpoints)
         report_out = fctl.report_label_match_metrics(
             report_metrics=report_metrics,
             metrics=metrics,
             label_matches=label_matches,
-            nonzero_only=nonzero_only)
+            nonzero_only=nonzero_only,
+        )
         self.assertEqual(report_out, self.learned_macs_result())
 
     def test_get_samples(self):
         """Test querying with get_samples"""
         self.write_prom_input_file(self.learned_macs_prom())
         samples = fctl.get_samples(
-            ['file://' + self.prom_input_file_name], 'learned_macs', {})
-        self.assertEqual(samples[0].value, self.DEFAULT_VALUES['value'])
+            ["file://" + self.prom_input_file_name], "learned_macs", {}
+        )
+        self.assertEqual(samples[0].value, self.DEFAULT_VALUES["value"])
 
 
 if __name__ == "__main__":
     unittest.main()  # pytype: disable=module-attr
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_main.py` & `c65faucet-1.0.47/tests/unit/faucet/test_main.py`

 * *Files 6% similar despite different names*

```diff
@@ -26,17 +26,19 @@
 
 class MainTestCase(unittest.TestCase):  # pytype: disable=module-attr
     """Test __main__ methods."""
 
     def test_parse_args(self):
         """Sanity check argument parsing."""
         self.assertFalse(parse_args([]).verbose)
-        self.assertTrue(parse_args(['--verbose']).verbose)
+        self.assertTrue(parse_args(["--verbose"]).verbose)
 
     def test_build_ryu_args(self):
         """Test build_ryu_args()."""
-        self.assertTrue(build_ryu_args(['faucet', '--use-stderr', '--use-syslog', '--verbose']))
-        self.assertFalse(build_ryu_args(['faucet', '--version']))
+        self.assertTrue(
+            build_ryu_args(["faucet", "--use-stderr", "--use-syslog", "--verbose"])
+        )
+        self.assertFalse(build_ryu_args(["faucet", "--version"]))
 
 
 if __name__ == "__main__":
     unittest.main()  # pytype: disable=module-attr
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_port.py` & `c65faucet-1.0.47/tests/unit/faucet/test_port.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,34 +1,41 @@
 """Unit tests for Port"""
 
 import unittest
 
 from faucet.port import Port
 from faucet.port import (
-    LACP_ACTOR_NOTCONFIGURED, LACP_PORT_NOTCONFIGURED,
-    LACP_ACTOR_INIT, LACP_ACTOR_UP, LACP_ACTOR_NOSYNC, LACP_ACTOR_NONE,
-    LACP_PORT_UNSELECTED, LACP_PORT_SELECTED, LACP_PORT_STANDBY)
+    LACP_ACTOR_NOTCONFIGURED,
+    LACP_PORT_NOTCONFIGURED,
+    LACP_ACTOR_INIT,
+    LACP_ACTOR_UP,
+    LACP_ACTOR_NOSYNC,
+    LACP_ACTOR_NONE,
+    LACP_PORT_UNSELECTED,
+    LACP_PORT_SELECTED,
+    LACP_PORT_STANDBY,
+)
 
 
-class MockVLAN():  # pylint: disable=too-few-public-methods
+class MockVLAN:  # pylint: disable=too-few-public-methods
     """Mock class for VLAN so we can inject into Port"""
 
     def __init__(self, name):
         self.name = name
 
 
 class FaucetPortMethodTest(unittest.TestCase):  # pytype: disable=module-attr
     """Test a range of methods on Port"""
 
     def test_vlans(self):
         """Test that the vlans() method behaves correctly"""
 
-        vlan100 = MockVLAN('v100')
-        vlan200 = MockVLAN('v200')
-        vlan300 = MockVLAN('v300')
+        vlan100 = MockVLAN("v100")
+        vlan200 = MockVLAN("v200")
+        vlan300 = MockVLAN("v300")
 
         tagged_vlans = [vlan200, vlan300]
         native_vlan = vlan100
 
         port = Port(1, 1, {})
         port.native_vlan = native_vlan
         self.assertIn(native_vlan, port.vlans())
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_valve.py` & `c65faucet-1.0.47/tests/unit/faucet/test_valve.py`

 * *Files 8% similar despite different names*

```diff
@@ -30,68 +30,89 @@
 from os_ken.ofproto import ofproto_v1_3_parser as parser
 
 from faucet import valve_of
 from faucet import valve_packet
 from faucet.config_parser_util import yaml_load, yaml_dump
 
 from clib.valve_test_lib import (
-    CONFIG, DP1_CONFIG, FAUCET_MAC, GROUP_DP1_CONFIG, IDLE_DP1_CONFIG,
-    ValveTestBases)
+    CONFIG,
+    DP1_CONFIG,
+    FAUCET_MAC,
+    GROUP_DP1_CONFIG,
+    IDLE_DP1_CONFIG,
+    ValveTestBases,
+)
 
 from clib.fakeoftable import CONTROLLER_PORT
 
 
-class ValveTestCase(ValveTestBases.ValveTestBig):  # pylint: disable=too-few-public-methods
+class ValveTestCase(
+    ValveTestBases.ValveTestBig
+):  # pylint: disable=too-few-public-methods
     """Run complete set of basic tests."""
 
 
 class ValveFuzzTestCase(ValveTestBases.ValveTestNetwork):
     """Test unknown ports/VLANs."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_fuzz_vlan(self):
         """Test unknown VIDs/ports."""
         for _ in range(0, 3):
             for i in range(0, 64):
-                self.rcv_packet(1, i, {
-                    'eth_src': self.P1_V100_MAC,
-                    'eth_dst': self.P2_V200_MAC,
-                    'ipv4_src': '10.0.0.2',
-                    'ipv4_dst': '10.0.0.3',
-                    'vid': i})
+                self.rcv_packet(
+                    1,
+                    i,
+                    {
+                        "eth_src": self.P1_V100_MAC,
+                        "eth_dst": self.P2_V200_MAC,
+                        "ipv4_src": "10.0.0.2",
+                        "ipv4_dst": "10.0.0.3",
+                        "vid": i,
+                    },
+                )
             for i in range(0, 64):
-                self.rcv_packet(i, 0x100, {
-                    'eth_src': self.P1_V100_MAC,
-                    'eth_dst': self.P2_V200_MAC,
-                    'ipv4_src': '10.0.0.2',
-                    'ipv4_dst': '10.0.0.3',
-                    'vid': 0x100})
+                self.rcv_packet(
+                    i,
+                    0x100,
+                    {
+                        "eth_src": self.P1_V100_MAC,
+                        "eth_dst": self.P2_V200_MAC,
+                        "ipv4_src": "10.0.0.2",
+                        "ipv4_dst": "10.0.0.3",
+                        "vid": 0x100,
+                    },
+                )
         # pylint: disable=no-member
         # pylint: disable=no-value-for-parameter
         cache_info = valve_packet.parse_packet_in_pkt.cache_info()
         self.assertGreater(cache_info.hits, cache_info.misses, msg=cache_info)
 
 
 class ValveCoprocessorTestCase(ValveTestBases.ValveTestNetwork):
     """Test direct packet output using coprocessor."""
 
-    CONFIG = """
+    CONFIG = (
+        """
 dps:
     s1:
 %s
         interfaces:
             p1:
                 number: 1
                 coprocessor: {strategy: vlan_vid, vlan_vid_base: 0x200}
@@ -111,92 +132,117 @@
             ipv4_src: 10.0.0.99
             dl_type: 0x0800
             actions:
                 allow: 0
         - rule:
             actions:
                 allow: 1
-""" % DP1_CONFIG
+"""
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic coprocessor config"""
         self.setup_valves(self.CONFIG)
 
     def test_output(self):
         # VID for direct output to port 2
         copro_vid_out = (0x200 + 2) | ofp.OFPVID_PRESENT
         direct_match = {
-            'in_port': 1, 'vlan_vid': copro_vid_out, 'eth_type': ether.ETH_TYPE_IP,
-            'eth_src': self.P1_V100_MAC, 'eth_dst': mac.BROADCAST_STR}
+            "in_port": 1,
+            "vlan_vid": copro_vid_out,
+            "eth_type": ether.ETH_TYPE_IP,
+            "eth_src": self.P1_V100_MAC,
+            "eth_dst": mac.BROADCAST_STR,
+        }
         table = self.network.tables[self.DP_ID]
         self.assertTrue(table.is_output(direct_match, port=2))
         p2_host_match = {
-            'eth_src': self.P1_V100_MAC, 'eth_dst': self.P2_V200_MAC,
-            'ipv4_src': '10.0.0.2', 'ipv4_dst': '10.0.0.3',
-            'eth_type': ether.ETH_TYPE_IP}
+            "eth_src": self.P1_V100_MAC,
+            "eth_dst": self.P2_V200_MAC,
+            "ipv4_src": "10.0.0.2",
+            "ipv4_dst": "10.0.0.3",
+            "eth_type": ether.ETH_TYPE_IP,
+        }
         p2_host_receive = copy.deepcopy(p2_host_match)
-        p2_host_receive.update({'in_port': 2})
+        p2_host_receive.update({"in_port": 2})
         # learn P2 host
         self.rcv_packet(2, 0x100, p2_host_receive)
         # copro can send to P2 via regular pipeline and is not subject to VLAN ACL.
         p2_copro_host_receive = copy.deepcopy(p2_host_match)
         p2_copro_host_receive.update(
-            {'in_port': 1,
-             'ipv4_src': '10.0.0.99', 'ipv4_dst': '10.0.0.3',
-             'eth_src': p2_host_match['eth_dst'],
-             'eth_dst': p2_host_match['eth_src']})
-        p2_copro_host_receive['vlan_vid'] = 0x100 | ofp.OFPVID_PRESENT
+            {
+                "in_port": 1,
+                "ipv4_src": "10.0.0.99",
+                "ipv4_dst": "10.0.0.3",
+                "eth_src": p2_host_match["eth_dst"],
+                "eth_dst": p2_host_match["eth_src"],
+            }
+        )
+        p2_copro_host_receive["vlan_vid"] = 0x100 | ofp.OFPVID_PRESENT
         self.assertTrue(table.is_output(p2_copro_host_receive, port=2, vid=0x100))
         # copro send to P2 was not flooded
         self.assertFalse(table.is_output(p2_copro_host_receive, port=3, vid=0x100))
 
 
 class ValveRestBcastTestCase(ValveTestBases.ValveTestNetwork):
     """Test restricted broadcast."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
                 restricted_bcast_arpnd: true
             p2:
                 number: 2
                 native_vlan: 0x100
             p3:
                 number: 3
                 native_vlan: 0x100
                 restricted_bcast_arpnd: true
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config with restricted broadcast enabled"""
         self.setup_valves(self.CONFIG)
 
     def test_rest_bcast(self):
         match = {
-            'in_port': 1, 'vlan_vid': 0, 'eth_type': ether.ETH_TYPE_IP,
-            'eth_src': self.P1_V100_MAC, 'eth_dst': mac.BROADCAST_STR}
+            "in_port": 1,
+            "vlan_vid": 0,
+            "eth_type": ether.ETH_TYPE_IP,
+            "eth_src": self.P1_V100_MAC,
+            "eth_dst": mac.BROADCAST_STR,
+        }
         table = self.network.tables[self.DP_ID]
         self.assertTrue(table.is_output(match, port=2))
         self.assertFalse(table.is_output(match, port=3))
         match = {
-            'in_port': 2, 'vlan_vid': 0, 'eth_type': ether.ETH_TYPE_IP,
-            'eth_src': self.P1_V100_MAC, 'eth_dst': mac.BROADCAST_STR}
+            "in_port": 2,
+            "vlan_vid": 0,
+            "eth_type": ether.ETH_TYPE_IP,
+            "eth_src": self.P1_V100_MAC,
+            "eth_dst": mac.BROADCAST_STR,
+        }
         self.assertTrue(table.is_output(match, port=1))
         self.assertTrue(table.is_output(match, port=3))
 
 
 class ValveUnusedMeterTestCase(ValveTestBases.ValveTestNetwork):
     """Test unused meters are not configured."""
 
-    CONFIG = """
+    CONFIG = (
+        """
 meters:
     unusedmeter:
         meter_id: 1
         entry:
             flags: "KBPS"
             bands:
                 [
@@ -226,23 +272,25 @@
     s1:
 %s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
                 acls_in: [meteracl]
-""" % DP1_CONFIG
+"""
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup meter and ACL config"""
         self.setup_valves(self.CONFIG)
 
     def test_usedmeter(self):
         valve = self.valves_manager.valves[self.DP_ID]
-        self.assertEqual(['usedmeter'], list(valve.dp.meters.keys()))
+        self.assertEqual(["usedmeter"], list(valve.dp.meters.keys()))
 
 
 class ValveOFErrorTestCase(ValveTestBases.ValveTestNetwork):
     """Test decoding of OFErrors."""
 
     def setUp(self):
         self.setup_valves(CONFIG)
@@ -253,32 +301,36 @@
             self.assertTrue(isinstance(type_code, int))
             type_str, error_codes = error_tuple
             self.assertTrue(isinstance(type_str, str))
             for error_code, error_str in error_codes.items():
                 self.assertTrue(isinstance(error_code, int))
                 self.assertTrue(isinstance(error_str, str))
         test_err = parser.OFPErrorMsg(
-            datapath=None, type_=ofp.OFPET_FLOW_MOD_FAILED, code=ofp.OFPFMFC_UNKNOWN)
+            datapath=None, type_=ofp.OFPET_FLOW_MOD_FAILED, code=ofp.OFPFMFC_UNKNOWN
+        )
         valve = self.valves_manager.valves[self.DP_ID]
         valve.oferror(test_err)
         test_unknown_type_err = parser.OFPErrorMsg(
-            datapath=None, type_=666, code=ofp.OFPFMFC_UNKNOWN)
+            datapath=None, type_=666, code=ofp.OFPFMFC_UNKNOWN
+        )
         valve.oferror(test_unknown_type_err)
         test_unknown_code_err = parser.OFPErrorMsg(
-            datapath=None, type_=ofp.OFPET_FLOW_MOD_FAILED, code=666)
+            datapath=None, type_=ofp.OFPET_FLOW_MOD_FAILED, code=666
+        )
         valve.oferror(test_unknown_code_err)
 
 
 class ValveGroupTestCase(ValveTestBases.ValveTestNetwork):
     """Tests for datapath with group support."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{GROUP_DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
             p2:
                 number: 2
                 native_vlan: v200
@@ -291,14 +343,16 @@
                 tagged_vlans: [v200]
 vlans:
     v100:
         vid: 0x100
     v200:
         vid: 0x200
 """
+        % GROUP_DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_unknown_eth_dst_rule(self):
         """Test that packets with unkown eth dst addrs get flooded correctly.
@@ -306,43 +360,36 @@
         They must be output to each port on the associated vlan, with the
         correct vlan tagging. And they must not be forwarded to a port not
         on the associated vlan
         """
         self.learn_hosts()
         matches = [
             {
-                'in_port': 3,
-                'vlan_vid': self.V100,
+                "in_port": 3,
+                "vlan_vid": self.V100,
             },
+            {"in_port": 2, "vlan_vid": 0, "eth_dst": self.P1_V100_MAC},
+            {"in_port": 1, "vlan_vid": 0, "eth_src": self.P1_V100_MAC},
             {
-                'in_port': 2,
-                'vlan_vid': 0,
-                'eth_dst': self.P1_V100_MAC
+                "in_port": 3,
+                "vlan_vid": self.V200,
+                "eth_src": self.P2_V200_MAC,
             },
-            {
-                'in_port': 1,
-                'vlan_vid': 0,
-                'eth_src': self.P1_V100_MAC
-            },
-            {
-                'in_port': 3,
-                'vlan_vid': self.V200,
-                'eth_src': self.P2_V200_MAC,
-            }
         ]
         self.verify_flooding(matches)
 
 
 class ValveIdleLearnTestCase(ValveTestBases.ValveTestNetwork):
     """Smoke test for idle-flow based learning. This feature is not currently reliable."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{IDLE_DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
             p2:
                 number: 2
                 native_vlan: v200
@@ -359,40 +406,50 @@
                 mirror: 4
 vlans:
     v100:
         vid: 0x100
     v200:
         vid: 0x200
 """
+        % IDLE_DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config with mirroring"""
         self.setup_valves(self.CONFIG)
 
     def test_known_eth_src_rule(self):
         """Test removal flow handlers."""
         self.learn_hosts()
         valve = self.valves_manager.valves[self.DP_ID]
         self.assertTrue(
             valve.flow_timeout(
                 self.mock_time(),
-                valve.dp.tables['eth_dst'].table_id,
-                {'vlan_vid': self.V100, 'eth_dst': self.P1_V100_MAC}))
+                valve.dp.tables["eth_dst"].table_id,
+                {"vlan_vid": self.V100, "eth_dst": self.P1_V100_MAC},
+            )
+        )
         self.assertFalse(
             valve.flow_timeout(
                 self.mock_time(),
-                valve.dp.tables['eth_src'].table_id,
-                {'vlan_vid': self.V100, 'in_port': 1, 'eth_src': self.P1_V100_MAC}))
+                valve.dp.tables["eth_src"].table_id,
+                {"vlan_vid": self.V100, "in_port": 1, "eth_src": self.P1_V100_MAC},
+            )
+        )
 
     def test_host_learn_coldstart(self):
         """Test flow learning, including cold-start cache invalidation"""
         valve = self.valves_manager.valves[self.DP_ID]
         match = {
-            'in_port': 3, 'vlan_vid': self.V100, 'eth_type': ether.ETH_TYPE_IP,
-            'eth_src': self.P3_V100_MAC, 'eth_dst': self.P1_V100_MAC}
+            "in_port": 3,
+            "vlan_vid": self.V100,
+            "eth_type": ether.ETH_TYPE_IP,
+            "eth_src": self.P3_V100_MAC,
+            "eth_dst": self.P1_V100_MAC,
+        }
         table = self.network.tables[self.DP_ID]
         self.assertTrue(table.is_output(match, port=1))
         self.assertTrue(table.is_output(match, port=2))
         self.assertTrue(table.is_output(match, port=CONTROLLER_PORT))
         self.learn_hosts()
         self.assertTrue(table.is_output(match, port=1))
         self.assertFalse(table.is_output(match, port=2))
@@ -407,18 +464,19 @@
         self.assertFalse(table.is_output(match, port=2))
         self.assertFalse(table.is_output(match, port=CONTROLLER_PORT))
 
 
 class ValveLACPTestCase(ValveTestBases.ValveTestNetwork):
     """Test LACP."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         lacp_timeout: 5
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
                 lacp: 1
             p2:
@@ -438,162 +496,175 @@
     v100:
         vid: 0x100
     v200:
         vid: 0x200
     v300:
         vid: 0x300
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup lacp config and activate ports"""
         self.setup_valves(self.CONFIG)
         self.activate_all_ports()
 
     def test_lacp(self):
         """Test LACP comes up."""
         test_port = 1
         labels = self.port_labels(test_port)
         valve = self.valves_manager.valves[self.DP_ID]
-        self.assertEqual(
-            1, int(self.get_prom('port_lacp_state', labels=labels)))
-        self.assertFalse(
-            valve.dp.ports[1].non_stack_forwarding())
-        self.rcv_packet(test_port, 0, {
-            'actor_system': '0e:00:00:00:00:02',
-            'partner_system': FAUCET_MAC,
-            'eth_dst': slow.SLOW_PROTOCOL_MULTICAST,
-            'eth_src': '0e:00:00:00:00:02',
-            'actor_state_synchronization': 1})
-        self.assertEqual(
-            3, int(self.get_prom('port_lacp_state', labels=labels)))
-        self.assertTrue(
-            valve.dp.ports[1].non_stack_forwarding())
+        self.assertEqual(1, int(self.get_prom("port_lacp_state", labels=labels)))
+        self.assertFalse(valve.dp.ports[1].non_stack_forwarding())
+        self.rcv_packet(
+            test_port,
+            0,
+            {
+                "actor_system": "0e:00:00:00:00:02",
+                "partner_system": FAUCET_MAC,
+                "eth_dst": slow.SLOW_PROTOCOL_MULTICAST,
+                "eth_src": "0e:00:00:00:00:02",
+                "actor_state_synchronization": 1,
+            },
+        )
+        self.assertEqual(3, int(self.get_prom("port_lacp_state", labels=labels)))
+        self.assertTrue(valve.dp.ports[1].non_stack_forwarding())
         self.learn_hosts()
         self.verify_expiry()
 
     def test_lacp_flap(self):
         """Test LACP handles state 0->1->0."""
         valve = self.valves_manager.valves[self.DP_ID]
         test_port = 1
         labels = self.port_labels(test_port)
-        self.assertEqual(
-            1, int(self.get_prom('port_lacp_state', labels=labels)))
-        self.assertFalse(
-            valve.dp.ports[1].non_stack_forwarding())
-        self.rcv_packet(test_port, 0, {
-            'actor_system': '0e:00:00:00:00:02',
-            'partner_system': FAUCET_MAC,
-            'eth_dst': slow.SLOW_PROTOCOL_MULTICAST,
-            'eth_src': '0e:00:00:00:00:02',
-            'actor_state_synchronization': 1})
-        self.assertEqual(
-            3, int(self.get_prom('port_lacp_state', labels=labels)))
-        self.assertTrue(
-            valve.dp.ports[1].non_stack_forwarding())
+        self.assertEqual(1, int(self.get_prom("port_lacp_state", labels=labels)))
+        self.assertFalse(valve.dp.ports[1].non_stack_forwarding())
+        self.rcv_packet(
+            test_port,
+            0,
+            {
+                "actor_system": "0e:00:00:00:00:02",
+                "partner_system": FAUCET_MAC,
+                "eth_dst": slow.SLOW_PROTOCOL_MULTICAST,
+                "eth_src": "0e:00:00:00:00:02",
+                "actor_state_synchronization": 1,
+            },
+        )
+        self.assertEqual(3, int(self.get_prom("port_lacp_state", labels=labels)))
+        self.assertTrue(valve.dp.ports[1].non_stack_forwarding())
         self.learn_hosts()
         self.verify_expiry()
-        self.rcv_packet(test_port, 0, {
-            'actor_system': '0e:00:00:00:00:02',
-            'partner_system': FAUCET_MAC,
-            'eth_dst': slow.SLOW_PROTOCOL_MULTICAST,
-            'eth_src': '0e:00:00:00:00:02',
-            'actor_state_synchronization': 0})
-        self.assertEqual(
-            5, int(self.get_prom('port_lacp_state', labels=labels)))
-        self.assertFalse(
-            valve.dp.ports[1].non_stack_forwarding())
+        self.rcv_packet(
+            test_port,
+            0,
+            {
+                "actor_system": "0e:00:00:00:00:02",
+                "partner_system": FAUCET_MAC,
+                "eth_dst": slow.SLOW_PROTOCOL_MULTICAST,
+                "eth_src": "0e:00:00:00:00:02",
+                "actor_state_synchronization": 0,
+            },
+        )
+        self.assertEqual(5, int(self.get_prom("port_lacp_state", labels=labels)))
+        self.assertFalse(valve.dp.ports[1].non_stack_forwarding())
         self.assertEqual(
-            4, int(self.get_prom('port_lacp_state_change_count_total', labels=labels)))
+            4, int(self.get_prom("port_lacp_state_change_count_total", labels=labels))
+        )
 
     def test_lacp_timeout(self):
         """Test LACP comes up and then times out."""
         valve = self.valves_manager.valves[self.DP_ID]
         test_port = 1
         labels = self.port_labels(test_port)
-        self.assertEqual(
-            1, int(self.get_prom('port_lacp_state', labels=labels)))
-        self.assertFalse(
-            valve.dp.ports[1].non_stack_forwarding())
-        self.rcv_packet(test_port, 0, {
-            'actor_system': '0e:00:00:00:00:02',
-            'partner_system': FAUCET_MAC,
-            'eth_dst': slow.SLOW_PROTOCOL_MULTICAST,
-            'eth_src': '0e:00:00:00:00:02',
-            'actor_state_synchronization': 1})
-        self.assertEqual(
-            3, int(self.get_prom('port_lacp_state', labels=labels)))
-        self.assertTrue(
-            valve.dp.ports[1].non_stack_forwarding())
+        self.assertEqual(1, int(self.get_prom("port_lacp_state", labels=labels)))
+        self.assertFalse(valve.dp.ports[1].non_stack_forwarding())
+        self.rcv_packet(
+            test_port,
+            0,
+            {
+                "actor_system": "0e:00:00:00:00:02",
+                "partner_system": FAUCET_MAC,
+                "eth_dst": slow.SLOW_PROTOCOL_MULTICAST,
+                "eth_src": "0e:00:00:00:00:02",
+                "actor_state_synchronization": 1,
+            },
+        )
+        self.assertEqual(3, int(self.get_prom("port_lacp_state", labels=labels)))
+        self.assertTrue(valve.dp.ports[1].non_stack_forwarding())
         future_now = self.mock_time(10)
         expire_ofmsgs = valve.state_expire(future_now, None)
         self.assertTrue(expire_ofmsgs)
-        self.assertEqual(
-            1, int(self.get_prom('port_lacp_state', labels=labels)))
-        self.assertFalse(
-            valve.dp.ports[1].non_stack_forwarding())
+        self.assertEqual(1, int(self.get_prom("port_lacp_state", labels=labels)))
+        self.assertFalse(valve.dp.ports[1].non_stack_forwarding())
 
     def test_dp_disconnect(self):
         """Test LACP state when disconnects."""
         test_port = 1
         labels = self.port_labels(test_port)
-        self.assertEqual(
-            1, int(self.get_prom('port_lacp_state', labels=labels)))
-        self.rcv_packet(test_port, 0, {
-            'actor_system': '0e:00:00:00:00:02',
-            'partner_system': FAUCET_MAC,
-            'eth_dst': slow.SLOW_PROTOCOL_MULTICAST,
-            'eth_src': '0e:00:00:00:00:02',
-            'actor_state_synchronization': 1})
-        self.assertEqual(
-            3, int(self.get_prom('port_lacp_state', labels=labels)))
+        self.assertEqual(1, int(self.get_prom("port_lacp_state", labels=labels)))
+        self.rcv_packet(
+            test_port,
+            0,
+            {
+                "actor_system": "0e:00:00:00:00:02",
+                "partner_system": FAUCET_MAC,
+                "eth_dst": slow.SLOW_PROTOCOL_MULTICAST,
+                "eth_src": "0e:00:00:00:00:02",
+                "actor_state_synchronization": 1,
+            },
+        )
+        self.assertEqual(3, int(self.get_prom("port_lacp_state", labels=labels)))
         self.disconnect_dp()
-        self.assertEqual(
-            0, int(self.get_prom('port_lacp_state', labels=labels)))
+        self.assertEqual(0, int(self.get_prom("port_lacp_state", labels=labels)))
 
 
 class ValveTFMSizeOverride(ValveTestBases.ValveTestNetwork):
     """Test TFM size override."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         table_sizes:
             eth_src: 999
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
 vlans:
     v100:
         vid: 0x100
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config with overriden TFM sizing"""
         self.setup_valves(self.CONFIG)
 
     def test_size(self):
         table = self.network.tables[self.DP_ID]
         tfm_by_name = {body.name: body for body in table.tfm.values()}
-        eth_src_table = tfm_by_name.get(b'eth_src', None)
+        eth_src_table = tfm_by_name.get(b"eth_src", None)
         self.assertTrue(eth_src_table)
         if eth_src_table is not None:
             self.assertEqual(999, eth_src_table.max_entries)
 
 
 class ValveTFMSize(ValveTestBases.ValveTestNetwork):
     """Test TFM sizer."""
 
     NUM_PORTS = 128
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         lacp_timeout: 5
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
                 lacp: 1
                 lacp_active: True
@@ -617,35 +688,38 @@
     v100:
         vid: 0x100
     v200:
         vid: 0x200
     v300:
         vid: 0x300
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_size(self):
         table = self.network.tables[self.DP_ID]
         tfm_by_name = {body.name: body for body in table.tfm.values()}
-        flood_table = tfm_by_name.get(b'flood', None)
+        flood_table = tfm_by_name.get(b"flood", None)
         self.assertTrue(flood_table)
         if flood_table is not None:
             self.assertGreater(flood_table.max_entries, self.NUM_PORTS * 2)
 
 
 class ValveActiveLACPTestCase(ValveTestBases.ValveTestNetwork):
     """Test LACP."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         lacp_timeout: 5
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
                 lacp: 1
                 lacp_active: True
@@ -666,61 +740,60 @@
     v100:
         vid: 0x100
     v200:
         vid: 0x200
     v300:
         vid: 0x300
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic lacp config and activate ports"""
         self.setup_valves(self.CONFIG)
         self.activate_all_ports()
 
     def test_lacp(self):
         """Test LACP comes up."""
         test_port = 1
         labels = self.port_labels(test_port)
-        self.assertEqual(
-            1, int(self.get_prom('port_lacp_state', labels=labels)))
+        self.assertEqual(1, int(self.get_prom("port_lacp_state", labels=labels)))
         # Ensure LACP packet sent.
         valve = self.valves_manager.valves[self.DP_ID]
         ofmsgs = valve.fast_advertise(self.mock_time(), None)[valve]
         self.assertTrue(ValveTestBases.packet_outs_from_flows(ofmsgs))
-        self.rcv_packet(test_port, 0, {
-            'actor_system': '0e:00:00:00:00:02',
-            'partner_system': FAUCET_MAC,
-            'eth_dst': slow.SLOW_PROTOCOL_MULTICAST,
-            'eth_src': '0e:00:00:00:00:02',
-            'actor_state_synchronization': 1})
-        self.assertEqual(
-            3, int(self.get_prom('port_lacp_state', labels=labels)))
+        self.rcv_packet(
+            test_port,
+            0,
+            {
+                "actor_system": "0e:00:00:00:00:02",
+                "partner_system": FAUCET_MAC,
+                "eth_dst": slow.SLOW_PROTOCOL_MULTICAST,
+                "eth_src": "0e:00:00:00:00:02",
+                "actor_state_synchronization": 1,
+            },
+        )
+        self.assertEqual(3, int(self.get_prom("port_lacp_state", labels=labels)))
         self.learn_hosts()
         self.verify_expiry()
 
 
 class ValveL2LearnTestCase(ValveTestBases.ValveTestNetwork):
     """Test L2 Learning"""
 
     def setUp(self):
         self.setup_valves(CONFIG)
 
     def test_expiry(self):
-        learn_labels = {
-            'vid': str(0x200),
-            'eth_src': self.P2_V200_MAC
-        }
-        self.assertEqual(
-            0, self.get_prom('learned_l2_port', labels=learn_labels))
+        learn_labels = {"vid": str(0x200), "eth_src": self.P2_V200_MAC}
+        self.assertEqual(0, self.get_prom("learned_l2_port", labels=learn_labels))
         self.learn_hosts()
-        self.assertEqual(
-            2.0, self.get_prom('learned_l2_port', labels=learn_labels))
+        self.assertEqual(2.0, self.get_prom("learned_l2_port", labels=learn_labels))
         self.verify_expiry()
-        self.assertEqual(
-            0, self.get_prom('learned_l2_port', labels=learn_labels))
+        self.assertEqual(0, self.get_prom("learned_l2_port", labels=learn_labels))
 
 
 class SoftPipelineTestCase(ValveTestBases.ValveTestNetwork):
     """Test warm starting match changes with soft pipeline."""
 
     REQUIRE_TFM = False
 
@@ -752,17 +825,17 @@
 
     def setUp(self):
         """Setup basic port and vlan config with ACLs"""
         self.setup_valves(self.CONFIG)
 
     def test_soft(self):
         config = yaml_load(self.CONFIG)
-        config['dps']['s1']['interfaces']['p1']['acls_in'] = ['acl2']
+        config["dps"]["s1"]["interfaces"]["p1"]["acls_in"] = ["acl2"]
         # We changed match conditions only, so this can be a warm start.
-        self.update_config(yaml_dump(config), reload_type='warm')
+        self.update_config(yaml_dump(config), reload_type="warm")
 
 
 class HardPipelineTestCase(ValveTestBases.ValveTestNetwork):
     """Test cold starting match conditions with hard pipeline."""
 
     CONFIG = """
 acls:
@@ -792,24 +865,26 @@
 
     def setUp(self):
         """Setup basic port and vlan config with ACLs"""
         self.setup_valves(self.CONFIG)
 
     def test_hard(self):
         config = yaml_load(self.CONFIG)
-        config['dps']['s1']['interfaces']['p1']['acls_in'] = ['acl2']
+        config["dps"]["s1"]["interfaces"]["p1"]["acls_in"] = ["acl2"]
         # Changed match conditions require restart.
-        self.update_config(yaml_dump(config), reload_type='cold')
+        self.update_config(yaml_dump(config), reload_type="cold")
 
 
 class ValveMirrorTestCase(ValveTestBases.ValveTestBig):
     """Test ACL and interface mirroring."""
+
     # TODO: check mirror packets are present/correct
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 acls:
     mirror_ospf:
         - rule:
             nw_dst: '224.0.0.5'
             dl_type: 0x800
             actions:
                 mirror: p5
@@ -819,15 +894,15 @@
             actions:
                 allow: 0
         - rule:
             actions:
                 allow: 1
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
                 lldp_beacon:
                     enable: True
                     system_name: "faucet"
@@ -876,76 +951,82 @@
             neighbor_as: 2
             port: 9179
             routerid: '1.1.1.1'
             server_addresses: ['127.0.0.1']
             neighbor_addresses: ['127.0.0.1']
             vlan: v100
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup complex config with routing, bgp, mirroring and ACLs"""
         self.setup_valves(self.CONFIG)
 
     def test_unmirror(self):
         config = yaml_load(self.CONFIG)
-        del config['dps']['s1']['interfaces']['p5']['mirror']
-        self.update_config(yaml_dump(config), reload_type='warm')
+        del config["dps"]["s1"]["interfaces"]["p5"]["mirror"]
+        self.update_config(yaml_dump(config), reload_type="warm")
 
 
 class ValvePortDescTestCase(ValveTestBases.ValveTestNetwork):
     """Test OFPMP_PORT_DESC reply handling."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
             p2:
                 number: 2
                 native_vlan: v100
 vlans:
     v100:
         vid: 0x100
     v200:
         vid: 0x200
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup simple configuration with no ports up"""
         ofmsgs = self.setup_valves(self.CONFIG, ports_up=[])[self.DP_ID]
         self.valve = self.valves_manager.valves[self.DP_ID]
 
         self.assertFalse(ofmsgs)
         self.assertFalse(self.valve.dp.dyn_up_port_nos)
 
     @staticmethod
     def _inport_flows(in_port, ofmsgs, table_id=0):
         return [
-            ofmsg for ofmsg in ValveTestBases.flowmods_from_flows(ofmsgs)
-            if ofmsg.match.get('in_port') == in_port
-            and ofmsg.table_id == table_id]
+            ofmsg
+            for ofmsg in ValveTestBases.flowmods_from_flows(ofmsgs)
+            if ofmsg.match.get("in_port") == in_port and ofmsg.table_id == table_id
+        ]
 
     @staticmethod
     def _build_port_descs(port_nos, port_nos_up=None):
         descs = []
         for port_no in port_nos:
-            desc = namedtuple('port_no', 'state')
+            desc = namedtuple("port_no", "state")
             desc.port_no = port_no
-            desc.state = (
-                0 if port_no in port_nos_up else valve_of.ofp.OFPPS_LINK_DOWN)
+            desc.state = 0 if port_no in port_nos_up else valve_of.ofp.OFPPS_LINK_DOWN
             descs.append(desc)
         return descs
 
     def _update_port_desc(self, port_nos, port_nos_up=None):
         descs = self._build_port_descs(port_nos, port_nos_up)
         ofmsgs_by_valve = self.valve.port_desc_stats_reply_handler(
-            descs, [], time.time())
+            descs, [], time.time()
+        )
         return ofmsgs_by_valve[self.valve]
 
     def test_unconfigured_ports(self):
         port_nos = [11, 12]
         port_nos_up = [12]
         ofmsgs = self._update_port_desc(port_nos, port_nos_up)
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_valve_config.py` & `c65faucet-1.0.47/tests/unit/faucet/test_valve_config.py`

 * *Files 10% similar despite different names*

```diff
@@ -28,95 +28,114 @@
 
 from os_ken.ofproto import ofproto_v1_3 as ofp
 
 from faucet import config_parser_util
 from faucet import valve_of
 
 from clib.fakeoftable import CONTROLLER_PORT
-from clib.valve_test_lib import BASE_DP1_CONFIG, CONFIG, DP1_CONFIG, FAUCET_MAC, ValveTestBases
+from clib.valve_test_lib import (
+    BASE_DP1_CONFIG,
+    CONFIG,
+    DP1_CONFIG,
+    FAUCET_MAC,
+    ValveTestBases,
+)
 
 
 class ValveIncludeTestCase(ValveTestBases.ValveTestNetwork):
     """Test include optional files."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 include-optional: ['/does/not/exist/']
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup config with non-existent optional include file"""
         self.setup_valves(self.CONFIG)
 
     def test_include_optional(self):
         """Test include optional files."""
-        self.assertEqual(1, int(self.get_prom('dp_status')))
+        self.assertEqual(1, int(self.get_prom("dp_status")))
 
 
 class ValveBadConfTestCase(ValveTestBases.ValveTestNetwork):
     """Test recovery from a bad config file."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
 """
+        % DP1_CONFIG
+    )
 
-    MORE_CONFIG = f"""
+    MORE_CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
             p2:
                 number: 2
                 native_vlan: 0x100
 """
+        % DP1_CONFIG
+    )
 
     BAD_CONFIG = """
 dps: {}
 """
 
     def setUp(self):
         """Setup invalid config"""
         self.setup_valves(self.CONFIG)
 
     def test_bad_conf(self):
         """Test various config types & config reloading"""
         for config, load_error in (
-                (self.CONFIG, 0),
-                (self.BAD_CONFIG, 1),
-                (self.CONFIG, 0),
-                (self.MORE_CONFIG, 0),
-                (self.BAD_CONFIG, 1),
-                (self.CONFIG, 0)):
-            with open(self.config_file, 'w', encoding='utf-8') as config_file:
+            (self.CONFIG, 0),
+            (self.BAD_CONFIG, 1),
+            (self.CONFIG, 0),
+            (self.MORE_CONFIG, 0),
+            (self.BAD_CONFIG, 1),
+            (self.CONFIG, 0),
+        ):
+            with open(self.config_file, "w", encoding="utf-8") as config_file:
                 config_file.write(config)
-            self.valves_manager.request_reload_configs(self.mock_time(), self.config_file)
+            self.valves_manager.request_reload_configs(
+                self.mock_time(), self.config_file
+            )
             self.assertEqual(
                 load_error,
-                self.get_prom('faucet_config_load_error', bare=True),
-                msg=f'{load_error}: {config}')
+                self.get_prom("faucet_config_load_error", bare=True),
+                msg="%u: %s" % (load_error, config),
+            )
 
 
 class ValveChangeVLANACLTestCase(ValveTestBases.ValveTestNetwork):
-
-    CONFIG = """
+    CONFIG = (
+        """
 acls:
   acl1:
   - rule:
       eth_type: 0x0806
       actions:
         allow: 1
 vlans:
@@ -126,17 +145,20 @@
     vid: 10
 dps:
     s1:
 %s
         interfaces:
             1:
                 native_vlan: vlan1
-""" % DP1_CONFIG
+"""
+        % DP1_CONFIG
+    )
 
-    MORE_CONFIG = """
+    MORE_CONFIG = (
+        """
 acls:
   acl1:
   - rule:
       eth_type: 0x0806
       actions:
         allow: 1
   - rule:
@@ -150,207 +172,245 @@
     vid: 10
 dps:
     s1:
 %s
         interfaces:
             1:
                 native_vlan: vlan1
-""" % DP1_CONFIG
+"""
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_change_vlan_acl(self):
         """Test vlan ACL change is detected."""
-        self.update_and_revert_config(self.CONFIG, self.MORE_CONFIG, 'cold')
+        self.update_and_revert_config(self.CONFIG, self.MORE_CONFIG, "cold")
 
 
 class ValveChangePortTestCase(ValveTestBases.ValveTestNetwork):
     """Test changes to config on ports."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
             p2:
                 number: 2
                 native_vlan: 0x200
                 permanent_learn: True
 """
+        % DP1_CONFIG
+    )
 
-    LESS_CONFIG = f"""
+    LESS_CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
             p2:
                 number: 2
                 native_vlan: 0x200
                 permanent_learn: False
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_delete_permanent_learn(self):
         """Test port permanent learn can deconfigured."""
         table = self.network.tables[self.DP_ID]
         before_table_state = table.table_state()
-        self.rcv_packet(2, 0x200, {
-            'eth_src': self.P2_V200_MAC,
-            'eth_dst': self.P3_V200_MAC,
-            'ipv4_src': '10.0.0.2',
-            'ipv4_dst': '10.0.0.3',
-            'vid': 0x200})
+        self.rcv_packet(
+            2,
+            0x200,
+            {
+                "eth_src": self.P2_V200_MAC,
+                "eth_dst": self.P3_V200_MAC,
+                "ipv4_src": "10.0.0.2",
+                "ipv4_dst": "10.0.0.3",
+                "vid": 0x200,
+            },
+        )
         self.update_and_revert_config(
-            self.CONFIG, self.LESS_CONFIG,
-            'warm', before_table_states={self.DP_ID: before_table_state})
+            self.CONFIG,
+            self.LESS_CONFIG,
+            "warm",
+            before_table_states={self.DP_ID: before_table_state},
+        )
 
 
 class ValveDeletePortTestCase(ValveTestBases.ValveTestNetwork):
     """Test deletion of a port."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 tagged_vlans: [0x100]
             p2:
                 number: 2
                 tagged_vlans: [0x100]
             p3:
                 number: 3
                 tagged_vlans: [0x100]
 """
+        % DP1_CONFIG
+    )
 
-    LESS_CONFIG = f"""
+    LESS_CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 tagged_vlans: [0x100]
             p2:
                 number: 2
                 tagged_vlans: [0x100]
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_port_delete(self):
         """Test port can be deleted."""
-        self.update_and_revert_config(self.CONFIG, self.LESS_CONFIG, 'cold')
+        self.update_and_revert_config(self.CONFIG, self.LESS_CONFIG, "cold")
 
 
 class ValveAddPortMirrorNoDelVLANTestCase(ValveTestBases.ValveTestNetwork):
     """Test addition of port mirroring does not cause a del VLAN."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 tagged_vlans: [0x100]
             p2:
                 number: 2
                 tagged_vlans: [0x100]
             p3:
                 number: 3
                 output_only: true
 """
+        % DP1_CONFIG
+    )
 
-    MORE_CONFIG = f"""
+    MORE_CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 tagged_vlans: [0x100]
             p2:
                 number: 2
                 tagged_vlans: [0x100]
             p3:
                 number: 3
                 output_only: true
                 mirror: [1]
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         _ = self.setup_valves(self.CONFIG)[self.DP_ID]
 
     def test_port_mirror(self):
         """Test addition of port mirroring is a warm start."""
-        _ = self.update_config(self.MORE_CONFIG, reload_type='warm')[self.DP_ID]
+        _ = self.update_config(self.MORE_CONFIG, reload_type="warm")[self.DP_ID]
 
 
 class ValveAddPortTestCase(ValveTestBases.ValveTestNetwork):
     """Test addition of a port."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 tagged_vlans: [0x100]
             p2:
                 number: 2
                 tagged_vlans: [0x100]
 """
+        % DP1_CONFIG
+    )
 
-    MORE_CONFIG = f"""
+    MORE_CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 tagged_vlans: [0x100]
             p2:
                 number: 2
                 tagged_vlans: [0x100]
             p3:
                 number: 3
                 tagged_vlans: [0x100]
 """
+        % DP1_CONFIG
+    )
 
     @staticmethod
     def _inport_flows(in_port, ofmsgs):
         return [
-            ofmsg for ofmsg in ValveTestBases.flowmods_from_flows(ofmsgs)
-            if ofmsg.match.get('in_port') == in_port]
+            ofmsg
+            for ofmsg in ValveTestBases.flowmods_from_flows(ofmsgs)
+            if ofmsg.match.get("in_port") == in_port
+        ]
 
     def setUp(self):
         """Setup basic port and vlan config"""
         initial_ofmsgs = self.setup_valves(self.CONFIG)[self.DP_ID]
         self.assertFalse(self._inport_flows(3, initial_ofmsgs))
 
     def test_port_add(self):
         """Test port can be added."""
-        reload_ofmsgs = self.update_config(self.MORE_CONFIG, reload_type='cold')[self.DP_ID]
+        reload_ofmsgs = self.update_config(self.MORE_CONFIG, reload_type="cold")[
+            self.DP_ID
+        ]
         self.assertTrue(self._inport_flows(3, reload_ofmsgs))
 
 
 class ValveAddPortTrafficTestCase(ValveTestBases.ValveTestNetwork):
     """Test addition of a port with traffic."""
 
     # NOTE: This needs to use 'Generic' hardware,
@@ -387,256 +447,289 @@
                 number: 3
                 tagged_vlans: [0x100]
 """
 
     @staticmethod
     def _inport_flows(in_port, ofmsgs):
         return [
-            ofmsg for ofmsg in ValveTestBases.flowmods_from_flows(ofmsgs)
-            if ofmsg.match.get('in_port') == in_port]
+            ofmsg
+            for ofmsg in ValveTestBases.flowmods_from_flows(ofmsgs)
+            if ofmsg.match.get("in_port") == in_port
+        ]
 
     def _learn(self, in_port):
         ucast_pkt = self.pkt_match(in_port, 1)
-        ucast_pkt['in_port'] = in_port
-        ucast_pkt['vlan_vid'] = self.V100
+        ucast_pkt["in_port"] = in_port
+        ucast_pkt["vlan_vid"] = self.V100
 
         table = self.network.tables[self.DP_ID]
         self.assertTrue(table.is_output(ucast_pkt, port=CONTROLLER_PORT))
         self.rcv_packet(in_port, self.V100, ucast_pkt)
 
     def _unicast_between(self, in_port, out_port, not_out=1):
         ucast_match = self.pkt_match(in_port, out_port)
-        ucast_match['in_port'] = in_port
-        ucast_match['vlan_vid'] = self.V100
+        ucast_match["in_port"] = in_port
+        ucast_match["vlan_vid"] = self.V100
 
         table = self.network.tables[self.DP_ID]
         self.assertTrue(table.is_output(ucast_match, port=out_port))
         self.assertFalse(table.is_output(ucast_match, port=not_out))
 
     def setUp(self):
         initial_ofmsgs = self.setup_valves(self.CONFIG)[self.DP_ID]
         self.assertFalse(self._inport_flows(3, initial_ofmsgs))
 
     def test_port_add_no_ofmsgs(self):
         """New config does not generate new flows."""
-        update_ofmsgs = self.update_config(self.MORE_CONFIG,
-                                           reload_type='warm')[self.DP_ID]
+        update_ofmsgs = self.update_config(self.MORE_CONFIG, reload_type="warm")[
+            self.DP_ID
+        ]
         self.assertFalse(self._inport_flows(3, update_ofmsgs))
 
     def test_port_add_link_state(self):
         """New port can be added in link-down state."""
-        self.update_config(self.MORE_CONFIG, reload_type='warm')
+        self.update_config(self.MORE_CONFIG, reload_type="warm")
 
         self.add_port(3, link_up=False)
         self.port_expected_status(3, 0)
 
         self.set_port_link_up(3)
         self.port_expected_status(3, 1)
 
     def test_port_add_traffic(self):
         """New port can be added, and pass traffic."""
-        self.update_config(self.MORE_CONFIG, reload_type='warm')
+        self.update_config(self.MORE_CONFIG, reload_type="warm")
 
         self.add_port(3)
 
         self._learn(2)
         self._learn(3)
 
         self._unicast_between(2, 3)
         self._unicast_between(3, 2)
 
 
 class ValveWarmStartVLANTestCase(ValveTestBases.ValveTestNetwork):
     """Test change of port VLAN only is a warm start."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 9
                 tagged_vlans: [0x100]
             p2:
                 number: 11
                 tagged_vlans: [0x100]
             p3:
                 number: 13
                 tagged_vlans: [0x100]
             p4:
                 number: 14
                 native_vlan: 0x200
 """
+        % DP1_CONFIG
+    )
 
-    WARM_CONFIG = f"""
+    WARM_CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 9
                 tagged_vlans: [0x100]
             p2:
                 number: 11
                 tagged_vlans: [0x100]
             p3:
                 number: 13
                 tagged_vlans: [0x100]
             p4:
                 number: 14
                 native_vlan: 0x300
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_warm_start(self):
         """Test VLAN change is warm startable and metrics maintained."""
-        self.update_and_revert_config(self.CONFIG, self.WARM_CONFIG, 'warm')
-        self.rcv_packet(9, 0x100, {
-            'eth_src': self.P1_V100_MAC,
-            'eth_dst': self.UNKNOWN_MAC,
-            'ipv4_src': '10.0.0.1',
-            'ipv4_dst': '10.0.0.2'})
-        vlan_labels = {'vlan': str(int(0x100))}
-        port_labels = {'port': 'p1', 'port_description': 'p1'}
+        self.update_and_revert_config(self.CONFIG, self.WARM_CONFIG, "warm")
+        self.rcv_packet(
+            9,
+            0x100,
+            {
+                "eth_src": self.P1_V100_MAC,
+                "eth_dst": self.UNKNOWN_MAC,
+                "ipv4_src": "10.0.0.1",
+                "ipv4_dst": "10.0.0.2",
+            },
+        )
+        vlan_labels = {"vlan": str(int(0x100))}
+        port_labels = {"port": "p1", "port_description": "p1"}
         port_labels.update(vlan_labels)
 
         def verify_func():
+            self.assertEqual(1, self.get_prom("vlan_hosts_learned", labels=vlan_labels))
             self.assertEqual(
-                1, self.get_prom('vlan_hosts_learned', labels=vlan_labels))
-            self.assertEqual(
-                1, self.get_prom('port_vlan_hosts_learned', labels=port_labels))
+                1, self.get_prom("port_vlan_hosts_learned", labels=port_labels)
+            )
 
         verify_func()
-        self.update_config(self.WARM_CONFIG, reload_type='warm')
+        self.update_config(self.WARM_CONFIG, reload_type="warm")
         verify_func()
 
 
 class ValveDeleteVLANTestCase(ValveTestBases.ValveTestNetwork):
     """Test deleting VLAN."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 tagged_vlans: [0x100, 0x200]
             p2:
                 number: 2
                 native_vlan: 0x200
 """
+        % DP1_CONFIG
+    )
 
-    LESS_CONFIG = f"""
+    LESS_CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 tagged_vlans: [0x200]
             p2:
                 number: 2
                 native_vlan: 0x200
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_delete_vlan(self):
         """Test VLAN can be deleted."""
-        self.update_and_revert_config(self.CONFIG, self.LESS_CONFIG, 'cold')
+        self.update_and_revert_config(self.CONFIG, self.LESS_CONFIG, "cold")
 
 
 class ValveChangeDPTestCase(ValveTestBases.ValveTestNetwork):
     """Test changing DP."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         priority_offset: 4321
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
             p2:
                 number: 2
                 native_vlan: 0x100
 """
+        % DP1_CONFIG
+    )
 
-    NEW_CONFIG = f"""
+    NEW_CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         priority_offset: 1234
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
             p2:
                 number: 2
                 native_vlan: 0x100
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config with priority offset"""
         self.setup_valves(self.CONFIG)
 
     def test_change_dp(self):
         """Test DP changed."""
-        self.update_and_revert_config(self.CONFIG, self.NEW_CONFIG, 'cold')
+        self.update_and_revert_config(self.CONFIG, self.NEW_CONFIG, "cold")
 
 
 class ValveAddVLANTestCase(ValveTestBases.ValveTestNetwork):
     """Test adding VLAN."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 tagged_vlans: [0x100, 0x200]
             p2:
                 number: 2
                 tagged_vlans: [0x100]
 """
+        % DP1_CONFIG
+    )
 
-    MORE_CONFIG = f"""
+    MORE_CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 tagged_vlans: [0x100, 0x200]
             p2:
                 number: 2
                 tagged_vlans: [0x100, 0x300]
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_add_vlan(self):
         """Test VLAN can added."""
-        self.update_and_revert_config(self.CONFIG, self.MORE_CONFIG, 'cold')
+        self.update_and_revert_config(self.CONFIG, self.MORE_CONFIG, "cold")
 
 
 class ValveChangeACLTestCase(ValveTestBases.ValveTestNetwork):
     """Test changes to ACL on a port."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 acls:
     acl_same_a:
         - rule:
             actions:
                 allow: 1
     acl_same_b:
         - rule:
@@ -644,26 +737,29 @@
                 allow: 1
     acl_diff_c:
         - rule:
             actions:
                 allow: 0
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
                 acl_in: acl_same_a
             p2:
                 number: 2
                 native_vlan: 0x200
 """
+        % DP1_CONFIG
+    )
 
-    SAME_CONTENT_CONFIG = f"""
+    SAME_CONTENT_CONFIG = (
+        """
 acls:
     acl_same_a:
         - rule:
             actions:
                 allow: 1
     acl_same_b:
         - rule:
@@ -671,26 +767,29 @@
                 allow: 1
     acl_diff_c:
         - rule:
             actions:
                 allow: 0
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
                 acl_in: acl_same_b
             p2:
                 number: 2
                 native_vlan: 0x200
 """
+        % DP1_CONFIG
+    )
 
-    DIFF_CONTENT_CONFIG = f"""
+    DIFF_CONTENT_CONFIG = (
+        """
 acls:
     acl_same_a:
         - rule:
             actions:
                 allow: 1
     acl_same_b:
         - rule:
@@ -698,134 +797,155 @@
                 allow: 1
     acl_diff_c:
         - rule:
             actions:
                 allow: 0
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
                 acl_in: acl_diff_c
             p2:
                 number: 2
                 native_vlan: 0x200
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic ACL config"""
         self.setup_valves(self.CONFIG)
 
     def test_change_port_acl(self):
         """Test port ACL can be changed."""
-        self.update_and_revert_config(self.CONFIG, self.SAME_CONTENT_CONFIG, 'warm')
-        self.update_config(self.SAME_CONTENT_CONFIG, reload_type='warm')
-        self.rcv_packet(1, 0x100, {
-            'eth_src': self.P1_V100_MAC,
-            'eth_dst': self.UNKNOWN_MAC,
-            'ipv4_src': '10.0.0.1',
-            'ipv4_dst': '10.0.0.2'})
-        vlan_labels = {'vlan': str(int(0x100))}
-        port_labels = {'port': 'p1', 'port_description': 'p1'}
+        self.update_and_revert_config(self.CONFIG, self.SAME_CONTENT_CONFIG, "warm")
+        self.update_config(self.SAME_CONTENT_CONFIG, reload_type="warm")
+        self.rcv_packet(
+            1,
+            0x100,
+            {
+                "eth_src": self.P1_V100_MAC,
+                "eth_dst": self.UNKNOWN_MAC,
+                "ipv4_src": "10.0.0.1",
+                "ipv4_dst": "10.0.0.2",
+            },
+        )
+        vlan_labels = {"vlan": str(int(0x100))}
+        port_labels = {"port": "p1", "port_description": "p1"}
         port_labels.update(vlan_labels)
 
         def verify_func():
+            self.assertEqual(1, self.get_prom("vlan_hosts_learned", labels=vlan_labels))
             self.assertEqual(
-                1, self.get_prom('vlan_hosts_learned', labels=vlan_labels))
-            self.assertEqual(
-                1, self.get_prom('port_vlan_hosts_learned', labels=port_labels))
+                1, self.get_prom("port_vlan_hosts_learned", labels=port_labels)
+            )
 
         verify_func()
         # ACL changed but we kept the learn cache.
-        self.update_config(self.DIFF_CONTENT_CONFIG, reload_type='warm')
+        self.update_config(self.DIFF_CONTENT_CONFIG, reload_type="warm")
         verify_func()
 
 
 class ValveChangeMirrorTestCase(ValveTestBases.ValveTestNetwork):
     """Test changes mirroring port."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
             p2:
                 number: 2
                 output_only: True
             p3:
                 number: 3
                 native_vlan: 0x200
 """
+        % DP1_CONFIG
+    )
 
-    MIRROR_CONFIG = f"""
+    MIRROR_CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
             p2:
                 number: 2
                 mirror: p1
             p3:
                 number: 3
                 native_vlan: 0x200
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_change_port_acl(self):
         """Test port ACL can be changed."""
-        self.update_and_revert_config(self.CONFIG, self.MIRROR_CONFIG, reload_type='warm')
+        self.update_and_revert_config(
+            self.CONFIG, self.MIRROR_CONFIG, reload_type="warm"
+        )
 
-        vlan_labels = {'vlan': str(int(0x100))}
-        port_labels = {'port': 'p1', 'port_description': 'p1'}
+        vlan_labels = {"vlan": str(int(0x100))}
+        port_labels = {"port": "p1", "port_description": "p1"}
         port_labels.update(vlan_labels)
 
         def verify_prom():
+            self.assertEqual(1, self.get_prom("vlan_hosts_learned", labels=vlan_labels))
             self.assertEqual(
-                1, self.get_prom('vlan_hosts_learned', labels=vlan_labels))
-            self.assertEqual(
-                1, self.get_prom('port_vlan_hosts_learned', labels=port_labels))
+                1, self.get_prom("port_vlan_hosts_learned", labels=port_labels)
+            )
 
-        self.rcv_packet(1, 0x100, {
-            'eth_src': self.P1_V100_MAC,
-            'eth_dst': self.UNKNOWN_MAC,
-            'ipv4_src': '10.0.0.1',
-            'ipv4_dst': '10.0.0.2'})
+        self.rcv_packet(
+            1,
+            0x100,
+            {
+                "eth_src": self.P1_V100_MAC,
+                "eth_dst": self.UNKNOWN_MAC,
+                "ipv4_src": "10.0.0.1",
+                "ipv4_dst": "10.0.0.2",
+            },
+        )
 
         verify_prom()
         # Now mirroring port 1 but we kept the cache.
-        self.update_config(self.MIRROR_CONFIG, reload_type='warm')
+        self.update_config(self.MIRROR_CONFIG, reload_type="warm")
         verify_prom()
         # Now unmirror again.
-        self.update_config(self.CONFIG, reload_type='warm')
+        self.update_config(self.CONFIG, reload_type="warm")
         verify_prom()
 
 
 class ValveACLTestCase(ValveTestBases.ValveTestNetwork):
     """Test ACL drop/allow and reloading."""
 
     def setUp(self):
         self.setup_valves(CONFIG)
 
     def test_vlan_acl_deny(self):
         """Test VLAN ACL denies a packet."""
-        acl_config = f"""
+        acl_config = (
+            """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
             p2:
                 number: 2
                 native_vlan: v200
@@ -855,57 +975,65 @@
             actions:
                 allow: 1
         - rule:
             dl_type: 0x800
             actions:
                 allow: 0
 """
+            % DP1_CONFIG
+        )
 
         drop_match = {
-            'in_port': 2,
-            'vlan_vid': 0,
-            'eth_type': 0x800,
-            'ipv4_dst': '192.0.2.1'}
+            "in_port": 2,
+            "vlan_vid": 0,
+            "eth_type": 0x800,
+            "ipv4_dst": "192.0.2.1",
+        }
         accept_match = {
-            'in_port': 2,
-            'vlan_vid': 0,
-            'eth_type': 0x800,
-            'ipv4_dst': '224.0.0.5'}
+            "in_port": 2,
+            "vlan_vid": 0,
+            "eth_type": 0x800,
+            "ipv4_dst": "224.0.0.5",
+        }
         table = self.network.tables[self.DP_ID]
 
         # base case
         for match in (drop_match, accept_match):
             self.assertTrue(
                 table.is_output(match, port=3, vid=self.V200),
-                msg='Packet not output before adding ACL')
+                msg="Packet not output before adding ACL",
+            )
 
         def verify_func():
             self.flap_port(2)
             self.assertFalse(
-                table.is_output(drop_match), msg='Packet not blocked by ACL')
+                table.is_output(drop_match), msg="Packet not blocked by ACL"
+            )
             self.assertTrue(
                 table.is_output(accept_match, port=3, vid=self.V200),
-                msg='Packet not allowed by ACL')
+                msg="Packet not allowed by ACL",
+            )
 
         self.update_and_revert_config(
-            CONFIG, acl_config, reload_type='cold', verify_func=verify_func)
+            CONFIG, acl_config, reload_type="cold", verify_func=verify_func
+        )
 
 
 class ValveEgressACLTestCase(ValveTestBases.ValveTestNetwork):
     """Test ACL drop/allow and reloading."""
 
     def setUp(self):
         self.setup_valves(CONFIG)
 
     def test_vlan_acl_deny(self):
         """Test VLAN ACL denies a packet."""
-        allow_host_v6 = 'fc00:200::1:1'
-        deny_host_v6 = 'fc00:200::1:2'
-        faucet_v100_vip = 'fc00:100::1'
-        faucet_v200_vip = 'fc00:200::1'
+        allow_host_v6 = "fc00:200::1:1"
+        deny_host_v6 = "fc00:200::1:2"
+        faucet_v100_vip = "fc00:100::1"
+        faucet_v200_vip = "fc00:200::1"
         acl_config = """
 dps:
     s1:
 {dp1_config}
         interfaces:
             p1:
                 number: 1
@@ -941,239 +1069,288 @@
             eth_type: 0x86DD
             actions:
                 allow: 1
         - rule:
             eth_type: 0x86DD
             actions:
                 allow: 0
-""".format(dp1_config=DP1_CONFIG, mac=FAUCET_MAC, v100_vip=faucet_v100_vip,
-           v200_vip=faucet_v200_vip, allow_host=allow_host_v6)
+""".format(
+            dp1_config=DP1_CONFIG,
+            mac=FAUCET_MAC,
+            v100_vip=faucet_v100_vip,
+            v200_vip=faucet_v200_vip,
+            allow_host=allow_host_v6,
+        )
 
         l2_drop_match = {
-            'in_port': 2,
-            'eth_dst': self.P3_V200_MAC,
-            'vlan_vid': 0,
-            'eth_type': 0x86DD,
-            'ipv6_dst': deny_host_v6}
+            "in_port": 2,
+            "eth_dst": self.P3_V200_MAC,
+            "vlan_vid": 0,
+            "eth_type": 0x86DD,
+            "ipv6_dst": deny_host_v6,
+        }
         l2_accept_match = {
-            'in_port': 3,
-            'eth_dst': self.P2_V200_MAC,
-            'vlan_vid': 0x200 | ofp.OFPVID_PRESENT,
-            'eth_type': 0x86DD,
-            'ipv6_dst': allow_host_v6}
-        v100_accept_match = {'in_port': 1, 'vlan_vid': 0}
+            "in_port": 3,
+            "eth_dst": self.P2_V200_MAC,
+            "vlan_vid": 0x200 | ofp.OFPVID_PRESENT,
+            "eth_type": 0x86DD,
+            "ipv6_dst": allow_host_v6,
+        }
+        v100_accept_match = {"in_port": 1, "vlan_vid": 0}
         table = self.network.tables[self.DP_ID]
 
         # base case
         for match in (l2_drop_match, l2_accept_match):
             self.assertTrue(
                 table.is_output(match, port=4),
-                msg='Packet not output before adding ACL')
+                msg="Packet not output before adding ACL",
+            )
 
         def verify_func():
             self.assertTrue(
                 table.is_output(v100_accept_match, port=3),
-                msg='Packet not output when on vlan with no ACL')
+                msg="Packet not output when on vlan with no ACL",
+            )
             self.assertFalse(
-                table.is_output(l2_drop_match, port=3),
-                msg='Packet not blocked by ACL')
+                table.is_output(l2_drop_match, port=3), msg="Packet not blocked by ACL"
+            )
             self.assertTrue(
                 table.is_output(l2_accept_match, port=2),
-                msg='Packet not allowed by ACL')
+                msg="Packet not allowed by ACL",
+            )
 
             # unicast
-            self.rcv_packet(2, 0x200, {
-                'eth_src': self.P2_V200_MAC,
-                'eth_dst': self.P3_V200_MAC,
-                'vid': 0x200,
-                'ipv6_src': allow_host_v6,
-                'ipv6_dst': deny_host_v6,
-                'neighbor_advert_ip': allow_host_v6})
-            self.rcv_packet(3, 0x200, {
-                'eth_src': self.P3_V200_MAC,
-                'eth_dst': self.P2_V200_MAC,
-                'vid': 0x200,
-                'ipv6_src': deny_host_v6,
-                'ipv6_dst': allow_host_v6,
-                'neighbor_advert_ip': deny_host_v6})
+            self.rcv_packet(
+                2,
+                0x200,
+                {
+                    "eth_src": self.P2_V200_MAC,
+                    "eth_dst": self.P3_V200_MAC,
+                    "vid": 0x200,
+                    "ipv6_src": allow_host_v6,
+                    "ipv6_dst": deny_host_v6,
+                    "neighbor_advert_ip": allow_host_v6,
+                },
+            )
+            self.rcv_packet(
+                3,
+                0x200,
+                {
+                    "eth_src": self.P3_V200_MAC,
+                    "eth_dst": self.P2_V200_MAC,
+                    "vid": 0x200,
+                    "ipv6_src": deny_host_v6,
+                    "ipv6_dst": allow_host_v6,
+                    "neighbor_advert_ip": deny_host_v6,
+                },
+            )
 
             self.assertTrue(
                 table.is_output(l2_accept_match, port=2),
-                msg='Packet not allowed by ACL')
+                msg="Packet not allowed by ACL",
+            )
             self.assertFalse(
-                table.is_output(l2_drop_match, port=3),
-                msg='Packet not blocked by ACL')
+                table.is_output(l2_drop_match, port=3), msg="Packet not blocked by ACL"
+            )
 
             # l3
             l3_drop_match = {
-                'in_port': 1,
-                'eth_dst': FAUCET_MAC,
-                'vlan_vid': 0,
-                'eth_type': 0x86DD,
-                'ipv6_dst': deny_host_v6}
+                "in_port": 1,
+                "eth_dst": FAUCET_MAC,
+                "vlan_vid": 0,
+                "eth_type": 0x86DD,
+                "ipv6_dst": deny_host_v6,
+            }
             l3_accept_match = {
-                'in_port': 1,
-                'eth_dst': FAUCET_MAC,
-                'vlan_vid': 0,
-                'eth_type': 0x86DD,
-                'ipv6_dst': allow_host_v6}
+                "in_port": 1,
+                "eth_dst": FAUCET_MAC,
+                "vlan_vid": 0,
+                "eth_type": 0x86DD,
+                "ipv6_dst": allow_host_v6,
+            }
 
             self.assertTrue(
                 table.is_output(l3_accept_match, port=2),
-                msg='Routed packet not allowed by ACL')
+                msg="Routed packet not allowed by ACL",
+            )
             self.assertFalse(
                 table.is_output(l3_drop_match, port=3),
-                msg='Routed packet not blocked by ACL')
+                msg="Routed packet not blocked by ACL",
+            )
 
         # multicast
-        self.update_and_revert_config(CONFIG, acl_config, 'cold', verify_func=verify_func)
+        self.update_and_revert_config(
+            CONFIG, acl_config, "cold", verify_func=verify_func
+        )
 
 
 class ValveReloadConfigProfile(ValveTestBases.ValveTestNetwork):
     """Test reload processing time."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{BASE_DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
 """
+        % BASE_DP1_CONFIG
+    )
     NUM_PORTS = 100
 
     baseline_total_tt = None
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(CONFIG)
 
     def test_profile_reload(self):
         """Test reload processing time."""
         orig_config = copy.copy(self.CONFIG)
 
         def load_orig_config():
-            pstats_out, _ = self.profile(
-                partial(self.update_config, orig_config))
-            self.baseline_total_tt = pstats_out.total_tt  # pytype: disable=attribute-error
+            pstats_out, _ = self.profile(partial(self.update_config, orig_config))
+            self.baseline_total_tt = (
+                pstats_out.total_tt
+            )  # pytype: disable=attribute-error
 
         for i in range(2, 100):
             self.CONFIG += """
             p%u:
                 number: %u
                 native_vlan: 0x100
-""" % (i, i)
+""" % (
+                i,
+                i,
+            )
 
         for i in range(5):
             load_orig_config()
             pstats_out, pstats_text = self.profile(
-                partial(self.update_config, self.CONFIG, reload_type='cold'))
+                partial(self.update_config, self.CONFIG, reload_type="cold")
+            )
+            cache_info = valve_of.output_non_output_actions.cache_info()
+            self.assertGreater(cache_info.hits, cache_info.misses, msg=cache_info)
             total_tt_prop = (
-                pstats_out.total_tt / self.baseline_total_tt)  # pytype: disable=attribute-error
+                pstats_out.total_tt / self.baseline_total_tt
+            )  # pytype: disable=attribute-error
             # must not be 20x slower, to ingest config for 100 interfaces than 1.
             # TODO: This test might have to be run separately,
             # since it is marginal on GitHub actions due to parallel test runs.
             if total_tt_prop < 20:
                 for valve in self.valves_manager.valves.values():
                     for table in valve.dp.tables.values():
-                        for cacheable_func in (table._trim_inst, table._verify_flowmod):  # pylint: disable=protected-access
-                            cache_info = cacheable_func.cache_info()
-                            self.assertGreater(cache_info.hits, 0, msg=(table.name, cacheable_func, cache_info))
+                        cache_info = (
+                            table._trim_inst.cache_info()
+                        )  # pylint: disable=protected-access
+                        self.assertGreater(
+                            cache_info.hits, cache_info.misses, msg=cache_info
+                        )
                 return
             time.sleep(i)
 
-        self.fail('{total_tt_prop}: {pstats_text}')
+        self.fail("%f: %s" % (total_tt_prop, pstats_text))
 
 
 class ValveTestVLANRef(ValveTestBases.ValveTestNetwork):
     """Test reference to same VLAN by name or VID."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 333
             p2:
                 number: 2
                 native_vlan: threes
 vlans:
     threes:
         vid: 333
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def test_vlan_refs(self):
         """Test same VLAN is referred to."""
         vlans = self.valves_manager.valves[self.DP_ID].dp.vlans
         self.assertEqual(1, len(vlans))
-        self.assertEqual('threes', vlans[333].name, vlans[333])
+        self.assertEqual("threes", vlans[333].name, vlans[333])
         self.assertEqual(2, len(vlans[333].untagged))
 
 
 class ValveTestConfigHash(ValveTestBases.ValveTestNetwork):
     """Verify faucet_config_hash_info update after config change"""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DP1_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
 """
+        % DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def _get_info(self, metric, name):
-        """"Return (single) info dict for metric"""
+        """ "Return (single) info dict for metric"""
         # There doesn't seem to be a nice API for this,
         # so we use the prometheus client internal API
         metrics = list(metric.collect())
         self.assertEqual(len(metrics), 1)
         samples = metrics[0].samples
         self.assertEqual(len(samples), 1)
         sample = samples[0]
         self.assertEqual(sample.name, name)
         return sample.labels
 
     def _check_hashes(self):
         """Verify and return faucet_config_hash_info labels"""
-        labels = self._get_info(metric=self.metrics.faucet_config_hash,
-                                name='faucet_config_hash_info')
-        files = labels['config_files'].split(',')
-        hashes = labels['hashes'].split(',')
+        labels = self._get_info(
+            metric=self.metrics.faucet_config_hash, name="faucet_config_hash_info"
+        )
+        files = labels["config_files"].split(",")
+        hashes = labels["hashes"].split(",")
         self.assertTrue(len(files) == len(hashes) == 1)
-        self.assertEqual(files[0], self.config_file, 'wrong config file')
+        self.assertEqual(files[0], self.config_file, "wrong config file")
         hash_value = config_parser_util.config_file_hash(self.config_file)
-        self.assertEqual(hashes[0], hash_value, 'hash validation failed')
+        self.assertEqual(hashes[0], hash_value, "hash validation failed")
         return labels
 
     def _change_config(self):
         """Change self.CONFIG"""
-        if '0x100' in self.CONFIG:
-            self.CONFIG = self.CONFIG.replace('0x100', '0x200')
+        if "0x100" in self.CONFIG:
+            self.CONFIG = self.CONFIG.replace("0x100", "0x200")
         else:
-            self.CONFIG = self.CONFIG.replace('0x200', '0x100')
+            self.CONFIG = self.CONFIG.replace("0x200", "0x100")
         self.update_config(self.CONFIG, reload_expected=True)
         return self.CONFIG
 
     def test_config_hash_func(self):
         """Verify that faucet_config_hash_func is set correctly"""
-        labels = self._get_info(metric=self.metrics.faucet_config_hash_func,
-                                name='faucet_config_hash_func')
+        labels = self._get_info(
+            metric=self.metrics.faucet_config_hash_func, name="faucet_config_hash_func"
+        )
         hash_funcs = list(labels.values())
         self.assertEqual(len(hash_funcs), 1, "found multiple hash functions")
         hash_func = hash_funcs[0]
         # Make sure that it matches and is supported in hashlib
         self.assertEqual(hash_func, config_parser_util.CONFIG_HASH_FUNC)
         self.assertTrue(hash_func in hashlib.algorithms_guaranteed)
 
@@ -1181,29 +1358,30 @@
         """Verify faucet_config_hash_info is properly updated after config"""
         # Verify that hashes change after config is changed
         old_config = self.CONFIG
         old_hashes = self._check_hashes()
         starting_hashes = old_hashes
         self._change_config()
         new_config = self.CONFIG
-        self.assertNotEqual(old_config, new_config, 'config not changed')
+        self.assertNotEqual(old_config, new_config, "config not changed")
         new_hashes = self._check_hashes()
-        self.assertNotEqual(old_hashes, new_hashes,
-                            'hashes not changed after config change')
+        self.assertNotEqual(
+            old_hashes, new_hashes, "hashes not changed after config change"
+        )
         # Verify that hashes don't change after config isn't changed
         old_hashes = new_hashes
         self.update_config(self.CONFIG, reload_expected=False)
         new_hashes = self._check_hashes()
-        self.assertEqual(old_hashes, new_hashes,
-                         "hashes changed when config didn't")
+        self.assertEqual(old_hashes, new_hashes, "hashes changed when config didn't")
         # Verify that hash is restored when config is restored
         self._change_config()
         new_hashes = self._check_hashes()
-        self.assertEqual(new_hashes, starting_hashes,
-                         'hashes should be restored to starting values')
+        self.assertEqual(
+            new_hashes, starting_hashes, "hashes should be restored to starting values"
+        )
 
 
 class ValveTestConfigRevert(ValveTestBases.ValveTestNetwork):
     """Test configuration revert"""
 
     CONFIG = """
 dps:
@@ -1220,27 +1398,32 @@
 
     def setUp(self):
         """Setup basic port and vlan config with hardware type set"""
         self.setup_valves(self.CONFIG)
 
     def test_config_revert(self):
         """Verify config is automatically reverted if bad."""
-        self.assertEqual(self.get_prom('faucet_config_load_error', bare=True), 0)
-        self.update_config('***broken***', reload_expected=True, error_expected=1)
-        self.assertEqual(self.get_prom('faucet_config_load_error', bare=True), 1)
-        with open(self.config_file, 'r', encoding='utf-8') as config_file:
+        self.assertEqual(self.get_prom("faucet_config_load_error", bare=True), 0)
+        self.update_config("***broken***", reload_expected=True, error_expected=1)
+        self.assertEqual(self.get_prom("faucet_config_load_error", bare=True), 1)
+        with open(self.config_file, "r", encoding="utf-8") as config_file:
             config_content = config_file.read()
         self.assertEqual(self.CONFIG, config_content)
-        self.update_config(self.CONFIG + '\n', reload_expected=False, error_expected=0)
-        more_config = self.CONFIG + """
+        self.update_config(self.CONFIG + "\n", reload_expected=False, error_expected=0)
+        more_config = (
+            self.CONFIG
+            + """
             p2:
                 number: 2
                 native_vlan: 0x100
         """
-        self.update_config(more_config, reload_expected=True, reload_type='warm', error_expected=0)
+        )
+        self.update_config(
+            more_config, reload_expected=True, reload_type="warm", error_expected=0
+        )
 
 
 class ValveTestConfigRevertBootstrap(ValveTestBases.ValveTestNetwork):
     """Test configuration auto reverted if bad"""
 
     BAD_CONFIG = """
     *** busted ***
@@ -1260,17 +1443,19 @@
 
     def setUp(self):
         """Setup invalid config"""
         self.setup_valves(self.BAD_CONFIG, error_expected=1)
 
     def test_config_revert(self):
         """Verify config is automatically reverted if bad."""
-        self.assertEqual(self.get_prom('faucet_config_load_error', bare=True), 1)
-        self.update_config(self.GOOD_CONFIG + '\n', reload_expected=False, error_expected=0)
-        self.assertEqual(self.get_prom('faucet_config_load_error', bare=True), 0)
+        self.assertEqual(self.get_prom("faucet_config_load_error", bare=True), 1)
+        self.update_config(
+            self.GOOD_CONFIG + "\n", reload_expected=False, error_expected=0
+        )
+        self.assertEqual(self.get_prom("faucet_config_load_error", bare=True), 0)
 
 
 class ValveTestConfigApplied(ValveTestBases.ValveTestNetwork):
     """Test cases for faucet_config_applied."""
 
     CONFIG = """
 dps:
@@ -1298,43 +1483,45 @@
     def setUp(self):
         """Setup basic port and vlan config with hardware type set"""
         self.setup_valves(self.CONFIG)
 
     def test_config_applied_update(self):
         """Verify that config_applied increments after DP connect"""
         # 100% for a single datapath
-        self.assertEqual(self.get_prom('faucet_config_applied', bare=True), 1.0)
+        self.assertEqual(self.get_prom("faucet_config_applied", bare=True), 1.0)
         # Add a second datapath, which currently isn't programmed
         self.CONFIG += """
     s2:
         dp_id: 0x2
         hardware: 'GenericTFM'
         interfaces:
             p1:
                 number: 1
                 native_vlan: 0x100
 """
         self.update_config(self.CONFIG, reload_expected=False)
         # Should be 50%
-        self.assertEqual(self.get_prom('faucet_config_applied', bare=True), .5)
+        self.assertEqual(self.get_prom("faucet_config_applied", bare=True), 0.5)
         # We don't have a way to simulate the second datapath connecting,
         # we update the statistic manually
         self.valves_manager.update_config_applied({0x2: True})
         # Should be 100% now
-        self.assertEqual(self.get_prom('faucet_config_applied', bare=True), 1.0)
+        self.assertEqual(self.get_prom("faucet_config_applied", bare=True), 1.0)
 
     def test_description_only(self):
         """Test updating config description"""
         self.update_config(self.NEW_DESCR_CONFIG, reload_expected=False)
 
 
-class ValveReloadConfigTestCase(ValveTestBases.ValveTestBig):  # pylint: disable=too-few-public-methods
+class ValveReloadConfigTestCase(
+    ValveTestBases.ValveTestBig
+):  # pylint: disable=too-few-public-methods
     """Repeats the tests after a config reload."""
 
     def setUp(self):
         super().setUp()
         self.flap_port(1)
-        self.update_config(CONFIG, reload_type='warm', reload_expected=False)
+        self.update_config(CONFIG, reload_type="warm", reload_expected=False)
 
 
 if __name__ == "__main__":
     unittest.main()  # pytype: disable=module-attr
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_valve_dot1x.py` & `c65faucet-1.0.47/tests/unit/faucet/test_valve_dot1x.py`

 * *Files 5% similar despite different names*

```diff
@@ -23,18 +23,19 @@
 
 from clib.valve_test_lib import DOT1X_CONFIG, DOT1X_ACL_CONFIG, ValveTestBases
 
 
 class ValveDot1xSmokeTestCase(ValveTestBases.ValveTestNetwork):
     """Smoke test to check dot1x can be initialized."""
 
-    CONFIG = f"""
+    CONFIG = (
+        """
 dps:
     s1:
-{DOT1X_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
                 dot1x: true
             p2:
                 number: 2
@@ -43,34 +44,39 @@
     v100:
         vid: 0x100
     student:
         vid: 0x200
         dot1x_assigned: True
 
 """
+        % DOT1X_CONFIG
+    )
 
     def setUp(self):
         """Setup basic 802.1x config"""
         self.setup_valves(self.CONFIG)
 
     def test_get_mac_str(self):
         """Test NFV port formatter."""
-        self.assertEqual('00:00:00:0f:01:01', faucet_dot1x.get_mac_str(15, 257))
+        self.assertEqual("00:00:00:0f:01:01", faucet_dot1x.get_mac_str(15, 257))
 
     def test_handlers(self):
         """Test dot1x logoff/failure handlers."""
         valve_index = self.dot1x.dp_id_to_valve_index[self.DP_ID]
         port_no = 1
-        vlan_name = 'student'
-        filter_id = 'block_http'
+        vlan_name = "student"
+        filter_id = "block_http"
         for handler in (self.dot1x.logoff_handler, self.dot1x.failure_handler):
-            handler('0e:00:00:00:00:ff', faucet_dot1x.get_mac_str(valve_index, port_no))
+            handler("0e:00:00:00:00:ff", faucet_dot1x.get_mac_str(valve_index, port_no))
         self.dot1x.auth_handler(
-            '0e:00:00:00:00:ff', faucet_dot1x.get_mac_str(valve_index, port_no),
-            vlan_name=vlan_name, filter_id=filter_id)
+            "0e:00:00:00:00:ff",
+            faucet_dot1x.get_mac_str(valve_index, port_no),
+            vlan_name=vlan_name,
+            filter_id=filter_id,
+        )
 
 
 class ValveDot1xACLSmokeTestCase(ValveDot1xSmokeTestCase):
     """Smoke test to check dot1x can be initialized with dot1x ACLs."""
 
     ACL_CONFIG = """
 acls:
@@ -80,19 +86,19 @@
                 allow: 1
     noauth_acl:
         - rule:
             actions:
                 allow: 0
 """
 
-    CONFIG = f"""
-{ACL_CONFIG}
+    CONFIG = """
+{}
 dps:
     s1:
-{DOT1X_ACL_CONFIG}
+{}
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
                 dot1x: true
                 dot1x_acl: True
             p2:
@@ -100,42 +106,48 @@
                 output_only: True
 vlans:
     v100:
         vid: 0x100
     student:
         vid: 0x200
         dot1x_assigned: True
-"""
+""".format(
+        ACL_CONFIG, DOT1X_ACL_CONFIG
+    )
 
 
 class ValveDot1xMABSmokeTestCase(ValveDot1xSmokeTestCase):
     """Smoke test to check dot1x can be initialized with dot1x MAB."""
 
-    CONFIG = f"""
+    CONFIG = """
 dps:
     s1:
-{DOT1X_CONFIG}
+{}
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
                 dot1x: true
                 dot1x_mab: True
             p2:
                 number: 2
                 output_only: True
 vlans:
     v100:
         vid: 0x100
-"""
+""".format(
+        DOT1X_CONFIG
+    )
 
 
 class ValveDot1xDynACLSmokeTestCase(ValveDot1xSmokeTestCase):
     """Smoke test to check dot1x can be initialized with dynamic dot1x ACLs."""
-    CONFIG = f"""
+
+    CONFIG = (
+        """
 acls:
     accept_acl:
         dot1x_assigned: True
         rules:
         - rule:
             dl_type: 0x800      # Allow ICMP / IPv4
             ip_proto: 1
@@ -143,43 +155,45 @@
                 allow: True
         - rule:
             dl_type: 0x0806     # ARP Packets
             actions:
                 allow: True
 dps:
     s1:
-{DOT1X_CONFIG}
+%s
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
                 dot1x: true
                 dot1x_dyn_acl: True
 
             p2:
                 number: 2
                 output_only: True
 vlans:
     v100:
         vid: 0x100
 """
+        % DOT1X_CONFIG
+    )
 
     def setUp(self):
         self.setup_valves(self.CONFIG)
 
     def test_handlers(self):
         valve_index = self.dot1x.dp_id_to_valve_index[self.DP_ID]
         port_no = 1
         vlan_name = None
-        filter_id = 'accept_acl'
-        for handler in (
-                self.dot1x.logoff_handler,
-                self.dot1x.failure_handler):
-            handler(
-                '0e:00:00:00:00:ff', faucet_dot1x.get_mac_str(valve_index, port_no))
+        filter_id = "accept_acl"
+        for handler in (self.dot1x.logoff_handler, self.dot1x.failure_handler):
+            handler("0e:00:00:00:00:ff", faucet_dot1x.get_mac_str(valve_index, port_no))
         self.dot1x.auth_handler(
-            '0e:00:00:00:00:ff', faucet_dot1x.get_mac_str(valve_index, port_no),
-            vlan_name=vlan_name, filter_id=filter_id)
+            "0e:00:00:00:00:ff",
+            faucet_dot1x.get_mac_str(valve_index, port_no),
+            vlan_name=vlan_name,
+            filter_id=filter_id,
+        )
 
 
 if __name__ == "__main__":
     unittest.main()  # pytype: disable=module-attr
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_valve_egress.py` & `c65faucet-1.0.47/tests/unit/faucet/test_valve_egress.py`

 * *Files 22% similar despite different names*

```diff
@@ -21,34 +21,39 @@
 import unittest
 
 from os_ken.ofproto import ofproto_v1_3 as ofp
 
 from clib.valve_test_lib import CONFIG, DP1_CONFIG, FAUCET_MAC, ValveTestBases
 
 
-class ValveTestEgressPipeline(ValveTestBases.ValveTestBig):  # pylint: disable=too-few-public-methods
+class ValveTestEgressPipeline(
+    ValveTestBases.ValveTestBig
+):  # pylint: disable=too-few-public-methods
     """Run complete set of basic tests."""
 
-    DP1_CONFIG = """
+    DP1_CONFIG = (
+        """
             egress_pipeline: True
-    """ + DP1_CONFIG
+    """
+        + DP1_CONFIG
+    )
 
 
 class ValveEgressACLTestCase(ValveTestBases.ValveTestNetwork):
     """Test ACL drop/allow and reloading."""
 
     def setUp(self):
         self.setup_valves(CONFIG)
 
     def test_vlan_acl_deny(self):
         """Test VLAN ACL denies a packet."""
-        ALLOW_HOST_V6 = 'fc00:200::1:1'  # pylint: disable=invalid-name
-        DENY_HOST_V6 = 'fc00:200::1:2'  # pylint: disable=invalid-name
-        FAUCET_V100_VIP = 'fc00:100::1'  # pylint: disable=invalid-name
-        FAUCET_V200_VIP = 'fc00:200::1'  # pylint: disable=invalid-name
+        ALLOW_HOST_V6 = "fc00:200::1:1"  # pylint: disable=invalid-name
+        DENY_HOST_V6 = "fc00:200::1:2"  # pylint: disable=invalid-name
+        FAUCET_V100_VIP = "fc00:100::1"  # pylint: disable=invalid-name
+        FAUCET_V200_VIP = "fc00:200::1"  # pylint: disable=invalid-name
         acl_config = """
 dps:
     s1:
 {dp1_config}
         interfaces:
             p1:
                 number: 1
@@ -84,93 +89,113 @@
             eth_type: 0x86DD
             actions:
                 allow: 1
         - rule:
             eth_type: 0x86DD
             actions:
                 allow: 0
-""".format(dp1_config=DP1_CONFIG, mac=FAUCET_MAC, v100_vip=FAUCET_V100_VIP,
-           v200_vip=FAUCET_V200_VIP, allow_host=ALLOW_HOST_V6)
+""".format(
+            dp1_config=DP1_CONFIG,
+            mac=FAUCET_MAC,
+            v100_vip=FAUCET_V100_VIP,
+            v200_vip=FAUCET_V200_VIP,
+            allow_host=ALLOW_HOST_V6,
+        )
 
         l2_drop_match = {
-            'in_port': 2,
-            'eth_dst': self.P3_V200_MAC,
-            'vlan_vid': 0,
-            'eth_type': 0x86DD,
-            'ipv6_dst': DENY_HOST_V6}
+            "in_port": 2,
+            "eth_dst": self.P3_V200_MAC,
+            "vlan_vid": 0,
+            "eth_type": 0x86DD,
+            "ipv6_dst": DENY_HOST_V6,
+        }
         l2_accept_match = {
-            'in_port': 3,
-            'eth_dst': self.P2_V200_MAC,
-            'vlan_vid': 0x200 | ofp.OFPVID_PRESENT,
-            'eth_type': 0x86DD,
-            'ipv6_dst': ALLOW_HOST_V6}
-        v100_accept_match = {'in_port': 1, 'vlan_vid': 0}
+            "in_port": 3,
+            "eth_dst": self.P2_V200_MAC,
+            "vlan_vid": 0x200 | ofp.OFPVID_PRESENT,
+            "eth_type": 0x86DD,
+            "ipv6_dst": ALLOW_HOST_V6,
+        }
+        v100_accept_match = {"in_port": 1, "vlan_vid": 0}
         table = self.network.tables[self.DP_ID]
 
         # base case
         for match in (l2_drop_match, l2_accept_match):
             self.assertTrue(
                 table.is_output(match, port=4),
-                msg='Packet not output before adding ACL')
+                msg="Packet not output before adding ACL",
+            )
 
         # multicast
-        self.update_config(acl_config, reload_type='cold')
+        self.update_config(acl_config, reload_type="cold")
         self.assertTrue(
             table.is_output(v100_accept_match, port=3),
-            msg='Packet not output when on vlan with no ACL'
+            msg="Packet not output when on vlan with no ACL",
         )
         self.assertFalse(
-            table.is_output(l2_drop_match, port=3),
-            msg='Packet not blocked by ACL')
+            table.is_output(l2_drop_match, port=3), msg="Packet not blocked by ACL"
+        )
         self.assertTrue(
-            table.is_output(l2_accept_match, port=2),
-            msg='Packet not allowed by ACL')
+            table.is_output(l2_accept_match, port=2), msg="Packet not allowed by ACL"
+        )
 
         # unicast
-        self.rcv_packet(2, 0x200, {
-            'eth_src': self.P2_V200_MAC,
-            'eth_dst': self.P3_V200_MAC,
-            'vid': 0x200,
-            'ipv6_src': ALLOW_HOST_V6,
-            'ipv6_dst': DENY_HOST_V6,
-            'neighbor_advert_ip': ALLOW_HOST_V6,
-        })
-        self.rcv_packet(3, 0x200, {
-            'eth_src': self.P3_V200_MAC,
-            'eth_dst': self.P2_V200_MAC,
-            'vid': 0x200,
-            'ipv6_src': DENY_HOST_V6,
-            'ipv6_dst': ALLOW_HOST_V6,
-            'neighbor_advert_ip': DENY_HOST_V6,
-        })
+        self.rcv_packet(
+            2,
+            0x200,
+            {
+                "eth_src": self.P2_V200_MAC,
+                "eth_dst": self.P3_V200_MAC,
+                "vid": 0x200,
+                "ipv6_src": ALLOW_HOST_V6,
+                "ipv6_dst": DENY_HOST_V6,
+                "neighbor_advert_ip": ALLOW_HOST_V6,
+            },
+        )
+        self.rcv_packet(
+            3,
+            0x200,
+            {
+                "eth_src": self.P3_V200_MAC,
+                "eth_dst": self.P2_V200_MAC,
+                "vid": 0x200,
+                "ipv6_src": DENY_HOST_V6,
+                "ipv6_dst": ALLOW_HOST_V6,
+                "neighbor_advert_ip": DENY_HOST_V6,
+            },
+        )
 
         self.assertTrue(
-            table.is_output(l2_accept_match, port=2),
-            msg='Packet not allowed by ACL')
+            table.is_output(l2_accept_match, port=2), msg="Packet not allowed by ACL"
+        )
         self.assertFalse(
-            table.is_output(l2_drop_match, port=3),
-            msg='Packet not blocked by ACL')
+            table.is_output(l2_drop_match, port=3), msg="Packet not blocked by ACL"
+        )
 
         # l3
         l3_drop_match = {
-            'in_port': 1,
-            'eth_dst': FAUCET_MAC,
-            'vlan_vid': 0,
-            'eth_type': 0x86DD,
-            'ipv6_dst': DENY_HOST_V6}
+            "in_port": 1,
+            "eth_dst": FAUCET_MAC,
+            "vlan_vid": 0,
+            "eth_type": 0x86DD,
+            "ipv6_dst": DENY_HOST_V6,
+        }
         l3_accept_match = {
-            'in_port': 1,
-            'eth_dst': FAUCET_MAC,
-            'vlan_vid': 0,
-            'eth_type': 0x86DD,
-            'ipv6_dst': ALLOW_HOST_V6}
+            "in_port": 1,
+            "eth_dst": FAUCET_MAC,
+            "vlan_vid": 0,
+            "eth_type": 0x86DD,
+            "ipv6_dst": ALLOW_HOST_V6,
+        }
 
         self.assertTrue(
             table.is_output(l3_accept_match, port=2),
-            msg='Routed packet not allowed by ACL')
+            msg="Routed packet not allowed by ACL",
+        )
         self.assertFalse(
             table.is_output(l3_drop_match, port=3),
-            msg='Routed packet not blocked by ACL')
+            msg="Routed packet not blocked by ACL",
+        )
 
 
 if __name__ == "__main__":
     unittest.main()  # pytype: disable=module-attr
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_valve_of.py` & `c65faucet-1.0.47/tests/unit/faucet/test_valve_of.py`

 * *Files 12% similar despite different names*

```diff
@@ -33,21 +33,37 @@
         reordered = valve_of.valve_flowreorder(flows, use_barriers=False)
         self.assertEqual(1, len(reordered))
 
     def test_delete_order(self):
         """Test delete ordering/deupdlication."""
         global_groupdel = valve_of.groupdel(group_id=valve_of.ofp.OFPG_ALL)
         global_flowdel = valve_of.flowmod(
-            cookie=None, hard_timeout=None, idle_timeout=None, match_fields=None, out_port=None,
-            table_id=valve_of.ofp.OFPTT_ALL, inst=(), priority=0, command=valve_of.ofp.OFPFC_DELETE,
-            out_group=valve_of.ofp.OFPG_ANY)
+            cookie=None,
+            hard_timeout=None,
+            idle_timeout=None,
+            match_fields=None,
+            out_port=None,
+            table_id=valve_of.ofp.OFPTT_ALL,
+            inst=(),
+            priority=0,
+            command=valve_of.ofp.OFPFC_DELETE,
+            out_group=valve_of.ofp.OFPG_ANY,
+        )
         flowdel = valve_of.flowmod(
-            cookie=None, hard_timeout=None, idle_timeout=None, match_fields=None, out_port=None,
-            table_id=9, inst=(), priority=0, command=valve_of.ofp.OFPFC_DELETE,
-            out_group=valve_of.ofp.OFPG_ANY)
+            cookie=None,
+            hard_timeout=None,
+            idle_timeout=None,
+            match_fields=None,
+            out_port=None,
+            table_id=9,
+            inst=(),
+            priority=0,
+            command=valve_of.ofp.OFPFC_DELETE,
+            out_group=valve_of.ofp.OFPG_ANY,
+        )
         flow = valve_of.output_port(1)
         flows = [flowdel, flow, flow, flow, global_flowdel, global_groupdel]
         reordered = valve_of.valve_flowreorder(flows, use_barriers=True)
         reordered_str = [str(r) for r in reordered]
         # global deletes come first
         self.assertTrue(valve_of.is_global_groupdel(reordered[0]), msg=reordered)
         self.assertTrue(valve_of.is_global_flowdel(reordered[1]), msg=reordered)
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_valve_stack.py` & `c65faucet-1.0.47/tests/unit/faucet/test_valve_stack.py`

 * *Files 16% similar despite different names*

```diff
@@ -25,22 +25,30 @@
 import ipaddress
 
 from os_ken.lib import mac
 from os_ken.ofproto import ofproto_v1_3 as ofp
 
 from faucet import valve_of
 from faucet.port import (
-    STACK_STATE_INIT, STACK_STATE_UP,
-    LACP_PORT_SELECTED, LACP_PORT_UNSELECTED)
+    STACK_STATE_INIT,
+    STACK_STATE_UP,
+    LACP_PORT_SELECTED,
+    LACP_PORT_UNSELECTED,
+)
 from faucet.config_parser_util import yaml_load, yaml_dump
 
 from clib.fakeoftable import CONTROLLER_PORT
 
 from clib.valve_test_lib import (
-    BASE_DP1_CONFIG, CONFIG, STACK_CONFIG, STACK_LOOP_CONFIG, ValveTestBases)
+    BASE_DP1_CONFIG,
+    CONFIG,
+    STACK_CONFIG,
+    STACK_LOOP_CONFIG,
+    ValveTestBases,
+)
 
 
 class ValveEdgeVLANTestCase(ValveTestBases.ValveTestNetwork):
     """Test edge VLAN operation"""
 
     CONFIG1 = """
 dps:
@@ -132,24 +140,25 @@
         self.assertFalse(s1.stack.is_edge())
         s2 = self.valves_manager.valves[2].dp
         self.assertFalse(s2.stack.is_root())
         self.assertFalse(s2.stack.is_edge())
         s3 = self.valves_manager.valves[3].dp
         self.assertFalse(s3.stack.is_root())
         self.assertTrue(s3.stack.is_edge())
-        match = {'in_port': 2, 'vlan_vid': 0, 'eth_src': self.P2_V100_MAC}
+        match = {"in_port": 2, "vlan_vid": 0, "eth_src": self.P2_V100_MAC}
         self.network.tables[3].is_output(match, port=3)
-        match = {'in_port': 3, 'vlan_vid': 0, 'eth_src': self.P2_V100_MAC}
+        match = {"in_port": 3, "vlan_vid": 0, "eth_src": self.P2_V100_MAC}
         self.network.tables[3].is_output(match, port=2)
 
 
 class ValveStackMCLAGTestCase(ValveTestBases.ValveTestNetwork):
     """Test stacked MCLAG"""
 
-    CONFIG = """
+    CONFIG = (
+        """
 dps:
     s1:
 %s
         stack:
             priority: 1
         interfaces:
             1:
@@ -184,15 +193,17 @@
                 description: p3
                 native_vlan: 100
                 lacp: 1
             4:
                 description: p4
                 native_vlan: 100
                 lacp: 1
-""" % BASE_DP1_CONFIG
+"""
+        % BASE_DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic loop config"""
         self.setup_valves(self.CONFIG)
 
     def test_dpid_nominations(self):
         """Test dpids are nominated correctly"""
@@ -203,70 +214,94 @@
                 if port.lacp:
                     lacp_ports.setdefault(valve.dp.dp_id, [])
                     lacp_ports[valve.dp.dp_id].append(port)
                     port.actor_up()
         valve = self.valves_manager.valves[0x1]
         other_valves = self.get_other_valves(valve)
         # Equal number of LAG ports, choose root DP
-        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(1, valve, other_valves)[0]
+        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(
+            1, valve, other_valves
+        )[0]
         self.assertEqual(
-            nominated_dpid, 0x1,
-            'Expected nominated DPID %s but found %s' % (0x1, nominated_dpid))
+            nominated_dpid,
+            0x1,
+            "Expected nominated DPID %s but found %s" % (0x1, nominated_dpid),
+        )
         # Choose DP with most UP LAG ports
         lacp_ports[0x1][0].actor_nosync()
-        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(1, valve, other_valves)[0]
+        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(
+            1, valve, other_valves
+        )[0]
         self.assertEqual(
-            nominated_dpid, 0x2,
-            'Expected nominated DPID %s but found %s' % (0x2, nominated_dpid))
+            nominated_dpid,
+            0x2,
+            "Expected nominated DPID %s but found %s" % (0x2, nominated_dpid),
+        )
         # Set all links to no_sync
         for valve in self.valves_manager.valves.values():
             for port in valve.dp.ports.values():
                 if port.lacp:
                     lacp_ports.setdefault(valve.dp.dp_id, [])
                     lacp_ports[valve.dp.dp_id].append(port)
                     port.actor_nosync()
         valve = self.valves_manager.valves[0x1]
         other_valves = self.get_other_valves(valve)
-        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(1, valve, other_valves)[0]
+        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(
+            1, valve, other_valves
+        )[0]
         self.assertEqual(
-            nominated_dpid, 0x1,
-            'Expected nominated DPID %s but found %s' % (0x1, nominated_dpid))
+            nominated_dpid,
+            0x1,
+            "Expected nominated DPID %s but found %s" % (0x1, nominated_dpid),
+        )
         # Set one link to up
         lacp_ports[0x1][0].actor_up()
-        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(1, valve, other_valves)[0]
+        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(
+            1, valve, other_valves
+        )[0]
         self.assertEqual(
-            nominated_dpid, 0x1,
-            'Expected nominated DPID %s but found %s' % (0x1, nominated_dpid))
+            nominated_dpid,
+            0x1,
+            "Expected nominated DPID %s but found %s" % (0x1, nominated_dpid),
+        )
         # Set DP 1 links to init
         lacp_ports[0x1][0].actor_init()
         lacp_ports[0x1][1].actor_init()
-        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(1, valve, other_valves)[0]
+        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(
+            1, valve, other_valves
+        )[0]
         self.assertEqual(
-            nominated_dpid, 0x2,
-            'Expected nominated DPID %s but found %s' % (0x2, nominated_dpid))
+            nominated_dpid,
+            0x2,
+            "Expected nominated DPID %s but found %s" % (0x2, nominated_dpid),
+        )
 
     def test_no_dpid_nominations(self):
         """Test dpid nomination doesn't nominate when no LACP ports are up"""
         self.activate_all_ports()
         valve = self.valves_manager.valves[0x1]
         other_valves = self.get_other_valves(valve)
         # No actors UP so should return None
-        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(1, valve, other_valves)[0]
+        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(
+            1, valve, other_valves
+        )[0]
         self.assertEqual(
-            nominated_dpid, None,
-            'Did not expect to nominate DPID %s' % nominated_dpid)
+            nominated_dpid, None, "Did not expect to nominate DPID %s" % nominated_dpid
+        )
         # No other valves so should return None
         for valve in self.valves_manager.valves.values():
             for port in valve.dp.ports.values():
                 if port.lacp:
                     port.actor_up()
-        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(1, valve, None)[0]
+        nominated_dpid = valve.switch_manager.get_lacp_dpid_nomination(1, valve, None)[
+            0
+        ]
         self.assertEqual(
-            nominated_dpid, None,
-            'Did not expect to nominate DPID %s' % nominated_dpid)
+            nominated_dpid, None, "Did not expect to nominate DPID %s" % nominated_dpid
+        )
 
     def test_nominated_dpid_port_selection(self):
         """Test a nominated port selection state is changed"""
         self.activate_all_ports()
         lacp_ports = {}
         for valve in self.valves_manager.valves.values():
             for port in valve.dp.ports.values():
@@ -276,108 +311,137 @@
                     port.actor_up()
         for valve, ports in lacp_ports.items():
             other_valves = self.get_other_valves(valve)
             for port in ports:
                 valve.lacp_update(port, True, 1, 1, other_valves)
                 # Testing accuracy of varz port_lacp_role
                 port_labels = {
-                    'port': port.name,
-                    'port_description': port.description,
-                    'dp_name': valve.dp.name,
-                    'dp_id': '0x%x' % valve.dp.dp_id
+                    "port": port.name,
+                    "port_description": port.description,
+                    "dp_name": valve.dp.name,
+                    "dp_id": "0x%x" % valve.dp.dp_id,
                 }
-                lacp_role = self.get_prom('port_lacp_role', labels=port_labels, bare=True)
+                lacp_role = self.get_prom(
+                    "port_lacp_role", labels=port_labels, bare=True
+                )
                 self.assertEqual(
-                    port.lacp_port_state(), lacp_role,
-                    'Port %s DP %s role %s differs from varz value %s'
-                    % (port, valve, port.lacp_port_state(), lacp_role))
+                    port.lacp_port_state(),
+                    lacp_role,
+                    "Port %s DP %s role %s differs from varz value %s"
+                    % (port, valve, port.lacp_port_state(), lacp_role),
+                )
                 if valve.dp.dp_id == 0x1:
                     self.assertEqual(
-                        port.lacp_port_state(), LACP_PORT_SELECTED,
-                        'Expected LACP port %s DP %s to be SELECTED' % (port, valve))
+                        port.lacp_port_state(),
+                        LACP_PORT_SELECTED,
+                        "Expected LACP port %s DP %s to be SELECTED" % (port, valve),
+                    )
                 else:
                     self.assertEqual(
-                        port.lacp_port_state(), LACP_PORT_UNSELECTED,
-                        'Expected LACP port %s DP %s to be UNSELECTED' % (port, valve))
+                        port.lacp_port_state(),
+                        LACP_PORT_UNSELECTED,
+                        "Expected LACP port %s DP %s to be UNSELECTED" % (port, valve),
+                    )
 
     def test_lag_flood(self):
         """Test flooding is allowed for UP & SELECTED LAG links only"""
         self.activate_all_ports()
         main_valve = self.valves_manager.valves[0x1]
         main_other_valves = self.get_other_valves(main_valve)
         # Start with all LAG links INIT & UNSELECTED
-        self.validate_flood(2, 0, 3, False, 'Flooded out UNSELECTED & INIT LAG port')
-        self.validate_flood(2, 0, 4, False, 'Flooded out UNSELECTED & INIT LAG port')
+        self.validate_flood(2, 0, 3, False, "Flooded out UNSELECTED & INIT LAG port")
+        self.validate_flood(2, 0, 4, False, "Flooded out UNSELECTED & INIT LAG port")
         # Set UP & SELECTED one s1 LAG link
         port3 = main_valve.dp.ports[3]
         port4 = main_valve.dp.ports[4]
         self.apply_ofmsgs(main_valve.lacp_update(port4, True, 1, 1, main_other_valves))
         self.apply_ofmsgs(main_valve.lacp_update(port3, False, 1, 1, main_other_valves))
-        self.validate_flood(2, 0, 3, False, 'Flooded out NOSYNC LAG port')
-        self.validate_flood(2, 0, 4, True, 'Did not flood out SELECTED LAG port')
+        self.validate_flood(2, 0, 3, False, "Flooded out NOSYNC LAG port")
+        self.validate_flood(2, 0, 4, True, "Did not flood out SELECTED LAG port")
         # Set UP & SELECTED s2 LAG links
         valve = self.valves_manager.valves[0x2]
         other_valves = self.get_other_valves(valve)
         for port in valve.dp.ports.values():
             if port.lacp:
                 valve.lacp_update(port, True, 1, 1, other_valves)
         self.apply_ofmsgs(main_valve.lacp_update(port4, True, 1, 1, main_other_valves))
         self.apply_ofmsgs(main_valve.lacp_update(port3, False, 1, 1, main_other_valves))
-        self.validate_flood(2, 0, 3, False, 'Flooded out UNSELECTED & NOSYNC LAG port')
-        self.validate_flood(2, 0, 4, False, 'Flooded out UNSELECTED LAG port')
+        self.validate_flood(2, 0, 3, False, "Flooded out UNSELECTED & NOSYNC LAG port")
+        self.validate_flood(2, 0, 4, False, "Flooded out UNSELECTED LAG port")
         # Set UP & SELECTED both s1 LAG links
         self.apply_ofmsgs(main_valve.lacp_update(port3, True, 1, 1, main_other_valves))
         self.apply_ofmsgs(main_valve.lacp_update(port4, True, 1, 1, main_other_valves))
-        self.validate_flood(2, 0, 3, True, 'Did not flood out SELECTED LAG port')
-        self.validate_flood(2, 0, 4, False, 'Flooded out multiple LAG ports')
+        self.validate_flood(2, 0, 3, True, "Did not flood out SELECTED LAG port")
+        self.validate_flood(2, 0, 4, False, "Flooded out multiple LAG ports")
 
     def test_lag_pipeline_accept(self):
         """Test packets entering through UP & SELECTED LAG links"""
         self.activate_all_ports()
         main_valve = self.valves_manager.valves[0x1]
         main_other_valves = self.get_other_valves(main_valve)
         # Packet initially rejected
         self.validate_flood(
-            3, 0, None, False, 'Packet incoming through UNSELECTED & INIT port was accepted')
+            3,
+            0,
+            None,
+            False,
+            "Packet incoming through UNSELECTED & INIT port was accepted",
+        )
         self.validate_flood(
-            4, 0, None, False, 'Packet incoming through UNSELECTED & INIT port was accepted')
+            4,
+            0,
+            None,
+            False,
+            "Packet incoming through UNSELECTED & INIT port was accepted",
+        )
         # Set one s1 LAG port 4 to SELECTED & UP
         port3 = main_valve.dp.ports[3]
         port4 = main_valve.dp.ports[4]
         self.apply_ofmsgs(main_valve.lacp_update(port4, True, 1, 1, main_other_valves))
         self.apply_ofmsgs(main_valve.lacp_update(port3, False, 1, 1, main_other_valves))
         self.validate_flood(
-            3, 0, None, False, 'Packet incoming through NOSYNC port was accepted')
+            3, 0, None, False, "Packet incoming through NOSYNC port was accepted"
+        )
         self.validate_flood(
-            4, 0, None, True, 'Packet incoming through SELECTED port was not accepted')
+            4, 0, None, True, "Packet incoming through SELECTED port was not accepted"
+        )
         # Set UP & SELECTED s2 LAG links, set one s1 port down
         valve = self.valves_manager.valves[0x2]
         other_valves = self.get_other_valves(valve)
         for port in valve.dp.ports.values():
             if port.lacp:
                 valve.lacp_update(port, True, 1, 1, other_valves)
         self.apply_ofmsgs(main_valve.lacp_update(port4, True, 1, 1, main_other_valves))
         self.apply_ofmsgs(main_valve.lacp_update(port3, False, 1, 1, main_other_valves))
         self.validate_flood(
-            3, 0, None, False, 'Packet incoming through UNSELECTED & NOSYNC port was accepted')
+            3,
+            0,
+            None,
+            False,
+            "Packet incoming through UNSELECTED & NOSYNC port was accepted",
+        )
         self.validate_flood(
-            4, 0, None, False, 'Packet incoming through UNSELECTED port was accepted')
+            4, 0, None, False, "Packet incoming through UNSELECTED port was accepted"
+        )
         # Set UP & SELECTED both s1 LAG links
         self.apply_ofmsgs(main_valve.lacp_update(port3, True, 1, 1, main_other_valves))
         self.apply_ofmsgs(main_valve.lacp_update(port4, True, 1, 1, main_other_valves))
         self.validate_flood(
-            3, 0, None, True, 'Packet incoming through SELECTED port was not accepted')
+            3, 0, None, True, "Packet incoming through SELECTED port was not accepted"
+        )
         self.validate_flood(
-            4, 0, None, True, 'Packet incoming through SELECTED port was not accepted')
+            4, 0, None, True, "Packet incoming through SELECTED port was not accepted"
+        )
 
 
 class ValveStackMCLAGRestartTestCase(ValveTestBases.ValveTestNetwork):
     """Test stacked MCLAG"""
 
-    CONFIG = """
+    CONFIG = (
+        """
 dps:
     s1:
 %s
         stack:
             priority: 1
         interfaces:
             1:
@@ -412,47 +476,54 @@
                 description: p3
                 native_vlan: 100
                 lacp: 1
             4:
                 description: p4
                 native_vlan: 100
                 lacp: 1
-""" % BASE_DP1_CONFIG
+"""
+        % BASE_DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic loop config"""
         self.setup_valves(self.CONFIG)
 
     def test_mclag_cold_start(self):
         """Test cold-starting a switch with a downed port resets LACP states"""
         self.activate_all_ports()
         valve = self.valves_manager.valves[0x1]
         other_valves = self.get_other_valves(valve)
         old_port = valve.dp.ports[3]
         # Make sure LACP state has been updated
-        self.assertTrue(valve.lacp_update(old_port, True, 1, 1, other_valves), 'No OFMSGS returned')
-        self.assertTrue(old_port.is_actor_up(), 'Actor not UP')
+        self.assertTrue(
+            valve.lacp_update(old_port, True, 1, 1, other_valves), "No OFMSGS returned"
+        )
+        self.assertTrue(old_port.is_actor_up(), "Actor not UP")
         # Set port DOWN
         valve.port_delete(3, other_valves=other_valves)
-        self.assertTrue(old_port.is_actor_none(), 'Actor not NONE')
+        self.assertTrue(old_port.is_actor_none(), "Actor not NONE")
         # Restart switch & LACP port
         self.cold_start()
         new_port = valve.dp.ports[3]
         # A full cold-start should change port object, but self.cold_start() is 'incomplete'
-        self.assertEqual(id(old_port), id(new_port), 'Port object changed')
-        self.assertTrue(valve.port_add(3), 'No OFMSGS returned')
+        self.assertEqual(id(old_port), id(new_port), "Port object changed")
+        self.assertTrue(valve.port_add(3), "No OFMSGS returned")
         # Successfully restart LACP from downed
-        self.assertTrue(valve.lacp_update(new_port, True, 1, 1, other_valves), 'No OFMSGS returned')
-        self.assertTrue(new_port.is_actor_up(), 'Actor not UP')
+        self.assertTrue(
+            valve.lacp_update(new_port, True, 1, 1, other_valves), "No OFMSGS returned"
+        )
+        self.assertTrue(new_port.is_actor_up(), "Actor not UP")
 
 
 class ValveStackMCLAGStandbyTestCase(ValveTestBases.ValveTestNetwork):
     """Test MCLAG with standby port option overrules unselected states"""
 
-    CONFIG = """
+    CONFIG = (
+        """
 dps:
     s1:
 %s
         stack:
             priority: 1
         interfaces:
             1:
@@ -485,15 +556,17 @@
                 lacp_standby: True
                 lacp: 1
             3:
                 description: p4
                 native_vlan: 100
                 lacp_standby: True
                 lacp: 1
-""" % BASE_DP1_CONFIG
+"""
+        % BASE_DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic loop config"""
         self.setup_valves(self.CONFIG)
 
     def test_mclag_standby_option(self):
         """Test MCLAG standby option forces standby state instead of unselected"""
@@ -502,30 +575,35 @@
         other_valve = self.valves_manager.valves[0x2]
         for port in valve.dp.ports.values():
             if port.lacp:
                 valve.lacp_update(port, True, 1, 1, self.get_other_valves(valve))
                 self.assertTrue(port.is_port_selected())
         for port in other_valve.dp.ports.values():
             if port.lacp:
-                other_valve.lacp_update(port, True, 1, 1, self.get_other_valves(other_valve))
+                other_valve.lacp_update(
+                    port, True, 1, 1, self.get_other_valves(other_valve)
+                )
                 self.assertTrue(port.is_port_standby())
         for port in valve.dp.ports.values():
             if port.lacp:
                 valve.lacp_update(port, False, 1, 1, self.get_other_valves(valve))
                 self.assertTrue(port.is_port_standby())
         for port in other_valve.dp.ports.values():
             if port.lacp:
-                other_valve.lacp_update(port, True, 1, 1, self.get_other_valves(other_valve))
+                other_valve.lacp_update(
+                    port, True, 1, 1, self.get_other_valves(other_valve)
+                )
                 self.assertTrue(port.is_port_selected())
 
 
 class ValveStackRootExtLoopProtectTestCase(ValveTestBases.ValveTestNetwork):
     """External loop protect test cases"""
 
-    CONFIG = """
+    CONFIG = (
+        """
 dps:
     s1:
 %s
         stack:
             priority: 1
         interfaces:
             1:
@@ -560,46 +638,51 @@
                 description: p3
                 native_vlan: 100
                 loop_protect_external: True
             4:
                 description: p4
                 native_vlan: 100
                 loop_protect_external: True
-""" % BASE_DP1_CONFIG
+"""
+        % BASE_DP1_CONFIG
+    )
 
     def setUp(self):
         self.setup_valves(self.CONFIG)
         self.set_stack_port_up(1)
 
     def test_loop_protect(self):
         """test basic loop protection"""
         mcast_match = {
-            'in_port': 2,
-            'eth_dst': mac.BROADCAST_STR,
-            'vlan_vid': 0,
-            'eth_type': 0x800,
-            'ipv4_dst': '224.0.0.5',
+            "in_port": 2,
+            "eth_dst": mac.BROADCAST_STR,
+            "vlan_vid": 0,
+            "eth_type": 0x800,
+            "ipv4_dst": "224.0.0.5",
         }
         table = self.network.tables[self.DP_ID]
         self.assertTrue(
             table.is_output(mcast_match, port=1),
-            msg='mcast packet not flooded to non-root stack')
+            msg="mcast packet not flooded to non-root stack",
+        )
         self.assertTrue(
             table.is_output(mcast_match, port=3),
-            msg='mcast packet not flooded locally on root')
+            msg="mcast packet not flooded locally on root",
+        )
         self.assertFalse(
             table.is_output(mcast_match, port=4),
-            msg='mcast packet multiply flooded externally on root')
+            msg="mcast packet multiply flooded externally on root",
+        )
 
 
 class ValveStackChainTest(ValveTestBases.ValveTestNetwork):
     """Test base class for loop stack config"""
 
     CONFIG = STACK_CONFIG
-    DP = 's2'
+    DP = "s2"
     DP_ID = 2
 
     def setUp(self):
         """Setup basic loop config"""
         self.setup_valves(self.CONFIG)
 
     def learn_stack_hosts(self):
@@ -610,64 +693,74 @@
             self.rcv_packet(4, 0, self.pkt_match(2, 1), dp_id=2)
             self.rcv_packet(1, 0, self.pkt_match(2, 1), dp_id=1)
             self.rcv_packet(1, 0, self.pkt_match(3, 2), dp_id=3)
             self.rcv_packet(3, 0, self.pkt_match(3, 2), dp_id=2)
 
     def _unicast_to(self, out_port, trace=False):
         ucast_match = {
-            'in_port': 4,
-            'eth_src': self.P2_V100_MAC,
-            'eth_dst': self.P1_V100_MAC,
-            'vlan_vid': 0,
-            'eth_type': 0x800,
+            "in_port": 4,
+            "eth_src": self.P2_V100_MAC,
+            "eth_dst": self.P1_V100_MAC,
+            "vlan_vid": 0,
+            "eth_type": 0x800,
         }
         table = self.network.tables[self.DP_ID]
         return table.is_output(ucast_match, port=out_port, trace=trace)
 
     def _learning_from_bcast(self, in_port, trace=False):
         ucast_match = {
-            'in_port': in_port,
-            'eth_src': self.P1_V100_MAC,
-            'eth_dst': self.BROADCAST_MAC,
-            'vlan_vid': self.V100,
-            'eth_type': 0x800,
+            "in_port": in_port,
+            "eth_src": self.P1_V100_MAC,
+            "eth_dst": self.BROADCAST_MAC,
+            "vlan_vid": self.V100,
+            "eth_type": 0x800,
         }
         table = self.network.tables[self.DP_ID]
         if trace:
             self.network.print_table(2)
         return table.is_output(ucast_match, port=CONTROLLER_PORT, trace=trace)
 
     def validate_edge_learn_ports(self):
         """Validate the switch behavior before learning, and then learn hosts"""
 
         # Before learning, unicast should flood to stack root and packet-in.
-        self.assertFalse(self._unicast_to(1), 'unlearned unicast to stack root')
-        self.assertFalse(self._unicast_to(2), 'unlearned unicast to stack root')
-        self.assertTrue(self._unicast_to(3), 'unlearned unicast away from stack root')
-        self.assertTrue(self._unicast_to(CONTROLLER_PORT), 'unlearned unicast learn')
-        self.assertFalse(self._learning_from_bcast(1), 'learn from stack root broadcast')
-        self.assertFalse(self._learning_from_bcast(4), 'learn from access port broadcast')
+        self.assertFalse(self._unicast_to(1), "unlearned unicast to stack root")
+        self.assertFalse(self._unicast_to(2), "unlearned unicast to stack root")
+        self.assertTrue(self._unicast_to(3), "unlearned unicast away from stack root")
+        self.assertTrue(self._unicast_to(CONTROLLER_PORT), "unlearned unicast learn")
+        self.assertFalse(
+            self._learning_from_bcast(1), "learn from stack root broadcast"
+        )
+        self.assertFalse(
+            self._learning_from_bcast(4), "learn from access port broadcast"
+        )
 
         self.learn_stack_hosts()
 
-        self.assertFalse(self._unicast_to(1), 'learned unicast to stack root')
-        self.assertFalse(self._unicast_to(2), 'learned unicast to stack root')
-        self.assertTrue(self._unicast_to(3), 'learned unicast away from stack root')
-        self.assertFalse(self._unicast_to(CONTROLLER_PORT), 'no learn from unicast')
-        self.assertFalse(self._learning_from_bcast(1), 'learn from stack root broadcast')
-        self.assertFalse(self._learning_from_bcast(4), 'learn from access port broadcast')
+        self.assertFalse(self._unicast_to(1), "learned unicast to stack root")
+        self.assertFalse(self._unicast_to(2), "learned unicast to stack root")
+        self.assertTrue(self._unicast_to(3), "learned unicast away from stack root")
+        self.assertFalse(self._unicast_to(CONTROLLER_PORT), "no learn from unicast")
+        self.assertFalse(
+            self._learning_from_bcast(1), "learn from stack root broadcast"
+        )
+        self.assertFalse(
+            self._learning_from_bcast(4), "learn from access port broadcast"
+        )
 
     def test_stack_learn_edge(self):
         """Test stack learned edge"""
         self.activate_all_ports()
         self.validate_edge_learn_ports()
 
     def test_stack_learn_not_root(self):
         """Test stack learned when not root"""
-        self.update_config(self._config_edge_learn_stack_root(False), reload_type='warm')
+        self.update_config(
+            self._config_edge_learn_stack_root(False), reload_type="warm"
+        )
         self.activate_all_ports()
         self.validate_edge_learn_ports()
 
 
 class ValveStackLoopTest(ValveTestBases.ValveTestNetwork):
     """Test base class for loop stack config"""
 
@@ -676,24 +769,24 @@
     def setUp(self):
         """Setup basic loop config"""
         self.setup_valves(self.CONFIG)
 
     def validate_flooding(self, rerouted=False, portup=True):
         """Validate the flooding state of the stack"""
         vid = self.V100
-        self.validate_flood(1, vid, 1, False, 'flooded out input stack port')
-        self.validate_flood(1, vid, 2, portup, 'not flooded to stack root')
-        self.validate_flood(1, vid, 3, portup, 'not flooded to external host')
-        self.validate_flood(2, vid, 1, rerouted, 'flooded out other stack port')
-        self.validate_flood(2, vid, 2, False, 'flooded out input stack port')
-        self.validate_flood(2, vid, 3, True, 'not flooded to external host')
+        self.validate_flood(1, vid, 1, False, "flooded out input stack port")
+        self.validate_flood(1, vid, 2, portup, "not flooded to stack root")
+        self.validate_flood(1, vid, 3, portup, "not flooded to external host")
+        self.validate_flood(2, vid, 1, rerouted, "flooded out other stack port")
+        self.validate_flood(2, vid, 2, False, "flooded out input stack port")
+        self.validate_flood(2, vid, 3, True, "not flooded to external host")
         vid = 0
-        self.validate_flood(3, vid, 1, rerouted, 'flooded out inactive port')
-        self.validate_flood(3, vid, 2, True, 'not flooded to stack root')
-        self.validate_flood(3, vid, 3, False, 'flooded out hairpin')
+        self.validate_flood(3, vid, 1, rerouted, "flooded out inactive port")
+        self.validate_flood(3, vid, 2, True, "not flooded to stack root")
+        self.validate_flood(3, vid, 3, False, "flooded out hairpin")
 
     def learn_stack_hosts(self):
         """Learn some hosts."""
         for _ in range(2):
             self.rcv_packet(3, 0, self.pkt_match(1, 2), dp_id=1)
             self.rcv_packet(2, 0, self.pkt_match(1, 2), dp_id=2)
             self.rcv_packet(3, 0, self.pkt_match(2, 1), dp_id=2)
@@ -701,116 +794,125 @@
 
 
 class ValveStackEdgeLearnTestCase(ValveStackLoopTest):
     """Edge learning test cases"""
 
     def _unicast_to(self, out_port):
         ucast_match = {
-            'in_port': 3,
-            'eth_src': self.P1_V100_MAC,
-            'eth_dst': self.P2_V100_MAC,
-            'vlan_vid': 0,
-            'eth_type': 0x800,
+            "in_port": 3,
+            "eth_src": self.P1_V100_MAC,
+            "eth_dst": self.P2_V100_MAC,
+            "vlan_vid": 0,
+            "eth_type": 0x800,
         }
         table = self.network.tables[self.DP_ID]
         return table.is_output(ucast_match, port=out_port)
 
     def _learning_from_bcast(self, in_port):
         bcast_match = {
-            'in_port': in_port,
-            'eth_src': self.P2_V100_MAC,
-            'eth_dst': self.BROADCAST_MAC,
-            'vlan_vid': self.V100,
-            'eth_type': 0x800,
+            "in_port": in_port,
+            "eth_src": self.P2_V100_MAC,
+            "eth_dst": self.BROADCAST_MAC,
+            "vlan_vid": self.V100,
+            "eth_type": 0x800,
         }
         table = self.network.tables[self.DP_ID]
         return table.is_output(bcast_match, port=CONTROLLER_PORT)
 
     def validate_edge_learn_ports(self):
         """Validate the switch behavior before learning, and then learn hosts"""
 
         # Before learning, unicast should flood to stack root and packet-in.
-        self.assertFalse(self._unicast_to(1), 'unicast direct to edge')
-        self.assertTrue(self._unicast_to(2), 'unicast to stack root')
-        self.assertTrue(self._unicast_to(CONTROLLER_PORT), 'learn from unicast')
+        self.assertFalse(self._unicast_to(1), "unicast direct to edge")
+        self.assertTrue(self._unicast_to(2), "unicast to stack root")
+        self.assertTrue(self._unicast_to(CONTROLLER_PORT), "learn from unicast")
 
-        self.assertTrue(self._learning_from_bcast(2), 'learn from stack root broadcast')
+        self.assertTrue(self._learning_from_bcast(2), "learn from stack root broadcast")
 
         self.learn_stack_hosts()
 
-        self.assertFalse(self._unicast_to(CONTROLLER_PORT), 'learn from unicast')
+        self.assertFalse(self._unicast_to(CONTROLLER_PORT), "learn from unicast")
 
     def test_edge_learn_edge_port(self):
         """Check the behavior of the basic edge_learn_port algorithm"""
 
-        self.update_config(self._config_edge_learn_stack_root(False), reload_type='warm')
+        self.update_config(
+            self._config_edge_learn_stack_root(False), reload_type="warm"
+        )
 
         self.activate_all_ports()
 
         self.validate_edge_learn_ports()
 
         # After learning, unicast should go direct to edge switch.
-        self.assertTrue(self._unicast_to(1), 'unicast direct to edge')
-        self.assertFalse(self._unicast_to(2), 'unicast to stack root')
+        self.assertTrue(self._unicast_to(1), "unicast direct to edge")
+        self.assertFalse(self._unicast_to(2), "unicast to stack root")
 
         # TODO: This should be False to prevent unnecessary packet-ins.
-        self.assertTrue(self._learning_from_bcast(2), 'learn from stack root broadcast')
+        self.assertTrue(self._learning_from_bcast(2), "learn from stack root broadcast")
 
     def test_edge_learn_stack_root(self):
         """Check the behavior of learning always towards stack root"""
 
         self.activate_all_ports()
 
         self.validate_edge_learn_ports()
 
         # After learning, unicast should go to stack root, and no more learning from root.
-        self.assertFalse(self._unicast_to(1), 'unicast direct to edge')
-        self.assertTrue(self._unicast_to(2), 'unicast to stack root')
-        self.assertFalse(self._learning_from_bcast(2), 'learn from stack root broadcast')
+        self.assertFalse(self._unicast_to(1), "unicast direct to edge")
+        self.assertTrue(self._unicast_to(2), "unicast to stack root")
+        self.assertFalse(
+            self._learning_from_bcast(2), "learn from stack root broadcast"
+        )
 
 
 class ValveStackRedundantLink(ValveStackLoopTest):
     """Check stack situations with a redundant link"""
 
     def test_loop_protect(self):
         """Basic loop protection check"""
         self.activate_all_ports()
         mcast_match = {
-            'in_port': 3,
-            'eth_dst': mac.BROADCAST_STR,
-            'vlan_vid': 0,
-            'eth_type': 0x800,
-            'ipv4_dst': '224.0.0.5',
+            "in_port": 3,
+            "eth_dst": mac.BROADCAST_STR,
+            "vlan_vid": 0,
+            "eth_type": 0x800,
+            "ipv4_dst": "224.0.0.5",
         }
         table = self.network.tables[self.DP_ID]
         valve = self.valves_manager.valves[self.DP_ID]
         self.assertTrue(
             table.is_output(mcast_match, port=2),
-            msg='mcast packet not flooded to root of stack')
+            msg="mcast packet not flooded to root of stack",
+        )
         self.assertFalse(valve.dp.ports[2].non_stack_forwarding())
         self.assertFalse(
             table.is_output(mcast_match, port=1),
-            msg='mcast packet flooded root of stack via not shortest path')
+            msg="mcast packet flooded root of stack via not shortest path",
+        )
         self.deactivate_stack_port(valve.dp.ports[2])
         self.assertFalse(valve.dp.ports[2].non_stack_forwarding())
         self.assertFalse(
             table.is_output(mcast_match, port=2),
-            msg='mcast packet flooded to root of stack via redundant path')
+            msg="mcast packet flooded to root of stack via redundant path",
+        )
         self.assertFalse(valve.dp.ports[2].non_stack_forwarding())
         self.assertTrue(
             table.is_output(mcast_match, port=1),
-            msg='mcast packet not flooded root of stack')
+            msg="mcast packet not flooded root of stack",
+        )
         self.assertFalse(valve.dp.ports[2].non_stack_forwarding())
         self.assertTrue(valve.dp.ports[3].non_stack_forwarding())
 
 
 class ValveStackNonRootExtLoopProtectTestCase(ValveTestBases.ValveTestNetwork):
     """Test non-root external loop protect"""
 
-    CONFIG = """
+    CONFIG = (
+        """
 dps:
     s1:
 %s
         interfaces:
             1:
                 description: p1
                 stack:
@@ -854,45 +956,51 @@
                 description: p1
                 stack:
                     dp: s2
                     port: 2
             2:
                 description: p2
                 native_vlan: 100
-""" % BASE_DP1_CONFIG
+"""
+        % BASE_DP1_CONFIG
+    )
 
     def setUp(self):
         self.setup_valves(self.CONFIG)
         self.set_stack_port_up(1)
 
     def test_loop_protect(self):
         """Test expected table outputs for external loop protect"""
         mcast_match = {
-            'in_port': 2,
-            'eth_dst': mac.BROADCAST_STR,
-            'vlan_vid': 0,
-            'eth_type': 0x800,
-            'ipv4_dst': '224.0.0.5',
+            "in_port": 2,
+            "eth_dst": mac.BROADCAST_STR,
+            "vlan_vid": 0,
+            "eth_type": 0x800,
+            "ipv4_dst": "224.0.0.5",
         }
         table = self.network.tables[self.DP_ID]
         self.assertTrue(
             table.is_output(mcast_match, port=1),
-            msg='mcast packet not flooded to root of stack')
+            msg="mcast packet not flooded to root of stack",
+        )
         self.assertFalse(
             table.is_output(mcast_match, port=3),
-            msg='mcast packet flooded locally on non-root')
+            msg="mcast packet flooded locally on non-root",
+        )
         self.assertFalse(
             table.is_output(mcast_match, port=4),
-            msg='mcast packet flooded locally on non-root')
+            msg="mcast packet flooded locally on non-root",
+        )
 
 
 class ValveStackAndNonStackTestCase(ValveTestBases.ValveTestNetwork):
     """Test stacked switches can exist with non-stacked switches"""
 
-    CONFIG = """
+    CONFIG = (
+        """
 dps:
     s1:
 %s
         stack:
             priority: 1
         interfaces:
             1:
@@ -921,23 +1029,27 @@
         interfaces:
             1:
                 description: p1
                 native_vlan: 0x100
             2:
                 description: p2
                 native_vlan: 0x100
-""" % BASE_DP1_CONFIG
+"""
+        % BASE_DP1_CONFIG
+    )
 
     def setUp(self):
         self.setup_valves(self.CONFIG)
 
     def test_nonstack_dp_port(self):
         """Test that finding a path from a stack swithc to a non-stack switch cannot happen"""
         self.assertIsNone(None, self.valves_manager.valves[0x3].dp.stack)
-        self.assertEqual(None, self.valves_manager.valves[0x1].dp.stack.shortest_path_port('s3'))
+        self.assertEqual(
+            None, self.valves_manager.valves[0x1].dp.stack.shortest_path_port("s3")
+        )
 
 
 class ValveStackRedundancyTestCase(ValveTestBases.ValveTestNetwork):
     """Valve test for root selection."""
 
     CONFIG = STACK_CONFIG
     STACK_ROOT_STATE_UPDATE_TIME = 10
@@ -965,103 +1077,122 @@
         self.trigger_stack_ports()
         # All switches are down to start with.
         for dp in [valve.dp for valve in self.valves_manager.valves.values()]:
             dp.dyn_running = False
             self.set_stack_all_ports_status(dp.name, STACK_STATE_INIT)
         for valve in self.valves_manager.valves.values():
             self.assertFalse(valve.dp.dyn_running)
-            self.assertEqual('s1', valve.dp.stack.root_name)
-            root_hop_port = valve.dp.stack.shortest_path_port('s1')
+            self.assertEqual("s1", valve.dp.stack.root_name)
+            root_hop_port = valve.dp.stack.shortest_path_port("s1")
             root_hop_port = root_hop_port.number if root_hop_port else 0
-            self.assertEqual(root_hop_port, self.get_prom('dp_root_hop_port', dp_id=valve.dp.dp_id))
+            self.assertEqual(
+                root_hop_port, self.get_prom("dp_root_hop_port", dp_id=valve.dp.dp_id)
+            )
         # From a cold start - we pick the s1 as root.
         self.assertEqual(None, self.valves_manager.meta_dp_state.stack_root_name)
         self.assertFalse(
-            self.valves_manager.maintain_stack_root(now, self.STACK_ROOT_STATE_UPDATE_TIME))
-        self.assertEqual('s1', self.valves_manager.meta_dp_state.stack_root_name)
-        self.assertEqual(1, self.get_prom('faucet_stack_root_dpid', bare=True))
-        self.assertTrue(self.get_prom('is_dp_stack_root', dp_id=1))
-        self.assertFalse(self.get_prom('is_dp_stack_root', dp_id=2))
-        self.assertEqual(1, self.get_prom('stack_root_change_count_total', bare=True))
-        now += (self.STACK_ROOT_DOWN_TIME * 2)
+            self.valves_manager.maintain_stack_root(
+                now, self.STACK_ROOT_STATE_UPDATE_TIME
+            )
+        )
+        self.assertEqual("s1", self.valves_manager.meta_dp_state.stack_root_name)
+        self.assertEqual(1, self.get_prom("faucet_stack_root_dpid", bare=True))
+        self.assertTrue(self.get_prom("is_dp_stack_root", dp_id=1))
+        self.assertFalse(self.get_prom("is_dp_stack_root", dp_id=2))
+        self.assertEqual(1, self.get_prom("stack_root_change_count_total", bare=True))
+        now += self.STACK_ROOT_DOWN_TIME * 2
         # Time passes, still no change, s1 is still the root.
         self.assertFalse(
-            self.valves_manager.maintain_stack_root(now, self.STACK_ROOT_STATE_UPDATE_TIME))
-        self.assertEqual('s1', self.valves_manager.meta_dp_state.stack_root_name)
-        self.assertEqual(1, self.get_prom('faucet_stack_root_dpid', bare=True))
-        self.assertTrue(self.get_prom('is_dp_stack_root', dp_id=1))
-        self.assertFalse(self.get_prom('is_dp_stack_root', dp_id=2))
-        self.assertEqual(1, self.get_prom('stack_root_change_count_total', bare=True))
+            self.valves_manager.maintain_stack_root(
+                now, self.STACK_ROOT_STATE_UPDATE_TIME
+            )
+        )
+        self.assertEqual("s1", self.valves_manager.meta_dp_state.stack_root_name)
+        self.assertEqual(1, self.get_prom("faucet_stack_root_dpid", bare=True))
+        self.assertTrue(self.get_prom("is_dp_stack_root", dp_id=1))
+        self.assertFalse(self.get_prom("is_dp_stack_root", dp_id=2))
+        self.assertEqual(1, self.get_prom("stack_root_change_count_total", bare=True))
         # s2 has come up, but has all stack ports down and s1 is still down.
-        self.valves_manager.meta_dp_state.dp_last_live_time['s2'] = now
-        now += (self.STACK_ROOT_STATE_UPDATE_TIME * 2)
+        self.valves_manager.meta_dp_state.dp_last_live_time["s2"] = now
+        now += self.STACK_ROOT_STATE_UPDATE_TIME * 2
         # We expect s2 to be the new root because now it has stack links up.
-        self.set_stack_all_ports_status('s2', STACK_STATE_UP)
-        now += (self.STACK_ROOT_STATE_UPDATE_TIME * 2)
-        self.valves_manager.meta_dp_state.dp_last_live_time['s2'] = now
+        self.set_stack_all_ports_status("s2", STACK_STATE_UP)
+        now += self.STACK_ROOT_STATE_UPDATE_TIME * 2
+        self.valves_manager.meta_dp_state.dp_last_live_time["s2"] = now
         self.assertTrue(
-            self.valves_manager.maintain_stack_root(now, self.STACK_ROOT_STATE_UPDATE_TIME))
-        self.assertEqual('s2', self.valves_manager.meta_dp_state.stack_root_name)
-        self.assertEqual(2, self.get_prom('faucet_stack_root_dpid', bare=True))
-        self.assertFalse(self.get_prom('is_dp_stack_root', dp_id=1))
-        self.assertTrue(self.get_prom('is_dp_stack_root', dp_id=2))
-        self.assertEqual(2, self.get_prom('stack_root_change_count_total', bare=True))
+            self.valves_manager.maintain_stack_root(
+                now, self.STACK_ROOT_STATE_UPDATE_TIME
+            )
+        )
+        self.assertEqual("s2", self.valves_manager.meta_dp_state.stack_root_name)
+        self.assertEqual(2, self.get_prom("faucet_stack_root_dpid", bare=True))
+        self.assertFalse(self.get_prom("is_dp_stack_root", dp_id=1))
+        self.assertTrue(self.get_prom("is_dp_stack_root", dp_id=2))
+        self.assertEqual(2, self.get_prom("stack_root_change_count_total", bare=True))
         # More time passes, s1 is still down, s2 is still the root.
-        now += (self.STACK_ROOT_DOWN_TIME * 2)
+        now += self.STACK_ROOT_DOWN_TIME * 2
         # s2 recently said something, s2 still the root.
-        self.valves_manager.meta_dp_state.dp_last_live_time['s2'] = now - 1
-        self.set_stack_all_ports_status('s2', STACK_STATE_UP)
+        self.valves_manager.meta_dp_state.dp_last_live_time["s2"] = now - 1
+        self.set_stack_all_ports_status("s2", STACK_STATE_UP)
         self.assertFalse(
-            self.valves_manager.maintain_stack_root(now, self.STACK_ROOT_STATE_UPDATE_TIME))
-        self.assertEqual('s2', self.valves_manager.meta_dp_state.stack_root_name)
-        self.assertEqual(2, self.get_prom('faucet_stack_root_dpid', bare=True))
-        self.assertFalse(self.get_prom('is_dp_stack_root', dp_id=1))
-        self.assertTrue(self.get_prom('is_dp_stack_root', dp_id=2))
-        self.assertEqual(2, self.get_prom('stack_root_change_count_total', bare=True))
+            self.valves_manager.maintain_stack_root(
+                now, self.STACK_ROOT_STATE_UPDATE_TIME
+            )
+        )
+        self.assertEqual("s2", self.valves_manager.meta_dp_state.stack_root_name)
+        self.assertEqual(2, self.get_prom("faucet_stack_root_dpid", bare=True))
+        self.assertFalse(self.get_prom("is_dp_stack_root", dp_id=1))
+        self.assertTrue(self.get_prom("is_dp_stack_root", dp_id=2))
+        self.assertEqual(2, self.get_prom("stack_root_change_count_total", bare=True))
         # now s1 came up too, but we stay on s2 because it's healthy.
-        self.valves_manager.meta_dp_state.dp_last_live_time['s1'] = now + 1
+        self.valves_manager.meta_dp_state.dp_last_live_time["s1"] = now + 1
         now += self.STACK_ROOT_STATE_UPDATE_TIME
         self.assertFalse(
-            self.valves_manager.maintain_stack_root(now, self.STACK_ROOT_STATE_UPDATE_TIME))
-        self.assertEqual('s2', self.valves_manager.meta_dp_state.stack_root_name)
-        self.assertEqual(2, self.get_prom('faucet_stack_root_dpid', bare=True))
-        self.assertFalse(self.get_prom('is_dp_stack_root', dp_id=1))
-        self.assertTrue(self.get_prom('is_dp_stack_root', dp_id=2))
-        self.assertEqual(2, self.get_prom('stack_root_change_count_total', bare=True))
+            self.valves_manager.maintain_stack_root(
+                now, self.STACK_ROOT_STATE_UPDATE_TIME
+            )
+        )
+        self.assertEqual("s2", self.valves_manager.meta_dp_state.stack_root_name)
+        self.assertEqual(2, self.get_prom("faucet_stack_root_dpid", bare=True))
+        self.assertFalse(self.get_prom("is_dp_stack_root", dp_id=1))
+        self.assertTrue(self.get_prom("is_dp_stack_root", dp_id=2))
+        self.assertEqual(2, self.get_prom("stack_root_change_count_total", bare=True))
 
 
 class ValveRootStackTestCase(ValveTestBases.ValveTestNetwork):
     """Test stacking/forwarding."""
 
-    DP = 's3'
+    DP = "s3"
     DP_ID = 0x3
 
     def setUp(self):
         self.setup_valves(CONFIG)
         self.set_stack_port_up(5)
 
     def test_stack_learn(self):
         """Test host learning on stack root."""
         self.prom_inc(
-            partial(self.rcv_packet, 1, 0x300, {
-                'eth_src': self.P1_V300_MAC,
-                'eth_dst': self.UNKNOWN_MAC,
-                'ipv4_src': '10.0.0.1',
-                'ipv4_dst': '10.0.0.2'}),
-            'vlan_hosts_learned',
-            labels={'vlan': str(int(0x300))})
+            partial(
+                self.rcv_packet,
+                1,
+                0x300,
+                {
+                    "eth_src": self.P1_V300_MAC,
+                    "eth_dst": self.UNKNOWN_MAC,
+                    "ipv4_src": "10.0.0.1",
+                    "ipv4_dst": "10.0.0.2",
+                },
+            ),
+            "vlan_hosts_learned",
+            labels={"vlan": str(int(0x300))},
+        )
 
     def test_stack_flood(self):
         """Test packet flooding when stacking."""
-        matches = [
-            {
-                'in_port': 1,
-                'vlan_vid': 0,
-                'eth_src': self.P1_V300_MAC
-            }]
+        matches = [{"in_port": 1, "vlan_vid": 0, "eth_src": self.P1_V300_MAC}]
         self.verify_flooding(matches)
 
     def test_stack_off_on(self):
         SIMPLE_DP_CONFIG = """
         dps:
             s3:
                 dp_id: 3
@@ -1084,54 +1215,58 @@
         self.assertTrue(dp.stack.is_root())
         self.assertFalse(dp.stack.is_edge())
 
 
 class ValveEdgeStackTestCase(ValveTestBases.ValveTestNetwork):
     """Test stacking/forwarding."""
 
-    DP = 's4'
+    DP = "s4"
     DP_ID = 0x4
 
     def setUp(self):
         self.setup_valves(CONFIG)
         self.set_stack_port_up(5)
 
     def test_stack_learn(self):
         """Test host learning on non-root switch."""
-        self.rcv_packet(1, 0x300, {
-            'eth_src': self.P1_V300_MAC,
-            'eth_dst': self.UNKNOWN_MAC,
-            'ipv4_src': '10.0.0.1',
-            'ipv4_dst': '10.0.0.2'})
-        self.rcv_packet(5, 0x300, {
-            'eth_src': self.P1_V300_MAC,
-            'eth_dst': self.UNKNOWN_MAC,
-            'vid': 0x300,
-            'ipv4_src': '10.0.0.1',
-            'ipv4_dst': '10.0.0.2'})
+        self.rcv_packet(
+            1,
+            0x300,
+            {
+                "eth_src": self.P1_V300_MAC,
+                "eth_dst": self.UNKNOWN_MAC,
+                "ipv4_src": "10.0.0.1",
+                "ipv4_dst": "10.0.0.2",
+            },
+        )
+        self.rcv_packet(
+            5,
+            0x300,
+            {
+                "eth_src": self.P1_V300_MAC,
+                "eth_dst": self.UNKNOWN_MAC,
+                "vid": 0x300,
+                "ipv4_src": "10.0.0.1",
+                "ipv4_dst": "10.0.0.2",
+            },
+        )
 
     def test_stack_flood(self):
         """Test packet flooding when stacking."""
-        matches = [
-            {
-                'in_port': 1,
-                'vlan_vid': 0,
-                'eth_src': self.P1_V300_MAC
-            }]
+        matches = [{"in_port": 1, "vlan_vid": 0, "eth_src": self.P1_V300_MAC}]
         self.verify_flooding(matches)
 
     def test_no_unexpressed_packetin(self):
         """Test host learning on stack root."""
         unexpressed_vid = 0x666 | ofp.OFPVID_PRESENT
-        match = {
-            'vlan_vid': unexpressed_vid,
-            'eth_dst': self.UNKNOWN_MAC}
+        match = {"vlan_vid": unexpressed_vid, "eth_dst": self.UNKNOWN_MAC}
         table = self.network.tables[self.DP_ID]
         self.assertFalse(
-            table.is_output(match, port=ofp.OFPP_CONTROLLER, vid=unexpressed_vid))
+            table.is_output(match, port=ofp.OFPP_CONTROLLER, vid=unexpressed_vid)
+        )
 
     def test_topo(self):
         """Test DP is assigned appropriate edge/root states"""
         dp = self.valves_manager.valves[self.DP_ID].dp
         self.assertFalse(dp.stack.is_root())
         self.assertTrue(dp.stack.is_edge())
 
@@ -1150,33 +1285,30 @@
         stack_port = valve.dp.ports[1]
         other_dp = self.valves_manager.valves[2].dp
         other_port = other_dp.ports[1]
         other_valves = self.valves_manager._other_running_valves(valve)
         self.assertTrue(stack_port.is_stack_none())
         valve.fast_state_expire(self.mock_time(), other_valves)
         self.assertTrue(stack_port.is_stack_init())
-        for change_func, check_func in [
-                ('stack_up', 'is_stack_up')]:
+        for change_func, check_func in [("stack_up", "is_stack_up")]:
             getattr(other_port, change_func)()
             self.rcv_lldp(stack_port, other_dp, other_port)
             self.assertTrue(getattr(stack_port, check_func)(), msg=change_func)
 
     def test_stack_miscabling(self):
         """Test probing stack with miscabling."""
         valve = self.valves_manager.valves[self.DP_ID]
         stack_port = valve.dp.ports[1]
         other_dp = self.valves_manager.valves[2].dp
         other_port = other_dp.ports[1]
         wrong_port = other_dp.ports[2]
         wrong_dp = self.valves_manager.valves[3].dp
         other_valves = self.valves_manager._other_running_valves(valve)
         valve.fast_state_expire(self.mock_time(), other_valves)
-        for remote_dp, remote_port in [
-                (wrong_dp, other_port),
-                (other_dp, wrong_port)]:
+        for remote_dp, remote_port in [(wrong_dp, other_port), (other_dp, wrong_port)]:
             self.rcv_lldp(stack_port, other_dp, other_port)
             self.assertTrue(stack_port.is_stack_up())
             self.rcv_lldp(stack_port, remote_dp, remote_port)
             self.assertTrue(stack_port.is_stack_bad())
 
     def test_stack_lost_lldp(self):
         """Test stacking when LLDP packets get dropped"""
@@ -1191,25 +1323,27 @@
         # simulate packet loss
         valve.fast_state_expire(self.mock_time(300), other_valves)
         self.assertTrue(stack_port.is_stack_gone())
         valve.fast_state_expire(self.mock_time(300), other_valves)
         self.rcv_lldp(stack_port, other_dp, other_port)
         self.assertTrue(stack_port.is_stack_up())
         port_labels = {
-            'port': stack_port.name,
-            'port_description': stack_port.description,
-            'dp_name': valve.dp.name,
-            'dp_id': '0x%x' % valve.dp.dp_id
+            "port": stack_port.name,
+            "port_description": stack_port.description,
+            "dp_name": valve.dp.name,
+            "dp_id": "0x%x" % valve.dp.dp_id,
         }
         stack_change_count = self.get_prom(
-            'port_stack_state_change_count_total', labels=port_labels, bare=True)
+            "port_stack_state_change_count_total", labels=port_labels, bare=True
+        )
         self.assertEqual(
-            4, stack_change_count,
-            'Port %s DP %s expected stack change count %s differs from varz value %s'
-            % (stack_port, valve.dp.name, 4, stack_change_count)
+            4,
+            stack_change_count,
+            "Port %s DP %s expected stack change count %s differs from varz value %s"
+            % (stack_port, valve.dp.name, 4, stack_change_count),
         )
 
 
 class ValveStackGraphUpdateTestCase(ValveTestBases.ValveTestNetwork):
     """Valve test for updating the stack graph."""
 
     CONFIG = STACK_CONFIG
@@ -1231,60 +1365,60 @@
                     test_func(edge in graph.edges(keys=True))
 
         num_edges = 3
         self.all_stack_up()
         verify_stack_learn_edges(num_edges)
         valve = self.valves_manager.valves[self.DP_ID]
         ports = [valve.dp.ports[1], valve.dp.ports[2]]
-        edges = [('s1', 's2', 's1:1-s2:1'), ('s1', 's2', 's1:2-s2:2')]
+        edges = [("s1", "s2", "s1:1-s2:1"), ("s1", "s2", "s1:2-s2:2")]
         for port, edge in zip(ports, edges):
             num_edges -= 1
             self.down_stack_port(port)
             verify_stack_learn_edges(num_edges, edge, self.assertFalse)
         self.up_stack_port(ports[0])
         verify_stack_learn_edges(2, edges[0], self.assertTrue)
 
 
 class ValveStackGraphBreakTestCase(ValveStackLoopTest):
     """Valve test for updating the stack graph."""
 
     def validate_flooding(self, rerouted=False, portup=True):
         """Validate the flooding state of the stack"""
         vid = self.V100
-        self.validate_flood(1, vid, 1, False, 'flooded out input stack port')
-        self.validate_flood(1, vid, 2, portup, 'not flooded to stack root')
-        self.validate_flood(1, vid, 3, portup, 'not flooded to external host')
-        self.validate_flood(2, vid, 1, rerouted, 'flooded out other stack port')
-        self.validate_flood(2, vid, 2, False, 'flooded out input stack port')
-        self.validate_flood(2, vid, 3, True, 'not flooded to external host')
+        self.validate_flood(1, vid, 1, False, "flooded out input stack port")
+        self.validate_flood(1, vid, 2, portup, "not flooded to stack root")
+        self.validate_flood(1, vid, 3, portup, "not flooded to external host")
+        self.validate_flood(2, vid, 1, rerouted, "flooded out other stack port")
+        self.validate_flood(2, vid, 2, False, "flooded out input stack port")
+        self.validate_flood(2, vid, 3, True, "not flooded to external host")
         vid = 0
-        self.validate_flood(3, vid, 1, rerouted, 'flooded out inactive port')
-        self.validate_flood(3, vid, 2, True, 'not flooded to stack root')
-        self.validate_flood(3, vid, 3, False, 'flooded out hairpin')
+        self.validate_flood(3, vid, 1, rerouted, "flooded out inactive port")
+        self.validate_flood(3, vid, 2, True, "not flooded to stack root")
+        self.validate_flood(3, vid, 3, False, "flooded out hairpin")
 
     def test_update_stack_graph(self):
         """Test stack graph port UP and DOWN updates"""
 
         self.activate_all_ports()
         self.validate_flooding(False)
         table = self.network.tables[self.DP_ID]
-        self.assertLessEqual(table.flow_count(), 33, 'table overflow')
+        self.assertLessEqual(table.flow_count(), 33, "table overflow")
         # Deactivate link between the two other switches, not the one under test.
         other_dp = self.valves_manager.valves[2].dp
         other_port = other_dp.ports[2]
         self.deactivate_stack_port(other_port)
         self.validate_flooding(rerouted=True)
 
     def _set_max_lldp_lost(self, new_value):
         """Set the interface config option max_lldp_lost"""
         config = yaml_load(self.CONFIG)
-        for dp in config['dps'].values():
-            for interface in dp['interfaces'].values():
-                if 'stack' in interface:
-                    interface['max_lldp_lost'] = new_value
+        for dp in config["dps"].values():
+            for interface in dp["interfaces"].values():
+                if "stack" in interface:
+                    interface["max_lldp_lost"] = new_value
         return yaml_dump(config)
 
     def test_max_lldp_timeout(self):
         """Check that timeout can be increased"""
 
         valve = self.valves_manager.valves[self.DP_ID]
         port = valve.dp.ports[1]
@@ -1304,48 +1438,52 @@
         # Validate expected normal behavior with the port down.
         self.validate_flooding(portup=False)
 
         # Restore everything and set max_lldp_lost to 100.
         self.activate_stack_port(port)
         self.validate_flooding()
         new_config = self._set_max_lldp_lost(100)
-        self.update_config(new_config, reload_expected=False, no_reload_no_table_change=False)
+        self.update_config(
+            new_config, reload_expected=False, no_reload_no_table_change=False
+        )
         self.activate_all_ports()
         self.validate_flooding()
 
         # Like above, deactivate the port (stops LLDP beacons).
         self.deactivate_stack_port(port, packets=10)
 
         # After 10 packets (more than before), it should still work.
         self.validate_flooding()
 
         # But, after 100 more port should be down b/c limit is set to 100.
         self.trigger_all_ports(packets=100)
         self.validate_flooding(portup=False)
 
 
-class ValveTestIPV4StackedRouting(ValveTestBases.ValveTestStackedRouting):  # pylint: disable=too-few-public-methods
+class ValveTestIPV4StackedRouting(
+    ValveTestBases.ValveTestStackedRouting
+):  # pylint: disable=too-few-public-methods
     """Test inter-vlan routing with stacking capabilities in an IPV4 network"""
 
-    VLAN100_FAUCET_VIPS = '10.0.1.254'
-    VLAN100_FAUCET_VIP_SPACE = '10.0.1.254/24'
-    VLAN200_FAUCET_VIPS = '10.0.2.254'
-    VLAN200_FAUCET_VIP_SPACE = '10.0.2.254/24'
+    VLAN100_FAUCET_VIPS = "10.0.1.254"
+    VLAN100_FAUCET_VIP_SPACE = "10.0.1.254/24"
+    VLAN200_FAUCET_VIPS = "10.0.2.254"
+    VLAN200_FAUCET_VIP_SPACE = "10.0.2.254/24"
 
     def setUp(self):
         self.setup_stack_routing()
 
 
 class ValveTestIPV4StackedRoutingDPOneVLAN(ValveTestBases.ValveTestStackedRouting):
     """Test stacked intervlan routing when each DP has only one of the routed VLANs"""
 
-    VLAN100_FAUCET_VIPS = '10.0.1.254'
-    VLAN100_FAUCET_VIP_SPACE = '10.0.1.254/24'
-    VLAN200_FAUCET_VIPS = '10.0.2.254'
-    VLAN200_FAUCET_VIP_SPACE = '10.0.2.254/24'
+    VLAN100_FAUCET_VIPS = "10.0.1.254"
+    VLAN100_FAUCET_VIP_SPACE = "10.0.1.254/24"
+    VLAN200_FAUCET_VIPS = "10.0.2.254"
+    VLAN200_FAUCET_VIP_SPACE = "10.0.2.254/24"
 
     V100_HOSTS = [1]
     V200_HOSTS = [2]
 
     NUM_PORTS = 64
 
     @staticmethod
@@ -1381,18 +1519,18 @@
     def setUp(self):
         self.setup_stack_routing()
 
 
 class ValveTestIPV4StackedRoutingPathNoVLANS(ValveTestBases.ValveTestStackedRouting):
     """Test stacked intervlan routing when DP in path contains no routed VLANs"""
 
-    VLAN100_FAUCET_VIPS = '10.0.1.254'
-    VLAN100_FAUCET_VIP_SPACE = '10.0.1.254/24'
-    VLAN200_FAUCET_VIPS = '10.0.2.254'
-    VLAN200_FAUCET_VIP_SPACE = '10.0.2.254/24'
+    VLAN100_FAUCET_VIPS = "10.0.1.254"
+    VLAN100_FAUCET_VIP_SPACE = "10.0.1.254/24"
+    VLAN200_FAUCET_VIPS = "10.0.2.254"
+    VLAN200_FAUCET_VIP_SPACE = "10.0.2.254/24"
 
     V100_HOSTS = [1]
     V200_HOSTS = [3]
 
     def create_config(self):
         """Create the config file"""
         # pylint: disable=attribute-defined-outside-init
@@ -1405,17 +1543,21 @@
         vlan200:
             vid: 0x200
             faucet_mac: '%s'
             faucet_vips: ['%s']
         vlan300:
             vid: 0x300
     %s
-           """ % (self.VLAN100_FAUCET_MAC, self.VLAN100_FAUCET_VIP_SPACE,
-                  self.VLAN200_FAUCET_MAC, self.VLAN200_FAUCET_VIP_SPACE,
-                  self.base_config())
+           """ % (
+            self.VLAN100_FAUCET_MAC,
+            self.VLAN100_FAUCET_VIP_SPACE,
+            self.VLAN200_FAUCET_MAC,
+            self.VLAN200_FAUCET_VIP_SPACE,
+            self.base_config(),
+        )
 
     @staticmethod
     def base_config():
         """Create the base config"""
         return """
     routers:
         router1:
@@ -1463,53 +1605,53 @@
     def setUp(self):
         self.setup_stack_routing()
 
 
 class ValveTestIPV6StackedRouting(ValveTestBases.ValveTestStackedRouting):
     """Test inter-vlan routing with stacking capabilities in an IPV6 network"""
 
-    VLAN100_FAUCET_VIPS = 'fc80::1:254'
-    VLAN200_FAUCET_VIPS = 'fc80::2:254'
-    VLAN100_FAUCET_VIP_SPACE = 'fc80::1:254/64'
-    VLAN200_FAUCET_VIP_SPACE = 'fc80::1:254/64'
+    VLAN100_FAUCET_VIPS = "fc80::1:254"
+    VLAN200_FAUCET_VIPS = "fc80::2:254"
+    VLAN100_FAUCET_VIP_SPACE = "fc80::1:254/64"
+    VLAN200_FAUCET_VIP_SPACE = "fc80::1:254/64"
 
     def setUp(self):
         self.setup_stack_routing()
 
     @staticmethod
     def create_ip(vindex, host):
         """Create a IP address string"""
-        return 'fc80::%u:%u' % (vindex, host)
+        return "fc80::%u:%u" % (vindex, host)
 
     @staticmethod
     def get_eth_type():
         """Returns IPV6 ether type"""
         return valve_of.ether.ETH_TYPE_IPV6
 
     def create_match(self, vindex, host, faucet_mac, faucet_vip, _code):
         """Create an NA message"""
         return {
-            'eth_src': self.create_mac(vindex, host),
-            'eth_dst': faucet_mac,
-            'ipv6_src': self.create_ip(vindex, host),
-            'ipv6_dst': faucet_vip,
-            'neighbor_advert_ip': self.create_ip(vindex, host)
+            "eth_src": self.create_mac(vindex, host),
+            "eth_dst": faucet_mac,
+            "ipv6_src": self.create_ip(vindex, host),
+            "ipv6_dst": faucet_vip,
+            "neighbor_advert_ip": self.create_ip(vindex, host),
         }
 
 
 class ValveInterVLANStackFlood(ValveTestBases.ValveTestNetwork):
     """Test that the stack ports get flooded to for interVLAN packets"""
 
-    VLAN100_FAUCET_MAC = '00:00:00:00:00:11'
-    VLAN200_FAUCET_MAC = '00:00:00:00:00:22'
-    VLAN100_FAUCET_VIPS = '10.1.0.254'
-    VLAN100_FAUCET_VIP_SPACE = '10.1.0.254/24'
-    VLAN200_FAUCET_VIPS = '10.2.0.254'
-    VLAN200_FAUCET_VIP_SPACE = '10.2.0.254/24'
-    DST_ADDRESS = ipaddress.IPv4Address('10.1.0.1')
+    VLAN100_FAUCET_MAC = "00:00:00:00:00:11"
+    VLAN200_FAUCET_MAC = "00:00:00:00:00:22"
+    VLAN100_FAUCET_VIPS = "10.1.0.254"
+    VLAN100_FAUCET_VIP_SPACE = "10.1.0.254/24"
+    VLAN200_FAUCET_VIPS = "10.2.0.254"
+    VLAN200_FAUCET_VIP_SPACE = "10.2.0.254/24"
+    DST_ADDRESS = ipaddress.IPv4Address("10.1.0.1")
 
     @staticmethod
     def base_config():
         """Create the base config"""
         return """
 routers:
     router1:
@@ -1571,84 +1713,104 @@
         faucet_mac: '%s'
         faucet_vips: ['%s']
     vlan200:
         vid: 200
         faucet_mac: '%s'
         faucet_vips: ['%s']
 %s
-        """ % (self.VLAN100_FAUCET_MAC, self.VLAN100_FAUCET_VIP_SPACE,
-               self.VLAN200_FAUCET_MAC, self.VLAN200_FAUCET_VIP_SPACE,
-               self.base_config())
+        """ % (
+            self.VLAN100_FAUCET_MAC,
+            self.VLAN100_FAUCET_VIP_SPACE,
+            self.VLAN200_FAUCET_MAC,
+            self.VLAN200_FAUCET_VIP_SPACE,
+            self.base_config(),
+        )
 
     def setUp(self):
         """Create a stacking config file."""
         self.create_config()
         self.setup_valves(self.CONFIG)
         self.trigger_stack_ports()
 
     @staticmethod
     def stack_manager_flood_ports(stack_manager):
         """Return list of port numbers that will be flooded to"""
         stack_manager.reset_peer_distances()
         ports = list()
         if stack_manager.stack.is_root():
-            ports = (stack_manager.away_ports - stack_manager.inactive_away_ports
-                     - stack_manager.pruned_away_ports)
+            ports = (
+                stack_manager.away_ports
+                - stack_manager.inactive_away_ports
+                - stack_manager.pruned_away_ports
+            )
         else:
             ports = [stack_manager.chosen_towards_port]
         return sorted([port.number for port in ports])
 
     def route_manager_ofmsgs(self, route_manager, vlan):
         """Return ofmsgs for route stack link flooding"""
         faucet_vip = list(vlan.faucet_vips_by_ipv(4))[0].ip
         ofmsgs = route_manager._flood_stack_links(
-            route_manager._gw_resolve_pkt(), vlan, route_manager.multi_out,
-            vlan.faucet_mac, valve_of.mac.BROADCAST_STR,
-            faucet_vip, self.DST_ADDRESS)
+            route_manager._gw_resolve_pkt(),
+            vlan,
+            route_manager.multi_out,
+            vlan.faucet_mac,
+            valve_of.mac.BROADCAST_STR,
+            faucet_vip,
+            self.DST_ADDRESS,
+        )
         return ofmsgs
 
     def test_flood_towards_root_from_s1(self):
         """Test intervlan flooding goes towards the root"""
         output_ports = [3]
         valve = self.valves_manager.valves[1]
         ports = self.stack_manager_flood_ports(valve.stack_manager)
-        self.assertEqual(output_ports, ports, 'InterVLAN flooding does not match expected')
+        self.assertEqual(
+            output_ports, ports, "InterVLAN flooding does not match expected"
+        )
         route_manager = valve._route_manager_by_ipv.get(4, None)
         vlan = valve.dp.vlans[100]
         ofmsgs = self.route_manager_ofmsgs(route_manager, vlan)
         self.assertTrue(ValveTestBases.packet_outs_from_flows(ofmsgs))
 
     def test_flood_away_from_root(self):
         """Test intervlan flooding goes away from the root"""
         output_ports = [3, 4]
         valve = self.valves_manager.valves[2]
         ports = self.stack_manager_flood_ports(valve.stack_manager)
-        self.assertEqual(output_ports, ports, 'InterVLAN flooding does not match expected')
+        self.assertEqual(
+            output_ports, ports, "InterVLAN flooding does not match expected"
+        )
         route_manager = valve._route_manager_by_ipv.get(4, None)
         vlan = valve.dp.vlans[100]
         ofmsgs = self.route_manager_ofmsgs(route_manager, vlan)
         self.assertTrue(ValveTestBases.packet_outs_from_flows(ofmsgs))
 
     def test_flood_towards_root_from_s3(self):
         """Test intervlan flooding only goes towards the root (s4 will get the reflection)"""
         output_ports = [3]
         valve = self.valves_manager.valves[3]
         ports = self.stack_manager_flood_ports(valve.stack_manager)
-        self.assertEqual(output_ports, ports, 'InterVLAN flooding does not match expected')
+        self.assertEqual(
+            output_ports, ports, "InterVLAN flooding does not match expected"
+        )
         route_manager = valve._route_manager_by_ipv.get(4, None)
         vlan = valve.dp.vlans[100]
         ofmsgs = self.route_manager_ofmsgs(route_manager, vlan)
         self.assertTrue(ValveTestBases.packet_outs_from_flows(ofmsgs))
 
     def test_flood_towards_root_from_s4(self):
         """Test intervlan flooding goes towards the root (through s3)"""
         output_ports = [3]
         valve = self.valves_manager.valves[4]
         ports = self.stack_manager_flood_ports(valve.stack_manager)
-        self.assertEqual(output_ports, ports, 'InterVLAN flooding does not match expected')
+        self.assertEqual(
+            output_ports, ports, "InterVLAN flooding does not match expected"
+        )
         route_manager = valve._route_manager_by_ipv.get(4, None)
         vlan = valve.dp.vlans[100]
         ofmsgs = self.route_manager_ofmsgs(route_manager, vlan)
         self.assertTrue(ValveTestBases.packet_outs_from_flows(ofmsgs))
 
 
 class ValveTestTunnel2DP(ValveTestBases.ValveTestTunnel):
@@ -1733,88 +1895,147 @@
         """Test tunnel rules when encapsulating and forwarding to the destination switch"""
         valve = self.valves_manager.valves[0x1]
         port = valve.dp.ports[3]
         # Apply tunnel to ofmsgs on valve
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should encapsulate and output packet towards tunnel destination s3
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.SRC_ID, True,
-            'Did not encapsulate and forward')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.SRC_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
         new_config_yaml = yaml_load(self.CONFIG)
-        new_config_yaml['dps']['s1']['interfaces'][1]['description'] = 'changed'
+        new_config_yaml["dps"]["s1"]["interfaces"][1]["description"] = "changed"
         self.update_config(yaml_dump(new_config_yaml), reload_type=None)
         self.activate_all_ports()
         # warm start with no topo change with tunnel.
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.SRC_ID, True,
-            'Did not encapsulate and forward')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.SRC_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
         # Set the chosen port down to force a recalculation on the tunnel path
         self.set_port_down(port.number)
         ofmsgs = valve.stack_manager.add_tunnel_acls()
-        self.assertTrue(ofmsgs, 'No tunnel ofmsgs returned after a topology change')
+        self.assertTrue(ofmsgs, "No tunnel ofmsgs returned after a topology change")
         self.apply_ofmsgs(ofmsgs)
         # Should encapsulate and output packet using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.SRC_ID, True,
-            'Did not encapsulate and forward out re-calculated port')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.SRC_ID,
+            True,
+            "Did not encapsulate and forward out re-calculated port",
+        )
         self.flap_port(1)
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.SRC_ID, True,
-            'Did not encapsulate and forward after port flap')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.SRC_ID,
+            True,
+            "Did not encapsulate and forward after port flap",
+        )
 
     def test_update_same_tunnel(self):
         """Test tunnel rules when outputting to host on the same switch as the source"""
         valve = self.valves_manager.valves[0x1]
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            2, 0, 1, 0, True, 'Did not forward to host on same DP')
+            self.DP_ID,
+            self.DP_ID,
+            2,
+            0,
+            1,
+            0,
+            True,
+            "Did not forward to host on same DP",
+        )
 
     def test_update_dst_tunnel(self):
         """Test a tunnel outputting to the correct tunnel destination"""
         valve = self.valves_manager.valves[0x1]
         port = valve.dp.ports[3]
         # Apply tunnel to ofmsgs on valve
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should accept encapsulated packet and output to the destination host
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            3, self.DST_ID, 1, 0, True, 'Did not output to host',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            3,
+            self.DST_ID,
+            1,
+            0,
+            True,
+            "Did not output to host",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
         # Set the chosen port down to force a recalculation on the tunnel path
         self.set_port_down(port.number)
         ofmsgs = valve.stack_manager.add_tunnel_acls()
-        self.assertTrue(ofmsgs, 'No tunnel ofmsgs returned after a topology change')
+        self.assertTrue(ofmsgs, "No tunnel ofmsgs returned after a topology change")
         self.apply_ofmsgs(ofmsgs)
         # Should accept encapsulated packet and output using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            4, self.DST_ID, 1, 0, True, 'Did not output to host',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            4,
+            self.DST_ID,
+            1,
+            0,
+            True,
+            "Did not output to host",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
 
     def test_update_none_tunnel(self):
         """Test tunnel on a switch not using a tunnel ACL"""
         valve = self.valves_manager.valves[0x1]
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should drop any packets received from the tunnel
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            5, self.NONE_ID, None, None, False,
-            'Should not output a packet')
+            self.DP_ID,
+            self.DP_ID,
+            5,
+            self.NONE_ID,
+            None,
+            None,
+            False,
+            "Should not output a packet",
+        )
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            6, self.NONE_ID, None, None, False,
-            'Should not output a packet')
+            self.DP_ID,
+            self.DP_ID,
+            6,
+            self.NONE_ID,
+            None,
+            None,
+            False,
+            "Should not output a packet",
+        )
 
 
-class ValveTestTransitTunnel(ValveTestBases.ValveTestTunnel):  # pylint: disable=too-few-public-methods
+class ValveTestTransitTunnel(
+    ValveTestBases.ValveTestTunnel
+):  # pylint: disable=too-few-public-methods
     """Test tunnel ACL implementation"""
 
     TRANSIT_ID = 2
 
     CONFIG = """
 acls:
     transit_acl:
@@ -1872,37 +2093,55 @@
         valve = self.valves_manager.valves[0x1]
         port1 = valve.dp.ports[3]
         port2 = valve.dp.ports[5]
         # Apply tunnel to ofmsgs on valve
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should accept packet from stack and output to the next switch
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            3, self.TRANSIT_ID, 5, self.TRANSIT_ID, True,
-            'Did not output to next switch',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            3,
+            self.TRANSIT_ID,
+            5,
+            self.TRANSIT_ID,
+            True,
+            "Did not output to next switch",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
         # Set the chosen port down to force a recalculation on the tunnel path
         self.set_port_down(port1.number)
         # Should accept encapsulated packet and output using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            4, self.TRANSIT_ID, 5, self.TRANSIT_ID, True,
-            'Did not output to next switch',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            4,
+            self.TRANSIT_ID,
+            5,
+            self.TRANSIT_ID,
+            True,
+            "Did not output to next switch",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
         # Set the chosen port to the next switch down to force a path recalculation
         self.set_port_down(port2.number)
         ofmsgs = valve.stack_manager.add_tunnel_acls()
-        self.assertTrue(ofmsgs, 'No tunnel ofmsgs returned after a topology change')
+        self.assertTrue(ofmsgs, "No tunnel ofmsgs returned after a topology change")
         self.apply_ofmsgs(ofmsgs)
         # Should accept encapsulated packet and output using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            4, self.TRANSIT_ID, 6, self.TRANSIT_ID, True,
-            'Did not output to next switch',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            4,
+            self.TRANSIT_ID,
+            6,
+            self.TRANSIT_ID,
+            True,
+            "Did not output to next switch",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
 
 
 class ValveTestMultipleTunnel(ValveTestBases.ValveTestTunnel):
     """Test tunnel ACL implementation with multiple hosts containing tunnel ACL"""
 
     TUNNEL_ID = 2
 
@@ -1947,48 +2186,72 @@
                 stack: {dp: s1, port: 3}
             4:
                 stack: {dp: s1, port: 4}
 """
 
     def test_new_tunnel_source(self):
         config = yaml_load(self.CONFIG)
-        config['dps']['s1']['interfaces'][5]['acls_in'] = ['tunnel_acl']
-        self.update_config(yaml_dump(config), reload_type='warm')
+        config["dps"]["s1"]["interfaces"][5]["acls_in"] = ["tunnel_acl"]
+        self.update_config(yaml_dump(config), reload_type="warm")
         self.activate_all_ports()
         self.test_tunnel_update_multiple_tunnels()
 
     def test_tunnel_update_multiple_tunnels(self):
         """Test having multiple hosts with the same tunnel"""
         valve = self.valves_manager.valves[0x1]
         port = valve.dp.ports[3]
         # Apply tunnel to ofmsgs on valve
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should encapsulate and output packet towards tunnel destination s3
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward')
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            2, 0, 3, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            2,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
         # Set the chosen port down to force a recalculation on the tunnel path
         self.set_port_down(port.number)
         ofmsgs = valve.stack_manager.add_tunnel_acls()
-        self.assertTrue(ofmsgs, 'No tunnel ofmsgs returned after a topology change')
+        self.assertTrue(ofmsgs, "No tunnel ofmsgs returned after a topology change")
         self.apply_ofmsgs(ofmsgs)
         # Should encapsulate and output packet using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward out re-calculated port')
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward out re-calculated port')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward out re-calculated port",
+        )
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward out re-calculated port",
+        )
 
 
 class ValveTestOrderedTunnel2DP(ValveTestBases.ValveTestTunnel):
     """Test Tunnel ACL implementation"""
 
     SRC_ID = 6
     DST_ID = 2
@@ -2075,79 +2338,133 @@
         """Test tunnel rules when encapsulating and forwarding to the destination switch"""
         valve = self.valves_manager.valves[0x1]
         port = valve.dp.ports[3]
         # Apply tunnel to ofmsgs on valve
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should encapsulate and output packet towards tunnel destination s3
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.SRC_ID, True,
-            'Did not encapsulate and forward')
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.SRC_ID, True,
-            'Did not encapsulate and forward',
-            eth_type=0x86dd, ip_proto=56)
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.SRC_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.SRC_ID,
+            True,
+            "Did not encapsulate and forward",
+            eth_type=0x86DD,
+            ip_proto=56,
+        )
         # Set the chosen port down to force a recalculation on the tunnel path
         self.set_port_down(port.number)
         ofmsgs = valve.stack_manager.add_tunnel_acls()
-        self.assertTrue(ofmsgs, 'No tunnel ofmsgs returned after a topology change')
+        self.assertTrue(ofmsgs, "No tunnel ofmsgs returned after a topology change")
         self.apply_ofmsgs(ofmsgs)
         # Should encapsulate and output packet using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.SRC_ID, True,
-            'Did not encapsulate and forward out re-calculated port')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.SRC_ID,
+            True,
+            "Did not encapsulate and forward out re-calculated port",
+        )
 
     def test_update_same_tunnel(self):
         """Test tunnel rules when outputting to host on the same switch as the source"""
         valve = self.valves_manager.valves[0x1]
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            2, 0, 1, 0, True, 'Did not forward to host on same DP')
+            self.DP_ID,
+            self.DP_ID,
+            2,
+            0,
+            1,
+            0,
+            True,
+            "Did not forward to host on same DP",
+        )
 
     def test_update_dst_tunnel(self):
         """Test a tunnel outputting to the correct tunnel destination"""
         valve = self.valves_manager.valves[0x1]
         port = valve.dp.ports[3]
         # Apply tunnel to ofmsgs on valve
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should accept encapsulated packet and output to the destination host
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            3, self.DST_ID, 1, 0, True, 'Did not output to host',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            3,
+            self.DST_ID,
+            1,
+            0,
+            True,
+            "Did not output to host",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
         # Set the chosen port down to force a recalculation on the tunnel path
         self.set_port_down(port.number)
         ofmsgs = valve.stack_manager.add_tunnel_acls()
-        self.assertTrue(ofmsgs, 'No tunnel ofmsgs returned after a topology change')
+        self.assertTrue(ofmsgs, "No tunnel ofmsgs returned after a topology change")
         self.apply_ofmsgs(ofmsgs)
         # Should accept encapsulated packet and output using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            4, self.DST_ID, 1, 0, True, 'Did not output to host',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            4,
+            self.DST_ID,
+            1,
+            0,
+            True,
+            "Did not output to host",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
 
     def test_update_none_tunnel(self):
         """Test tunnel on a switch not using a tunnel ACL"""
         valve = self.valves_manager.valves[0x1]
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should drop any packets received from the tunnel
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            5, self.NONE_ID, None, None, False,
-            'Should not output a packet')
+            self.DP_ID,
+            self.DP_ID,
+            5,
+            self.NONE_ID,
+            None,
+            None,
+            False,
+            "Should not output a packet",
+        )
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            6, self.NONE_ID, None, None, False,
-            'Should not output a packet')
+            self.DP_ID,
+            self.DP_ID,
+            6,
+            self.NONE_ID,
+            None,
+            None,
+            False,
+            "Should not output a packet",
+        )
 
 
-class ValveTestTransitOrderedTunnel(ValveTestBases.ValveTestTunnel):  # pylint: disable=too-few-public-methods
+class ValveTestTransitOrderedTunnel(
+    ValveTestBases.ValveTestTunnel
+):  # pylint: disable=too-few-public-methods
     """Test tunnel ACL implementation"""
 
     TRANSIT_ID = 2
 
     CONFIG = """
 acls:
     transit_acl:
@@ -2205,40 +2522,60 @@
         valve = self.valves_manager.valves[0x1]
         port1 = valve.dp.ports[3]
         port2 = valve.dp.ports[5]
         # Apply tunnel to ofmsgs on valve
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should accept packet from stack and output to the next switch
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            3, self.TRANSIT_ID, 5, self.TRANSIT_ID, True,
-            'Did not output to next switch',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            3,
+            self.TRANSIT_ID,
+            5,
+            self.TRANSIT_ID,
+            True,
+            "Did not output to next switch",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
         # Set the chosen port down to force a recalculation on the tunnel path
         self.set_port_down(port1.number)
         # Should accept encapsulated packet and output using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            4, self.TRANSIT_ID, 5, self.TRANSIT_ID, True,
-            'Did not output to next switch',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            4,
+            self.TRANSIT_ID,
+            5,
+            self.TRANSIT_ID,
+            True,
+            "Did not output to next switch",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
         # Set the chosen port to the next switch down to force a path recalculation
         self.set_port_down(port2.number)
         ofmsgs = valve.stack_manager.add_tunnel_acls()
-        self.assertTrue(ofmsgs, 'No tunnel ofmsgs returned after a topology change')
+        self.assertTrue(ofmsgs, "No tunnel ofmsgs returned after a topology change")
         self.apply_ofmsgs(ofmsgs)
         # Should accept encapsulated packet and output using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            4, self.TRANSIT_ID, 6, self.TRANSIT_ID, True,
-            'Did not output to next switch',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            4,
+            self.TRANSIT_ID,
+            6,
+            self.TRANSIT_ID,
+            True,
+            "Did not output to next switch",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
 
 
-class ValveTestMultipleOrderedTunnel(ValveTestBases.ValveTestTunnel):  # pylint: disable=too-few-public-methods
+class ValveTestMultipleOrderedTunnel(
+    ValveTestBases.ValveTestTunnel
+):  # pylint: disable=too-few-public-methods
     """Test tunnel ACL implementation with multiple hosts containing tunnel ACL"""
 
     TUNNEL_ID = 2
 
     CONFIG = """
 acls:
     tunnel_acl:
@@ -2284,38 +2621,64 @@
         """Test having multiple hosts with the same tunnel"""
         valve = self.valves_manager.valves[0x1]
         port = valve.dp.ports[3]
         # Apply tunnel to ofmsgs on valve
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should encapsulate and output packet towards tunnel destination s3
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward')
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            2, 0, 3, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            2,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
         # Set the chosen port down to force a recalculation on the tunnel path
         self.set_port_down(port.number)
         ofmsgs = valve.stack_manager.add_tunnel_acls()
-        self.assertTrue(ofmsgs, 'No tunnel ofmsgs returned after a topology change')
+        self.assertTrue(ofmsgs, "No tunnel ofmsgs returned after a topology change")
         self.apply_ofmsgs(ofmsgs)
         # Should encapsulate and output packet using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward out re-calculated port')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward out re-calculated port",
+        )
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward out re-calculated port')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward out re-calculated port",
+        )
 
 
-class ValveTestMultipleOrderedDPTunnelACL(ValveTestBases.ValveTestTunnel):  # pylint: disable=too-few-public-methods
+class ValveTestMultipleOrderedDPTunnelACL(
+    ValveTestBases.ValveTestTunnel
+):  # pylint: disable=too-few-public-methods
     """Test tunnel DP ACL implementation with multiple hosts/DP containing tunnel ACL"""
 
     TUNNEL_ID = 2
 
     CONFIG = """
 acls:
     tunnel_acl:
@@ -2361,35 +2724,59 @@
         """Test having multiple hosts with the same tunnel"""
         valve = self.valves_manager.valves[0x1]
         port = valve.dp.ports[3]
         # Apply tunnel to ofmsgs on valve
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should encapsulate and output packet towards tunnel destination s3
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward')
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            2, 0, 3, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            2,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
         # Set the chosen port down to force a recalculation on the tunnel path
         self.set_port_down(port.number)
         ofmsgs = valve.stack_manager.add_tunnel_acls()
-        self.assertTrue(ofmsgs, 'No tunnel ofmsgs returned after a topology change')
+        self.assertTrue(ofmsgs, "No tunnel ofmsgs returned after a topology change")
         self.apply_ofmsgs(ofmsgs)
         # Should encapsulate and output packet using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward out re-calculated port')
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward out re-calculated port')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward out re-calculated port",
+        )
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward out re-calculated port",
+        )
 
 
 class ValveTestMultipleOrderedTunnelDestinationDPACL(
     ValveTestBases.ValveTestTunnel
 ):  # pylint: disable=too-few-public-methods
     """Test tunnel DP ACL implementation with a tunnel ACL with a DP destination"""
 
@@ -2440,38 +2827,64 @@
         """Test having multiple hosts with the same tunnel"""
         valve = self.valves_manager.valves[0x1]
         port = valve.dp.ports[3]
         # Apply tunnel to ofmsgs on valve
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         # Should encapsulate and output packet towards tunnel destination s3
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward')
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            2, 0, 3, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            2,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
         # Set the chosen port down to force a recalculation on the tunnel path
         self.set_port_down(port.number)
         ofmsgs = valve.stack_manager.add_tunnel_acls()
-        self.assertTrue(ofmsgs, 'No tunnel ofmsgs returned after a topology change')
+        self.assertTrue(ofmsgs, "No tunnel ofmsgs returned after a topology change")
         self.apply_ofmsgs(ofmsgs)
         # Should encapsulate and output packet using the new path
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward out re-calculated port')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward out re-calculated port",
+        )
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 4, self.TUNNEL_ID, True,
-            'Did not encapsulate and forward out re-calculated port')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            4,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward out re-calculated port",
+        )
 
 
-class ValveTestOrderedTunnelExitInstructions(ValveTestBases.ValveTestTunnel):  # pylint: disable=too-few-public-methods
+class ValveTestOrderedTunnelExitInstructions(
+    ValveTestBases.ValveTestTunnel
+):  # pylint: disable=too-few-public-methods
     """Test tunnel DP ACL implementation with a tunnel ACL with exit instructions"""
 
     TUNNEL_ID = 2
 
     CONFIG = """
 acls:
     tunnel_acl:
@@ -2509,23 +2922,39 @@
             3:
                 stack: {dp: s1, port: 3}
 """
 
     def test_tunnel_additional_exit_instructions(self):
         """Test having additional exit instructions"""
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.TUNNEL_ID, True, 'Did not encapsulate and forward')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
         self.validate_tunnel(
-            int(0x2), int(0x2),
-            3, self.TUNNEL_ID, 1, 101, True, 'Did not apply additional exit instructions',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            int(0x2),
+            int(0x2),
+            3,
+            self.TUNNEL_ID,
+            1,
+            101,
+            True,
+            "Did not apply additional exit instructions",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
 
 
-class ValveTestRemoteDHCPCoprocessorTunnelACL(ValveTestBases.ValveTestTunnel):  # pylint: disable=too-few-public-methods
+class ValveTestRemoteDHCPCoprocessorTunnelACL(
+    ValveTestBases.ValveTestTunnel
+):  # pylint: disable=too-few-public-methods
     """Test bi_directional tunnel implementation to a remote coprocessor port with a DHCP server"""
 
     SW1_TUNNEL_ID = 101
     SW2_TUNNEL_ID = 102
 
     CONFIG = """
 acls:
@@ -2633,43 +3062,70 @@
         faucet_vips: [10.1.0.254/24]
         vid: 100
 """
 
     def test_tunnel_remote_bi_directional_tunnel_coprocessor(self):
         """Test having a bi_directional tunnel to a remote coprocessor"""
         dhcp_options = {
-            'ip_proto': 17,
-            'eth_type': 0x0800,
-            'udp_dst': 67,
-            'udp_src': 68}
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            3, 0, 4, self.SW1_TUNNEL_ID, True,
-            'Did not encapsulate and output to coprocessor on same switch',
-            packet_match=dhcp_options)
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            4, [self.SW1_TUNNEL_ID, 100], 3, 0, True,
-            'Did not output reverse, return DHCP packet to host on the same switch',
+            "ip_proto": 17,
+            "eth_type": 0x0800,
+            "udp_dst": 67,
+            "udp_src": 68,
+        }
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            3,
+            0,
+            4,
+            self.SW1_TUNNEL_ID,
+            True,
+            "Did not encapsulate and output to coprocessor on same switch",
+            packet_match=dhcp_options,
+        )
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            4,
+            [self.SW1_TUNNEL_ID, 100],
+            3,
+            0,
+            True,
+            "Did not output reverse, return DHCP packet to host on the same switch",
             pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
-            packet_match=dhcp_options)
+            packet_match=dhcp_options,
+        )
+        self.validate_tunnel(
+            2,
+            self.DP_ID,
+            3,
+            0,
+            4,
+            self.SW2_TUNNEL_ID,
+            True,
+            "Did not encapsulate and output to coprocessor on remote switch",
+            packet_match=dhcp_options,
+        )
         self.validate_tunnel(
-            2, self.DP_ID,
-            3, 0, 4, self.SW2_TUNNEL_ID, True,
-            'Did not encapsulate and output to coprocessor on remote switch',
-            packet_match=dhcp_options)
-        self.validate_tunnel(
-            self.DP_ID, 2,
-            4, [self.SW2_TUNNEL_ID, 100], 3, 0, True,
-            'Did not output reverse, return DHCP packet to host on the remote switch',
+            self.DP_ID,
+            2,
+            4,
+            [self.SW2_TUNNEL_ID, 100],
+            3,
+            0,
+            True,
+            "Did not output reverse, return DHCP packet to host on the remote switch",
             pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
-            packet_match=dhcp_options)
+            packet_match=dhcp_options,
+        )
 
 
-class ValveTestOrderedBiDirectionalTunnelACL(ValveTestBases.ValveTestTunnel):  # pylint: disable=too-few-public-methods
+class ValveTestOrderedBiDirectionalTunnelACL(
+    ValveTestBases.ValveTestTunnel
+):  # pylint: disable=too-few-public-methods
     """Test tunnel DP ACL implementation with a tunnel ACL with bidirectionality"""
 
     TUNNEL_ID = 2
 
     CONFIG = """
 acls:
     tunnel_acl:
@@ -2709,23 +3165,35 @@
 """
 
     def test_tunnel_bidirectionality(self):
         """Test bidirectionality on a tunnel"""
         valve = self.valves_manager.valves[0x1]
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         self.validate_tunnel(
-            int(0x2), int(0x2),
-            1, self.TUNNEL_ID, 3, self.TUNNEL_ID, True,
-            'Did not accept reverse tunnel packet',
-            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG)
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            3, self.TUNNEL_ID, 1, 0, True,
-            'Did not output to original source, the reverse tunnelled packet',
-            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG)
+            int(0x2),
+            int(0x2),
+            1,
+            self.TUNNEL_ID,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not accept reverse tunnel packet",
+            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
+        )
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            3,
+            self.TUNNEL_ID,
+            1,
+            0,
+            True,
+            "Did not output to original source, the reverse tunnelled packet",
+            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
+        )
 
 
 class ValveTestOrderedMaintainTunnelEncapsulationACL(
     ValveTestBases.ValveTestTunnel
 ):  # pylint: disable=too-few-public-methods
     """Test tunnel maintains encapsulation with maintain_encapsulation option"""
 
@@ -2771,20 +3239,34 @@
 """
 
     def test_tunnel_maintain_encapsulation(self):
         """Test having tunnel with maintain_encapsulation option, maintains encapsulation"""
         valve = self.valves_manager.valves[0x1]
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.TUNNEL_ID, True, 'Did not encapsulate and forward')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
         self.validate_tunnel(
-            int(0x2), int(0x2),
-            3, self.TUNNEL_ID, 1, self.TUNNEL_ID, True, 'Did not maintain tunnel encapsulation',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            int(0x2),
+            int(0x2),
+            3,
+            self.TUNNEL_ID,
+            1,
+            self.TUNNEL_ID,
+            True,
+            "Did not maintain tunnel encapsulation",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
 
 
 class ValveTestOrderedBiDirectionalDPTunnelACL(
     ValveTestBases.ValveTestTunnel
 ):  # pylint: disable=too-few-public-methods
     """Test tunnel DP ACL implementation with a tunnel DP ACL with bidirectionality"""
 
@@ -2831,22 +3313,35 @@
 """
 
     def test_tunnel_bi_directional_dp_acl(self):
         """Test bi-directionality on a DP ACL"""
         valve = self.valves_manager.valves[0x1]
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         self.validate_tunnel(
-            int(0x2), int(0x2),
-            1, self.TUNNEL_ID, 3, self.TUNNEL_ID, True, 'Did not accept reverse tunnel packet',
-            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG)
-        self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            3, [self.TUNNEL_ID, 1], 1, 0,
-            True, 'Did not output to original source, the reverse tunnelled packet',
-            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG)
+            int(0x2),
+            int(0x2),
+            1,
+            self.TUNNEL_ID,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not accept reverse tunnel packet",
+            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
+        )
+        self.validate_tunnel(
+            self.DP_ID,
+            self.DP_ID,
+            3,
+            [self.TUNNEL_ID, 1],
+            1,
+            0,
+            True,
+            "Did not output to original source, the reverse tunnelled packet",
+            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
+        )
 
 
 class ValveTestOrderedBidirectionalTunnelACLwithExitInstructions(
     ValveTestBases.ValveTestTunnel
 ):  # pylint: disable=too-few-public-methods
     """Test tunnel implementation with bi-directionality and exit instructions"""
 
@@ -2894,32 +3389,61 @@
 
     def test_tunnel_bi_directional_with_exit_instructions(self):
         """Test bi-directionality tunnel with exit instructions"""
         # The exit instructions will only apply to the forward tunnel
         valve = self.valves_manager.valves[0x1]
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            1, 0, 3, self.TUNNEL_ID, True, 'Did not encapsulate and forward')
+            self.DP_ID,
+            self.DP_ID,
+            1,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and forward",
+        )
         self.validate_tunnel(
-            int(0x2), int(0x2),
-            3, self.TUNNEL_ID, 1, 101, True, 'Did not apply additional exit instructions',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            int(0x2),
+            int(0x2),
+            3,
+            self.TUNNEL_ID,
+            1,
+            101,
+            True,
+            "Did not apply additional exit instructions",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
         self.validate_tunnel(
-            int(0x2), int(0x2),
-            1, self.TUNNEL_ID, 3, self.TUNNEL_ID, True, 'Did not accept reverse tunnel packet',
-            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG)
+            int(0x2),
+            int(0x2),
+            1,
+            self.TUNNEL_ID,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not accept reverse tunnel packet",
+            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
+        )
         self.validate_tunnel(
-            self.DP_ID, self.DP_ID,
-            3, [self.TUNNEL_ID, 1], 1, 0,
-            True, 'Did not output to original source, the reverse tunnelled packet',
-            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG)
+            self.DP_ID,
+            self.DP_ID,
+            3,
+            [self.TUNNEL_ID, 1],
+            1,
+            0,
+            True,
+            "Did not output to original source, the reverse tunnelled packet",
+            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
+        )
 
 
-class ValveTestOrderedReverseTunnelOption(ValveTestBases.ValveTestTunnel):  # pylint: disable=too-few-public-methods
+class ValveTestOrderedReverseTunnelOption(
+    ValveTestBases.ValveTestTunnel
+):  # pylint: disable=too-few-public-methods
     """Test tunnel implementation with reverse tunnel option"""
 
     TUNNEL_ID = 3
 
     CONFIG = """
 acls:
     forward_acl:
@@ -2972,22 +3496,34 @@
 """
 
     def test_tunnel_reverse_option(self):
         """Test separate reverse tunnel ACL using the `reverse` tunnel option"""
         valve = self.valves_manager.valves[0x1]
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         self.validate_tunnel(
-            int(0x2), int(0x2),
-            1, 1, 3, self.TUNNEL_ID,
-            True, 'Did not output to original source, the reverse tunnelled packet')
-        self.validate_tunnel(
-            int(0x1), int(0x1),
-            3, self.TUNNEL_ID, 1, 0,
-            True, 'Did not output to original source, the reverse tunnelled packet',
-            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG)
+            int(0x2),
+            int(0x2),
+            1,
+            1,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not output to original source, the reverse tunnelled packet",
+        )
+        self.validate_tunnel(
+            int(0x1),
+            int(0x1),
+            3,
+            self.TUNNEL_ID,
+            1,
+            0,
+            True,
+            "Did not output to original source, the reverse tunnelled packet",
+            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
+        )
 
 
 class ValveTestOrderedBiDirectionalDPACLTunnelDPDestination(
     ValveTestBases.ValveTestTunnel
 ):  # pylint: disable=too-few-public-methods
     """Test tunnel configured as a DP ACL with bi-directionality and destination as a DP"""
 
@@ -3032,44 +3568,78 @@
 """
 
     def test_dp_destination_tunnel_bi_directional_dp_acl(self):
         """Test bi-directionality on a DP ACL with DP destination"""
         valve = self.valves_manager.valves[0x1]
         self.apply_ofmsgs(valve.stack_manager.add_tunnel_acls())
         self.validate_tunnel(
-            int(0x1), int(0x1),
-            1, 0, 3, self.TUNNEL_ID,
-            True, 'Did not encapsulate and output towards destination')
-        self.validate_tunnel(
-            int(0x2), int(0x2),
-            3, [self.TUNNEL_ID, 1], 1, 0,
-            True, 'Did not output to host as flood',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
-        self.validate_tunnel(
-            int(0x2), int(0x2),
-            1, self.TUNNEL_ID, 3, self.TUNNEL_ID,
-            True, 'Did not output reverse packet towards reverse destination',
-            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG)
-        self.validate_tunnel(
-            int(0x1), int(0x1),
-            3, [self.TUNNEL_ID, 1], 1, 0,
-            True, 'Did not output reverse packet to host as flood',
-            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG)
+            int(0x1),
+            int(0x1),
+            1,
+            0,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not encapsulate and output towards destination",
+        )
+        self.validate_tunnel(
+            int(0x2),
+            int(0x2),
+            3,
+            [self.TUNNEL_ID, 1],
+            1,
+            0,
+            True,
+            "Did not output to host as flood",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
+        self.validate_tunnel(
+            int(0x2),
+            int(0x2),
+            1,
+            self.TUNNEL_ID,
+            3,
+            self.TUNNEL_ID,
+            True,
+            "Did not output reverse packet towards reverse destination",
+            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
+        )
+        self.validate_tunnel(
+            int(0x1),
+            int(0x1),
+            3,
+            [self.TUNNEL_ID, 1],
+            1,
+            0,
+            True,
+            "Did not output reverse packet to host as flood",
+            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
+        )
         self.validate_tunnel(
-            int(0x1), int(0x1),
-            3, self.TUNNEL_ID, None, None,
+            int(0x1),
+            int(0x1),
+            3,
+            self.TUNNEL_ID,
+            None,
+            None,
             False,
-            'Expected a drop of the reverse tunnel packet as it fell through to the flood table without a VID',
-            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG)
+            "Expected a drop of the reverse tunnel packet as it fell through to the flood table without a VID",
+            pcp=valve_of.PCP_TUNNEL_REVERSE_DIRECTION_FLAG,
+        )
         self.validate_tunnel(
-            int(0x2), int(0x2),
-            3, self.TUNNEL_ID, None, None,
+            int(0x2),
+            int(0x2),
+            3,
+            self.TUNNEL_ID,
+            None,
+            None,
             False,
-            'Expected a drop of the tunnel packet as it fell through to the flood table without a VID',
-            pcp=valve_of.PCP_TUNNEL_FLAG)
+            "Expected a drop of the tunnel packet as it fell through to the flood table without a VID",
+            pcp=valve_of.PCP_TUNNEL_FLAG,
+        )
 
 
 class ValveTwoDpRoot(ValveTestBases.ValveTestNetwork):
     """Test simple stack topology from root."""
 
     CONFIG = """
 dps:
@@ -3131,15 +3701,15 @@
     def test_topo(self):
         """Test topology functions."""
         dp = self.valves_manager.valves[self.DP_ID].dp
         self.assertTrue(dp.stack.is_root())
         self.assertFalse(dp.stack.is_edge())
 
     def test_add_remove_port(self):
-        self.update_and_revert_config(self.CONFIG, self.CONFIG3, 'warm')
+        self.update_and_revert_config(self.CONFIG, self.CONFIG3, "warm")
 
 
 class ValveTwoDpRootEdge(ValveTestBases.ValveTestNetwork):
     """Test simple stack topology from edge."""
 
     CONFIG = """
 dps:
@@ -3201,15 +3771,15 @@
     def test_topo(self):
         """Test topology functions."""
         dp_obj = self.valves_manager.valves[self.DP_ID].dp
         self.assertFalse(dp_obj.stack.is_root())
         self.assertTrue(dp_obj.stack.is_edge())
 
     def test_add_remove_port(self):
-        self.update_and_revert_config(self.CONFIG, self.CONFIG3, 'warm')
+        self.update_and_revert_config(self.CONFIG, self.CONFIG3, "warm")
 
 
 class GroupDeleteACLTestCase(ValveTestBases.ValveTestNetwork):
     """Test that a group ACL creates a groupdel for the group_id"""
 
     CONFIG = """
 acls:
@@ -3261,26 +3831,40 @@
     def test_groupdel_exists(self):
         """Test valve_flowreorder doesn't remove groupmods unless expected"""
         valve = self.valves_manager.valves[0x1]
         port = valve.dp.ports[1]
         ofmsgs = valve.acl_manager.add_port(port)
         self.check_groupmods_exist(valve_of.valve_flowreorder(ofmsgs))
         global_flowmod = valve_of.flowmod(
-            0, ofp.OFPFC_DELETE, ofp.OFPTT_ALL,
-            0, ofp.OFPP_CONTROLLER, ofp.OFPP_CONTROLLER,
-            valve_of.match_from_dict({}), (), 0, 0, 0)
+            0,
+            ofp.OFPFC_DELETE,
+            ofp.OFPTT_ALL,
+            0,
+            ofp.OFPP_CONTROLLER,
+            ofp.OFPP_CONTROLLER,
+            valve_of.match_from_dict({}),
+            (),
+            0,
+            0,
+            0,
+        )
         self.check_groupmods_exist(
-            valve_of.valve_flowreorder(ofmsgs + [global_flowmod]))
+            valve_of.valve_flowreorder(ofmsgs + [global_flowmod])
+        )
         global_metermod = valve_of.meterdel()
         self.check_groupmods_exist(
-            valve_of.valve_flowreorder(ofmsgs + [global_flowmod, global_metermod]))
+            valve_of.valve_flowreorder(ofmsgs + [global_flowmod, global_metermod])
+        )
         global_groupmod = valve_of.groupdel()
         self.check_groupmods_exist(
             valve_of.valve_flowreorder(
-                ofmsgs + [global_flowmod, global_metermod, global_groupmod]), False)
+                ofmsgs + [global_flowmod, global_metermod, global_groupmod]
+            ),
+            False,
+        )
 
     def test_all_offset(self):
         """Test groups with the redundant controller offset check for all possible offsets"""
         valve = self.valves_manager.valves[0x1]
         port = valve.dp.ports[1]
         ofmsgs = valve.acl_manager.add_port(port)
         self.apply_ofmsgs(ofmsgs, 0x1, all_offsets=True)
@@ -3448,59 +4032,67 @@
 
     def setUp(self):
         """Setup network and start stack ports"""
         self.setup_valves(self.CONFIG)
 
     def test_reload_topology_change_warmstart(self):
         """Test reload with topology change forces stack ports down, only warm starts"""
-        self.update_and_revert_config(
-            self.CONFIG, self.NEW_PORT_CONFIG, 'warm')
+        self.update_and_revert_config(self.CONFIG, self.NEW_PORT_CONFIG, "warm")
 
     def test_reload_topology_change(self):
         """Test reload with topology change forces stack ports down"""
-        with open(self.config_file, 'w', encoding='utf-8') as config_file:
+        with open(self.config_file, "w", encoding="utf-8") as config_file:
             config_file.write(self.NEW_PORT_CONFIG)
         new_dps = self.valves_manager.parse_configs(self.config_file)
         for new_dp in new_dps:
             valve = self.valves_manager.valves[new_dp.dp_id]
             changes = valve.dp.get_config_changes(valve.logger, new_dp)
             changed_ports, all_ports_changed = changes[1], changes[6]
             for port in valve.dp.stack_ports():
                 if not all_ports_changed:
                     self.assertIn(
-                        port.number, changed_ports,
-                        'Stack port not detected as changed on topology change')
+                        port.number,
+                        changed_ports,
+                        "Stack port not detected as changed on topology change",
+                    )
 
     def test_reload_vlan_change_warmstart(self):
         """Test reload with topology change, only do a warm start"""
         valve = self.valves_manager.valves[1]
         self.assertEqual(valve.dp.ports[3].native_vlan.vid, 200)
         self.assertTrue(valve.dp.ports[3].dyn_update_time)
 
         # DPID 1 Port 3 changes vlan from 200, to 100, back to 200
         self.update_and_revert_config(
-            self.CONFIG, self.NEW_VLAN_CONFIG, 'warm',
-            verify_func=lambda: self.assertEqual(valve.dp.ports[3].native_vlan.vid, 100))
+            self.CONFIG,
+            self.NEW_VLAN_CONFIG,
+            "warm",
+            verify_func=lambda: self.assertEqual(
+                valve.dp.ports[3].native_vlan.vid, 100
+            ),
+        )
 
         self.assertTrue(valve.dp.ports[3].dyn_update_time)
         self.assertEqual(valve.dp.ports[3].native_vlan.vid, 200)
 
     def test_reload_vlan_change(self):
         """Test reload with topology change, stack ports stay up"""
-        with open(self.config_file, 'w', encoding='utf-8') as config_file:
+        with open(self.config_file, "w", encoding="utf-8") as config_file:
             config_file.write(self.NEW_VLAN_CONFIG)
         new_dps = self.valves_manager.parse_configs(self.config_file)
         for new_dp in new_dps:
             valve = self.valves_manager.valves[new_dp.dp_id]
             changed = valve.dp.get_config_changes(valve.logger, new_dp)
             changed_ports = changed[1]
             for port in valve.dp.stack_ports():
                 self.assertNotIn(
-                    port.number, changed_ports,
-                    'Stack port detected as changed on non-topology change')
+                    port.number,
+                    changed_ports,
+                    "Stack port detected as changed on non-topology change",
+                )
 
 
 class ValveStackHealthTest(ValveTestBases.ValveTestNetwork):
     """Test stack root health metrics"""
 
     UPDATE_TIME = 10
 
@@ -3575,88 +4167,105 @@
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
-        self.assertTrue(dps[0].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
-        self.assertFalse(dps[0].stack.update_health(
-            120, last_live_times, self.UPDATE_TIME)[0])
-        self.assertTrue(dps[1].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
-        self.assertFalse(dps[1].stack.update_health(
-            130, last_live_times, self.UPDATE_TIME)[0])
-        self.assertTrue(dps[2].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
-        self.assertFalse(dps[2].stack.update_health(
-            140, last_live_times, self.UPDATE_TIME)[0])
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
+        self.assertTrue(
+            dps[0].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
+        self.assertFalse(
+            dps[0].stack.update_health(120, last_live_times, self.UPDATE_TIME)[0]
+        )
+        self.assertTrue(
+            dps[1].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
+        self.assertFalse(
+            dps[1].stack.update_health(130, last_live_times, self.UPDATE_TIME)[0]
+        )
+        self.assertTrue(
+            dps[2].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
+        self.assertFalse(
+            dps[2].stack.update_health(140, last_live_times, self.UPDATE_TIME)[0]
+        )
 
     def test_lacp_down(self):
         """Test stack health on LACP ports being DOWN"""
         dps = [valve.dp for valve in self.valves_manager.valves.values()]
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
-        self.assertTrue(dps[0].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
+        self.assertTrue(
+            dps[0].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
         for port in dps[0].ports.values():
             if port.lacp:
                 port.actor_notconfigured()
-        self.assertFalse(dps[0].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
-        self.assertTrue(dps[1].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
+        self.assertFalse(
+            dps[0].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
+        self.assertTrue(
+            dps[1].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
         for port in dps[1].ports.values():
             if port.lacp:
                 port.actor_nosync()
-        self.assertFalse(dps[1].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
-        self.assertTrue(dps[2].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
+        self.assertFalse(
+            dps[1].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
+        self.assertTrue(
+            dps[2].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
 
     def test_stack_port_down(self):
         """Test stack health on stack ports being DOWN"""
         dps = [valve.dp for valve in self.valves_manager.valves.values()]
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
-        self.assertTrue(dps[0].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
+        self.assertTrue(
+            dps[0].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
         for port in dps[0].ports.values():
             if port.stack:
                 port.stack_bad()
-        self.assertFalse(dps[0].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
-        self.assertTrue(dps[1].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
+        self.assertFalse(
+            dps[0].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
+        self.assertTrue(
+            dps[1].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
         for port in dps[1].ports.values():
             if port.stack:
                 port.stack_gone()
-        self.assertFalse(dps[1].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
-        self.assertTrue(dps[2].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
+        self.assertFalse(
+            dps[1].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
+        self.assertTrue(
+            dps[2].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
         for port in dps[2].ports.values():
             if port.stack:
                 port.stack_admin_down()
-        self.assertFalse(dps[2].stack.update_health(
-            110, last_live_times, self.UPDATE_TIME)[0])
+        self.assertFalse(
+            dps[2].stack.update_health(110, last_live_times, self.UPDATE_TIME)[0]
+        )
 
 
 class ValveVariableRootHealthTest(ValveTestBases.ValveTestNetwork):
     """Test stack root health metrics"""
 
     UPDATE_TIME = 10
 
@@ -3722,27 +4331,31 @@
 """
 
     def setUp(self):
         """Start network for test"""
         self.setup_valves(self.CONFIG)
 
     def other_valves(self, root_valve):
-        return [valve for valve in self.valves_manager.valves.values() if valve != root_valve]
+        return [
+            valve
+            for valve in self.valves_manager.valves.values()
+            if valve != root_valve
+        ]
 
     def test_sw3_lacp(self):
         """Test LACP health metrics with SW3"""
         dps = [valve.dp for valve in self.valves_manager.valves.values()]
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
         # SW3 has no LACP ports, so LACP health percentage should be 0.0,
         #   but overall should be considered healthy
         dps[2].stack.update_health(100, last_live_times, self.UPDATE_TIME)
         self.assertEqual(dps[2].stack.dyn_healthy_info[2], 0.0)
         self.assertTrue(dps[2].stack.dyn_healthy)
 
     def test_sw1_lacp_down(self):
@@ -3751,15 +4364,15 @@
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
         # Take down some LACP ports, health percentage configured so
         #   SW1 will still be healthy
         for port in dps[0].ports.values():
             if port.lacp:
                 port.actor_nosync()
                 port.deselect_port()
                 break
@@ -3781,15 +4394,15 @@
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
         # Take down some LACP ports, health percentage configured
         #   so SW2 will still be healthy
         for port_num in [4, 5]:
             port = dps[1].ports[port_num]
             if port.lacp:
                 port.actor_nosync()
                 port.deselect_port()
@@ -3812,15 +4425,15 @@
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
         # All switches running, stack and LACP UP
         for port_num in [2, 3]:
             port = dps[0].ports[port_num]
             if port.stack:
                 port.stack_bad()
         dps[0].stack.update_health(100, last_live_times, self.UPDATE_TIME)
         self.assertEqual(dps[0].stack.dyn_healthy_info[1], 0.5)
@@ -3839,15 +4452,15 @@
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
         # All switches running, stack and LACP UP
         for port_num in [2]:
             port = dps[1].ports[port_num]
             if port.stack:
                 port.stack_bad()
         dps[1].stack.update_health(100, last_live_times, self.UPDATE_TIME)
         self.assertEqual(dps[1].stack.dyn_healthy_info[1], 0.5)
@@ -3928,99 +4541,135 @@
 """
 
     def setUp(self):
         """Start network for test"""
         self.setup_valves(self.CONFIG)
 
     def other_valves(self, root_valve):
-        return [valve for valve in self.valves_manager.valves.values() if valve != root_valve]
+        return [
+            valve
+            for valve in self.valves_manager.valves.values()
+            if valve != root_valve
+        ]
 
     def test_lacp_root_nomination(self):
         """Test root selection health"""
         dps = [valve.dp for valve in self.valves_manager.valves.values()]
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
         valves = self.valves_manager.valves
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
         # Start not root currently selected, all valves should select root sw1
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME), 'sw1')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME
+                ),
+                "sw1",
+            )
         # Create some LACP ports DOWN, but maintain above the health threshold, Equal percentage
         #   of LACP ports taken down so LACP stays the same
         for port_num in [4]:
             port = dps[0].ports[port_num]
             port.actor_nosync()
         for port_num in [4, 5]:
             port = dps[1].ports[port_num]
             port.actor_nosync()
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME), 'sw1')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME
+                ),
+                "sw1",
+            )
         # Change it so SW2 has greater percentage of LACP ports UP, so should be elected
         for port_num in [4]:
             port = dps[1].ports[port_num]
             port.actor_up()
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME), 'sw2')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME
+                ),
+                "sw2",
+            )
         # Stats of Sw1 and Sw2 the same, should choose lower priority: SW1
         for port_num in [4]:
             port = dps[1].ports[port_num]
             port.actor_nosync()
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME), 'sw1')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME
+                ),
+                "sw1",
+            )
         # SW2 considered DOWN, so should only nominate SW1
         for port_num in [4, 5]:
             port = dps[1].ports[port_num]
             port.actor_nosync()
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME), 'sw1')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME
+                ),
+                "sw1",
+            )
 
     def test_stack_root_nomination(self):
         """Test root selection health"""
         dps = [valve.dp for valve in self.valves_manager.valves.values()]
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
         valves = self.valves_manager.valves
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
         # Start not root currently selected, all valves should select root sw1
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME), 'sw1')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME
+                ),
+                "sw1",
+            )
         # Create some stack ports DOWN, but maintain above the health threshold, Equal percentage
         #   of stack ports taken down so stack stays the same
         for port_num in [2]:
             port = dps[1].ports[port_num]
             port.stack_bad()
         for port_num in [2, 3]:
             port = dps[0].ports[port_num]
             port.stack_bad()
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME), 'sw1')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME
+                ),
+                "sw1",
+            )
         # Change it so SW2 has greater percentage of stack ports UP, so should be elected
         for port_num in [2]:
             port = dps[1].ports[port_num]
             port.stack_up()
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME), 'sw2')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME
+                ),
+                "sw2",
+            )
 
 
 class ValveRootNominationTest(ValveStackHealthTest):
     """Test ValveStackManager root nomination calculations"""
 
     UPDATE_TIME = 10
 
@@ -4087,63 +4736,98 @@
 
     def setUp(self):
         """Start network for test"""
         self.setup_valves(self.CONFIG)
 
     def other_valves(self, root_valve):
         """Return a list of the other valves"""
-        return [valve for valve in self.valves_manager.valves.values() if valve != root_valve]
+        return [
+            valve
+            for valve in self.valves_manager.valves.values()
+            if valve != root_valve
+        ]
 
     def test_root_nomination(self):
         """Test root selection health"""
         dps = [valve.dp for valve in self.valves_manager.valves.values()]
         for dp in dps:
             for port in dp.ports.values():
                 if port.lacp:
                     port.actor_up()
                     port.select_port()
                 if port.stack:
                     port.stack_up()
         valves = self.valves_manager.valves
-        last_live_times = {'sw1': 100, 'sw2': 100, 'sw3': 100}
+        last_live_times = {"sw1": 100, "sw2": 100, "sw3": 100}
         # Start not root currently selected, all valves should select root sw1
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME), 'sw1')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    None, list(valves.values()), 100, last_live_times, self.UPDATE_TIME
+                ),
+                "sw1",
+            )
         # timeout SW1, all valves should select sw2
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                valves[1], self.other_valves(valves[1]), 111,
-                last_live_times, self.UPDATE_TIME), 'sw2')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    valves[1],
+                    self.other_valves(valves[1]),
+                    111,
+                    last_live_times,
+                    self.UPDATE_TIME,
+                ),
+                "sw2",
+            )
         # timeout sw2, should return None because there are no healthy switches
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                valves[2], self.other_valves(valves[2]),
-                121, last_live_times, self.UPDATE_TIME), None)
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    valves[2],
+                    self.other_valves(valves[2]),
+                    121,
+                    last_live_times,
+                    self.UPDATE_TIME,
+                ),
+                None,
+            )
         # timeout sw1, despite being unhealthy, all valves should select sw2
         for port in valves[2].dp.ports.values():
             if port.stack:
                 port.stack_bad()
         for valve in valves.values():
-            self.assertEqual(valve.stack_manager.nominate_stack_root(
-                valves[1], self.other_valves(valves[1]), 111,
-                last_live_times, self.UPDATE_TIME), 'sw2')
+            self.assertEqual(
+                valve.stack_manager.nominate_stack_root(
+                    valves[1],
+                    self.other_valves(valves[1]),
+                    111,
+                    last_live_times,
+                    self.UPDATE_TIME,
+                ),
+                "sw2",
+            )
 
     def test_consistent_roots(self):
         """Test inconsistent root detection"""
         valves = self.valves_manager.valves
         for valve in valves.values():
-            valve.dp.stack.root_name = 'sw1'
+            valve.dp.stack.root_name = "sw1"
         for valve in valves.values():
-            self.assertTrue(valve.stack_manager.consistent_roots(
-                'sw1', valve, self.other_valves(valve)))
-        valves[1].dp.stack.root_name = 'sw2'
+            self.assertTrue(
+                valve.stack_manager.consistent_roots(
+                    "sw1", valve, self.other_valves(valve)
+                )
+            )
+        valves[1].dp.stack.root_name = "sw2"
         for valve in valves.values():
-            self.assertFalse(valve.stack_manager.consistent_roots(
-                'sw1', valve, self.other_valves(valve)))
+            self.assertFalse(
+                valve.stack_manager.consistent_roots(
+                    "sw1", valve, self.other_valves(valve)
+                )
+            )
 
 
 class ValveStackConfigTest(ValveTestBases.ValveTestNetwork):
     """Test recompiling Stack into YAML config object"""
 
     CONFIG = """
 vlans:
@@ -4211,36 +4895,37 @@
         self.setup_valves(self.CONFIG)
 
     def test_stack(self):
         """Test getting config for stack with correct config"""
         dp = self.valves_manager.valves[1].dp
         stack_conf = yaml_load(dp.stack.to_conf())
         self.assertIsInstance(stack_conf, dict)
-        self.assertIn('priority', stack_conf)
-        self.assertIn('down_time_multiple', stack_conf)
-        self.assertIn('route_learning', stack_conf)
-        self.assertNotIn('dyn_healthy', stack_conf)
-        self.assertNotIn('canonical_port_order', stack_conf)
-        self.assertNotIn('graph', stack_conf)
-        self.assertNotIn('name', stack_conf)
+        self.assertIn("priority", stack_conf)
+        self.assertIn("down_time_multiple", stack_conf)
+        self.assertIn("route_learning", stack_conf)
+        self.assertNotIn("dyn_healthy", stack_conf)
+        self.assertNotIn("canonical_port_order", stack_conf)
+        self.assertNotIn("graph", stack_conf)
+        self.assertNotIn("name", stack_conf)
 
     def test_dp_stack(self):
         """Test getting config for DP with correct subconfig stack"""
         dp = self.valves_manager.valves[1].dp
         dp_conf = yaml_load(dp.to_conf())
         stack_conf = yaml_load(dp.stack.to_conf())
-        self.assertIn('stack', dp_conf)
-        self.assertIsInstance(dp_conf['stack'], dict)
-        self.assertEqual(dp_conf['stack'], stack_conf)
+        self.assertIn("stack", dp_conf)
+        self.assertIsInstance(dp_conf["stack"], dict)
+        self.assertEqual(dp_conf["stack"], stack_conf)
 
 
 class ValveStackLLDPRestartTestCase(ValveTestBases.ValveTestNetwork):
     """Test restarting stacked LLDP"""
 
-    CONFIG = """
+    CONFIG = (
+        """
 dps:
     s1:
 %s
         stack:
             priority: 1
         interfaces:
             1:
@@ -4261,45 +4946,53 @@
                 description: p1
                 stack:
                     dp: s1
                     port: 1
             2:
                 description: p2
                 native_vlan: 100
-""" % BASE_DP1_CONFIG
+"""
+        % BASE_DP1_CONFIG
+    )
 
     def setUp(self):
         """Setup basic loop config"""
         self.setup_valves(self.CONFIG)
 
     def test_lldp_cold_start(self):
         """Test cold-starting a switch preserves LLDP states"""
-        self.migrate_stack_root('s1')
+        self.migrate_stack_root("s1")
         self.activate_all_ports()
         valve = self.valves_manager.valves[0x1]
         old_port = valve.dp.ports[1]
-        self.assertTrue(old_port.is_stack_up(), 'Port stack not UP')
+        self.assertTrue(old_port.is_stack_up(), "Port stack not UP")
         init_events = self.get_events()
-        init_ports = [event for event in init_events if 'PORTS_STATUS' in event]
-        self.assertEqual(2, len(init_ports), 'Expected 2 PORTS_STATUS events')
+        init_ports = [event for event in init_events if "PORTS_STATUS" in event]
+        self.assertEqual(2, len(init_ports), "Expected 2 PORTS_STATUS events")
 
-        self.migrate_stack_root('s2')
+        self.migrate_stack_root("s2")
         migrate_events = self.get_events()
 
         def is_coldstart(event):
-            return 'CONFIG_CHANGE' in event and event['CONFIG_CHANGE']['restart_type'] == 'cold'
+            return (
+                "CONFIG_CHANGE" in event
+                and event["CONFIG_CHANGE"]["restart_type"] == "cold"
+            )
+
         migrate_coldstarts = [event for event in migrate_events if is_coldstart(event)]
-        self.assertEqual(2, len(migrate_coldstarts), 'Expected 2 coldstart events')
+        self.assertEqual(2, len(migrate_coldstarts), "Expected 2 coldstart events")
 
-        migrate_stack = [event for event in migrate_events if 'STACK_STATE' in event]
-        self.assertEqual(0, len(migrate_stack), 'Expected 0 stack state events')
+        migrate_stack = [event for event in migrate_events if "STACK_STATE" in event]
+        self.assertEqual(0, len(migrate_stack), "Expected 0 stack state events")
 
-        migrate_topo = [event for event in migrate_events if 'STACK_TOPO_CHANGE' in event]
-        self.assertEqual(2, len(migrate_topo), 'Expected 2 topo change events')
+        migrate_topo = [
+            event for event in migrate_events if "STACK_TOPO_CHANGE" in event
+        ]
+        self.assertEqual(2, len(migrate_topo), "Expected 2 topo change events")
 
         new_port = valve.dp.ports[1]
-        self.assertNotEqual(id(old_port), id(new_port), 'Port object not changed')
-        self.assertTrue(new_port.is_stack_up(), 'Port stack not UP')
+        self.assertNotEqual(id(old_port), id(new_port), "Port object not changed")
+        self.assertTrue(new_port.is_stack_up(), "Port stack not UP")
 
 
 if __name__ == "__main__":
     unittest.main()  # pytype: disable=module-attr
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_valveapp_smoke.py` & `c65faucet-1.0.47/tests/unit/faucet/test_valveapp_smoke.py`

 * *Files 6% similar despite different names*

```diff
@@ -30,44 +30,43 @@
 
 
 class OSKenAppSmokeTest(unittest.TestCase):  # pytype: disable=module-attr
     """Test bare instantiation of controller classes."""
 
     @staticmethod
     def _fake_dp():
-        datapath = namedtuple('datapath', ['id', 'close'])(0, lambda: None)
+        datapath = namedtuple("datapath", ["id", "close"])(0, lambda: None)
         return datapath
 
     def test_faucet(self):
         """Test FAUCET can be initialized."""
-        os.environ['FAUCET_CONFIG'] = '/dev/null'
-        os.environ['FAUCET_LOG'] = '/dev/null'
-        os.environ['FAUCET_EXCEPTION_LOG'] = '/dev/null'
-        os_ken_app = faucet.Faucet(
-            dpset={},
-            reg=CollectorRegistry())
+        os.environ["FAUCET_CONFIG"] = "/dev/null"
+        os.environ["FAUCET_LOG"] = "/dev/null"
+        os.environ["FAUCET_EXCEPTION_LOG"] = "/dev/null"
+        os_ken_app = faucet.Faucet(dpset={}, reg=CollectorRegistry())
         os_ken_app.reload_config(None)
         self.assertFalse(os_ken_app._config_files_changed())
         os_ken_app.metric_update(None)
         event_dp = dpset.EventDPReconnected(dp=self._fake_dp())
         for enter in (True, False):
             event_dp.enter = enter
             os_ken_app.connect_or_disconnect_handler(event_dp)
         for event_handler in (
-                os_ken_app.error_handler,
-                os_ken_app.features_handler,
-                os_ken_app.packet_in_handler,
-                os_ken_app.desc_stats_reply_handler,
-                os_ken_app.port_desc_stats_reply_handler,
-                os_ken_app.port_status_handler,
-                os_ken_app.flowremoved_handler,
-                os_ken_app.reconnect_handler,
-                os_ken_app._datapath_connect,
-                os_ken_app._datapath_disconnect):
-            msg = namedtuple('msg', ['datapath'])(self._fake_dp())
+            os_ken_app.error_handler,
+            os_ken_app.features_handler,
+            os_ken_app.packet_in_handler,
+            os_ken_app.desc_stats_reply_handler,
+            os_ken_app.port_desc_stats_reply_handler,
+            os_ken_app.port_status_handler,
+            os_ken_app.flowremoved_handler,
+            os_ken_app.reconnect_handler,
+            os_ken_app._datapath_connect,
+            os_ken_app._datapath_disconnect,
+        ):
+            msg = namedtuple("msg", ["datapath"])(self._fake_dp())
             event = EventOFPMsgBase(msg=msg)
             event.dp = msg.datapath
             event_handler(event)
         os_ken_app._check_thread_exception()
         os_ken_app._thread_jitter(1)
```

### Comparing `c65faucet-1.0.46/tests/unit/faucet/test_vlan.py` & `c65faucet-1.0.47/tests/unit/faucet/test_vlan.py`

 * *Files 22% similar despite different names*

```diff
@@ -14,43 +14,33 @@
 
         vlan = VLAN(1, 1, {})
         self.assertEqual(len(vlan.ipvs()), 0)
 
     def test_ipvs_ipv4(self):
         """Tests the ipvs() method with an IPv4 vip"""
 
-        vlan_config = {
-            'faucet_vips': ['10.0.0.254/24']
-        }
+        vlan_config = {"faucet_vips": ["10.0.0.254/24"]}
 
         vlan = VLAN(1, 1, vlan_config)
         self.assertIn(4, vlan.ipvs())
         self.assertNotIn(6, vlan.ipvs())
 
     def test_ipvs_ipv6(self):
         """Tests the ipvs() method with an IPv6 vip"""
 
-        vlan_config = {
-            'faucet_vips': ['2001::1/16']
-        }
+        vlan_config = {"faucet_vips": ["2001::1/16"]}
 
         vlan = VLAN(1, 1, vlan_config)
         self.assertIn(6, vlan.ipvs())
         self.assertNotIn(4, vlan.ipvs())
 
     def test_ipvs_ipv4_ipv6(self):
         """Tests the ipvs() method with both IPv4 and IPv6 vips"""
 
-        vlan_config = {
-            'faucet_vips': [
-                '2001::1/16',
-                'fe80::1/64',
-                '10.0.0.254/24'
-            ]
-        }
+        vlan_config = {"faucet_vips": ["2001::1/16", "fe80::1/64", "10.0.0.254/24"]}
 
         vlan = VLAN(1, 1, vlan_config)
         self.assertIn(4, vlan.ipvs())
         self.assertIn(6, vlan.ipvs())
 
     def test_faucet_vips_by_ipv_none(self):
         """Tests the faucet_vips_by_ipv() method when there are no vips"""
@@ -58,30 +48,24 @@
         vlan = VLAN(1, 1, {})
         self.assertEqual(len(vlan.faucet_vips_by_ipv(4)), 0)
         self.assertEqual(len(vlan.faucet_vips_by_ipv(6)), 0)
 
     def test_faucet_vips_by_ipv_both(self):
         """Tests the faucet_vips_by_ipv() method when there are both IPv4 and IPv6 vips"""
 
-        vlan_config = {
-            'faucet_vips': [
-                '2001::1/16',
-                'fe80::1/64',
-                '10.0.0.254/24'
-            ]
-        }
+        vlan_config = {"faucet_vips": ["2001::1/16", "fe80::1/64", "10.0.0.254/24"]}
 
         vlan = VLAN(1, 1, vlan_config)
-        self.assertEqual(set(vlan.faucet_vips_by_ipv(4)), set([
-            ip_interface('10.0.0.254/24')
-        ]))
-        self.assertEqual(set(vlan.faucet_vips_by_ipv(6)), set([
-            ip_interface('2001::1/16'),
-            ip_interface('fe80::1/64')
-        ]))
+        self.assertEqual(
+            set(vlan.faucet_vips_by_ipv(4)), set([ip_interface("10.0.0.254/24")])
+        )
+        self.assertEqual(
+            set(vlan.faucet_vips_by_ipv(6)),
+            set([ip_interface("2001::1/16"), ip_interface("fe80::1/64")]),
+        )
 
     def test_routes_by_ipv_none(self):
         """Tests the routes_by_ipv() and route_count_by_ipv() methods with no routes"""
 
         vlan = VLAN(1, 1, {})
         self.assertEqual(vlan.routes_by_ipv(4), {})
         self.assertEqual(vlan.routes_by_ipv(6), {})
@@ -89,147 +73,177 @@
         self.assertEqual(vlan.route_count_by_ipv(6), 0)
 
     def test_routes_by_ipv_both(self):
         """Tests the routes_by_ipv() and route_count_by_ipv() methods with both
         IPv4 and IPv6 routes"""
 
         vlan_config = {
-            'routes': [
-                {'route': {'ip_dst': '10.99.99.0/24', 'ip_gw': '10.0.0.1'}},
-                {'route': {'ip_dst': '10.99.98.0/24', 'ip_gw': '10.0.0.99'}},
-                {'route': {'ip_dst': '10.99.97.0/24', 'ip_gw': '10.0.0.99'}},
-                {'route': {'ip_dst': 'fc00::10:0/112', 'ip_gw': 'fc00::1:1'}},
-                {'route': {'ip_dst': 'fc00::20:0/112', 'ip_gw': 'fc00::1:99'}}
+            "routes": [
+                {"route": {"ip_dst": "10.99.99.0/24", "ip_gw": "10.0.0.1"}},
+                {"route": {"ip_dst": "10.99.98.0/24", "ip_gw": "10.0.0.99"}},
+                {"route": {"ip_dst": "10.99.97.0/24", "ip_gw": "10.0.0.99"}},
+                {"route": {"ip_dst": "fc00::10:0/112", "ip_gw": "fc00::1:1"}},
+                {"route": {"ip_dst": "fc00::20:0/112", "ip_gw": "fc00::1:99"}},
             ],
         }
 
         vlan = VLAN(1, 1, vlan_config)
 
-        self.assertEqual(vlan.routes_by_ipv(4), {
-            ip_network('10.99.99.0/24'): ip_address('10.0.0.1'),
-            ip_network('10.99.98.0/24'): ip_address('10.0.0.99'),
-            ip_network('10.99.97.0/24'): ip_address('10.0.0.99'),
-        })
-        self.assertEqual(vlan.routes_by_ipv(6), {
-            ip_network('fc00::10:0/112'): ip_address('fc00::1:1'),
-            ip_network('fc00::20:0/112'): ip_address('fc00::1:99'),
-        })
+        self.assertEqual(
+            vlan.routes_by_ipv(4),
+            {
+                ip_network("10.99.99.0/24"): ip_address("10.0.0.1"),
+                ip_network("10.99.98.0/24"): ip_address("10.0.0.99"),
+                ip_network("10.99.97.0/24"): ip_address("10.0.0.99"),
+            },
+        )
+        self.assertEqual(
+            vlan.routes_by_ipv(6),
+            {
+                ip_network("fc00::10:0/112"): ip_address("fc00::1:1"),
+                ip_network("fc00::20:0/112"): ip_address("fc00::1:99"),
+            },
+        )
         self.assertEqual(vlan.route_count_by_ipv(4), 3)
         self.assertEqual(vlan.route_count_by_ipv(6), 2)
 
     def test_modify_routes_v4(self):
         """Tests the add_route() and remove_route() methods with IPv4 routes"""
 
         vlan = VLAN(1, 1, {})
 
         self.assertEqual(vlan.routes_by_ipv(4), {})
-        vlan.add_route(ip_network('10.99.99.0/24'), ip_address('10.0.0.1'))
-        vlan.add_route(ip_network('10.99.98.0/24'), ip_address('10.0.0.99'))
-        self.assertEqual(vlan.routes_by_ipv(4), {
-            ip_network('10.99.99.0/24'): ip_address('10.0.0.1'),
-            ip_network('10.99.98.0/24'): ip_address('10.0.0.99')
-        })
+        vlan.add_route(ip_network("10.99.99.0/24"), ip_address("10.0.0.1"))
+        vlan.add_route(ip_network("10.99.98.0/24"), ip_address("10.0.0.99"))
+        self.assertEqual(
+            vlan.routes_by_ipv(4),
+            {
+                ip_network("10.99.99.0/24"): ip_address("10.0.0.1"),
+                ip_network("10.99.98.0/24"): ip_address("10.0.0.99"),
+            },
+        )
         self.assertEqual(vlan.route_count_by_ipv(4), 2)
-        vlan.del_route(ip_network('10.99.99.0/24'))
-        self.assertEqual(vlan.routes_by_ipv(4), {
-            ip_network('10.99.98.0/24'): ip_address('10.0.0.99')
-        })
+        vlan.del_route(ip_network("10.99.99.0/24"))
+        self.assertEqual(
+            vlan.routes_by_ipv(4),
+            {ip_network("10.99.98.0/24"): ip_address("10.0.0.99")},
+        )
         self.assertEqual(vlan.route_count_by_ipv(4), 1)
-        vlan.del_route(ip_network('10.99.98.0/24'))
+        vlan.del_route(ip_network("10.99.98.0/24"))
         self.assertEqual(vlan.route_count_by_ipv(4), 0)
         self.assertEqual(vlan.routes_by_ipv(4), {})
 
     def test_modify_routes_v6(self):
         """Tests the add_route() and remove_route() methods with IPv4 routes"""
 
         vlan = VLAN(1, 1, {})
 
         self.assertEqual(vlan.routes_by_ipv(6), {})
-        vlan.add_route(ip_network('fc00::10:0/112'), ip_address('fc00::1:1'))
-        vlan.add_route(ip_network('fc00::20:0/112'), ip_address('fc00::1:99'))
-        self.assertEqual(vlan.routes_by_ipv(6), {
-            ip_network('fc00::10:0/112'): ip_address('fc00::1:1'),
-            ip_network('fc00::20:0/112'): ip_address('fc00::1:99')
-        })
+        vlan.add_route(ip_network("fc00::10:0/112"), ip_address("fc00::1:1"))
+        vlan.add_route(ip_network("fc00::20:0/112"), ip_address("fc00::1:99"))
+        self.assertEqual(
+            vlan.routes_by_ipv(6),
+            {
+                ip_network("fc00::10:0/112"): ip_address("fc00::1:1"),
+                ip_network("fc00::20:0/112"): ip_address("fc00::1:99"),
+            },
+        )
         self.assertEqual(vlan.route_count_by_ipv(6), 2)
-        vlan.del_route(ip_network('fc00::10:0/112'))
-        self.assertEqual(vlan.routes_by_ipv(6), {
-            ip_network('fc00::20:0/112'): ip_address('fc00::1:99')
-        })
+        vlan.del_route(ip_network("fc00::10:0/112"))
+        self.assertEqual(
+            vlan.routes_by_ipv(6),
+            {ip_network("fc00::20:0/112"): ip_address("fc00::1:99")},
+        )
         self.assertEqual(vlan.route_count_by_ipv(6), 1)
-        vlan.del_route(ip_network('fc00::20:0/112'))
+        vlan.del_route(ip_network("fc00::20:0/112"))
         self.assertEqual(vlan.route_count_by_ipv(6), 0)
         self.assertEqual(vlan.routes_by_ipv(6), {})
 
     def test_modify_routes_static_v4(self):
         """Tests the add_route() and remove_route() methods,
         starting with configured static routes for IPv4"""
 
         vlan_config = {
-            'routes': [
-                {'route': {'ip_dst': '10.99.97.0/24', 'ip_gw': '10.0.0.99'}},
+            "routes": [
+                {"route": {"ip_dst": "10.99.97.0/24", "ip_gw": "10.0.0.99"}},
             ],
         }
 
         vlan = VLAN(1, 1, vlan_config)
 
-        self.assertEqual(vlan.routes_by_ipv(4), {
-            ip_network('10.99.97.0/24'): ip_address('10.0.0.99')
-        })
-        vlan.add_route(ip_network('10.99.99.0/24'), ip_address('10.0.0.1'))
-        vlan.add_route(ip_network('10.99.98.0/24'), ip_address('10.0.0.99'))
-        self.assertEqual(vlan.routes_by_ipv(4), {
-            ip_network('10.99.99.0/24'): ip_address('10.0.0.1'),
-            ip_network('10.99.98.0/24'): ip_address('10.0.0.99'),
-            ip_network('10.99.97.0/24'): ip_address('10.0.0.99')
-        })
+        self.assertEqual(
+            vlan.routes_by_ipv(4),
+            {ip_network("10.99.97.0/24"): ip_address("10.0.0.99")},
+        )
+        vlan.add_route(ip_network("10.99.99.0/24"), ip_address("10.0.0.1"))
+        vlan.add_route(ip_network("10.99.98.0/24"), ip_address("10.0.0.99"))
+        self.assertEqual(
+            vlan.routes_by_ipv(4),
+            {
+                ip_network("10.99.99.0/24"): ip_address("10.0.0.1"),
+                ip_network("10.99.98.0/24"): ip_address("10.0.0.99"),
+                ip_network("10.99.97.0/24"): ip_address("10.0.0.99"),
+            },
+        )
         self.assertEqual(vlan.route_count_by_ipv(4), 3)
-        vlan.del_route(ip_network('10.99.99.0/24'))
-        self.assertEqual(vlan.routes_by_ipv(4), {
-            ip_network('10.99.97.0/24'): ip_address('10.0.0.99'),
-            ip_network('10.99.98.0/24'): ip_address('10.0.0.99')
-        })
+        vlan.del_route(ip_network("10.99.99.0/24"))
+        self.assertEqual(
+            vlan.routes_by_ipv(4),
+            {
+                ip_network("10.99.97.0/24"): ip_address("10.0.0.99"),
+                ip_network("10.99.98.0/24"): ip_address("10.0.0.99"),
+            },
+        )
         self.assertEqual(vlan.route_count_by_ipv(4), 2)
-        vlan.del_route(ip_network('10.99.98.0/24'))
+        vlan.del_route(ip_network("10.99.98.0/24"))
         self.assertEqual(vlan.route_count_by_ipv(4), 1)
-        self.assertEqual(vlan.routes_by_ipv(4), {
-            ip_network('10.99.97.0/24'): ip_address('10.0.0.99')
-        })
+        self.assertEqual(
+            vlan.routes_by_ipv(4),
+            {ip_network("10.99.97.0/24"): ip_address("10.0.0.99")},
+        )
 
     def test_modify_routes_static_v6(self):
         """Tests the add_route() and remove_route() methods,
         starting with configured static routes for IPv6"""
 
         vlan_config = {
-            'routes': [
-                {'route': {'ip_dst': 'fc00::30:0/112', 'ip_gw': 'fc00::1:99'}},
+            "routes": [
+                {"route": {"ip_dst": "fc00::30:0/112", "ip_gw": "fc00::1:99"}},
             ],
         }
 
         vlan = VLAN(1, 1, vlan_config)
 
-        self.assertEqual(vlan.routes_by_ipv(6), {
-            ip_network('fc00::30:0/112'): ip_address('fc00::1:99')
-        })
-        vlan.add_route(ip_network('fc00::10:0/112'), ip_address('fc00::1:1'))
-        vlan.add_route(ip_network('fc00::20:0/112'), ip_address('fc00::1:99'))
-        self.assertEqual(vlan.routes_by_ipv(6), {
-            ip_network('fc00::10:0/112'): ip_address('fc00::1:1'),
-            ip_network('fc00::20:0/112'): ip_address('fc00::1:99'),
-            ip_network('fc00::30:0/112'): ip_address('fc00::1:99')
-        })
+        self.assertEqual(
+            vlan.routes_by_ipv(6),
+            {ip_network("fc00::30:0/112"): ip_address("fc00::1:99")},
+        )
+        vlan.add_route(ip_network("fc00::10:0/112"), ip_address("fc00::1:1"))
+        vlan.add_route(ip_network("fc00::20:0/112"), ip_address("fc00::1:99"))
+        self.assertEqual(
+            vlan.routes_by_ipv(6),
+            {
+                ip_network("fc00::10:0/112"): ip_address("fc00::1:1"),
+                ip_network("fc00::20:0/112"): ip_address("fc00::1:99"),
+                ip_network("fc00::30:0/112"): ip_address("fc00::1:99"),
+            },
+        )
         self.assertEqual(vlan.route_count_by_ipv(6), 3)
-        vlan.del_route(ip_network('fc00::10:0/112'))
-        self.assertEqual(vlan.routes_by_ipv(6), {
-            ip_network('fc00::30:0/112'): ip_address('fc00::1:99'),
-            ip_network('fc00::20:0/112'): ip_address('fc00::1:99')
-        })
+        vlan.del_route(ip_network("fc00::10:0/112"))
+        self.assertEqual(
+            vlan.routes_by_ipv(6),
+            {
+                ip_network("fc00::30:0/112"): ip_address("fc00::1:99"),
+                ip_network("fc00::20:0/112"): ip_address("fc00::1:99"),
+            },
+        )
         self.assertEqual(vlan.route_count_by_ipv(6), 2)
-        vlan.del_route(ip_network('fc00::20:0/112'))
+        vlan.del_route(ip_network("fc00::20:0/112"))
         self.assertEqual(vlan.route_count_by_ipv(6), 1)
-        self.assertEqual(vlan.routes_by_ipv(6), {
-            ip_network('fc00::30:0/112'): ip_address('fc00::1:99')
-        })
+        self.assertEqual(
+            vlan.routes_by_ipv(6),
+            {ip_network("fc00::30:0/112"): ip_address("fc00::1:99")},
+        )
 
 
 if __name__ == "__main__":
     unittest.main()  # pytype: disable=module-attr
```

### Comparing `c65faucet-1.0.46/tests/unit/gauge/test_config_gauge.py` & `c65faucet-1.0.47/tests/unit/gauge/test_config_gauge.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/unit/gauge/test_gauge.py` & `c65faucet-1.0.47/tests/unit/gauge/test_gauge.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/unit/gauge/test_main.py` & `c65faucet-1.0.47/tests/unit/gauge/test_main.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.46/tests/unit/packaging/test_packaging.py` & `c65faucet-1.0.47/tests/unit/packaging/test_packaging.py`

 * *Files identical despite different names*

