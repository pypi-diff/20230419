# Comparing `tmp/azureml_acft_image_components-0.0.6-py3-none-any.whl.zip` & `tmp/azureml_acft_image_components-0.0.6.post1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,137 +1,138 @@
-Zip file size: 298829 bytes, number of entries: 135
--rw-rw-rw-  2.0 fat      251 b- defN 23-Apr-10 23:37 azureml/__init__.py
--rw-rw-rw-  2.0 fat      295 b- defN 23-Apr-10 23:37 azureml/acft/__init__.py
--rw-rw-rw-  2.0 fat      822 b- defN 23-Apr-10 23:37 azureml/acft/image/__init__.py
--rw-rw-rw-  2.0 fat   600282 b- defN 23-Apr-10 23:37 azureml/acft/image/components/NOTICE.txt
--rw-rw-rw-  2.0 fat      297 b- defN 23-Apr-10 23:37 azureml/acft/image/components/__init__.py
--rw-rw-rw-  2.0 fat       34 b- defN 23-Apr-10 23:44 azureml/acft/image/components/_version.py
--rw-rw-rw-  2.0 fat      312 b- defN 23-Apr-10 23:37 azureml/acft/image/components/common/__init__.py
--rw-rw-rw-  2.0 fat      583 b- defN 23-Apr-10 23:37 azureml/acft/image/components/common/utils.py
--rw-rw-rw-  2.0 fat      320 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/__init__.py
--rw-rw-rw-  2.0 fat     7891 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/finetune_runner.py
--rw-rw-rw-  2.0 fat      466 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/__init__.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/utils.py
--rw-rw-rw-  2.0 fat      313 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/augmentation/__init__.py
--rw-rw-rw-  2.0 fat     9017 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/augmentation/albumentations_augmentation.py
--rw-rw-rw-  2.0 fat    17791 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/augmentation/augmentation_config_utils.py
--rw-rw-rw-  2.0 fat     3152 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/augmentation/base_augmentation.py
--rw-rw-rw-  2.0 fat    10881 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/augmentation/custom_augmentations.py
--rw-rw-rw-  2.0 fat    26914 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/augmentation/model_preproc_extractor.py
--rw-rw-rw-  2.0 fat      320 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/augmentation/configs/__init__.py
--rw-rw-rw-  2.0 fat     1929 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/augmentation/configs/albumentations_classification.yaml
--rw-rw-rw-  2.0 fat     2474 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/augmentation/configs/albumentations_od_is.yaml
--rw-rw-rw-  2.0 fat     1314 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/augmentation/configs/hf_albumentations_classification.yaml
--rw-rw-rw-  2.0 fat      309 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/constants/__init__.py
--rw-rw-rw-  2.0 fat     1817 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/constants/augmentation_constants.py
--rw-rw-rw-  2.0 fat     4475 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/constants/constants.py
--rw-rw-rw-  2.0 fat      304 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/data/__init__.py
--rw-rw-rw-  2.0 fat     4848 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/data/base_dataset.py
--rw-rw-rw-  2.0 fat     8655 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/data/classification_dataset.py
--rw-rw-rw-  2.0 fat     5469 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/data/classification_hf_dataset.py
--rw-rw-rw-  2.0 fat    10245 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/data/download_manager.py
--rw-rw-rw-  2.0 fat     7569 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/data/runtime_detection_dataset_adapter.py
--rw-rw-rw-  2.0 fat      338 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/mlflow/__init__.py
--rw-rw-rw-  2.0 fat     2333 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/mlflow/common_constants.py
--rw-rw-rw-  2.0 fat     6225 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/mlflow/common_utils.py
--rw-rw-rw-  2.0 fat     6194 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/mlflow/hf_test_predict.py
--rw-rw-rw-  2.0 fat     3222 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/mlflow/hf_utils.py
--rw-rw-rw-  2.0 fat      102 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/mlflow/mmdet-requirements.txt
--rw-rw-rw-  2.0 fat     5123 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/mlflow/mmdet_mlflow_model_wrapper.py
--rw-rw-rw-  2.0 fat     6775 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/mlflow/mmdet_modules.py
--rw-rw-rw-  2.0 fat     8293 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/common/mlflow/mmdet_utils.py
--rw-rw-rw-  2.0 fat      329 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/defaults/__init__.py
--rw-rw-rw-  2.0 fat     3091 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/defaults/classification_models_defaults.py
--rw-rw-rw-  2.0 fat     3507 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/defaults/constants.py
--rw-rw-rw-  2.0 fat     1430 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/defaults/hf_trainer_defaults.py
--rw-rw-rw-  2.0 fat      823 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/defaults/instance_segmentation_models_defaults.py
--rw-rw-rw-  2.0 fat      808 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/defaults/object_detection_models_defaults.py
--rw-rw-rw-  2.0 fat     2021 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/defaults/task_defaults.py
--rw-rw-rw-  2.0 fat     7761 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/defaults/training_defaults.py
--rw-rw-rw-  2.0 fat      333 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/factory/__init__.py
--rw-rw-rw-  2.0 fat      764 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/factory/mappings.py
--rw-rw-rw-  2.0 fat     1750 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/factory/model_factory.py
--rw-rw-rw-  2.0 fat      548 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/factory/task_definitions.py
--rw-rw-rw-  2.0 fat     5988 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/__init__.py
--rw-rw-rw-  2.0 fat      347 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/classification/__init__.py
--rw-rw-rw-  2.0 fat     8922 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/classification/data_cls.py
--rw-rw-rw-  2.0 fat     2367 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/classification/finetune_cls.py
--rw-rw-rw-  2.0 fat     5355 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/classification/inference_cls.py
--rw-rw-rw-  2.0 fat     1706 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/classification/trainer_classes.py
--rw-rw-rw-  2.0 fat      339 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/common/__init__.py
--rw-rw-rw-  2.0 fat      951 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/common/constants.py
--rw-rw-rw-  2.0 fat    18975 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/common/hf_image_interfaces.py
--rw-rw-rw-  2.0 fat      350 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/mlflow/__init__.py
--rw-rw-rw-  2.0 fat     8760 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/huggingface/mlflow/hf_test_predict.py
--rw-rw-rw-  2.0 fat      265 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/interfaces/__init__.py
--rw-rw-rw-  2.0 fat    10706 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/interfaces/azml_interface.py
--rw-rw-rw-  2.0 fat     2145 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/__init__.py
--rw-rw-rw-  2.0 fat      249 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/common/__init__.py
--rw-rw-rw-  2.0 fat      759 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/common/constants.py
--rw-rw-rw-  2.0 fat     9959 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/common/data_class.py
--rw-rw-rw-  2.0 fat     8592 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/common/dataset.py
--rw-rw-rw-  2.0 fat     1108 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/common/image_metadata.py
--rw-rw-rw-  2.0 fat     3015 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/common/inference.py
--rw-rw-rw-  2.0 fat    10173 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/common/metrics.py
--rw-rw-rw-  2.0 fat     7234 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/common/model.py
--rw-rw-rw-  2.0 fat     1832 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/common/trainer_arguments.py
--rw-rw-rw-  2.0 fat     1784 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/common/trainer_classes.py
--rw-rw-rw-  2.0 fat      264 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/instance_segmentation/__init__.py
--rw-rw-rw-  2.0 fat     7586 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/instance_segmentation/model_wrapper.py
--rw-rw-rw-  2.0 fat      259 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/object_detection/__init__.py
--rw-rw-rw-  2.0 fat    10810 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/mmdetection/object_detection/model_wrapper.py
--rw-rw-rw-  2.0 fat      337 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/__init__.py
--rw-rw-rw-  2.0 fat      298 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/classification/__init__.py
--rw-rw-rw-  2.0 fat      298 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/classification/common/__init__.py
--rw-rw-rw-  2.0 fat    27116 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/classification/common/classification_utils.py
--rw-rw-rw-  2.0 fat     7507 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/classification/common/constants.py
--rw-rw-rw-  2.0 fat     2601 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/classification/common/transforms.py
--rw-rw-rw-  2.0 fat      331 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/__init__.py
--rw-rw-rw-  2.0 fat     1903 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/aml_dataset_base_wrapper.py
--rw-rw-rw-  2.0 fat    10450 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/artifacts_utils.py
--rw-rw-rw-  2.0 fat     2025 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/average_meter.py
--rw-rw-rw-  2.0 fat     2233 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/base_model_factory.py
--rw-rw-rw-  2.0 fat     1231 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/base_model_settings.py
--rw-rw-rw-  2.0 fat    22490 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/constants.py
--rw-rw-rw-  2.0 fat     2826 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/data_utils.py
--rw-rw-rw-  2.0 fat     3200 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/dataloaders.py
--rw-rw-rw-  2.0 fat    18975 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/dataset_helper.py
--rw-rw-rw-  2.0 fat    21071 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/distributed_utils.py
--rw-rw-rw-  2.0 fat      672 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/errors.py
--rw-rw-rw-  2.0 fat     2379 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/exceptions.py
--rw-rw-rw-  2.0 fat     2528 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/logging_utils.py
--rw-rw-rw-  2.0 fat    25090 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/model_export_utils.py
--rw-rw-rw-  2.0 fat    14792 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/parameters.py
--rw-rw-rw-  2.0 fat    10358 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/prediction_dataset.py
--rw-rw-rw-  2.0 fat    37139 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/pretrained_model_utilities.py
--rw-rw-rw-  2.0 fat     3033 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/sku_validation.py
--rw-rw-rw-  2.0 fat     9854 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/system_meter.py
--rw-rw-rw-  2.0 fat     5482 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/tiling_dataset_element.py
--rw-rw-rw-  2.0 fat     6632 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/tiling_utils.py
--rw-rw-rw-  2.0 fat     1039 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/torch_utils.py
--rw-rw-rw-  2.0 fat    65467 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/common/utils.py
--rw-rw-rw-  2.0 fat      302 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/__init__.py
--rw-rw-rw-  2.0 fat      311 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/common/__init__.py
--rw-rw-rw-  2.0 fat     9651 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/common/augmentations.py
--rw-rw-rw-  2.0 fat     7384 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/common/boundingbox.py
--rw-rw-rw-  2.0 fat     3072 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/common/coco_eval_box_converter.py
--rw-rw-rw-  2.0 fat    11010 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/common/constants.py
--rw-rw-rw-  2.0 fat    11945 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/common/masktools.py
--rw-rw-rw-  2.0 fat    27549 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/common/object_detection_utils.py
--rw-rw-rw-  2.0 fat     5003 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/common/parameters.py
--rw-rw-rw-  2.0 fat    25989 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/common/tiling_helper.py
--rw-rw-rw-  2.0 fat      306 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/data/__init__.py
--rw-rw-rw-  2.0 fat     5012 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/data/dataset_wrappers.py
--rw-rw-rw-  2.0 fat    39886 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/data/datasets.py
--rw-rw-rw-  2.0 fat     6166 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/data/loaders.py
--rw-rw-rw-  2.0 fat    11905 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/data/object_annotation.py
--rw-rw-rw-  2.0 fat     5041 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/data/tiling_distributed_sampler.py
--rw-rw-rw-  2.0 fat    11382 b- defN 23-Apr-10 23:37 azureml/acft/image/components/finetune/runtime_common/object_detection/data/utils.py
--rw-rw-rw-  2.0 fat      321 b- defN 23-Apr-10 23:37 azureml/acft/image/components/model_selector/__init__.py
--rw-rw-rw-  2.0 fat    10217 b- defN 23-Apr-10 23:37 azureml/acft/image/components/model_selector/component.py
--rw-rw-rw-  2.0 fat      494 b- defN 23-Apr-10 23:37 azureml/acft/image/components/model_selector/constants.py
--rw-rw-rw-  2.0 fat      859 b- defN 23-Apr-10 23:44 azureml_acft_image_components-0.0.6.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     1485 b- defN 23-Apr-10 23:44 azureml_acft_image_components-0.0.6.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-Apr-10 23:44 azureml_acft_image_components-0.0.6.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-Apr-10 23:44 azureml_acft_image_components-0.0.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    17555 b- defN 23-Apr-10 23:44 azureml_acft_image_components-0.0.6.dist-info/RECORD
-135 files, 1445119 bytes uncompressed, 268679 bytes compressed:  81.4%
+Zip file size: 302149 bytes, number of entries: 136
+-rw-rw-rw-  2.0 fat      251 b- defN 23-Apr-19 08:56 azureml/__init__.py
+-rw-rw-rw-  2.0 fat      295 b- defN 23-Apr-19 08:57 azureml/acft/__init__.py
+-rw-rw-rw-  2.0 fat      822 b- defN 23-Apr-19 08:58 azureml/acft/image/__init__.py
+-rw-rw-rw-  2.0 fat   600282 b- defN 23-Apr-19 08:59 azureml/acft/image/components/NOTICE.txt
+-rw-rw-rw-  2.0 fat      297 b- defN 23-Apr-19 08:59 azureml/acft/image/components/__init__.py
+-rw-rw-rw-  2.0 fat       40 b- defN 23-Apr-19 09:08 azureml/acft/image/components/_version.py
+-rw-rw-rw-  2.0 fat      312 b- defN 23-Apr-19 09:00 azureml/acft/image/components/common/__init__.py
+-rw-rw-rw-  2.0 fat      583 b- defN 23-Apr-19 09:00 azureml/acft/image/components/common/utils.py
+-rw-rw-rw-  2.0 fat      320 b- defN 23-Apr-19 09:00 azureml/acft/image/components/finetune/__init__.py
+-rw-rw-rw-  2.0 fat     7953 b- defN 23-Apr-19 09:00 azureml/acft/image/components/finetune/finetune_runner.py
+-rw-rw-rw-  2.0 fat      466 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/common/__init__.py
+-rw-rw-rw-  2.0 fat     1052 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/common/utils.py
+-rw-rw-rw-  2.0 fat      313 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/augmentation/__init__.py
+-rw-rw-rw-  2.0 fat     6267 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/augmentation/albumentations_augmentation.py
+-rw-rw-rw-  2.0 fat    14648 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/augmentation/augmentation_config_utils.py
+-rw-rw-rw-  2.0 fat     3152 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/augmentation/base_augmentation.py
+-rw-rw-rw-  2.0 fat    27652 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/augmentation/model_preproc_extractor.py
+-rw-rw-rw-  2.0 fat      320 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/augmentation/configs/__init__.py
+-rw-rw-rw-  2.0 fat     1929 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/augmentation/configs/albumentations_classification.yaml
+-rw-rw-rw-  2.0 fat     2474 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/augmentation/configs/albumentations_od_is.yaml
+-rw-rw-rw-  2.0 fat     1314 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/augmentation/configs/hf_albumentations_classification.yaml
+-rw-rw-rw-  2.0 fat      309 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/constants/__init__.py
+-rw-rw-rw-  2.0 fat     1550 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/constants/augmentation_constants.py
+-rw-rw-rw-  2.0 fat     4536 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/constants/constants.py
+-rw-rw-rw-  2.0 fat      304 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/data/__init__.py
+-rw-rw-rw-  2.0 fat     5216 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/data/base_dataset.py
+-rw-rw-rw-  2.0 fat     9513 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/data/classification_dataset.py
+-rw-rw-rw-  2.0 fat     5469 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/data/classification_hf_dataset.py
+-rw-rw-rw-  2.0 fat    10779 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/data/download_manager.py
+-rw-rw-rw-  2.0 fat     7569 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/data/runtime_detection_dataset_adapter.py
+-rw-rw-rw-  2.0 fat      338 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/__init__.py
+-rw-rw-rw-  2.0 fat     8766 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/augmentation_helper.py
+-rw-rw-rw-  2.0 fat     3095 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/common_constants.py
+-rw-rw-rw-  2.0 fat     6644 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/common_utils.py
+-rw-rw-rw-  2.0 fat    10881 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/custom_augmentations.py
+-rw-rw-rw-  2.0 fat     6194 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/hf_test_predict.py
+-rw-rw-rw-  2.0 fat     3222 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/hf_utils.py
+-rw-rw-rw-  2.0 fat      125 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/mmdet-requirements.txt
+-rw-rw-rw-  2.0 fat     6153 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/mmdet_mlflow_model_wrapper.py
+-rw-rw-rw-  2.0 fat     6775 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/mmdet_modules.py
+-rw-rw-rw-  2.0 fat     9358 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/common/mlflow/mmdet_utils.py
+-rw-rw-rw-  2.0 fat      329 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/defaults/__init__.py
+-rw-rw-rw-  2.0 fat     3091 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/defaults/classification_models_defaults.py
+-rw-rw-rw-  2.0 fat     3507 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/defaults/constants.py
+-rw-rw-rw-  2.0 fat     1430 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/defaults/hf_trainer_defaults.py
+-rw-rw-rw-  2.0 fat      823 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/defaults/instance_segmentation_models_defaults.py
+-rw-rw-rw-  2.0 fat      808 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/defaults/object_detection_models_defaults.py
+-rw-rw-rw-  2.0 fat     2021 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/defaults/task_defaults.py
+-rw-rw-rw-  2.0 fat     7761 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/defaults/training_defaults.py
+-rw-rw-rw-  2.0 fat      333 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/factory/__init__.py
+-rw-rw-rw-  2.0 fat      764 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/factory/mappings.py
+-rw-rw-rw-  2.0 fat     2253 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/factory/model_factory.py
+-rw-rw-rw-  2.0 fat      548 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/factory/task_definitions.py
+-rw-rw-rw-  2.0 fat     5944 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/huggingface/__init__.py
+-rw-rw-rw-  2.0 fat      347 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/huggingface/classification/__init__.py
+-rw-rw-rw-  2.0 fat     8922 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/huggingface/classification/data_cls.py
+-rw-rw-rw-  2.0 fat     2367 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/huggingface/classification/finetune_cls.py
+-rw-rw-rw-  2.0 fat     5178 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/huggingface/classification/inference_cls.py
+-rw-rw-rw-  2.0 fat     1706 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/huggingface/classification/trainer_classes.py
+-rw-rw-rw-  2.0 fat      339 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/huggingface/common/__init__.py
+-rw-rw-rw-  2.0 fat      951 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/huggingface/common/constants.py
+-rw-rw-rw-  2.0 fat    18943 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/huggingface/common/hf_image_interfaces.py
+-rw-rw-rw-  2.0 fat      350 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/huggingface/mlflow/__init__.py
+-rw-rw-rw-  2.0 fat     8760 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/huggingface/mlflow/hf_test_predict.py
+-rw-rw-rw-  2.0 fat      265 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/interfaces/__init__.py
+-rw-rw-rw-  2.0 fat    10706 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/interfaces/azml_interface.py
+-rw-rw-rw-  2.0 fat     2327 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/mmdetection/__init__.py
+-rw-rw-rw-  2.0 fat      249 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/common/__init__.py
+-rw-rw-rw-  2.0 fat      759 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/common/constants.py
+-rw-rw-rw-  2.0 fat    10253 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/common/data_class.py
+-rw-rw-rw-  2.0 fat     8739 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/common/dataset.py
+-rw-rw-rw-  2.0 fat     1108 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/common/image_metadata.py
+-rw-rw-rw-  2.0 fat     3015 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/common/inference.py
+-rw-rw-rw-  2.0 fat    10173 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/common/metrics.py
+-rw-rw-rw-  2.0 fat     7234 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/common/model.py
+-rw-rw-rw-  2.0 fat     1832 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/common/trainer_arguments.py
+-rw-rw-rw-  2.0 fat     1784 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/common/trainer_classes.py
+-rw-rw-rw-  2.0 fat      264 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/instance_segmentation/__init__.py
+-rw-rw-rw-  2.0 fat     7586 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/instance_segmentation/model_wrapper.py
+-rw-rw-rw-  2.0 fat      259 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/object_detection/__init__.py
+-rw-rw-rw-  2.0 fat    10810 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/mmdetection/object_detection/model_wrapper.py
+-rw-rw-rw-  2.0 fat      337 b- defN 23-Apr-19 09:01 azureml/acft/image/components/finetune/runtime_common/__init__.py
+-rw-rw-rw-  2.0 fat      298 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/classification/__init__.py
+-rw-rw-rw-  2.0 fat      298 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/classification/common/__init__.py
+-rw-rw-rw-  2.0 fat    27116 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/classification/common/classification_utils.py
+-rw-rw-rw-  2.0 fat     7507 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/classification/common/constants.py
+-rw-rw-rw-  2.0 fat     2601 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/classification/common/transforms.py
+-rw-rw-rw-  2.0 fat      331 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/__init__.py
+-rw-rw-rw-  2.0 fat     1903 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/aml_dataset_base_wrapper.py
+-rw-rw-rw-  2.0 fat    10450 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/artifacts_utils.py
+-rw-rw-rw-  2.0 fat     2025 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/average_meter.py
+-rw-rw-rw-  2.0 fat     2233 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/base_model_factory.py
+-rw-rw-rw-  2.0 fat     1231 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/base_model_settings.py
+-rw-rw-rw-  2.0 fat    22490 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/constants.py
+-rw-rw-rw-  2.0 fat     2826 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/data_utils.py
+-rw-rw-rw-  2.0 fat     3200 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/dataloaders.py
+-rw-rw-rw-  2.0 fat    18975 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/dataset_helper.py
+-rw-rw-rw-  2.0 fat    21071 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/distributed_utils.py
+-rw-rw-rw-  2.0 fat      672 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/errors.py
+-rw-rw-rw-  2.0 fat     2379 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/exceptions.py
+-rw-rw-rw-  2.0 fat     2528 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/logging_utils.py
+-rw-rw-rw-  2.0 fat    25090 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/model_export_utils.py
+-rw-rw-rw-  2.0 fat    14792 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/parameters.py
+-rw-rw-rw-  2.0 fat    10358 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/prediction_dataset.py
+-rw-rw-rw-  2.0 fat    37139 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/pretrained_model_utilities.py
+-rw-rw-rw-  2.0 fat     3033 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/sku_validation.py
+-rw-rw-rw-  2.0 fat     9854 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/system_meter.py
+-rw-rw-rw-  2.0 fat     5482 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/tiling_dataset_element.py
+-rw-rw-rw-  2.0 fat     6632 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/tiling_utils.py
+-rw-rw-rw-  2.0 fat     1039 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/torch_utils.py
+-rw-rw-rw-  2.0 fat    65467 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/common/utils.py
+-rw-rw-rw-  2.0 fat      302 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/__init__.py
+-rw-rw-rw-  2.0 fat      311 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/common/__init__.py
+-rw-rw-rw-  2.0 fat     9651 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/common/augmentations.py
+-rw-rw-rw-  2.0 fat     7384 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/common/boundingbox.py
+-rw-rw-rw-  2.0 fat     3072 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/common/coco_eval_box_converter.py
+-rw-rw-rw-  2.0 fat    11010 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/common/constants.py
+-rw-rw-rw-  2.0 fat    11945 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/common/masktools.py
+-rw-rw-rw-  2.0 fat    27549 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/common/object_detection_utils.py
+-rw-rw-rw-  2.0 fat     5003 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/common/parameters.py
+-rw-rw-rw-  2.0 fat    25989 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/common/tiling_helper.py
+-rw-rw-rw-  2.0 fat      306 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/data/__init__.py
+-rw-rw-rw-  2.0 fat     5012 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/data/dataset_wrappers.py
+-rw-rw-rw-  2.0 fat    39886 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/data/datasets.py
+-rw-rw-rw-  2.0 fat     6166 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/data/loaders.py
+-rw-rw-rw-  2.0 fat    11905 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/data/object_annotation.py
+-rw-rw-rw-  2.0 fat     5041 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/data/tiling_distributed_sampler.py
+-rw-rw-rw-  2.0 fat    11382 b- defN 23-Apr-19 09:02 azureml/acft/image/components/finetune/runtime_common/object_detection/data/utils.py
+-rw-rw-rw-  2.0 fat      321 b- defN 23-Apr-19 09:00 azureml/acft/image/components/model_selector/__init__.py
+-rw-rw-rw-  2.0 fat    13149 b- defN 23-Apr-19 09:00 azureml/acft/image/components/model_selector/component.py
+-rw-rw-rw-  2.0 fat      757 b- defN 23-Apr-19 09:00 azureml/acft/image/components/model_selector/constants.py
+-rw-rw-rw-  2.0 fat      859 b- defN 23-Apr-19 09:08 azureml_acft_image_components-0.0.6.post1.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     1491 b- defN 23-Apr-19 09:08 azureml_acft_image_components-0.0.6.post1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-Apr-19 09:08 azureml_acft_image_components-0.0.6.post1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 23-Apr-19 09:08 azureml_acft_image_components-0.0.6.post1.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    17713 b- defN 23-Apr-19 09:08 azureml_acft_image_components-0.0.6.post1.dist-info/RECORD
+136 files, 1458042 bytes uncompressed, 271725 bytes compressed:  81.4%
```

## zipnote {}

```diff
@@ -42,17 +42,14 @@
 
 Filename: azureml/acft/image/components/finetune/common/augmentation/augmentation_config_utils.py
 Comment: 
 
 Filename: azureml/acft/image/components/finetune/common/augmentation/base_augmentation.py
 Comment: 
 
-Filename: azureml/acft/image/components/finetune/common/augmentation/custom_augmentations.py
-Comment: 
-
 Filename: azureml/acft/image/components/finetune/common/augmentation/model_preproc_extractor.py
 Comment: 
 
 Filename: azureml/acft/image/components/finetune/common/augmentation/configs/__init__.py
 Comment: 
 
 Filename: azureml/acft/image/components/finetune/common/augmentation/configs/albumentations_classification.yaml
@@ -90,20 +87,26 @@
 
 Filename: azureml/acft/image/components/finetune/common/data/runtime_detection_dataset_adapter.py
 Comment: 
 
 Filename: azureml/acft/image/components/finetune/common/mlflow/__init__.py
 Comment: 
 
+Filename: azureml/acft/image/components/finetune/common/mlflow/augmentation_helper.py
+Comment: 
+
 Filename: azureml/acft/image/components/finetune/common/mlflow/common_constants.py
 Comment: 
 
 Filename: azureml/acft/image/components/finetune/common/mlflow/common_utils.py
 Comment: 
 
+Filename: azureml/acft/image/components/finetune/common/mlflow/custom_augmentations.py
+Comment: 
+
 Filename: azureml/acft/image/components/finetune/common/mlflow/hf_test_predict.py
 Comment: 
 
 Filename: azureml/acft/image/components/finetune/common/mlflow/hf_utils.py
 Comment: 
 
 Filename: azureml/acft/image/components/finetune/common/mlflow/mmdet-requirements.txt
@@ -384,23 +387,23 @@
 
 Filename: azureml/acft/image/components/model_selector/component.py
 Comment: 
 
 Filename: azureml/acft/image/components/model_selector/constants.py
 Comment: 
 
-Filename: azureml_acft_image_components-0.0.6.dist-info/LICENSE.txt
+Filename: azureml_acft_image_components-0.0.6.post1.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_acft_image_components-0.0.6.dist-info/METADATA
+Filename: azureml_acft_image_components-0.0.6.post1.dist-info/METADATA
 Comment: 
 
-Filename: azureml_acft_image_components-0.0.6.dist-info/WHEEL
+Filename: azureml_acft_image_components-0.0.6.post1.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_acft_image_components-0.0.6.dist-info/top_level.txt
+Filename: azureml_acft_image_components-0.0.6.post1.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_acft_image_components-0.0.6.dist-info/RECORD
+Filename: azureml_acft_image_components-0.0.6.post1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/acft/image/components/_version.py

```diff
@@ -1,2 +1,2 @@
 ver = "0.0.6"
-selfver = "0.0.6"
+selfver = "0.0.6.post1"
```

## azureml/acft/image/components/finetune/finetune_runner.py

```diff
@@ -23,39 +23,39 @@
 
 from argparse import Namespace
 from functools import partial
 
 from azureml.acft.accelerator.constants import HfTrainerType
 from azureml.acft.accelerator.finetune import AzuremlDatasetArgs, AzuremlFinetuneArgs
 from azureml.acft.accelerator.finetune import AzuremlTrainer
-from azureml.acft.accelerator.utils.decorators import swallow_all_exceptions
 
 from azureml.acft.common_components import get_logger_app
 from azureml.acft.image.components.finetune.factory.mappings import MODEL_FAMILY_CLS
 from azureml.acft.image.components.finetune.factory.model_factory import ModelFactory
-from azureml.acft.image.components.finetune.defaults.training_defaults import (
-    TrainingDefaults,
-)
 from azureml.acft.image.components.finetune.common.constants.constants import (
     SettingLiterals,
 )
 from azureml.evaluate import mlflow
 
-
+# need to add the common folder to the path for importing common augmentation.
+sys.path.append(os.path.join(os.path.dirname(__file__), 'common', 'mlflow'))
 logger = get_logger_app(__name__)
 
 
-@swallow_all_exceptions(logger)
 def finetune_runner(component_args: Namespace) -> None:
     """
     finetune runner for all image tasks
 
     :param component_args: args from the finetune component
     :type component_args: Namespace
     """
+    # importing here after common/mlfow is added to the path.
+    from azureml.acft.image.components.finetune.defaults.training_defaults import (
+        TrainingDefaults,
+    )
 
     logger.info("Starting finetune runner")
     logger.info(f"Task name: {component_args.task_name}")
 
     model_factory = ModelFactory(
         component_args.model_family,
         component_args.model_name_or_path,
@@ -75,16 +75,16 @@
     finetune_obj = finetune_cls(vars(component_args))
 
     # get the component args dict
     custom_finetune_args = finetune_obj.get_finetune_args()
     component_args_dict = vars(component_args)
     component_args_dict.update(custom_finetune_args)
 
-    # get the training defaults if auto defaults is set
-    if component_args_dict.get(SettingLiterals.AUTO_DEFAULTS, False):
+    # get the training defaults if auto hyperparameter selection is set
+    if component_args_dict.get(SettingLiterals.AUTO_HYPERPARAMETER_SELECTION, False):
         training_defaults = TrainingDefaults(
             task=component_args.task_name,
             model_name_or_path=component_args.model_name_or_path,
         )
         component_args_dict.update(training_defaults.defaults_dict)
 
     # log the component args dict
@@ -174,15 +174,14 @@
                 config=model.config,
                 hf_conf=hf_conf,
                 code_paths=code_paths,
             )
         elif component_args.model_family == MODEL_FAMILY_CLS.MMDETECTION_IMAGE:
             requirements_file = os.path.join(os.path.dirname(__file__), "common", "mlflow", "mmdet-requirements.txt")
             # importing directly from acft package is resulting in our package dependencies.
-            sys.path.append(os.path.join(os.path.dirname(__file__), 'common', 'mlflow'))
             from mmdet_utils import save_mmdet_mlflow_pyfunc_model
             save_mmdet_mlflow_pyfunc_model(
                 model_output_dir=component_args.pytorch_model_folder,
                 mlflow_output_dir=component_args.mlflow_model_folder,
                 model_name=os.path.basename(component_args.model_name).split('.')[0],
                 pip_requirements=requirements_file,
                 task_type=component_args.task_name,
```

## azureml/acft/image/components/finetune/common/utils.py

```diff
@@ -1,14 +1,19 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """ Utilities function for finetuning module """
 
 import torch
 
+from azureml.acft.common_components.utils.logging_utils import get_logger_app
+
+
+logger = get_logger_app(__name__)
+
 
 def get_current_device() -> torch.device:
     """Get current cuda device
     :return: current device
     :rtype: torch.device
     """
 
@@ -17,11 +22,12 @@
         try:
             # get the current device index
             device_idx = torch.distributed.get_rank()
         except RuntimeError as ex:
             if 'Default process group has not been initialized'.lower() in str(ex).lower():
                 device_idx = 0
             else:
+                logger.error(str(ex))
                 raise ex
         return torch.device(type="cuda", index=device_idx)
     else:
         return torch.device(type="cpu")
```

## azureml/acft/image/components/finetune/common/augmentation/albumentations_augmentation.py

```diff
@@ -2,33 +2,34 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 """AzureML ACFT Image finetune component - albumentation augmentation."""
 
 import os
 
-from typing import List, Dict
 
 from azureml.acft.common_components import get_logger_app
 from azureml.acft.image.components.finetune.factory.task_definitions import Tasks
 from azureml.acft.image.components.finetune.common.augmentation.base_augmentation import (
     BaseAugmentation,
 )
-from azureml.acft.image.components.finetune.common.augmentation.custom_augmentations import albumentations
+from azureml.acft.image.components.finetune.common.mlflow.custom_augmentations import albumentations
 from azureml.acft.image.components.finetune.common.augmentation.augmentation_config_utils import (
     update_augmentation_dict_with_model_preproc_config,
 )
 from azureml.acft.image.components.finetune.common.constants.augmentation_constants import (
     AugmentationConfigKeys,
-    AugmentationConfigFileNames,
-    AlbumentationParamNames
+    AugmentationConfigFileNames
 )
 from azureml.acft.image.components.finetune.common.constants.constants import (
     SettingLiterals
 )
+from azureml.acft.image.components.finetune.common.mlflow.augmentation_helper import (
+    get_transform, save_augmentations_to_disk
+)
 
 logger = get_logger_app(__name__)
 
 
 class AlbumentationsAugmentation(BaseAugmentation):
     """
     This class expects a yaml file in following format. And composes albumentation transforms from it.
@@ -95,14 +96,19 @@
         # Bounding box is only required for OD and IS tasks
         self.is_bbox_required = kwargs[SettingLiterals.TASK_NAME] in \
             [Tasks.MM_OBJECT_DETECTION, Tasks.MM_INSTANCE_SEGMENTATION]
 
         # read the yaml, get processed dictionary
         self.augmentation_dict = self._get_augmentation_dict()
 
+        output_directory = kwargs.get(SettingLiterals.OUTPUT_DIR, None)
+        if output_directory:
+            # Dump the augmentation dictionary to disk so that it could be reconstructed later for inference
+            save_augmentations_to_disk(output_directory, self.augmentation_dict)
+
     def _get_augmentation_dict(self) -> dict:
         """ Get the augmentation dictionary from augmentation-config updated with the model
         preprocessing param values.
 
         :return: augmentation config dictionary containing function name, updated function param's values
                  & values for train and valid
                 {
@@ -113,106 +119,37 @@
         """
         return update_augmentation_dict_with_model_preproc_config(
             config_path=self.config_path,
             model_preprocessing_params_dict=self.model_preprocessing_params_dict,
             **self.task_params,
         )
 
-    def _get_transform_list(self, augmentation_list: List[Dict]) -> List[Dict]:
-        """
-        Given the list in transformation names, convert it into list of transformation objects.
-
-        :param augmentation_list: List of transformations
-        :type augmentation_list: List[Dict]
-
-        :return List of transformation objects to be applied
-        :rtype: List[Dict]
-
-        Example:
-        augmentation_list = [{
-            "HorizontalFlip": {"p": 0.5}
-        }]
-        _get_transform_list(augmentation_list) would return [albumentations.HorizontalFlip(p=0.5)]
-
-        augmentation_list = [{
-            "OneOf": {
-                "p": 1,
-                "transforms": {
-                    "HorizontalFlip": {"p": 0.5},
-                    "RandomResizedCrop": {"width": 224, "height": 224, "p": 0.5}
-                }
-            }
-        }]
-        _get_transform_list(augmentation_list) would return [
-            albumentations.OneOf(p=1, transforms=[
-                albumentations.HorizontalFlip(p=0.5),
-                albumentations.RandomResizedCrop(width=224, height=224, p=0.5)
-            ])
-        ]
-        """
-        tranform_list = []
-        for augmentation_dict_item in augmentation_list:
-            augmentation_function_name = list(augmentation_dict_item.keys())[0]
-            augmentation_function_params_dict = augmentation_dict_item[augmentation_function_name]
-            if AlbumentationParamNames.TRANSFORMS_KEY in augmentation_function_params_dict:
-                # If transforms list is present inside the augmentation, then iterate over the augmentations
-                # This is required for compositional albumentation augmentations such as OneOf, Sequential, SomeOf etc.
-                augmentation_function_params_dict[AlbumentationParamNames.TRANSFORMS_KEY] = \
-                    self._get_transform_list(
-                        augmentation_function_params_dict.get(AlbumentationParamNames.TRANSFORMS_KEY, [])
-                )
-
-            # Append to list of transforms
-            tranform_list.append(
-                getattr(albumentations, augmentation_function_name)(
-                    **augmentation_function_params_dict
-                )
-            )
-        return tranform_list
-
-    def _get_transform(
-        self, phase_key: str, **kwargs
-    ) -> albumentations.core.composition.Compose:
-        """ Get transform for specified phase <train/valid>
-
-        :param phase_key: Name of the phase (one of train, valid) for which transform is required.
-        :type phase_key: str
-
-        :return: Albumentation transform
-        :rtype: albumentations.core.composition.Compose
-        """
-        tranform_list = self._get_transform_list(self.augmentation_dict[phase_key])
-
-        # Albumentation requires extra bounding box related parameters for processing bbox.
-        extra_params = {
-            AlbumentationParamNames.BBOX_PARAMS: albumentations.BboxParams(
-                format=AlbumentationParamNames.PASCAL_VOC, label_fields=[AlbumentationParamNames.CLASS_LABELS]
-            )} if self.is_bbox_required else dict()
-        albumentations_transforms = albumentations.Compose(transforms=tranform_list, **extra_params)
-        return albumentations_transforms
-
     def get_train_transform(self) -> albumentations.core.composition.Compose:
         """ Get training transform
 
         :return: Albumentation transform
         :rtype: albumentations.core.composition.Compose
         """
-        train_transforms = self._get_transform(
-            phase_key=AugmentationConfigKeys.TRAINING_PHASE_KEY
+        train_transforms = get_transform(
+            phase_key=AugmentationConfigKeys.TRAINING_PHASE_KEY,
+            augmentation_dict=self.augmentation_dict,
+            is_bbox_required=self.is_bbox_required
         )
         logger.info(f"Train transform: {train_transforms}")
         return train_transforms
 
     def get_valid_transform(self) -> albumentations.core.composition.Compose:
         """ Get validation transform
 
         :return: Albumentation transform
         :rtype: albumentations.core.composition.Compose
         """
         if AugmentationConfigKeys.VALIDATION_PHASE_KEY not in self.augmentation_dict:
             valid_transforms = None
         else:
-            valid_transforms = self._get_transform(
-                phase_key=AugmentationConfigKeys.VALIDATION_PHASE_KEY
+            valid_transforms = get_transform(
+                phase_key=AugmentationConfigKeys.VALIDATION_PHASE_KEY,
+                augmentation_dict=self.augmentation_dict,
+                is_bbox_required=self.is_bbox_required
             )
         logger.info(f"Valid transform: {valid_transforms}")
         return valid_transforms
```

## azureml/acft/image/components/finetune/common/augmentation/augmentation_config_utils.py

```diff
@@ -7,141 +7,50 @@
 import inspect
 import os
 import yaml
 
 from types import ModuleType
 from typing import List
 
-from azureml.acft.image.components.finetune.common.augmentation.custom_augmentations import albumentations
+from azureml.acft.image.components.finetune.common.mlflow.custom_augmentations import albumentations
 from azureml.acft.common_components import get_logger_app
 from azureml.acft.image.components.finetune.factory.mappings import MODEL_FAMILY_CLS
-from azureml.acft.image.components.finetune.common.constants.augmentation_constants import (
-    AugmentationConfigKeys,
-    AugmentationConfigFileExts,
-    AlbumentationParamNames
-)
+from azureml.acft.image.components.finetune.common.constants.augmentation_constants import AugmentationConfigKeys
+from azureml.acft.image.components.finetune.common.mlflow.common_constants import AlbumentationParameterNames
 from azureml.acft.image.components.finetune.common.constants.constants import (
     SettingLiterals
 )
 from azureml.acft.image.components.finetune.factory.task_definitions import Tasks
 from azureml.acft.image.components.finetune.common.augmentation.model_preproc_extractor import (
     HfModelPreProcExtractor,
     ModelPreProcExtractor,
     MMDModelPreProcExtractor
 )
+from azureml.acft.image.components.finetune.common.mlflow.augmentation_helper import (
+    load_augmentation_dict_from_config
+)
 
 logger = get_logger_app(__name__)
 
 
-def load_augmentation_dict_from_yaml(config_path: str) -> dict:
-    """
-    This function expects a yaml file in following format, and load the augmentations config file into a dictionary
-
-    augmentation_library_name: <albumentations>
-    train:
-        - <albumentations_function_name_1>:
-              <function_1_parameter_1>: <value_1>
-              <function_1_parameter_2>: <value_2>
-              ...
-        - <albumentations_function_name_2>:
-              <function_2_parameter_1>: <value_3>
-              <function_2_parameter_2>: <value_4>
-              ...
-        ...
-    validation:
-        - <albumentations_function_name_1>:
-              <function_1_parameter_1>: <value_1>
-              <function_1_parameter_2>: <value_2>
-              ...
-        - <albumentations_function_name_2>:
-              <function_2_parameter_1>: <value_3>
-              <function_2_parameter_2>: <value_4>
-              ...
-        ...
-
-    :param config_path: Path to augmentation config
-    :type config_path: str
-    :return: augmentation config dictionary containing
-            function name, function params & values for
-            train and validation
-            {
-                "augmentation_library_name": <"albumentations"> or <"kornia"> or <"torchvision">,
-                "train": [{"function_name": {function_param_name: value, ...}, ...}],
-                "validation": [{"function_name": {function_param_name: value, ...}, ...}],
-            }
-    :rtype: dict
-    """
-    config_path = os.path.abspath(config_path)
-    with open(config_path, "r") as af:
-        incoming_augmentation_dict = yaml.load(af, Loader=yaml.FullLoader)
-
-    incoming_keys = list(incoming_augmentation_dict.keys())
-    if AugmentationConfigKeys.AUGMENTATION_LIBRARY_NAME not in incoming_keys:
-        # Ensure augmentation_library_name is provided in config
-        raise KeyError(
-            f"{AugmentationConfigKeys.AUGMENTATION_LIBRARY_NAME} not in yaml keys: {incoming_keys}"
-        )
-    if AugmentationConfigKeys.TRAINING_PHASE_KEY not in incoming_keys:
-        raise KeyError(
-            f"{AugmentationConfigKeys.TRAINING_PHASE_KEY} not in yaml keys: {incoming_keys}"
-        )
-
-    if (
-        len(incoming_keys) == 3
-        and AugmentationConfigKeys.VALIDATION_PHASE_KEY not in incoming_keys
-    ):
-        # Check for validation key, only when there are >2 keys present
-        raise KeyError(
-            f"{AugmentationConfigKeys.VALIDATION_PHASE_KEY} not in yaml keys: {incoming_keys}"
-        )
-
-    return incoming_augmentation_dict
-
-
-def load_augmentation_dict_from_config(config_path: str) -> dict:
-    """Load the augmentations config file into a dictionary
-
-    :param config_path: Path to augmentation config
-    :typr config_path: str
-
-    :return: augmentation config dictionary containing
-            function name, function params & values for
-            train and valid
-            {
-                "augmentation_library_name": <"albumentations"> or <"kornia"> or <"torchvision">,
-                "train": {"function_name": {function_param_name: value, ...}, ...},
-                "valid": {"function_name": {function_param_name: value, ...}, ...},
-            }
-    :rtype: dict
-    """
-    load_augmentation_dict_from_config_factory = {
-        AugmentationConfigFileExts.YAML: load_augmentation_dict_from_yaml
-    }
-    config_file_type = os.path.splitext(config_path)[-1]
-    if config_file_type not in load_augmentation_dict_from_config_factory:
-        raise NotImplementedError(
-            f"Augmentation config {config_path}, file type {config_file_type} is not supported."
-        )
-    return load_augmentation_dict_from_config_factory[config_file_type](
-        config_path=config_path
-    )
-
-
 def get_augmentation_library(augmentation_library_name: str) -> ModuleType:
     """ Get Augmentation lib corresponding to augmentation_library_name used in config file
 
     :param augmentation_function_name: name of augmentaion library
     :type augmentation_function_name: string
 
     :return: augmentation library used, for example albumentations, kornia, torchvision
     :rtype: ModuleType
     """
     AUGMENTATION_LIB_NAME_TO_LIB_MAPPING = {"albumentations": albumentations}
 
     if augmentation_library_name not in AUGMENTATION_LIB_NAME_TO_LIB_MAPPING.keys():
+        logger.error(
+            f"{augmentation_library_name} not in supported libraries - {AUGMENTATION_LIB_NAME_TO_LIB_MAPPING.keys()}"
+        )
         raise NotImplementedError(
             f"{augmentation_library_name} not in supported libraries - {AUGMENTATION_LIB_NAME_TO_LIB_MAPPING.keys()}"
         )
     return AUGMENTATION_LIB_NAME_TO_LIB_MAPPING[augmentation_library_name]
 
 
 def get_model_preproc_extractor(
@@ -173,14 +82,17 @@
         )
     elif model_family == MODEL_FAMILY_CLS.MMDETECTION_IMAGE:
         return MMDModelPreProcExtractor(
             model_preprocessing_params=model_preprocessing_params_dict,
             augmentation_library_name=augmentation_library_name,
         )
     else:
+        logger.error(
+            f"Model Preprocessing Params Extractor is not yet implemented for {model_family}."
+        )
         raise NotImplementedError(
             f"Model Preprocessing Params Extractor is not yet implemented for "
             f"{model_family}."
         )
 
 
 def validate_transform_function_and_parameter_names(
@@ -200,14 +112,17 @@
 
     :return: None
     :rtype: None
     """
 
     # Ensure that the function is present in the library
     if not hasattr(augmentation_library, augmentation_function_name):
+        logger.error(
+            f"{augmentation_function_name} is not present in {augmentation_library.__name__}."
+        )
         raise NameError(
             f"{augmentation_function_name} is not present in {augmentation_library.__name__}."
         )
 
     # Get list of expected parameters for a function
     expected_func_param_names_list = list(
         inspect.signature(
@@ -326,15 +241,17 @@
         AugmentationConfigKeys.VALIDATION_PHASE_KEY: get_task_image_label_params_dict(
             phase_name=AugmentationConfigKeys.VALIDATION_PHASE_KEY,
             task_params_dict=task_params,
         ),
     }
 
     # Get the updated augmentation config dictionary
-    updated_augmentation_dict = dict()
+    updated_augmentation_dict = {
+        AugmentationConfigKeys.AUGMENTATION_LIBRARY_NAME: augmentation_library_name
+    }
     for phase_key in [
         AugmentationConfigKeys.TRAINING_PHASE_KEY,
         AugmentationConfigKeys.VALIDATION_PHASE_KEY,
     ]:
         if phase_key not in augmentation_dict.keys():
             logger.info(
                 f"{phase_key} is not present in the Augmentation config. Skipping processing for it."
@@ -353,24 +270,24 @@
             # Update aug params, if needed
             update_fn_params = {
                 AugmentationConfigKeys.FUNC_NAME: augmentation_function_name,
                 AugmentationConfigKeys.FUNC_PARAMS_DICT: augmentation_function_params_dict,
                 AugmentationConfigKeys.PHASE_NAME: phase_key,
                 AugmentationConfigKeys.TASK_PARAM_DICT: task_image_label_params_dict[phase_key]
             }
-            if AlbumentationParamNames.TRANSFORMS_KEY in result_dict[augmentation_function_name].keys():
+            if AlbumentationParameterNames.TRANSFORMS_KEY in result_dict[augmentation_function_name].keys():
                 # Processing the transformations such as OneOf, SomeOf etc which contains child transformations
-                augmentation_function_params_update = {AlbumentationParamNames.TRANSFORMS_KEY: []}
-                for transform in result_dict[augmentation_function_name][AlbumentationParamNames.TRANSFORMS_KEY]:
+                augmentation_function_params_update = {AlbumentationParameterNames.TRANSFORMS_KEY: []}
+                for transform in result_dict[augmentation_function_name][AlbumentationParameterNames.TRANSFORMS_KEY]:
                     tfms_name = list(transform.keys())[0]
                     default_param_dict = transform[tfms_name]
                     update_fn_params[AugmentationConfigKeys.FUNC_NAME] = tfms_name
                     update_fn_params[AugmentationConfigKeys.FUNC_PARAMS_DICT] = default_param_dict
                     updated_params_dict = model_preproc_extractor.extract_augmentation_params_dict(**update_fn_params)
-                    augmentation_function_params_update[AlbumentationParamNames.TRANSFORMS_KEY].append({
+                    augmentation_function_params_update[AlbumentationParameterNames.TRANSFORMS_KEY].append({
                         tfms_name: {**default_param_dict, **updated_params_dict}
                     })
             else:
                 augmentation_function_params_update = \
                     model_preproc_extractor.extract_augmentation_params_dict(**update_fn_params)
 
             # Update parameters in the dict
@@ -385,16 +302,16 @@
                 augmentation_library=augmentation_library,
                 augmentation_function_name=augmentation_function_name,
                 augmentation_function_param_names_list=list(
                     result_dict[augmentation_function_name].keys()
                 ),
             )
             # Validate the parameters for compositional transforms such as OneOf, SomeOf etc.
-            if AlbumentationParamNames.TRANSFORMS_KEY in result_dict[augmentation_function_name].keys():
-                for transform in result_dict[augmentation_function_name][AlbumentationParamNames.TRANSFORMS_KEY]:
+            if AlbumentationParameterNames.TRANSFORMS_KEY in result_dict[augmentation_function_name].keys():
+                for transform in result_dict[augmentation_function_name][AlbumentationParameterNames.TRANSFORMS_KEY]:
                     tfms_name = list(transform.keys())[0]
                     validate_transform_function_and_parameter_names(
                         augmentation_library=augmentation_library,
                         augmentation_function_name=tfms_name,
                         augmentation_function_param_names_list=list(
                             transform[tfms_name].keys()
                         )
```

## azureml/acft/image/components/finetune/common/augmentation/model_preproc_extractor.py

```diff
@@ -64,14 +64,18 @@
         :return: <Lib>ParamNames class object, which contains specific param names for this <Lib>
         :rtype: Union[AlbumentationParamNames, ...]
         """
         if (
             self.augmentation_library_name
             not in self.AUGMENTATION_LIB_NAME_TO_PARAMNAMES_CLS_MAPPING.keys()
         ):
+            logger.error(
+                f"{self.augmentation_library_name} not in "
+                f"{self.AUGMENTATION_LIB_NAME_TO_PARAMNAMES_CLS_MAPPING.keys()}"
+            )
             raise NotImplementedError(
                 f"{self.augmentation_library_name} not in "
                 f"{self.AUGMENTATION_LIB_NAME_TO_PARAMNAMES_CLS_MAPPING.keys()}"
             )
 
         return self.AUGMENTATION_LIB_NAME_TO_PARAMNAMES_CLS_MAPPING[
             self.augmentation_library_name
@@ -261,14 +265,19 @@
         """ Get height/width from feature extractor
 
         :return: A dictionary containing height and width extracted form feature extractor / preprocess_config
         :rtype: dict
         """
 
         if HfProcessorParamNames.SIZE_KEY not in self.model_preprocessing_params:
+            logger.error(
+                f"{HfProcessorParamNames.SIZE_KEY} is not present in model preprocess config - "
+                f"{self.model_preprocessing_params}. "
+                f"We don't know what size to use for the model."
+            )
             raise KeyError(
                 f"{HfProcessorParamNames.SIZE_KEY} is not present in model preprocess config - "
                 f"{self.model_preprocessing_params}. "
                 f"We don't know what size to use for the model."
             )
 
         # get the model specific height/width from preprocess_config
@@ -335,14 +344,19 @@
                     height = size_dict[HfProcessorParamNames.SHORTEST_EDGE_KEY]
                     width = size_dict[HfProcessorParamNames.SHORTEST_EDGE_KEY]
             else:
                 height = self.model_preprocessing_params[HfProcessorParamNames.SIZE_KEY]
                 width = self.model_preprocessing_params[HfProcessorParamNames.SIZE_KEY]
 
         else:
+            logger.error(
+                f"Neither {HfProcessorParamNames.CROP_SIZE_KEY}, nor {HfProcessorParamNames.SIZE_KEY} "
+                f"is being used in preprocess_config.json of the model. Please check model's"
+                f"preprocess_config.json file."
+            )
             raise KeyError(
                 f"Neither {HfProcessorParamNames.CROP_SIZE_KEY}, nor {HfProcessorParamNames.SIZE_KEY} "
                 f"is being used in preprocess_config.json of the model. Please check model's"
                 f"preprocess_config.json file."
             )
 
         height_width_dict = {
```

## azureml/acft/image/components/finetune/common/constants/augmentation_constants.py

```diff
@@ -30,31 +30,20 @@
     OD_IS_ALBUMENTATIONS_CONFIG = "configs/albumentations_od_is.yaml"
     HF_CLASSIFICATION_ALBUMENTATIONS_CONFIG = (
         "configs/hf_albumentations_classification.yaml"
     )
 
 
 @dataclass
-class AugmentationConfigFileExts:
-    """Various Augmentation Config file types supported"""
-
-    YAML = ".yaml"
-
-
-@dataclass
 class AlbumentationParamNames:
     """Albumentations parameter and function names"""
 
     LIB_NAME = "albumentations"
 
     HEIGHT_KEY = "height"
     WIDTH_KEY = "width"
 
     NORMALIZE_FUNC_NAME = "Normalize"
     MEAN_KEY = "mean"
     STD_KEY = "std"
-    TRANSFORMS_KEY = "transforms"
     CONSTRAINT_RESIZE_KEY = "img_scale"
     KEEP_RATIO = "keep_ratio"
-    BBOX_PARAMS = "bbox_params"
-    PASCAL_VOC = 'pascal_voc'
-    CLASS_LABELS = 'class_labels'
```

## azureml/acft/image/components/finetune/common/constants/constants.py

```diff
@@ -78,15 +78,15 @@
     TASK_NAME = "task_name"
     VALIDATION_MLTABLE_PATH = "valid_mltable_path"
     PROB_THRESHOLD = "prob_threshold"
     REMOVE_UNUSED_COLUMNS = "remove_unused_columns"
     LABEL_NAMES = "label_names"
     LABEL2ID = "label2id"
     ID2LABEL = "id2label"
-    AUTO_DEFAULTS = "auto_defaults"
+    AUTO_HYPERPARAMETER_SELECTION = "auto_hyperparameter_selection"
     MODEL_NAME_OR_PATH = "model_name_or_path"
 
     # Model family
     MODEL_FAMILY = "model_family"
     MODEL_NAME = "model_name"
 
     # Augmentations
@@ -158,7 +158,8 @@
     HEIGHT = "height"
     WIDTH = "width"
     MASKS = "masks"
     BOXES = "boxes"
     LABELS = "labels"
     ISCROWD = "iscrowd"
     IMAGEFILENAME = "filename"
+    TRANSFORM = "transform"
```

## azureml/acft/image/components/finetune/common/data/base_dataset.py

```diff
@@ -9,15 +9,19 @@
 import pandas as pd
 
 from abc import abstractmethod, ABC
 from torch.utils.data import Dataset
 from sklearn.model_selection import train_test_split
 from typing import Dict, Optional, Tuple
 
-from azureml.acft.accelerator.utils.error_handling.exceptions import DataException
+from azureml._common._error_definition.azureml_error import AzureMLError
+
+from azureml.acft.common_components.utils.error_handling.error_definitions import ACFTUserError
+from azureml.acft.common_components.utils.error_handling.exceptions import ACFTDataException
+
 from azureml.acft.image.components.finetune.common.constants.constants import (
     VisionDatasetConstants,
     ImageDataFrameConstants,
 )
 from azureml.acft.image.components.finetune.common.data.download_manager import (
     DownloadManager,
 )
@@ -55,15 +59,20 @@
         super().__init__()
         self.label2id = dict()
         self.id2label = dict()
 
         if images_df is not None:
             # images df and download directory is available
             if data_dir is None:
-                raise DataException("data_dir cannot be None if image_df is specified.")
+                raise ACFTDataException._with_error(
+                    AzureMLError.create(
+                        ACFTUserError,
+                        pii_safe_message="data_dir cannot be None if image_df is specified.",
+                    )
+                )
             self.images_df = images_df
             self.data_dir = data_dir
             self.image_column_name = image_column_name
             self.label_column_name = label_column_name
 
         else:
             # create dataframe from mltable and download the images
```

## azureml/acft/image/components/finetune/common/data/classification_dataset.py

```diff
@@ -2,33 +2,42 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 """AzureML ACFT Image finetune component - classification dataset."""
 
 import albumentations
 import os
+
 import numpy as np
 import pandas as pd
 
 from PIL import Image
 from typing import Any, Callable, Dict, Optional
 
-from azureml.acft.accelerator.utils.error_handling.exceptions import DataException
+from azureml._common._error_definition.azureml_error import AzureMLError
+
+from azureml.acft.common_components.utils.logging_utils import get_logger_app
+from azureml.acft.common_components.utils.error_handling.error_definitions import ACFTUserError
+from azureml.acft.common_components.utils.error_handling.exceptions import ACFTDataException
+
 from azureml.acft.image.components.finetune.common.constants.augmentation_constants import (
     AlbumentationParamNames,
 )
 from azureml.acft.image.components.finetune.common.constants.constants import (
     ImageDataFrameConstants,
     ImageDataItemLiterals,
 )
 from azureml.acft.image.components.finetune.common.data.base_dataset import (
     BaseDataset,
 )
 
 
+logger = get_logger_app(__name__)
+
+
 class ImageClassificationDataset(BaseDataset):
     """Image Classification Dataset for HF"""
 
     SUPPORTED_TRANSFORM_LIB_NAME_MAPPING = {
         albumentations.core.composition.Compose: AlbumentationParamNames.LIB_NAME
     }
 
@@ -103,14 +112,16 @@
         # Check for supported augmentation transform
         if self.transform and not any(
             [
                 isinstance(self.transform, supported_transform)
                 for supported_transform in self.SUPPORTED_TRANSFORM_LIB_NAME_MAPPING.keys()
             ]
         ):
+            logger.error(f"{type(self.transform)} is not supported. Only"
+                         f" {list( self.SUPPORTED_TRANSFORM_LIB_NAME_MAPPING.keys())} are supported for now.")
             raise NotImplementedError(
                 f"{type(self.transform)} is not supported. "
                 f"Only {list( self.SUPPORTED_TRANSFORM_LIB_NAME_MAPPING.keys())} are supported for now."
             )
 
     def _set_augmentation_library_name_from_transform(self):
         """Extract augmentation library name based on the transform and set it."""
@@ -209,18 +220,25 @@
             ImageDataItemLiterals.DEFAULT_LABEL_KEY: label,
         }
         return example
 
     def validate_image_dataframe(self) -> None:
         """Validate image classification dataframe."""
         if self.images_df.empty:
-            raise DataException("Image dataframe should not be empty.")
-
+            raise ACFTDataException._with_error(
+                AzureMLError.create(
+                    ACFTUserError,
+                    pii_safe_message="Image dataframe should not be empty.")
+            )
         if self.label_column_name not in self.images_df.columns:
-            raise DataException(
-                f"{self.label_column_name} is not present in image dataframe."
+            raise ACFTDataException._with_error(
+                AzureMLError.create(
+                    ACFTUserError,
+                    pii_safe_message=f"{self.label_column_name} is not present in image dataframe.")
             )
 
         if self.image_column_name not in self.images_df.columns:
-            raise DataException(
-                f"{self.image_column_name} is not present in image dataframe."
+            raise ACFTDataException._with_error(
+                AzureMLError.create(
+                    ACFTUserError,
+                    pii_safe_message=f"{self.image_column_name} is not present in image dataframe.")
             )
```

## azureml/acft/image/components/finetune/common/data/download_manager.py

```diff
@@ -16,20 +16,18 @@
 from azureml.core.workspace import Workspace
 from azureml.data.abstract_dataset import AbstractDataset
 from azureml.dataprep import ExecutionError
 from azureml.dataprep.api.engineapi.typedefinitions import FieldType
 from azureml.exceptions import UserErrorException
 
 from azureml.acft.common_components import get_logger_app
+from azureml.acft.common_components.utils.error_handling.error_definitions import ACFTUserError, ACFTSystemError
+from azureml.acft.common_components.utils.error_handling.exceptions import ACFTDataException, ACFTSystemException
+
 from azureml.acft.image.components.common.utils import get_workspace
-from azureml.acft.accelerator.utils.error_handling.exceptions import (
-    LLMException,
-    DataException,
-)
-from azureml.acft.accelerator.utils.error_handling.error_definitions import UserError
 from azureml.acft.image.components.finetune.common.constants.constants import (
     ImageDataFrameConstants,
 )
 
 logger = get_logger_app(__name__)
 
 
@@ -94,26 +92,30 @@
         :type workspace: azureml.core.Workspace
         :return: The dataset corresponding to given label.
         :rtype: AbstractDataset
         """
 
         dataset = None
         if mltable_path is None:
-            raise LLMException._with_error(
-                AzureMLError.create(UserError, error=f"Mltable path is not provided, is {mltable_path}.")
+            raise ACFTDataException._with_error(
+                AzureMLError.create(ACFTUserError, pii_safe_message="Mltable path is not provided.")
             )
         else:
             try:
                 dataset = DownloadManager._load_abstract_dataset(mltable_path)
             except (UserErrorException, ValueError) as e:
                 msg = f"MLTable input is invalid. {e}"
-                raise DataException(msg)
+                raise ACFTDataException._with_error(
+                    AzureMLError.create(ACFTUserError, pii_safe_message=msg)
+                ) from e
             except Exception as e:
                 msg = f"Error in loading MLTable. {e}"
-                raise LLMException(msg)
+                raise ACFTSystemException._with_error(
+                    AzureMLError.create(ACFTSystemError, pii_safe_message=msg)
+                ) from e
 
         return dataset
 
     @staticmethod
     def _load_abstract_dataset(mltable_path: str) -> AbstractDataset:
         """Get abstract dataset  from mltable.
 
@@ -223,18 +225,18 @@
             ds.download(
                 stream_column=image_column_name,
                 target_path=data_dir,
                 ignore_not_found=True,
                 overwrite=True,
             )
         except (ExecutionError, UserErrorException) as e:
-            raise DataException(
-                "Could not download dataset files. "
-                f"Please check the logs for more details. Error Code: {e}"
-            )
+            msg = f"Could not download dataset files. Please check the logs for more details. Error Code: {e}"
+            raise ACFTDataException._with_error(
+                AzureMLError.create(ACFTUserError, pii_safe_message=msg)
+            ) from e
 
         logger.info(
             f"Downloading image files took {time.perf_counter() - start_time:.2f} seconds"
         )
 
     @staticmethod
     def _validate_image_column(ds: AmlDataset, image_column_name: str) -> None:
@@ -243,17 +245,19 @@
         :param ds: Aml Dataset object
         :type ds: TabularDataset
         :param image_column_name: The column name for the image file.
         :type image_column_name: str
         """
         dtypes = ds._dataflow.dtypes
         if image_column_name not in dtypes:
-            raise DataException(
-                f"Image URL column '{image_column_name}' is not present in the dataset."
+            msg = f"Image URL column '{image_column_name}' is not present in the dataset."
+            raise ACFTDataException._with_error(
+                AzureMLError.create(ACFTUserError, pii_safe_message=msg)
             )
-
         image_column_dtype = dtypes.get(image_column_name)
         if image_column_dtype != FieldType.STREAM:
-            raise DataException(
-                f"The data type of image URL column '{image_column_name}' is {image_column_dtype.name}, "
-                f"but it should be {FieldType.STREAM.name}."
+            msg = f"The data type of image URL column '{image_column_name}' is {image_column_dtype.name}, " \
+                  f"but it should be {FieldType.STREAM.name}."
+
+            raise ACFTDataException._with_error(
+                AzureMLError.create(ACFTUserError, pii_safe_message=msg)
             )
```

## azureml/acft/image/components/finetune/common/mlflow/common_constants.py

```diff
@@ -1,13 +1,42 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 """Mlflow PythonModel wrapper helper constants."""
 
+from dataclasses import dataclass
+
+
+@dataclass
+class AugmentationConfigKeys:
+    """Keys in augmentation configs"""
+
+    TRAINING_PHASE_KEY = "train"
+    VALIDATION_PHASE_KEY = "validation"
+    AUGMENTATION_LIBRARY_NAME = "augmentation_library_name"
+    OUTPUT_AUG_FILENAME = "augmentations.yaml"
+    ALBUMENTATIONS = "albumentations"
+
+
+@dataclass
+class AlbumentationParameterNames:
+    """ keys for Albumentations parameters"""
+    TRANSFORMS_KEY = "transforms"
+    BBOX_PARAMS = "bbox_params"
+    PASCAL_VOC = 'pascal_voc'
+    CLASS_LABELS = 'class_labels'
+
+
+@dataclass
+class AugmentationConfigFileExts:
+    """Various Augmentation Config file types supported"""
+
+    YAML = ".yaml"
+
 
 class Tasks:
     "Tasks supported for All Frameworks"
 
     HF_MULTI_CLASS_IMAGE_CLASSIFICATION = "image-classification"
     HF_MULTI_LABEL_IMAGE_CLASSIFICATION = "image-classification-multilabel"
     MM_OBJECT_DETECTION = "image-object-detection"
@@ -50,14 +79,15 @@
     THRESHOLD = "threshold"
 
 
 class MMDetLiterals:
     """MMDetection constants"""
     CONFIG_PATH = "config_path"
     WEIGHTS_PATH = "weights_path"
+    AUGMENTATIONS_PATH = "augmentations_path"
 
 
 class MmDetectionDatasetLiterals:
     """MMDetection dataset constants"""
 
     IMG = "img"
     IMG_METAS = "img_metas"
```

## azureml/acft/image/components/finetune/common/mlflow/common_utils.py

```diff
@@ -16,14 +16,19 @@
 import re
 import requests
 
 from PIL import Image
 from typing import Any, Dict, Optional
 from mlflow.models.signature import ModelSignature
 from mlflow.types.schema import ColSpec, Schema
+
+from azureml._common._error_definition.azureml_error import AzureMLError
+from azureml.acft.common_components.utils.error_handling.error_definitions import TaskNotSupported
+from azureml.acft.common_components.utils.error_handling.exceptions import ACFTValidationException
+
 from common_constants import (
     Tasks,
     MLFlowSchemaLiterals,
     MMDetLiterals
 )
 
 logger = logging.getLogger(__name__)
@@ -98,15 +103,17 @@
                 ColSpec(
                     MLFlowSchemaLiterals.OUTPUT_COLUMN_DATA_TYPE,
                     MLFlowSchemaLiterals.OUTPUT_COLUMN_BOXES,
                 ),
             ]
         )
     else:
-        raise ValueError(f"invalid task type {task_type}")
+        raise ACFTValidationException._with_error(
+            AzureMLError.create(TaskNotSupported, TaskName=task_type)
+        )
 
     return ModelSignature(inputs=input_schema, outputs=output_schema)
 
 
 def save_mlflow_model(
     model_output_dir: str,
     mlflow_output_dir: str,
@@ -144,14 +151,15 @@
             artifacts=artifacts_dict,
             pip_requirements=pip_requirements,
             signature=options[MLFlowSchemaLiterals.SCHEMA_SIGNATURE],
             code_path=code_path
         )
         logger.info("Saved mlflow model successfully.")
     except Exception as e:
+        logger.error(f"Failed to save the mlflow model {str(e)}")
         raise Exception(f"failed to save the mlflow model {str(e)}")
 
 
 def process_image(img: pd.Series) -> pd.Series:
     """If input image is in base64 string format, decode it to bytes. If input image is in url format,
     download it and return bytes.
     https://github.com/mlflow/mlflow/blob/master/examples/flower_classifier/image_pyfunc.py
```

## azureml/acft/image/components/finetune/common/mlflow/mmdet-requirements.txt

```diff
@@ -1,7 +1,8 @@
 mlflow
 cloudpickle==2.2.0
 datasets==2.3.2
 openmim
 torch==1.11.0
 torchvision
-transformers==4.25.1
+transformers==4.25.1
+albumentations==1.3.0
```

## azureml/acft/image/components/finetune/common/mlflow/mmdet_mlflow_model_wrapper.py

```diff
@@ -6,29 +6,39 @@
 
 import logging
 import mlflow
 import pandas as pd
 import sys
 import subprocess
 import tempfile
-
+import torch
 
 from common_utils import process_image, create_temp_file
 from transformers import TrainingArguments
-
-from common_constants import (
-    HFMiscellaneousLiterals,
-    MMDetLiterals,
-    MLFlowSchemaLiterals,
-    Tasks,
+from augmentation_helper import (
+    load_augmentation_dict_from_config,
+    get_transform
 )
 
+from common_constants import (AugmentationConfigKeys,
+                              HFMiscellaneousLiterals, Tasks,
+                              MMDetLiterals,
+                              MLFlowSchemaLiterals)
+
 logger = logging.getLogger(__name__)
 
 
+def get_device() -> str:
+    """Returns the currently existing device type.
+    Returns:
+        str: cuda | cpu.
+    """
+    return "cuda" if torch.cuda.is_available() else "cpu"
+
+
 class ImagesMLFlowModelWrapper(mlflow.pyfunc.PythonModel):
     """MLFlow model wrapper for AutoML for Images models."""
 
     def __init__(
         self,
         task_type: str,
     ) -> None:
@@ -46,39 +56,56 @@
         :param context: Mlflow context containing artifacts that the model can use for inference.
         :type context: mlflow.pyfunc.PythonModelContext
         """
         logger.info("Inside load_context()")
 
         if self._task_type == Tasks.MM_OBJECT_DETECTION:
             # Install mmcv and mmdet using mim, with pip installation is not working
-            subprocess.check_call([sys.executable, "-m", "mim", "install", "mmcv-full"])
-            subprocess.check_call([sys.executable, "-m", "mim", "install", "mmdet"])
+            subprocess.check_call([sys.executable, "-m", "mim", "install", "mmcv-full==1.7.1"])
+            subprocess.check_call([sys.executable, "-m", "mim", "install", "mmdet==2.28.2"])
 
             # importing mmdet/mmcv afte installing using mim
             from mmdet.models import build_detector
             from mmcv import Config
             from mmcv.runner import load_checkpoint
             from mmdet_modules import ObjectDetectionModelWrapper
 
             try:
                 if self._task_type == Tasks.MM_OBJECT_DETECTION:
                     model_config_path = context.artifacts[MMDetLiterals.CONFIG_PATH]
                     model_weights_path = context.artifacts[MMDetLiterals.WEIGHTS_PATH]
 
                     self._config = Config.fromfile(model_config_path)
                     self._model = build_detector(self._config.model)
-                    load_checkpoint(self._model, model_weights_path)
+                    load_checkpoint(self._model, model_weights_path, map_location=get_device())
                     self._model = ObjectDetectionModelWrapper(self._model, self._config, model_weights_path)
                     logger.info("Model loaded successfully")
+
                 else:
                     raise ValueError(f"invalid task type {self._task_type}")
             except Exception:
                 logger.warning("Failed to load the the model.")
                 raise
 
+            aug_config_path = context.artifacts[MMDetLiterals.AUGMENTATIONS_PATH]
+            aug_config_dict = load_augmentation_dict_from_config(aug_config_path)
+            self.test_transforms = get_transform(AugmentationConfigKeys.VALIDATION_PHASE_KEY,
+                                                 aug_config_dict,
+                                                 # Bbox is not required at test time
+                                                 is_bbox_required=False)
+            # arguments for Trainer
+            self.test_args = TrainingArguments(
+                output_dir=".",
+                do_train=False,
+                do_predict=True,
+                per_device_eval_batch_size=1,
+                dataloader_drop_last=False,
+                remove_unused_columns=False
+            )
+
     def predict(
         self, context: mlflow.pyfunc.PythonModelContext, input_data: pd.DataFrame
     ) -> pd.DataFrame:
         """This method performs inference on the input data.
 
         :param context: Mlflow context containing artifacts that the model can use for inference.
         :type context: mlflow.pyfunc.PythonModelContext
@@ -92,39 +119,30 @@
         task = self._task_type
 
         # process the images in image column
         processed_images = input_data.loc[
             :, [MLFlowSchemaLiterals.INPUT_COLUMN_IMAGE]
         ].apply(axis=1, func=process_image)
 
-        # arguments for Trainer
-        test_args = TrainingArguments(
-            output_dir=".",
-            do_train=False,
-            do_predict=True,
-            per_device_eval_batch_size=1,
-            dataloader_drop_last=False,
-            remove_unused_columns=False,
-        )
-
         # To Do: change image height and width based on kwargs.
 
         with tempfile.TemporaryDirectory() as tmp_output_dir:
             image_path_list = (
                 processed_images.iloc[:, 0]
                 .map(lambda row: create_temp_file(row, tmp_output_dir))
                 .tolist()
             )
             if task in [
                 Tasks.MM_OBJECT_DETECTION,
                 Tasks.MM_INSTANCE_SEGMENTATION
             ]:
                 from mmdet_utils import mmdet_run_inference_batch
                 result = mmdet_run_inference_batch(
-                    test_args,
+                    self.test_args,
                     model=self._model,
                     id2label=self._config[HFMiscellaneousLiterals.ID2LABEL],
                     image_path_list=image_path_list,
-                    task_type=task
+                    task_type=task,
+                    test_transforms=self.test_transforms,
                 )
 
         return pd.DataFrame(result)
```

## azureml/acft/image/components/finetune/common/mlflow/mmdet_utils.py

```diff
@@ -11,18 +11,20 @@
 import torch
 
 
 from datasets import load_dataset
 from dataclasses import asdict
 from torchvision.transforms import functional as F
 from transformers import Trainer, TrainingArguments
-from typing import Dict, List, Optional, Tuple, Any
+from typing import Dict, List, Optional, Any, Callable
 
 
-from common_constants import (HFMiscellaneousLiterals, Tasks,
+from common_constants import (AugmentationConfigKeys,
+                              HFMiscellaneousLiterals,
+                              Tasks,
                               MMDetLiterals,
                               MLFlowSchemaLiterals,
                               MmDetectionDatasetLiterals,
                               ODLiterals)
 from common_utils import get_mlflow_signature
 from mmdet_mlflow_model_wrapper import ImagesMLFlowModelWrapper
 from mmdet_modules import ImageMetadata
@@ -50,37 +52,41 @@
     :type model_name: str
     :param pip_requirements: Path to the pip requirements file.
     :type pip_requirements: Optional[os.PathLike]
     """
 
     config_path = os.path.join(model_output_dir, model_name + ".py")
     model_weights_path = os.path.join(model_output_dir, model_name + ".pth")
+    augmentations_path = os.path.join(model_output_dir, AugmentationConfigKeys.OUTPUT_AUG_FILENAME)
     artifacts_dict = {
         MMDetLiterals.CONFIG_PATH : config_path,
-        MMDetLiterals.WEIGHTS_PATH : model_weights_path
+        MMDetLiterals.WEIGHTS_PATH : model_weights_path,
+        MMDetLiterals.AUGMENTATIONS_PATH: augmentations_path
     }
 
     logger.info(f"Saving mlflow pyfunc model to {mlflow_output_dir}.")
 
     try:
         logging.getLogger("mlflow").setLevel(logging.DEBUG)
         dir = os.path.dirname(__file__)
         files_to_include = ['common_constants.py', 'common_utils.py', 'mmdet_mlflow_model_wrapper.py',
-                            'mmdet_modules.py', 'mmdet_utils.py']
+                            'mmdet_modules.py', 'mmdet_utils.py', 'augmentation_helper.py',
+                            'custom_augmentations.py']
         code_path = [os.path.join(dir, x) for x in files_to_include]
         mlflow.pyfunc.save_model(
             path=mlflow_output_dir,
             python_model=options[MLFlowSchemaLiterals.WRAPPER],
             artifacts=artifacts_dict,
             pip_requirements=pip_requirements,
             signature=options[MLFlowSchemaLiterals.SCHEMA_SIGNATURE],
             code_path=code_path
         )
         logger.info("Saved mlflow model successfully.")
     except Exception as e:
+        logger.error(f"Failed to save the mlflow model {str(e)}")
         raise Exception(f"failed to save the mlflow model {str(e)}")
 
 
 def save_mmdet_mlflow_pyfunc_model(
     task_type: str,
     model_output_dir: str,
     mlflow_output_dir: str,
@@ -122,27 +128,31 @@
 
 def mmdet_run_inference_batch(
     test_args: TrainingArguments,
     model: torch.nn.Module,
     id2label: Dict[int, str],
     image_path_list: List,
     task_type: Tasks,
+    test_transforms: Callable,
 ) -> List:
     """This method performs inference on batch of input images.
 
     :param test_args: Training arguments path.
     :type test_args: transformers.TrainingArguments
     :param image_processor: Preprocessing configuration loader.
     :type image_processor: transformers.AutoImageProcessor
     :param model: Pytorch model weights.
     :type model: transformers.AutoModelForImageClassification
     :param image_path_list: list of image paths for inferencing.
     :type image_path_list: List
     :param task_type: Task type of the model.
     :type task_type: constants.Tasks
+    :param test_transforms: Transformations to apply to the test dataset before
+                            sending it to the model.
+    :param test_transforms: Callable
     :return: list of dict.
     :rtype: list
     """
 
     def collate_fn(examples: List[Dict[str, Dict]]) -> Dict[str, Dict]:
         # Filter out invalid examples
         valid_examples = [example for example in examples if example is not None]
@@ -154,28 +164,37 @@
                 logger.info(f"{num_invalid_examples} invalid images found.")
                 logger.info("Replacing invalid images with randomly selected valid images from the current batch")
                 new_example_indices = np.random.choice(np.arange(len(valid_examples)), num_invalid_examples)
                 for ind in new_example_indices:
                     # Padding the batch with valid examples
                     valid_examples.append(valid_examples[ind])
 
-        # Pre processing code should come here once the PR for mmdet aug
-        # is merged
-        pixel_values = torch.stack(
-            [
-                F.to_tensor(example[HFMiscellaneousLiterals.DEFAULT_IMAGE_KEY]).to(dtype=torch.float)
-                for example in valid_examples
-            ]
-        )
+        # Pre processing Image
+        if test_transforms is not None:
+            for example in valid_examples:
+                example[HFMiscellaneousLiterals.DEFAULT_IMAGE_KEY] = test_transforms(
+                    image=np.array(example[HFMiscellaneousLiterals.DEFAULT_IMAGE_KEY])
+                )[HFMiscellaneousLiterals.DEFAULT_IMAGE_KEY]
+
+        def to_tensor_fn(img):
+            return torch.from_numpy(img.transpose(2, 0, 1)).to(dtype=torch.float)
+
+        pixel_values = torch.stack([
+            to_tensor_fn(example[HFMiscellaneousLiterals.DEFAULT_IMAGE_KEY])
+            for example in valid_examples
+        ])
 
         img_metas = []
         for i, example in enumerate(valid_examples):
             image = example[HFMiscellaneousLiterals.DEFAULT_IMAGE_KEY]
-            width, height = image.size
-            no_ch = len(image.getbands())
+            if test_transforms:
+                width, height, no_ch = image.shape
+            else:
+                width, height = image.size
+                no_ch = len(image.getbands())
             img_metas.append(
                 asdict(ImageMetadata(ori_shape=(width, height, no_ch), filename=f"test_{i}.jpg"))
             )
 
         # input to mmdet model should contain image and image meta data
         output = {
             MmDetectionDatasetLiterals.IMG: pixel_values,
@@ -201,17 +220,17 @@
 
     proc_op = []
     for bboxes, labels in zip(output[MmDetectionDatasetLiterals.BBOXES], output[MmDetectionDatasetLiterals.LABELS]):
         curimage_preds = {ODLiterals.BOXES : []}
         for bbox, label in zip(bboxes, labels):
             curimage_preds[ODLiterals.BOXES].append({
                 ODLiterals.BOX : {
-                    ODLiterals.TOP_X : bbox[0],
-                    ODLiterals.TOP_Y : bbox[1],
-                    ODLiterals.BOTTOM_X : bbox[2],
-                    ODLiterals.BOTTOM_Y : bbox[3],
+                    ODLiterals.TOP_X : str(bbox[0]),
+                    ODLiterals.TOP_Y : str(bbox[1]),
+                    ODLiterals.BOTTOM_X : str(bbox[2]),
+                    ODLiterals.BOTTOM_Y : str(bbox[3]),
                 },
                 ODLiterals.LABEL : id2label[label],
-                ODLiterals.SCORE : bbox[4],
+                ODLiterals.SCORE : str(bbox[4]),
             })
         proc_op.append(curimage_preds)
     return proc_op
```

## azureml/acft/image/components/finetune/factory/model_factory.py

```diff
@@ -2,15 +2,18 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 """Fetching and validating models."""
 
 import importlib
 
+from azureml._common._error_definition.azureml_error import AzureMLError
 from azureml.acft.common_components import get_logger_app
+from azureml.acft.common_components.utils.error_handling.error_definitions import ModelFamilyNotSupported
+from azureml.acft.common_components.utils.error_handling.exceptions import ACFTValidationException
 from azureml.acft.image.components.finetune.factory.task_definitions import Tasks
 from azureml.acft.image.components.finetune.factory.mappings import (
     MODEL_FAMILY_CLS,
     MODEL_FAMILY_MODULE_IMPORT_PATH_MAP
 )
 
 
@@ -22,23 +25,27 @@
 
     def __init__(self, model_family: MODEL_FAMILY_CLS, model_id: str,
                  task_name: str=Tasks.HF_MULTI_CLASS_IMAGE_CLASSIFICATION):
         """
         init function for ModelFactory
         """
         if model_family not in MODEL_FAMILY_MODULE_IMPORT_PATH_MAP:
-            raise ValueError(
-                f"{model_family} not supported. "
-                f"Supported model families: {list(MODEL_FAMILY_MODULE_IMPORT_PATH_MAP.keys())}."
+            raise ACFTValidationException._with_error(
+                AzureMLError.create(
+                    ModelFamilyNotSupported,
+                    model_family=model_family,
+                    supported_model_families=list(MODEL_FAMILY_MODULE_IMPORT_PATH_MAP.keys())
+                )
             )
 
         # Load the module
         try:
             module = importlib.import_module(MODEL_FAMILY_MODULE_IMPORT_PATH_MAP[model_family])
         except ImportError as e:
+            logger.error(f"Unable to import the module. model family: {model_family}. Error: {e}")
             raise ImportError(f"Unable to import the module. model family: {model_family}. Error: {e}")
 
         # Set the model factory output
         trainer_classes_obj = getattr(module, "TrainerClasses")(model_family, model_id, task_name)
         self.trainer_classes = trainer_classes_obj.get_trainer_classes_mapping()
 
     @property
```

## azureml/acft/image/components/finetune/huggingface/__init__.py

```diff
@@ -27,22 +27,18 @@
     MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES,
 )
 
 from azureml.acft.image.components.finetune.factory.mappings import MODEL_FAMILY_CLS
 from azureml.acft.image.components.finetune.factory.task_definitions import Tasks
 from azureml.acft.image.components.finetune.common.constants.constants import HfConstants
 from azureml._common._error_definition.azureml_error import AzureMLError  # type: ignore
-from azureml.acft.accelerator.utils.error_handling.error_definitions import (
-    ModelIncompatibleWithTask,
-    # InvalidModelData, to be added in future.
-)
-from azureml.acft.accelerator.utils.error_handling.exceptions import (
-    ValidationException,
-)
+
 from azureml.acft.common_components import get_logger_app
+from azureml.acft.common_components.utils.error_handling.exceptions import ACFTValidationException
+from azureml.acft.common_components.utils.error_handling.error_definitions import ModelIncompatibleWithTask
 from azureml.acft.image.components.finetune.huggingface.classification.trainer_classes import (
     ImageClassificationTrainerClasses,
 )
 
 logger = get_logger_app(__name__)
 
 
@@ -88,15 +84,15 @@
             return os.environ[HfConstants.HFModelType]
         try:
             config = AutoConfig.from_pretrained(model_name_or_path)
         except Exception as e:
             # TO DO: differentiate between azureml supported models and user resgistered
             # models and raise system/user errors.
             # change this to invalid model data error after its support in acft.accelerator
-            raise ValidationException._with_error(
+            raise ACFTValidationException._with_error(
                 AzureMLError.create(ModelIncompatibleWithTask,
                                     error=str(e),
                                     ModelName=model_name_or_path,
                                     TaskName=Tasks.HF_MULTI_CLASS_IMAGE_CLASSIFICATION,
                                     file_name="config.json")
             )
         if config is None or not hasattr(config, "model_type"):
@@ -107,15 +103,15 @@
         os.environ[HfConstants.HFModelType] = getattr(config, "model_type")
         return getattr(config, "model_type")
 
     def _check_task_model_family_compatibility(self):
         """Check if the given model supports the given task in the case of Hugging Face Models."""
         task_supported_model_families = TASK_SUPPORTED_FAMILY_MAP[self.task_name]
         if self.model_type not in task_supported_model_families:
-            raise ValidationException._with_error(
+            raise ACFTValidationException._with_error(
                 AzureMLError.create(
                     ModelIncompatibleWithTask,
                     TaskName=self.task_name,
                     ModelName=self.model_name_or_path,
                 )
             )
         else:
```

## azureml/acft/image/components/finetune/huggingface/classification/inference_cls.py

```diff
@@ -24,16 +24,14 @@
 import numpy as np
 import torch
 
 from azureml._common._error_definition.azureml_error import AzureMLError  # type: ignore
 from azureml.metrics import compute_metrics
 from azureml.metrics import constants as metrics_constants
 from azureml.metrics import list_metrics
-from azureml.acft.accelerator.utils.error_handling.error_definitions import LLMInternalError
-from azureml.acft.accelerator.utils.error_handling.exceptions import LLMException
 
 from azureml.acft.common_components import get_logger_app
 from azureml.acft.image.components.finetune.common.constants.constants import InferenceParameters, SettingLiterals
 from azureml.acft.image.components.finetune.huggingface.common.constants import HfProblemType
 from azureml.acft.image.components.finetune.interfaces.azml_interface import AzmlInferenceInterface
 
 logger = get_logger_app(__name__)
```

## azureml/acft/image/components/finetune/huggingface/common/hf_image_interfaces.py

```diff
@@ -16,32 +16,31 @@
 # limitations under the License.
 # ---------------------------------------------------------
 
 """Common image interfaces for huggingface."""
 
 import os
 import time
+from azureml.acft.common_components.utils.error_handling.error_definitions import (
+    TaskNotSupported,
+    ACFTUserError,
+)
+from azureml.acft.common_components.utils.error_handling.exceptions import ACFTValidationException
 import transformers
 
 from transformers import (
     AutoConfig,
     AutoImageProcessor,
     PretrainedConfig,
     PreTrainedModel,
 )
 from transformers.image_processing_utils import BaseImageProcessor
 from typing import Optional, Tuple, Union
 
 from azureml._common._error_definition.azureml_error import AzureMLError  # type: ignore
-from azureml.acft.accelerator.utils.error_handling.error_definitions import (
-    BadData,
-)
-from azureml.acft.accelerator.utils.error_handling.exceptions import (
-    ValidationException,
-)
 
 from azureml.acft.common_components import get_logger_app
 from azureml.acft.image.components.finetune.common.constants.constants import (
     SettingLiterals,
     HfProcessorParamNames
 )
 from azureml.acft.image.components.finetune.huggingface.common.constants import (
@@ -255,83 +254,79 @@
         :rtype: BaseImageProcessor
         """
         # this function is to be used by all hugging face image models
         # task_name and model_specific_args is to facilitate any future
         # model_specific requirements.
         task_name = kwargs.pop(SettingLiterals.TASK_NAME, None)
         if task_name is None:
-            raise ValueError(f"Invalid task_name: {task_name}")
+            raise ACFTValidationException._with_error(
+                AzureMLError.create(TaskNotSupported,
+                                    TaskName=task_name))
         start_time = time.time()
 
         model_specific_args = {}
-        try:
-            image_processor = AutoImageProcessor.from_pretrained(
-                hf_image_model_name_or_path, **model_specific_args,
-            )
-            logger.info(f"Original feature extractor: {image_processor.to_dict()}")
-            # The general order in which the transforms are applied in HF
-            # RGB -> resize -> center_crop -> rescale -> normalize
-            do_center_crop = hasattr(
-                image_processor, HfProcessorParamNames.DO_CENTER_CROP_KEY
-            ) and getattr(image_processor, HfProcessorParamNames.DO_CENTER_CROP_KEY)
-            do_resize = hasattr(
-                image_processor, HfProcessorParamNames.DO_RESIZE_KEY
-            ) and getattr(image_processor, HfProcessorParamNames.DO_RESIZE_KEY)
-            if do_center_crop and do_resize:
-                image_processor.size = AzmlHfImageFeatureExtractor.update_resize_size_dict(
-                    image_processor=image_processor,
-                    image_height=kwargs[SettingLiterals.IMAGE_HEIGHT],
-                    image_width=kwargs[SettingLiterals.IMAGE_WIDTH],
-                    do_center_crop=do_center_crop,
-                )
-                image_processor.crop_size = {
-                    HfProcessorParamNames.HEIGHT_KEY: kwargs[
-                        SettingLiterals.IMAGE_HEIGHT
-                    ],
-                    HfProcessorParamNames.WIDTH_KEY: kwargs[
-                        SettingLiterals.IMAGE_WIDTH
-                    ],
-                }
-            elif do_center_crop:
-                image_processor.crop_size = {
-                    HfProcessorParamNames.HEIGHT_KEY: kwargs[
-                        SettingLiterals.IMAGE_HEIGHT
-                    ],
-                    HfProcessorParamNames.WIDTH_KEY: kwargs[
-                        SettingLiterals.IMAGE_WIDTH
-                    ],
-                }
-            elif do_resize:
-                image_processor.size = AzmlHfImageFeatureExtractor.update_resize_size_dict(
-                    image_processor=image_processor,
-                    image_height=kwargs[SettingLiterals.IMAGE_HEIGHT],
-                    image_width=kwargs[SettingLiterals.IMAGE_WIDTH],
-                    do_center_crop=False,
-                )
-            logger.info(
-                f"Updating Feature Extractor with: {image_processor.to_dict()}"
-            )
-            # Todo - if we find a better way, update it.
-            # Get the feature extractor using the updated dict. This way save_pretrained will save the right values.
-            image_processor = AutoImageProcessor.from_pretrained(
-                hf_image_model_name_or_path, **image_processor.to_dict()
-            )
-        except Exception as e:
-            raise ValidationException._with_error(
-                AzureMLError.create(
-                    BadData, error=str(e), file_name="preprocessor_config.json"
-                )
+
+        image_processor = AutoImageProcessor.from_pretrained(
+            hf_image_model_name_or_path, **model_specific_args,
+        )
+        logger.info(f"Original feature extractor: {image_processor.to_dict()}")
+        # The general order in which the transforms are applied in HF
+        # RGB -> resize -> center_crop -> rescale -> normalize
+        do_center_crop = hasattr(
+            image_processor, HfProcessorParamNames.DO_CENTER_CROP_KEY
+        ) and getattr(image_processor, HfProcessorParamNames.DO_CENTER_CROP_KEY)
+        do_resize = hasattr(
+            image_processor, HfProcessorParamNames.DO_RESIZE_KEY
+        ) and getattr(image_processor, HfProcessorParamNames.DO_RESIZE_KEY)
+        if do_center_crop and do_resize:
+            image_processor.size = AzmlHfImageFeatureExtractor.update_resize_size_dict(
+                image_processor=image_processor,
+                image_height=kwargs[SettingLiterals.IMAGE_HEIGHT],
+                image_width=kwargs[SettingLiterals.IMAGE_WIDTH],
+                do_center_crop=do_center_crop,
+            )
+            image_processor.crop_size = {
+                HfProcessorParamNames.HEIGHT_KEY: kwargs[
+                    SettingLiterals.IMAGE_HEIGHT
+                ],
+                HfProcessorParamNames.WIDTH_KEY: kwargs[
+                    SettingLiterals.IMAGE_WIDTH
+                ],
+            }
+        elif do_center_crop:
+            image_processor.crop_size = {
+                HfProcessorParamNames.HEIGHT_KEY: kwargs[
+                    SettingLiterals.IMAGE_HEIGHT
+                ],
+                HfProcessorParamNames.WIDTH_KEY: kwargs[
+                    SettingLiterals.IMAGE_WIDTH
+                ],
+            }
+        elif do_resize:
+            image_processor.size = AzmlHfImageFeatureExtractor.update_resize_size_dict(
+                image_processor=image_processor,
+                image_height=kwargs[SettingLiterals.IMAGE_HEIGHT],
+                image_width=kwargs[SettingLiterals.IMAGE_WIDTH],
+                do_center_crop=False,
             )
+        logger.info(
+            f"Updating Feature Extractor with: {image_processor.to_dict()}"
+        )
+        # Todo - if we find a better way, update it.
+        # Get the feature extractor using the updated dict. This way save_pretrained will save the right values.
+        image_processor = AutoImageProcessor.from_pretrained(
+            hf_image_model_name_or_path, **image_processor.to_dict()
+        )
 
         end_time = time.time()
         logger.info(
             f"Feature Extractor loaded for model_name_or_path {hf_image_model_name_or_path} "
             f"in {round(end_time - start_time, 3)} seconds."
         )
-        logger.debug(f"Loaded Feature Extractor : {image_processor.to_dict()}")
+        logger.info(f"Loaded Feature Extractor : {image_processor.to_dict()}")
 
         return image_processor
 
 
 class AzmlHfImageConfig:
     """Get Config based on the model_name or path."""
 
@@ -348,22 +343,29 @@
         config = AutoConfig.from_pretrained(hf_image_model_name_or_path, **kwargs,)
         # Update the image size in the model
         if hasattr(config, HfImageModelConstants.HF_MODEL_CONFIG_IMAGE_SIZE_KEY):
             image_size = getattr(
                 config, HfImageModelConstants.HF_MODEL_CONFIG_IMAGE_SIZE_KEY
             )
             if isinstance(image_size, int):
-                assert (
+                if (
                     kwargs[SettingLiterals.IMAGE_WIDTH]
-                    == kwargs[SettingLiterals.IMAGE_HEIGHT]
-                ), (
-                    f"The model expects same values for image height and width. "
-                    f"Inputs given, height: {kwargs[SettingLiterals.IMAGE_HEIGHT]}, "
-                    f"width: {kwargs[SettingLiterals.IMAGE_WIDTH]}"
-                )
+                    != kwargs[SettingLiterals.IMAGE_HEIGHT]
+                ):
+                    error_string = (
+                        f"The model expects same values for image height and width. "
+                        f"Inputs given, height: {kwargs[SettingLiterals.IMAGE_HEIGHT]}, "
+                        f"width: {kwargs[SettingLiterals.IMAGE_WIDTH]}"
+                    )
+
+                    raise ACFTValidationException._with_error(
+                        AzureMLError.create(
+                            ACFTUserError, pii_safe_message=error_string
+                        )
+                    )
                 image_size_update = {
                     HfImageModelConstants.HF_MODEL_CONFIG_IMAGE_SIZE_KEY: kwargs[
                         SettingLiterals.IMAGE_HEIGHT
                     ]
                 }
             else:
                 assert isinstance(image_size, list) or isinstance(
```

## azureml/acft/image/components/finetune/mmdetection/__init__.py

```diff
@@ -5,14 +5,16 @@
 """AzureML ACFT Image Components package - finetuning component MMDetection."""
 
 import os
 from typing import Union
 
 from azureml._common._error_definition.azureml_error import AzureMLError  # type: ignore
 from azureml.acft.common_components import get_logger_app
+from azureml.acft.common_components.utils.error_handling.error_definitions import TaskNotSupported
+from azureml.acft.common_components.utils.error_handling.exceptions import ACFTValidationException
 
 from azureml.acft.image.components.finetune.factory.mappings import MODEL_FAMILY_CLS
 from azureml.acft.image.components.finetune.factory.task_definitions import Tasks
 from azureml.acft.image.components.finetune.mmdetection.common.trainer_classes import (
     DetectionTrainer,
 )
 
@@ -44,11 +46,10 @@
         """get trainer class based on task_name"""
         if self.task_name in [
             Tasks.MM_OBJECT_DETECTION,
             Tasks.MM_INSTANCE_SEGMENTATION,
         ]:
             return DetectionTrainer
         else:
-            raise NotImplementedError(
-                f"Task not supported. Supported task: "
-                f"{Tasks.MM_OBJECT_DETECTION, Tasks.MM_INSTANCE_SEGMENTATION}"
-            )
+            raise ACFTValidationException._with_error(
+                AzureMLError.create(TaskNotSupported,
+                                    TaskName=self.task_name))
```

## azureml/acft/image/components/finetune/mmdetection/common/data_class.py

```diff
@@ -4,15 +4,19 @@
 """MMDetection object detection data class"""
 
 import numpy as np
 import os
 import torch
 from typing import Callable, Dict, List, Optional, Tuple
 
+from azureml._common._error_definition import AzureMLError
+
 from azureml.acft.common_components import get_logger_app
+from azureml.acft.common_components.utils.error_handling.error_definitions import ACFTUserError
+from azureml.acft.common_components.utils.error_handling.exceptions import ACFTDataException
 from azureml.acft.image.components.finetune.common.augmentation.albumentations_augmentation import (
     AlbumentationsAugmentation as AlbumAugmentations,
 )
 from azureml.acft.image.components.finetune.common.constants.augmentation_constants import (
     AugmentationConfigFileNames,
 )
 from azureml.acft.image.components.finetune.common.constants.constants import (
@@ -25,15 +29,15 @@
 from azureml.acft.image.components.finetune.mmdetection.common.dataset import (
     MmObjectDetectionDataset
 )
 from azureml.acft.image.components.finetune.common.data.runtime_detection_dataset_adapter import (
     get_object_detection_dataset
 )
 from azureml.acft.image.components.finetune.interfaces.azml_interface import AzmlDataInterface
-from azureml.acft.image.components.finetune.runtime_common.common.exceptions import AutoMLVisionDataException
+
 from azureml.acft.image.components.finetune.mmdetection.common.model import DetectionConfigBuilder
 
 logger = get_logger_app(__name__)
 
 
 class AzmlMMDImageDataClass(AzmlDataInterface):
     """Data Class for MMDetection Image Models"""
@@ -192,15 +196,18 @@
             examples: List[Dict[str, Dict]]
         ) -> Dict[str, Dict]:
 
             # Filter out invalid examples
             valid_examples = [example for example in examples if example is not None]
             if len(valid_examples) != len(examples):
                 if len(valid_examples) == 0:
-                    raise AutoMLVisionDataException("All images in the current batch are invalid.")
+                    raise ACFTDataException._with_error(
+                        AzureMLError.create(ACFTUserError,
+                                            pii_safe_message="All images in the current batch are invalid.")
+                    )
                 else:
                     num_invalid_examples = len(examples) - len(valid_examples)
                     logger.info(f"{num_invalid_examples} invalid images found.")
                     logger.info("Replacing invalid images with randomly selected valid images from the current batch")
                     new_example_indices = np.random.choice(np.arange(len(valid_examples)), num_invalid_examples)
                     for ind in new_example_indices:
                         # Padding the batch with valid examples
```

## azureml/acft/image/components/finetune/mmdetection/common/dataset.py

```diff
@@ -26,17 +26,20 @@
 from dataclasses import asdict
 from mmdet.core.mask import BitmapMasks
 from typing import Union, Dict, Any, Callable, Tuple, Optional, List
 
 from azureml.acft.image.components.finetune.common.constants.augmentation_constants import (
     AlbumentationParamNames,
 )
+from azureml.acft.image.components.finetune.common.mlflow.common_constants import (
+    AlbumentationParameterNames
+)
 from azureml.acft.image.components.finetune.common.constants.constants import (
     DetectionDatasetLiterals,
-    ImageDataItemLiterals,
+    ImageDataItemLiterals
 )
 from azureml.acft.image.components.finetune.common.data.runtime_detection_dataset_adapter import (
     RuntimeDetectionDatasetAdapter,
 )
 from azureml.acft.image.components.finetune.mmdetection.common.constants import (
     MmDetectionDatasetLiterals,
 )
@@ -68,15 +71,15 @@
             return None
 
         gtbboxes = training_labels[DetectionDatasetLiterals.BOXES]
         gtlabels = training_labels[DetectionDatasetLiterals.LABELS]
         gtmasks = [mask for mask in training_labels[DetectionDatasetLiterals.MASKS].numpy()] \
             if DetectionDatasetLiterals.MASKS in training_labels else None
         # Convert np.ndarray to List[np.ndarray] since albumentation expects list[np.ndarray]
-        if hasattr(self, 'transform') and self.transform is not None:
+        if hasattr(self, DetectionDatasetLiterals.TRANSFORM) and self.transform is not None:
             transformed = self._apply_transform(
                 # Move channel to last dimension. i.e. (C,H,W) to (H,W,C)
                 image=image.numpy().transpose(1, 2, 0),
                 bboxes=gtbboxes,
                 labels=gtlabels,
                 masks=gtmasks
             )
@@ -84,15 +87,15 @@
             if isinstance(image, np.ndarray):
                 # Move channel to first dimension. i.e. (H,W,C) to (C,H,W)
                 image = torch.from_numpy(image.transpose(2, 0, 1))
 
             gtbboxes = transformed[ImageDataItemLiterals.ALBUMENTATIONS_BBOXES_KEY]
             if not torch.is_tensor(gtbboxes):
                 gtbboxes = torch.as_tensor(gtbboxes, dtype=torch.float32)
-            gtlabels = transformed[AlbumentationParamNames.CLASS_LABELS]
+            gtlabels = transformed[AlbumentationParameterNames.CLASS_LABELS]
             if not torch.is_tensor(gtlabels):
                 gtlabels = torch.as_tensor(gtlabels, dtype=torch.long)
             gtmasks = transformed[DetectionDatasetLiterals.MASKS]
 
         channels, height, width = image.shape
 
         img_metas = ImageMetadata(ori_shape=(height, width, channels),
```

## azureml/acft/image/components/model_selector/component.py

```diff
@@ -1,26 +1,33 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """AzureML ACFT Image Components - model selector component code."""
 import glob
 import json
 import os
-
+import urllib
+import yaml
+from os.path import dirname
 from azureml._common._error_definition.azureml_error import AzureMLError  # type: ignore
-
-from azureml.acft.accelerator.utils.error_handling.error_definitions import LLMInternalError
+from azureml.acft.accelerator.utils.error_handling.error_definitions import (
+    LLMInternalError,
+)
 from azureml.acft.accelerator.utils.error_handling.exceptions import LLMException
-
 from azureml.acft.common_components import get_logger_app
 from azureml.acft.common_components.model_selector.component import ModelSelector
-from azureml.acft.common_components.model_selector.constants import ModelSelectorDefaults, ModelSelectorConstants
-
+from azureml.acft.common_components.model_selector.constants import (
+    ModelSelectorDefaults,
+    ModelSelectorConstants,
+)
 from azureml.acft.image.components.finetune.factory.mappings import MODEL_FAMILY_CLS
-from azureml.acft.image.components.model_selector.constants import ImageModelSelectorConstants
+from azureml.acft.image.components.model_selector.constants import (
+    ImageModelSelectorConstants,
+    MMDetectionModelZooConfigConstants,
+)
 
 
 logger = get_logger_app(__name__)
 
 
 class ImageModelSelector(ModelSelector):
     """Implementation for image model selector."""
@@ -42,22 +49,14 @@
         :param model_family: model family (like HuggingFace, MMDetection), defaults to None
         :type model_family: str, optional
         :param model_name: model name from the framework (i.e., HF), defaults to None
         :type model_name: str, optional
         :param output_dir: path to store arguments and model, defaults to None
         :type output_dir: str, optional
         """
-        # Todo - Remove when we start supporting all the models from MMDetection model zoo.
-        if model_family == MODEL_FAMILY_CLS.MMDETECTION_IMAGE:
-            if pytorch_model is None and mlflow_model is None:
-                raise NotImplementedError(
-                    f"For {model_family}, we are only supporting models, which are registered. "
-                    f"We don't provide support for all models from MMDetection Model Zoo yet."
-                )
-
         super().__init__(
             pytorch_model=pytorch_model,
             mlflow_model=mlflow_model,
             model_name=model_name,
             output_dir=output_dir,
             **kwargs,
         )
@@ -149,43 +148,95 @@
 
     def _prepare_mmlab_arguments_from_model_zoo_config(self) -> dict:
         """Prepared arguments for MMLAB/MMDetection models using the model name as in MMDetection model zoo.
 
         :return: A dictinary conatining argument name to value mapping to update.
         :rtype: dictionary
         """
-        # Todo - add the functionality to prepare from model zoo config
-        raise NotImplementedError(
-            "Supporting from MMDetection model zoo is not implemented yet."
-        )
-        # # Uncomment the below when enabling the support
-        # if self.pytorch_model is None and self.mlflow_model is None and self.model_name is None:
-        #     error_string = (
-        #             f"All, model_name:{self.model_name}, mlflow_model: {self.mlflow_model},"
-        #             f" pytorch_model: {self.pytorch_model} can not be None together."
-        #             f"Please provide either a model via pytorch_model or mlflow_model port; Or, "
-        #             f"provide name of the model from MMDetcetion model zoo, as specified in respective model"
-        #             " model family's metafile.yaml."
-        #         )
-        #     raise LLMException._with_error(
-        #         AzureMLError.create(LLMInternalError, error=error_string)
-        #     )
+        if (
+            self.pytorch_model is None
+            and self.mlflow_model is None
+            and self.model_name is None
+        ):
+            error_string = (
+                "All, model_name, mlflow_model, pytorch_model can not be None at the same time."
+                "Please provide either a model via pytorch_model or mlflow_model port; Or, "
+                "provide name of the model from MMDetection model zoo, as specified in respective model"
+                "family's metafile.yaml."
+            )
+            raise LLMException._with_error(
+                AzureMLError.create(LLMInternalError, error=error_string)
+            )
+
+        meta_file = ImageModelSelector._search_model_name_in_mmd_model_zoo(self.model_name)
+        model_data = None
+        if meta_file is not None:
+            # read yml file and get the model data
+            with open(meta_file, "r") as f:
+                data = yaml.safe_load(f)
+                for model in data[MMDetectionModelZooConfigConstants.MODEL_ZOO_MODELS]:
+                    if self.model_name in model[MMDetectionModelZooConfigConstants.MODEL_ZOO_MODEL_NAME]:
+                        model_data = model
+                        break
+        else:
+            error_string = f"Not able to find the meta file {meta_file}."
+            raise LLMException._with_error(
+                AzureMLError.create(LLMInternalError, error=error_string)
+            )
+
+        if model_data is None:
+            error_string = f"Ensure that {self.model_name} data exists in the meta file."
+            raise LLMException._with_error(
+                AzureMLError.create(LLMInternalError, error=error_string)
+            )
+        abs_config_folder_path = ImageModelSelector._get_mmdet_config_path()
+        abs_mmlab_config_path = os.path.join(
+            dirname(abs_config_folder_path),
+            model_data[MMDetectionModelZooConfigConstants.MODEL_ZOO_CONFIG],
+        )
+
+        if not os.path.exists(abs_mmlab_config_path):
+            error_string = f"Ensure that {self.model_name}.py exists in the model zoo configs folder."
+            raise LLMException._with_error(
+                AzureMLError.create(LLMInternalError, error=error_string)
+            )
+
+        # Get the model weight file path
+        url = model_data[MMDetectionModelZooConfigConstants.MODEL_ZOO_WEIGHTS]
+
+        weights_file_name = self.model_name + "_weights.pth"
+
+        weights_path = os.path.join(self.output_dir, weights_file_name)
+
+        # download the file
+        urllib.request.urlretrieve(
+            url,
+            weights_path,
+        )
+
+        self.pytorch_model = abs_mmlab_config_path
+
+        return {
+            ModelSelectorConstants.MODEL_NAME: self.model_name,
+            ModelSelectorConstants.MLFLOW_MODEL_PATH: self.mlflow_model,
+            ModelSelectorConstants.PYTORCH_MODEL_PATH: self.pytorch_model,
+            ImageModelSelectorConstants.MMLAB_MODEL_WEIGHTS_PATH_OR_URL: weights_file_name,
+        }
 
     def _prepare_mmlab_arguments(self) -> dict:
         """Prepare an update for the keyword arguments (if present) with required key-val items for MMLab/MMDetection
         models.
 
         :return: A dictinary conatining argument name to value mapping to update.
         :rtype: dictionary
         """
 
         if self.pytorch_model is not None or self.mlflow_model is not None:
             return self._prepare_mmlab_arguments_from_input_model()
         else:
-            # Todo - add the support for reading from model zoo config.
             return self._prepare_mmlab_arguments_from_model_zoo_config()
 
     def _prepare_and_logs_arguments(self) -> None:
         """Update the keyword arguments (if present) with required key-val items and
         Store the model selector arguments to json file.
         """
         arguments = {
@@ -208,7 +259,36 @@
             self.output_dir, ModelSelectorDefaults.MODEL_SELECTOR_ARGS_SAVE_PATH
         )
         logger.info(
             f"Saving the model selector args to {model_selector_args_save_path}"
         )
         with open(model_selector_args_save_path, "w") as output_file:
             json.dump(self.keyword_arguments, output_file, indent=2)
+
+    @staticmethod
+    def _search_model_name_in_mmd_model_zoo(model_name):
+        """
+        Search for model name in all the metafile.yaml files present in model zoo configs folder
+        """
+        for dirpath, _, filenames in os.walk(ImageModelSelector._get_mmdet_config_path()):
+            for file_name in filenames:
+                file_path = os.path.abspath(os.path.join(dirpath, file_name))
+                if file_path.endswith("metafile.yml"):
+                    with open(file_path, "r") as f:
+                        if model_name in f.read():
+                            return file_path
+        error_string = f"Model {model_name} was not found in the metafile.yml files of the model zoo configs folder."
+        raise LLMException._with_error(
+            AzureMLError.create(LLMInternalError, error=error_string)
+        )
+
+    @staticmethod
+    def _get_mmdet_config_path() -> str:
+        """Get the path to the MMDetection config file.
+
+        :return: Path to the MMDetection config file.
+        :rtype: str
+        """
+        import mmdet
+        # Note: mmdet should be installed via mim to access the model zoo config folder.
+        CONFIG_FOLDER_PATH = os.path.join(mmdet.__path__[0], ".mim", "configs")
+        return CONFIG_FOLDER_PATH
```

## azureml/acft/image/components/model_selector/constants.py

```diff
@@ -1,13 +1,25 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """Constants used for image model selector component."""
+import os
 
 
 class ImageModelSelectorConstants:
     """String constants for model selector component."""
 
     MODEL_FAMILY = "model_family"
 
     MMLAB_MODEL_WEIGHTS_PATH_OR_URL = "model_weights_path_or_url"
     MMLAB_MODEL_METAFILE_NAME = "metafile.json"
+
+
+class MMDetectionModelZooConfigConstants:
+    """
+    Constants for MMDetection model zoo config.
+    """
+
+    MODEL_ZOO_MODELS = "Models"
+    MODEL_ZOO_MODEL_NAME = "Name"
+    MODEL_ZOO_CONFIG = "Config"
+    MODEL_ZOO_WEIGHTS = "Weights"
```

## Comparing `azureml/acft/image/components/finetune/common/augmentation/custom_augmentations.py` & `azureml/acft/image/components/finetune/common/mlflow/custom_augmentations.py`

 * *Files identical despite different names*

## Comparing `azureml_acft_image_components-0.0.6.dist-info/LICENSE.txt` & `azureml_acft_image_components-0.0.6.post1.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_acft_image_components-0.0.6.dist-info/METADATA` & `azureml_acft_image_components-0.0.6.post1.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-acft-image-components
-Version: 0.0.6
+Version: 0.0.6.post1
 Summary: Contains the code for vision model's components.
 Home-page: https://msdata.visualstudio.com/Vienna/_git/sdk-cli-v2?version=GBtraining_finetune
 Author: Microsoft Corp
 License: inline_license
 Platform: UNKNOWN
 Classifier: Development Status :: 2 - Pre-Alpha
 Classifier: Intended Audience :: Developers
```

## Comparing `azureml_acft_image_components-0.0.6.dist-info/RECORD` & `azureml_acft_image_components-0.0.6.post1.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,77 +1,78 @@
 azureml/__init__.py,sha256=n0xtZ3iWcoVg5Qognsb7InYAUVAK8s3iaVeHB5GOaNA,251
 azureml/acft/__init__.py,sha256=qqupVla6VEo6r82whlPnifStK1J2C4O_mvyitUj6Gtk,295
 azureml/acft/image/__init__.py,sha256=WGGwHkYOREBwKsKBoq01dly26dZbULYG-kEjJllgnD4,822
 azureml/acft/image/components/NOTICE.txt,sha256=EQf8zdNpXgRtMJBAPCxdMMmqBLxxbRMbufcg3O2Yi_s,600282
 azureml/acft/image/components/__init__.py,sha256=ggjdDitC6cCpqW_xEnB1ruomGpvAjlAIfzJ7H0rbyB8,297
-azureml/acft/image/components/_version.py,sha256=zEK9qNwSOrrnJnVdRoBiTn9QiUMB122Z8Pf9WuceS2M,34
+azureml/acft/image/components/_version.py,sha256=uh0VWzHdEQl7gw9VpWtMeegnzWKSHzOpEVYk20d7uEs,40
 azureml/acft/image/components/common/__init__.py,sha256=67NkDhf7k9I_TLGAq1Bc3h3_EHkorbP9cthI9SlxOZk,312
 azureml/acft/image/components/common/utils.py,sha256=DAhC43RDSFpU4t90x4BxlFlz-lXh6IstJnuKM8IFcDM,583
 azureml/acft/image/components/finetune/__init__.py,sha256=zdA-QvDl7dTfTAapxHHMdHstIVbe-2WdgP7HPmhugMY,320
-azureml/acft/image/components/finetune/finetune_runner.py,sha256=tceSIGenrRw9k8FZ1dWqTHNe_61WyFmtSGta1ZiX4NE,7891
+azureml/acft/image/components/finetune/finetune_runner.py,sha256=TazZ-IVuaecoFlVMxX2Iru4xneExXsei1m9GHYDkwAY,7953
 azureml/acft/image/components/finetune/common/__init__.py,sha256=ApiQpNyjcUthAQZPkkoK6-WHW7iMjbQ0H2ijqHnsikU,466
-azureml/acft/image/components/finetune/common/utils.py,sha256=VXJTIaaFUDYEY60B5FaW1dgy0Pv35HfDpmUvEYH89hY,893
+azureml/acft/image/components/finetune/common/utils.py,sha256=1sWQMassg4tJ-HXTXz7MGB38oOrHVp-nPOoVVIt2UoM,1052
 azureml/acft/image/components/finetune/common/augmentation/__init__.py,sha256=XzmI_8D7RLFYFYhIHWlQEzNoidm6vueDIhVYs23Ltk4,313
-azureml/acft/image/components/finetune/common/augmentation/albumentations_augmentation.py,sha256=GXkOzYP2sVikWbIujvENr2U8MZDf1zxn6aSpBC7UZRA,9017
-azureml/acft/image/components/finetune/common/augmentation/augmentation_config_utils.py,sha256=1_w6-did9pkB94BLBwh8r-elfFqNRmevy0yOXgS9h_s,17791
+azureml/acft/image/components/finetune/common/augmentation/albumentations_augmentation.py,sha256=adVNzRTXCQWJCqk_W0xkYEkFmQS16lbOHRRjKOUhuLQ,6267
+azureml/acft/image/components/finetune/common/augmentation/augmentation_config_utils.py,sha256=s09P7WtXdeOZNXlqZ6AFhSATZ6LLz44GTwnmcXhda5Q,14648
 azureml/acft/image/components/finetune/common/augmentation/base_augmentation.py,sha256=HAsPVKQzoVn0CIWYycpUtWy6-wxXe7LE1rhQ58f1QgA,3152
-azureml/acft/image/components/finetune/common/augmentation/custom_augmentations.py,sha256=7ZEV4x8Bx4mALttHNUPENuYfA2GIkaQNm0m3ogBA1K4,10881
-azureml/acft/image/components/finetune/common/augmentation/model_preproc_extractor.py,sha256=n6YGscC2xDdfZF80tt9PES1PwYly8x4BTpPeXUt-hJM,26914
+azureml/acft/image/components/finetune/common/augmentation/model_preproc_extractor.py,sha256=JbvuvwIPC6KCeRR9qnT8Y3mb-2nrPNzpZ0h2BX6XjiE,27652
 azureml/acft/image/components/finetune/common/augmentation/configs/__init__.py,sha256=JG43IYmXg2fXS1FNoebLt63IkV2hypGITnXUcz8dfCc,320
 azureml/acft/image/components/finetune/common/augmentation/configs/albumentations_classification.yaml,sha256=4Uo-vR8XQSNcMO2C3Eaj-vMfa7JuJ0Zlx9VyTBdqtFM,1929
 azureml/acft/image/components/finetune/common/augmentation/configs/albumentations_od_is.yaml,sha256=Iys2djRrJmPVpJBE_CZvHp17s_GSRubScfSNxlx_WtM,2474
 azureml/acft/image/components/finetune/common/augmentation/configs/hf_albumentations_classification.yaml,sha256=Ai-zj88CJDxFR1rj9OkdoI5yx8nMH46dIsp57e7etiY,1314
 azureml/acft/image/components/finetune/common/constants/__init__.py,sha256=c9AETSCQhjx9UqOeXZvZ6DVap4R3qW-FxMJ39HCsO_8,309
-azureml/acft/image/components/finetune/common/constants/augmentation_constants.py,sha256=BPFD2vkpmw4hcNHoG-O-G2V0iLPqR8BOKXf5zfH7oko,1817
-azureml/acft/image/components/finetune/common/constants/constants.py,sha256=AWgBOoJUAxeJdpENzSCVVCF5lSyFM4NwhzGIHREMq7g,4475
+azureml/acft/image/components/finetune/common/constants/augmentation_constants.py,sha256=E1dGL46EIvm9S7JgNeHZ3l4HKYxrYrNcmGoSDJx3O4s,1550
+azureml/acft/image/components/finetune/common/constants/constants.py,sha256=Up87BBKjhBtHTjjCT1tEbDqLG23d3JUVwPcJ1tdfxrk,4536
 azureml/acft/image/components/finetune/common/data/__init__.py,sha256=Iqha0pV_7OWQ1pBcLUeBZW-Yo_GGxSzRRUvjZsW27Hw,304
-azureml/acft/image/components/finetune/common/data/base_dataset.py,sha256=q3X3Nf2chT-DMjalw2G5MW1rYM6b6PtDd2UDrAzdUNo,4848
-azureml/acft/image/components/finetune/common/data/classification_dataset.py,sha256=tv1-MtQCfc7V9v4-JQDd0r48pfRmpbCrai6zatlTlxA,8655
+azureml/acft/image/components/finetune/common/data/base_dataset.py,sha256=DnvZM-dCQL6coxbE1blL-bsSnNaDPJUE2-YBILyO0_Y,5216
+azureml/acft/image/components/finetune/common/data/classification_dataset.py,sha256=DRPXxk3rAu3LiBLxHgPu8qhfTZyL8NlALiFPIh__fzw,9513
 azureml/acft/image/components/finetune/common/data/classification_hf_dataset.py,sha256=lKqSOy60LPU9dbvv2woj0xZxhsoQ5seoPwt-n3uVtCk,5469
-azureml/acft/image/components/finetune/common/data/download_manager.py,sha256=BEBr75BRTmx_u_xK24PEjSTmdrdqWxiDpPDUOqXY2ZM,10245
+azureml/acft/image/components/finetune/common/data/download_manager.py,sha256=0-rYQlv_q_xNRpCGI3oPuZHnzall0LIZLaJexDh3x-M,10779
 azureml/acft/image/components/finetune/common/data/runtime_detection_dataset_adapter.py,sha256=Lb2HkNBG2h-K6Q8yps7OS-SXv9zp0ApLPjz1asayIgU,7569
 azureml/acft/image/components/finetune/common/mlflow/__init__.py,sha256=D1Fxlq34-Arr84Ey1nucvWPYmkjmcrelBuwOvQVDSF4,338
-azureml/acft/image/components/finetune/common/mlflow/common_constants.py,sha256=f0VhSs-aTf2kRpsCbTLvyjr5JB221lNzEix3yGWR9qc,2333
-azureml/acft/image/components/finetune/common/mlflow/common_utils.py,sha256=Vxw5iYi0582jhU0pr2t-S6RClxofBDFg5uIshbmB0OI,6225
+azureml/acft/image/components/finetune/common/mlflow/augmentation_helper.py,sha256=RnCBSSm-baboCsYEOn_mfaes_69huqflJkW2fmbyCsc,8766
+azureml/acft/image/components/finetune/common/mlflow/common_constants.py,sha256=ICFakv9GT_MYFU8rsnwlJG6vRSb-6aUQnMtCp08_SOY,3095
+azureml/acft/image/components/finetune/common/mlflow/common_utils.py,sha256=RfcRo-MdCJj41cvmNj6RhtE1Y4j5jGbM8dQP0koxwD0,6644
+azureml/acft/image/components/finetune/common/mlflow/custom_augmentations.py,sha256=7ZEV4x8Bx4mALttHNUPENuYfA2GIkaQNm0m3ogBA1K4,10881
 azureml/acft/image/components/finetune/common/mlflow/hf_test_predict.py,sha256=8s4d99ltap3BkArCfdQDQfGgJASc72cv7NdLSmrdI3U,6194
 azureml/acft/image/components/finetune/common/mlflow/hf_utils.py,sha256=E729Bkb19eO2tYLquLvK2zyJL6NMLs7ROS15a8jzntA,3222
-azureml/acft/image/components/finetune/common/mlflow/mmdet-requirements.txt,sha256=JPr_e3mLhLRuHzjkAy0If-NAdMvkkwPla5-zMyXV70U,102
-azureml/acft/image/components/finetune/common/mlflow/mmdet_mlflow_model_wrapper.py,sha256=BfSLSbMd_oyomlSW3BHa1JfLBdLaeayzFPh_9haEBDs,5123
+azureml/acft/image/components/finetune/common/mlflow/mmdet-requirements.txt,sha256=tZ15cmof2eQEB9M74a2c7Gn0MRFiATxaHGreraYBMLw,125
+azureml/acft/image/components/finetune/common/mlflow/mmdet_mlflow_model_wrapper.py,sha256=78gifMXvEETil-zsU_rq_NJ-CqFn5495OL2Qpc3yFFg,6153
 azureml/acft/image/components/finetune/common/mlflow/mmdet_modules.py,sha256=zBThexJ4WoabbirsbE-PADMOzXSNouJIgM09nfyaaoQ,6775
-azureml/acft/image/components/finetune/common/mlflow/mmdet_utils.py,sha256=WXGW17R2Fu0LfzVN0dKL-Y3NE-_biuMEP3i9cWilmQY,8293
+azureml/acft/image/components/finetune/common/mlflow/mmdet_utils.py,sha256=BH3Gr2P_fVyl_bGF_rkv15GKXWzqmCXc-__aTLDQNCE,9358
 azureml/acft/image/components/finetune/defaults/__init__.py,sha256=lwuuzJJAKaTscSfrN5IK4YYV45l1Nllq1MM7CUkGx3M,329
 azureml/acft/image/components/finetune/defaults/classification_models_defaults.py,sha256=NoyWIeXGqCRGkKmdySOQWHlGkZZRWf4W9TTOszDIlHQ,3091
 azureml/acft/image/components/finetune/defaults/constants.py,sha256=M2MihYAofHs9k6NA3n0tzqs-2xxds4uZ-AIBDbl_Pqk,3507
 azureml/acft/image/components/finetune/defaults/hf_trainer_defaults.py,sha256=yOUlcDsdnLmbmm3LPTwltT38SQrMdrrOvg_ZWVenjZs,1430
 azureml/acft/image/components/finetune/defaults/instance_segmentation_models_defaults.py,sha256=hgPHsBVbRxUZJzqUEK0yBujO-Obg7goqez6bcDv1pyM,823
 azureml/acft/image/components/finetune/defaults/object_detection_models_defaults.py,sha256=a8Halx2uj6JZuGQWDMlmEBkyTlpg6cfr_WoKyhosITE,808
 azureml/acft/image/components/finetune/defaults/task_defaults.py,sha256=2VEBUYVE8nalnRt6QjaJ3UqanxHhcP7DaM59kIe1IpM,2021
 azureml/acft/image/components/finetune/defaults/training_defaults.py,sha256=oXyJaeeTg6hMpV50exL6veIMRQTGDqqpsUApP0sBYVU,7761
 azureml/acft/image/components/finetune/factory/__init__.py,sha256=Ht92Ed5qLyCM4oyG7VsOAIDo-KhSxtXzeqJzFCnL8vw,333
 azureml/acft/image/components/finetune/factory/mappings.py,sha256=StW_tjTrsjhHo3cmlQNqQaEIT7iOaehAQ0HTgPAWUfk,764
-azureml/acft/image/components/finetune/factory/model_factory.py,sha256=pGSfP1gatSe_-3U9Lsos9QX09wwztZdD9wCjKoaaH2k,1750
+azureml/acft/image/components/finetune/factory/model_factory.py,sha256=vDw01IgHjV5a8z3gAProiSVkpeLQuBfvJYd5Hs3iuaI,2253
 azureml/acft/image/components/finetune/factory/task_definitions.py,sha256=xrpwIttCpexwQeUeq1PNURJcyKsiFtv5rqJmSK_7iPw,548
-azureml/acft/image/components/finetune/huggingface/__init__.py,sha256=GrtPf5ARDc92_vTARr_msNP4sH6sVKj4RVjgVrGzgrg,5988
+azureml/acft/image/components/finetune/huggingface/__init__.py,sha256=iBl14XpvoQpEuXuLYgoel3QoEbV_XCJUyVE6RodoKzk,5944
 azureml/acft/image/components/finetune/huggingface/classification/__init__.py,sha256=zn62FtSZn05t8m69tj0WxP6XVYqKjv6ZGkbCzEkWuh0,347
 azureml/acft/image/components/finetune/huggingface/classification/data_cls.py,sha256=byqp0MPlHi9_kmWzOA2nzmz3EwfHGTPQgvuN7MjZJMc,8922
 azureml/acft/image/components/finetune/huggingface/classification/finetune_cls.py,sha256=rNhwvyUb5H9PdxsqDT2uJKgmeEY5V51vAJXbuNwa3OE,2367
-azureml/acft/image/components/finetune/huggingface/classification/inference_cls.py,sha256=acKbeR8zn4h12RH1V-_jIfzl9Tu_LQNQP10DXJW5EUg,5355
+azureml/acft/image/components/finetune/huggingface/classification/inference_cls.py,sha256=AwfXL1apBCtLRB_HxQcvEypgzbzt5qAjbSzk5xfumDE,5178
 azureml/acft/image/components/finetune/huggingface/classification/trainer_classes.py,sha256=bTnSEyJhXU9XJgLMKKieMbyNb99qbcZx490aE524Cxg,1706
 azureml/acft/image/components/finetune/huggingface/common/__init__.py,sha256=dV1ARKal1bfCgWCrBdxsbYnWi-P93vInIXCkw32IzWY,339
 azureml/acft/image/components/finetune/huggingface/common/constants.py,sha256=P2JWfLsSPUXenUOlhMoMtrCVAgKwvPSElMGfUpjTK8g,951
-azureml/acft/image/components/finetune/huggingface/common/hf_image_interfaces.py,sha256=C5jzPqGauy_tiVpSl2NjyGIgEU-lGRktkbn5VMYQGwU,18975
+azureml/acft/image/components/finetune/huggingface/common/hf_image_interfaces.py,sha256=Z1cV27VSvtDPBzlm6CklLhAQCL6yyEPqP_FTsRI1VbI,18943
 azureml/acft/image/components/finetune/huggingface/mlflow/__init__.py,sha256=DJU_AOCFlm8r_D-7nloUhNg2WjRwdextrnprtKLFbzo,350
 azureml/acft/image/components/finetune/huggingface/mlflow/hf_test_predict.py,sha256=b70-zfc7uxbYVpaPD8KkmsofqCRM4CaTwV1HAX14p28,8760
 azureml/acft/image/components/finetune/interfaces/__init__.py,sha256=u68Sh6HeK2tX3EWq4aLftcM84RH5fJC0I7h5mGJOsm4,265
 azureml/acft/image/components/finetune/interfaces/azml_interface.py,sha256=-N8_ryynhz94xmdTTmHLLhU1Aht35PhyYnLlmNwB3nA,10706
-azureml/acft/image/components/finetune/mmdetection/__init__.py,sha256=RIRGo3xcLsuILpMJos88iLapK4m3bRg5lLutOvGPQY4,2145
+azureml/acft/image/components/finetune/mmdetection/__init__.py,sha256=GspVcHM5uoSi1hjTkxnqsAFzoqZfxxxP-y-5xxPMuyI,2327
 azureml/acft/image/components/finetune/mmdetection/common/__init__.py,sha256=2hE4bBrLwd8UgBHayiYE1IfNgiGL7T-e-bC4fGDT4Qk,249
 azureml/acft/image/components/finetune/mmdetection/common/constants.py,sha256=GUZlQPJZ9cduoJ_7GDdKBObQqH3IMBKZHjofKu-yjNg,759
-azureml/acft/image/components/finetune/mmdetection/common/data_class.py,sha256=z31lX7X2vK5U9XqqXv2ioike_QpmDuCDKfXq8J9wnHU,9959
-azureml/acft/image/components/finetune/mmdetection/common/dataset.py,sha256=-1M6omUmxq_EP357inCk_P6DptQDOlVf-I5Ro9p2FGs,8592
+azureml/acft/image/components/finetune/mmdetection/common/data_class.py,sha256=u_tcYFHfIXY-Zjh7V5ML9n1FMMFEyQnbBGk-95R1Psw,10253
+azureml/acft/image/components/finetune/mmdetection/common/dataset.py,sha256=1C-j_Zejv0B4f2Ys_CwIdjGVB06MLwAkA7u2G8Kga3s,8739
 azureml/acft/image/components/finetune/mmdetection/common/image_metadata.py,sha256=tKMsva7j7wtvsUa--ZIUEfvgQLAKsxn8wTp7-AMKAYU,1108
 azureml/acft/image/components/finetune/mmdetection/common/inference.py,sha256=_tgfaxwkaTNbQuGOQaGdJpTN8CSLlHLujGaKaD06H6k,3015
 azureml/acft/image/components/finetune/mmdetection/common/metrics.py,sha256=kVWtuEbk_ggSxocoCjlhUw2IOKFeJPNWjyOPkQwMuEs,10173
 azureml/acft/image/components/finetune/mmdetection/common/model.py,sha256=jk1W41dnOn8_-beYJcryJpZpDEgHRPLbSgCz43ntKo0,7234
 azureml/acft/image/components/finetune/mmdetection/common/trainer_arguments.py,sha256=u8hgY4HMuN4dDUdnaJMk9ow_okoki3_wxgwwg7ok0dk,1832
 azureml/acft/image/components/finetune/mmdetection/common/trainer_classes.py,sha256=PG5Dn91CGMFPWXde1PM8QumhDy_o6Y25wjLY7AOTBs4,1784
 azureml/acft/image/components/finetune/mmdetection/instance_segmentation/__init__.py,sha256=71dEttL5_De4UI9esPQaS80moyIqM5sZQKf8AqC6rmE,264
@@ -122,14 +123,14 @@
 azureml/acft/image/components/finetune/runtime_common/object_detection/data/dataset_wrappers.py,sha256=5YZoijCmcPNyMjJOx20Ve6kMIpsj54LhzVMl8I282Pc,5012
 azureml/acft/image/components/finetune/runtime_common/object_detection/data/datasets.py,sha256=3d-bpF0_9jLVikKWPIOsL4qe8Wtcr4iO0xqm5YZdgpA,39886
 azureml/acft/image/components/finetune/runtime_common/object_detection/data/loaders.py,sha256=E_NpFNadRMgamB03QH6JL1UVn2wMWIrQktB3JEn0NC4,6166
 azureml/acft/image/components/finetune/runtime_common/object_detection/data/object_annotation.py,sha256=xjghhlGVQrGkF1WskhNGYy2FI-CrGkN9nRj7r5LeEQo,11905
 azureml/acft/image/components/finetune/runtime_common/object_detection/data/tiling_distributed_sampler.py,sha256=julAbXivQhaws1KRxSlbV-Lk2mb3GLCCo1rdxsAykug,5041
 azureml/acft/image/components/finetune/runtime_common/object_detection/data/utils.py,sha256=ExAvxTBzwlYpCcoRSGIvMpYD_Fkc03etDyUkBDhhqx8,11382
 azureml/acft/image/components/model_selector/__init__.py,sha256=Beo_K6DsSl7BGWXFdQGv9B8qo9oY-qWKJmNJvmsvZpo,321
-azureml/acft/image/components/model_selector/component.py,sha256=rKGZ7iYEDUrV6vSp7MJpdRa07N0Jv_C1znDtLh4IFpU,10217
-azureml/acft/image/components/model_selector/constants.py,sha256=_oIrIGKFmS9ZHb8upSv3sxBQ2htkWtLie2rapvBaXRs,494
-azureml_acft_image_components-0.0.6.dist-info/LICENSE.txt,sha256=GBoIyZ-6vJ4xjRc8U3wTw4EfkuaEdVTm_gbr1Nm8uDI,859
-azureml_acft_image_components-0.0.6.dist-info/METADATA,sha256=Ql-sLCPttjYr4jo9tyf0TKCy0PXcBZPzSn9aMSko5lk,1485
-azureml_acft_image_components-0.0.6.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_acft_image_components-0.0.6.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
-azureml_acft_image_components-0.0.6.dist-info/RECORD,,
+azureml/acft/image/components/model_selector/component.py,sha256=DpbzmYgqawThvYxrhM8-wmq5_FocAGvrbkl8We_G1xA,13149
+azureml/acft/image/components/model_selector/constants.py,sha256=BNRbjiH70C4UbLmBOp_dfC1Cgxo-fQU_UFcNr-bM8fw,757
+azureml_acft_image_components-0.0.6.post1.dist-info/LICENSE.txt,sha256=GBoIyZ-6vJ4xjRc8U3wTw4EfkuaEdVTm_gbr1Nm8uDI,859
+azureml_acft_image_components-0.0.6.post1.dist-info/METADATA,sha256=i-F_g105bhYNsZg2d359XFIfOjbLersfMja9jeWjmu8,1491
+azureml_acft_image_components-0.0.6.post1.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_acft_image_components-0.0.6.post1.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
+azureml_acft_image_components-0.0.6.post1.dist-info/RECORD,,
```

