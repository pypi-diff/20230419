# Comparing `tmp/azureml_metrics-0.0.6-py3-none-any.whl.zip` & `tmp/azureml_metrics-0.0.6.post1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,47 +1,48 @@
-Zip file size: 147706 bytes, number of entries: 45
--rw-rw-rw-  2.0 fat      267 b- defN 23-Apr-10 23:37 azureml/__init__.py
--rw-rw-rw-  2.0 fat   260065 b- defN 23-Apr-10 23:37 azureml/metrics/NOTICE
--rw-rw-rw-  2.0 fat      805 b- defN 23-Apr-10 23:37 azureml/metrics/__init__.py
--rw-rw-rw-  2.0 fat    49711 b- defN 23-Apr-10 23:37 azureml/metrics/_classification.py
--rw-rw-rw-  2.0 fat     7829 b- defN 23-Apr-10 23:37 azureml/metrics/_dataset_binning.py
--rw-rw-rw-  2.0 fat    24070 b- defN 23-Apr-10 23:37 azureml/metrics/_forecasting.py
--rw-rw-rw-  2.0 fat     5034 b- defN 23-Apr-10 23:37 azureml/metrics/_metric_base.py
--rw-rw-rw-  2.0 fat    25843 b- defN 23-Apr-10 23:37 azureml/metrics/_regression.py
--rw-rw-rw-  2.0 fat    27931 b- defN 23-Apr-10 23:37 azureml/metrics/_score.py
--rw-rw-rw-  2.0 fat    32735 b- defN 23-Apr-10 23:37 azureml/metrics/_scoring.py
--rw-rw-rw-  2.0 fat    23940 b- defN 23-Apr-10 23:37 azureml/metrics/_scoring_confidence.py
--rw-rw-rw-  2.0 fat    34427 b- defN 23-Apr-10 23:37 azureml/metrics/_scoring_utilities.py
--rw-rw-rw-  2.0 fat     2213 b- defN 23-Apr-10 23:37 azureml/metrics/_seq2seq_fill_mask.py
--rw-rw-rw-  2.0 fat     5135 b- defN 23-Apr-10 23:37 azureml/metrics/_seq2seq_qa.py
--rw-rw-rw-  2.0 fat     2137 b- defN 23-Apr-10 23:37 azureml/metrics/_seq2seq_summarization.py
--rw-rw-rw-  2.0 fat     1968 b- defN 23-Apr-10 23:37 azureml/metrics/_seq2seq_translation.py
--rw-rw-rw-  2.0 fat     5023 b- defN 23-Apr-10 23:37 azureml/metrics/_token_classification.py
--rw-rw-rw-  2.0 fat    44604 b- defN 23-Apr-10 23:37 azureml/metrics/_validation.py
--rw-rw-rw-  2.0 fat       34 b- defN 23-Apr-10 23:44 azureml/metrics/_version.py
--rw-rw-rw-  2.0 fat    14181 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_classification_metrics.py
--rw-rw-rw-  2.0 fat     3975 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_fill_mask_metrics.py
--rw-rw-rw-  2.0 fat    15561 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_forecasting_metrics.py
--rw-rw-rw-  2.0 fat     6621 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_metrics.py
--rw-rw-rw-  2.0 fat     4523 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_qa_metrics.py
--rw-rw-rw-  2.0 fat     9157 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_regression_metrics.py
--rw-rw-rw-  2.0 fat     3858 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_summarization_metrics.py
--rw-rw-rw-  2.0 fat     7329 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_text_classification_metrics.py
--rw-rw-rw-  2.0 fat     3985 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_text_generation_metrics.py
--rw-rw-rw-  2.0 fat     4200 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_text_ner_metrics.py
--rw-rw-rw-  2.0 fat     3646 b- defN 23-Apr-10 23:37 azureml/metrics/azureml_translation_metrics.py
--rw-rw-rw-  2.0 fat     4601 b- defN 23-Apr-10 23:37 azureml/metrics/bleu.py
--rw-rw-rw-  2.0 fat    33784 b- defN 23-Apr-10 23:37 azureml/metrics/constants.py
--rw-rw-rw-  2.0 fat     4820 b- defN 23-Apr-10 23:37 azureml/metrics/contract.py
--rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-10 23:37 azureml/metrics/exceptions.py
--rw-rw-rw-  2.0 fat    47616 b- defN 23-Apr-10 23:37 azureml/metrics/reference_codes.py
--rw-rw-rw-  2.0 fat     6424 b- defN 23-Apr-10 23:37 azureml/metrics/scoring.py
--rw-rw-rw-  2.0 fat    13269 b- defN 23-Apr-10 23:37 azureml/metrics/utilities.py
--rw-rw-rw-  2.0 fat      239 b- defN 23-Apr-10 23:37 azureml/metrics/od_is_eval/__init__.py
--rw-rw-rw-  2.0 fat    21196 b- defN 23-Apr-10 23:37 azureml/metrics/od_is_eval/incremental_voc_evaluator.py
--rw-rw-rw-  2.0 fat    17250 b- defN 23-Apr-10 23:37 azureml/metrics/od_is_eval/metric_computation_utils.py
--rw-rw-rw-  2.0 fat      859 b- defN 23-Apr-10 23:44 azureml_metrics-0.0.6.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     1800 b- defN 23-Apr-10 23:44 azureml_metrics-0.0.6.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-Apr-10 23:44 azureml_metrics-0.0.6.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-Apr-10 23:44 azureml_metrics-0.0.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     4158 b- defN 23-Apr-10 23:44 azureml_metrics-0.0.6.dist-info/RECORD
-45 files, 793552 bytes uncompressed, 140982 bytes compressed:  82.2%
+Zip file size: 151244 bytes, number of entries: 46
+-rw-rw-rw-  2.0 fat      267 b- defN 23-Apr-19 08:56 azureml/__init__.py
+-rw-rw-rw-  2.0 fat   260065 b- defN 23-Apr-19 08:57 azureml/metrics/NOTICE
+-rw-rw-rw-  2.0 fat      805 b- defN 23-Apr-19 08:57 azureml/metrics/__init__.py
+-rw-rw-rw-  2.0 fat    49711 b- defN 23-Apr-19 08:57 azureml/metrics/_classification.py
+-rw-rw-rw-  2.0 fat     7829 b- defN 23-Apr-19 08:57 azureml/metrics/_dataset_binning.py
+-rw-rw-rw-  2.0 fat    24070 b- defN 23-Apr-19 08:57 azureml/metrics/_forecasting.py
+-rw-rw-rw-  2.0 fat     5034 b- defN 23-Apr-19 08:57 azureml/metrics/_metric_base.py
+-rw-rw-rw-  2.0 fat    25843 b- defN 23-Apr-19 08:57 azureml/metrics/_regression.py
+-rw-rw-rw-  2.0 fat    32574 b- defN 23-Apr-19 08:57 azureml/metrics/_score.py
+-rw-rw-rw-  2.0 fat    32735 b- defN 23-Apr-19 08:57 azureml/metrics/_scoring.py
+-rw-rw-rw-  2.0 fat    23940 b- defN 23-Apr-19 08:57 azureml/metrics/_scoring_confidence.py
+-rw-rw-rw-  2.0 fat    34427 b- defN 23-Apr-19 08:57 azureml/metrics/_scoring_utilities.py
+-rw-rw-rw-  2.0 fat     2213 b- defN 23-Apr-19 08:57 azureml/metrics/_seq2seq_fill_mask.py
+-rw-rw-rw-  2.0 fat     5135 b- defN 23-Apr-19 08:57 azureml/metrics/_seq2seq_qa.py
+-rw-rw-rw-  2.0 fat     2137 b- defN 23-Apr-19 08:57 azureml/metrics/_seq2seq_summarization.py
+-rw-rw-rw-  2.0 fat     1968 b- defN 23-Apr-19 08:57 azureml/metrics/_seq2seq_translation.py
+-rw-rw-rw-  2.0 fat     5023 b- defN 23-Apr-19 08:57 azureml/metrics/_token_classification.py
+-rw-rw-rw-  2.0 fat    44604 b- defN 23-Apr-19 08:57 azureml/metrics/_validation.py
+-rw-rw-rw-  2.0 fat       40 b- defN 23-Apr-19 09:08 azureml/metrics/_version.py
+-rw-rw-rw-  2.0 fat    14894 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_classification_metrics.py
+-rw-rw-rw-  2.0 fat     3975 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_fill_mask_metrics.py
+-rw-rw-rw-  2.0 fat    15561 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_forecasting_metrics.py
+-rw-rw-rw-  2.0 fat     6621 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_metrics.py
+-rw-rw-rw-  2.0 fat    13109 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_od_is_metrics.py
+-rw-rw-rw-  2.0 fat     4523 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_qa_metrics.py
+-rw-rw-rw-  2.0 fat     9157 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_regression_metrics.py
+-rw-rw-rw-  2.0 fat     3858 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_summarization_metrics.py
+-rw-rw-rw-  2.0 fat     7329 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_text_classification_metrics.py
+-rw-rw-rw-  2.0 fat     3985 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_text_generation_metrics.py
+-rw-rw-rw-  2.0 fat     4200 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_text_ner_metrics.py
+-rw-rw-rw-  2.0 fat     3646 b- defN 23-Apr-19 08:57 azureml/metrics/azureml_translation_metrics.py
+-rw-rw-rw-  2.0 fat     4601 b- defN 23-Apr-19 08:57 azureml/metrics/bleu.py
+-rw-rw-rw-  2.0 fat    35559 b- defN 23-Apr-19 08:57 azureml/metrics/constants.py
+-rw-rw-rw-  2.0 fat     4820 b- defN 23-Apr-19 08:57 azureml/metrics/contract.py
+-rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-19 08:57 azureml/metrics/exceptions.py
+-rw-rw-rw-  2.0 fat    47616 b- defN 23-Apr-19 08:57 azureml/metrics/reference_codes.py
+-rw-rw-rw-  2.0 fat     6424 b- defN 23-Apr-19 08:57 azureml/metrics/scoring.py
+-rw-rw-rw-  2.0 fat    13249 b- defN 23-Apr-19 08:57 azureml/metrics/utilities.py
+-rw-rw-rw-  2.0 fat      239 b- defN 23-Apr-19 08:59 azureml/metrics/od_is_eval/__init__.py
+-rw-rw-rw-  2.0 fat    21196 b- defN 23-Apr-19 08:59 azureml/metrics/od_is_eval/incremental_voc_evaluator.py
+-rw-rw-rw-  2.0 fat    17250 b- defN 23-Apr-19 08:59 azureml/metrics/od_is_eval/metric_computation_utils.py
+-rw-rw-rw-  2.0 fat      859 b- defN 23-Apr-19 09:08 azureml_metrics-0.0.6.post1.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     1806 b- defN 23-Apr-19 09:08 azureml_metrics-0.0.6.post1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-Apr-19 09:08 azureml_metrics-0.0.6.post1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 23-Apr-19 09:08 azureml_metrics-0.0.6.post1.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     4286 b- defN 23-Apr-19 09:08 azureml_metrics-0.0.6.post1.dist-info/RECORD
+46 files, 813912 bytes uncompressed, 144304 bytes compressed:  82.3%
```

## zipnote {}

```diff
@@ -63,14 +63,17 @@
 
 Filename: azureml/metrics/azureml_forecasting_metrics.py
 Comment: 
 
 Filename: azureml/metrics/azureml_metrics.py
 Comment: 
 
+Filename: azureml/metrics/azureml_od_is_metrics.py
+Comment: 
+
 Filename: azureml/metrics/azureml_qa_metrics.py
 Comment: 
 
 Filename: azureml/metrics/azureml_regression_metrics.py
 Comment: 
 
 Filename: azureml/metrics/azureml_summarization_metrics.py
@@ -114,23 +117,23 @@
 
 Filename: azureml/metrics/od_is_eval/incremental_voc_evaluator.py
 Comment: 
 
 Filename: azureml/metrics/od_is_eval/metric_computation_utils.py
 Comment: 
 
-Filename: azureml_metrics-0.0.6.dist-info/LICENSE.txt
+Filename: azureml_metrics-0.0.6.post1.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_metrics-0.0.6.dist-info/METADATA
+Filename: azureml_metrics-0.0.6.post1.dist-info/METADATA
 Comment: 
 
-Filename: azureml_metrics-0.0.6.dist-info/WHEEL
+Filename: azureml_metrics-0.0.6.post1.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_metrics-0.0.6.dist-info/top_level.txt
+Filename: azureml_metrics-0.0.6.post1.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_metrics-0.0.6.dist-info/RECORD
+Filename: azureml_metrics-0.0.6.post1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/metrics/_score.py

```diff
@@ -15,14 +15,15 @@
 from azureml.metrics.azureml_regression_metrics import AzureMLRegressionMetrics
 from azureml.metrics.azureml_translation_metrics import AzureMLTranslationMetrics
 from azureml.metrics.azureml_summarization_metrics import AzureMLSummarizationMetrics
 from azureml.metrics.azureml_qa_metrics import AzureMLQAMetrics
 from azureml.metrics.azureml_text_ner_metrics import AzureMLTextNERMetrics
 from azureml.metrics.azureml_fill_mask_metrics import AzureMLFillMaskMetrics
 from azureml.metrics.azureml_text_generation_metrics import AzureMLTextGenerationMetrics
+from azureml.metrics.azureml_od_is_metrics import AzureMLODISMetrics, AzureMLODMetrics, AzureMLISMetrics
 
 logger = logging.getLogger(__name__)
 
 
 def compute_metrics(*,
                     task_type: constants.TASKS,
                     y_test: Optional[Union[np.ndarray, pd.DataFrame, List]] = None,
@@ -32,15 +33,16 @@
                     **kwargs) -> Dict[str, Dict[str, Any]]:
     """Given task type, y_test, y_pred or y_pred_proba compute metrics for the respective task.
 
         :param task_type: Accepts an argument of type constants.Tasks for which metrics have to be computed.
             Can accept from any of the values constants.Tasks.CLASSIFICATION, constants.Tasks.REGRESSION,
             constants.Tasks.TEXT_CLASSIFICATION, constants.Tasks.TEXT_CLASSIFICATION_MULTILABEL,
             constants.Tasks.TEXT_NER, constants.Tasks.SUMMARIZATION, constants.Tasks.TRANSLATION,
-            constants.Tasks.QUESTION_ANSWERING.
+            constants.Tasks.QUESTION_ANSWERING, constants.Tasks.IMAGE_OBJECT_DETECTION,
+            constants.Tasks.IMAGE_INSTANCE_SEGMENTATION.
         :param y_test: Ground truths or reference values.
             optional for computing few of language_modeling metrics.
         :param y_pred: Prediction values.
         :param y_pred_proba: Predicted probability values.
 
         Example for multiclass classification:
         --------------------------------------
@@ -109,14 +111,71 @@
 
         Example for text_generation:
         ----------------------------
         from azureml.metrics import compute_metrics, constants
         y_pred = ["hello there general kenobi","foo bar foobar", "blue & red"]
         y_test = [["hello there general kenobi san"], ["foo bar foobar"], ["blue & green"]]
         result = compute_metrics(task_type=constants.Tasks.TEXT_GENERATION, y_test=y_test, y_pred=y_pred)
+
+        Example for object-detection:
+        -------------------------------
+        from azureml.metrics import compute_metrics
+        y_test = [{
+                "boxes": np.array([[160, 120, 320, 240]], dtype=np.float32),
+                "classes": np.array([1])
+            }]
+        image_meta_info = [{
+                "areas": [60000],
+                "iscrowd": [0],
+                "filename": "image_1.jpg",
+                "height": 640,
+                "width": 480,
+                "original_width": 640,
+                "original_height": 480,
+            }]
+        y_pred = [{
+                "boxes": np.array([[160, 120, 320, 240]], dtype=np.float32),
+                "classes": np.array([1]),
+                "scores": np.array([0.75]),
+            }]
+        result = compute_metrics(task_type=constants.Tasks.IMAGE_OBJECT_DETECTION, y_test=y_test,
+                                    y_pred=y_pred,image_meta_info=image_meta_info)
+
+        Example for instance-segmentation:
+        -------------------------------
+        from azureml.metrics import compute_metrics
+        from pycocotools import mask as pycoco_mask
+        def _rle_mask_from_bbox(bbox, height, width):
+            x1, y1, x2, y2 = bbox
+            polygon = [[x1, y1, x2, y1, x2, y2, x1, y2, x1, y1]]
+            rle_masks = pycoco_mask.frPyObjects(polygon, height, width)
+            return rle_masks[0]
+
+        y_test = [{
+                "boxes": np.array([[160, 120, 320, 240]], dtype=np.float32),
+                "classes": np.array([1]),
+                "masks": [_rle_mask_from_bbox([1, 0, 2, 100], 640, 640)],
+            }]
+        image_meta_info = [{
+                "areas": [60000],
+                "iscrowd": [0],
+                "filename": "image_1.jpg",
+                "height": 640,
+                "width": 480,
+                "original_width": 640,
+                "original_height": 480,
+            }]
+        y_pred = [{
+                "boxes": np.array([[160, 120, 320, 240]], dtype=np.float32),
+                "masks": [_rle_mask_from_bbox([1, 0, 2, 100], 640, 640)],
+                "classes": np.array([1]),
+                "scores": np.array([0.75]),
+            }]
+        result = compute_metrics(task_type=constants.Tasks.IMAGE_INSTANCE_SEGMENTATION, y_test=y_test,
+                                        y_pred=y_pred,image_meta_info=image_meta_info)
     """
     # Step 1: Check either y_pred or y_pred_proba exist
     # Step 2: Instantiate Metrics Class object on the basis of task type
     #   and pass in necessary parameters while creating object
     # Step 3: Call compute method of class object to compute and fetch metrics
     if y_test is None:
         if task_type in [constants.Tasks.FILL_MASK]:
@@ -362,19 +421,43 @@
             aggregation_method=aggregation_method,
             custom_dimensions=custom_dimensions,
             y_min_dict=y_min_dict,
             y_max_dict=y_max_dict,
             log_activity=log_activity,
             log_traceback=log_traceback)
         computed_metrics = metrics.compute(y_test=y_test, y_pred=y_pred, X_test=X_test)
+    elif task_type in [constants.Tasks.IMAGE_OBJECT_DETECTION, constants.Tasks.IMAGE_INSTANCE_SEGMENTATION]:
+        metrics_list = kwargs.pop("metrics", None)
+        task_is_detection = task_type == constants.Tasks.IMAGE_OBJECT_DETECTION
+        num_classes = kwargs.pop("num_classes", None)
+        iou_threshold = kwargs.pop("iou_threshold", None)
+        if num_classes is None:
+            raise Exception("The number of classes must be specified for {} tasks.".format(task_type))
+
+        # Extract the additional image-related argument required for object detection / instance segmentation.
+        image_meta_info = kwargs.pop("image_meta_info", None)
+        if image_meta_info is None:
+            raise Exception("The image meta information must be specified for {} tasks.".format(task_type))
+
+        od_is_kwargs = ["metrics", "num_classes", "iou_threshold", "image_meta_info"]
+        check_kwargs(kwargs, task_type, od_is_kwargs, common_args)
+
+        metrics = AzureMLODISMetrics(
+            task_is_detection=task_is_detection,
+            num_classes=num_classes,
+            iou_threshold=iou_threshold,
+            metrics=metrics_list,
+        )
+        computed_metrics = metrics.compute(y_test=y_test, y_pred=y_pred, image_meta_info=image_meta_info)
     else:
         supported_tasks = [constants.Tasks.FORECASTING, constants.Tasks.CLASSIFICATION,
                            constants.Tasks.REGRESSION, constants.Tasks.SUMMARIZATION,
                            constants.Tasks.TRANSLATION, constants.Tasks.FILL_MASK,
-                           constants.Tasks.QUESTION_ANSWERING] + constants.Tasks.ALL_TEXT
+                           constants.Tasks.QUESTION_ANSWERING, constants.Tasks.IMAGE_OBJECT_DETECTION,
+                           constants.Tasks.IMAGE_INSTANCE_SEGMENTATION] + constants.Tasks.ALL_TEXT
         raise Exception(f"Invalid task type. Please choose among the following task types : {supported_tasks}")
     return computed_metrics
 
 
 def score(*,
           task_type: constants.TASKS,
           model: Any,
@@ -498,35 +581,47 @@
         >>>list_metrics(task_type=constants.Tasks.SUMMARIZATION)
 
         Example for question answering:
         -------------------------------
         >>>from azureml.metrics import list_metrics, constants
         >>>list_metrics(task_type=constants.Tasks.QUESTION_ANSWERING)
 
+        Example for text generation:
+        -------------------------------
+        >>>from azureml.metrics import list_metrics, constants
+        >>>list_metrics(task_type=constants.Tasks.TEXT_GENERATION)
+
         Example for language modeling:
         -------------------------------
         >>>from azureml.metrics import list_metrics, constants
         >>>list_metrics(task_type=constants.Tasks.FILL_MASK)
 
-        Example for text generation:
+        Example for object detection:
+        --------------------------
+        >>>from azureml.metrics import list_metrics, constants
+        >>>list_metrics(task_type=constants.Tasks.IMAGE_OBJECT_DETECTION)
+
+        Example for instance segmentation:
         -------------------------------
         >>>from azureml.metrics import list_metrics, constants
-        >>>list_metrics(task_type=constants.Tasks.TEXT_GENERATION)
+        >>>list_metrics(task_type=constants.Tasks.IMAGE_INSTANCE_SEGMENTATION)
     """
     task_options = {
         constants.Tasks.CLASSIFICATION: AzureMLClassificationMetrics,
         constants.Tasks.REGRESSION: AzureMLRegressionMetrics,
         constants.Tasks.TEXT_CLASSIFICATION: AzureMLClassificationMetrics,
         constants.Tasks.TEXT_CLASSIFICATION_MULTILABEL: AzureMLClassificationMetrics,
         constants.Tasks.TEXT_NER: AzureMLTextNERMetrics,
         constants.Tasks.TRANSLATION: AzureMLTranslationMetrics,
         constants.Tasks.SUMMARIZATION: AzureMLSummarizationMetrics,
         constants.Tasks.QUESTION_ANSWERING: AzureMLQAMetrics,
         constants.Tasks.FILL_MASK: AzureMLFillMaskMetrics,
         constants.Tasks.TEXT_GENERATION: AzureMLTextGenerationMetrics,
+        constants.Tasks.IMAGE_OBJECT_DETECTION: AzureMLODMetrics,
+        constants.Tasks.IMAGE_INSTANCE_SEGMENTATION: AzureMLISMetrics,
         constants.Tasks.FORECASTING: AzureMLForecastingMetrics
     }
 
     result = task_options.get(task_type, None)
 
     if result is None:
         return f"Metrics are not implemented for provided task type : {task_type}."
```

## azureml/metrics/_version.py

```diff
@@ -1,2 +1,2 @@
 ver = "0.0.6"
-selfver = "0.0.6"
+selfver = "0.0.6.post1"
```

## azureml/metrics/azureml_classification_metrics.py

```diff
@@ -192,14 +192,27 @@
 
         if (y_pred_probs is not None) and (y_pred is not None):
             if len(self.train_labels) != len(y_pred_probs[0]):
                 y_pred_probs = None
                 logger.warning("Ignoring y_pred_proba as we found mismatch in length"
                                " of class labels identified from y_test, y_pred and y_pred_proba")
 
+            elif not self.multilabel:
+                y_pred_from_probs = np.argmax(y_pred_probs, axis=1)
+
+                class_label_map = {key: label for key, label in enumerate(self.class_labels)}
+                y_pred_from_probs = np.array([class_label_map[key] for key in y_pred_from_probs])
+
+                same_prediction = (y_pred == y_pred_from_probs).all()
+                if not same_prediction:
+                    y_pred_probs = None
+                    logger.warning("Ignoring y_pred_proba as predictions indicated from"
+                                   " y_pred_probs do not equal y_pred. Send class_labels "
+                                   "in same order of y_pred_proba.")
+
         scored_metrics = _scoring._score_classification(
             self._log_activity,
             self._log_traceback,
             y_test,
             y_pred,
             y_pred_probs,
             self.metrics,
```

## azureml/metrics/constants.py

```diff
@@ -123,25 +123,32 @@
     # QA
     QAExactMatch = "exact_match"
     QAF1Score = "f1_score"
 
     # Fill Masking Metrics
     FMPerplexity = "perplexities"
 
-    # Image Multi Label Classification
+    # Image multilabel classification
     IOU = "iou"  # Intersection Over Union
 
-    # Image Object Detection
-    MEAN_AVERAGE_PRECISION = 'mean_average_precision'
-    AVERAGE_PRECISION = 'average_precision'
-    PRECISION = 'precision'
-    RECALL = 'recall'
-    PER_LABEL_METRICS = 'per_label_metrics'
-    IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS = 'image_level_binary_classsifier_metrics'
-    CONFUSION_MATRICES_PER_SCORE_THRESHOLD = 'confusion_matrices_per_score_threshold'
+    # Image object detection and instance segmentation
+    MEAN_AVERAGE_PRECISION = "mean_average_precision"
+    AVERAGE_PRECISION = "average_precision"
+    PRECISION = "precision"
+    RECALL = "recall"
+    PER_LABEL_METRICS = "per_label_metrics"
+    IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS = "image_level_binary_classsifier_metrics"
+    CONFUSION_MATRICES_PER_SCORE_THRESHOLD = "confusion_matrices_per_score_threshold"
+    MEAN_AVERAGE_PRECISION = "mean_average_precision"
+    AVERAGE_PRECISION = "average_precision"
+    PRECISION = "precision"
+    RECALL = "recall"
+    PER_LABEL_METRICS = "per_label_metrics"
+    IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS = "image_level_binary_classsifier_metrics"
+    CONFUSION_MATRICES_PER_SCORE_THRESHOLD = "confusion_matrices_per_score_threshold"
 
     SCALAR_CLASSIFICATION_SET = {
         AUCBinary,
         AUCMacro,
         AUCMicro,
         AUCWeighted,
         Accuracy,
@@ -265,20 +272,23 @@
 
     REGRESSION_PRIMARY_SET = {Spearman, NormRMSE, R2Score, NormMeanAbsError}
 
     IMAGE_CLASSIFICATION_PRIMARY_SET = {Accuracy}
 
     IMAGE_CLASSIFICATION_MULTILABEL_PRIMARY_SET = {IOU}
 
-    IMAGE_OBJECT_DETECTION_PRIMARY_SET = {
-        MEAN_AVERAGE_PRECISION,
-    }
+    IMAGE_OBJECT_DETECTION_PRIMARY_SET = {MEAN_AVERAGE_PRECISION}
 
     IMAGE_OBJECT_DETECTION_SET = {
-        MEAN_AVERAGE_PRECISION,
+        MEAN_AVERAGE_PRECISION, RECALL, PRECISION, PER_LABEL_METRICS,
+        IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS, CONFUSION_MATRICES_PER_SCORE_THRESHOLD
+    }
+
+    IMAGE_INSTANCE_SEGMENTATION_SET = {
+        MEAN_AVERAGE_PRECISION, RECALL, PRECISION, PER_LABEL_METRICS
     }
 
     SAMPLE_WEIGHTS_UNSUPPORTED_SET = {
         WeightedAccuracy,
         Spearman,
         MedianAbsError,
         NormMedianAbsError,
@@ -688,23 +698,43 @@
 
 IMAGE_CLASSIFICATION_SET = {ACCURACY}
 
 IMAGE_CLASSIFICATION_MULTILABEL_CLASSIFICATION_SET = {IOU}
 
 # Image Object Detection Metrics
 MEAN_AVERAGE_PRECISION = "mean_average_precision"
+AVERAGE_PRECISION = "average_precision"
+PRECISION = "precision"
+RECALL = "recall"
+PER_LABEL_METRICS = "per_label_metrics"
+IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS = "image_level_binary_classsifier_metrics"
+CONFUSION_MATRICES_PER_SCORE_THRESHOLD = "confusion_matrices_per_score_threshold"
 
 IMAGE_OBJECT_DETECTION_SCALAR_SET = {
-    MEAN_AVERAGE_PRECISION,
+    MEAN_AVERAGE_PRECISION, RECALL, PRECISION
 }
 
-IMAGE_OBJECT_DETECTION_SET = {
-    MEAN_AVERAGE_PRECISION,
+IMAGE_OBJECT_DETECTION_CLASSWISE_SET = {PER_LABEL_METRICS}
+
+IMAGE_OBJECT_DETECTION_NONSCALAR_SET = {
+    IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS, CONFUSION_MATRICES_PER_SCORE_THRESHOLD
 }
 
+IMAGE_OBJECT_DETECTION_SET = (
+    IMAGE_OBJECT_DETECTION_SCALAR_SET | IMAGE_OBJECT_DETECTION_CLASSWISE_SET | IMAGE_OBJECT_DETECTION_NONSCALAR_SET
+)
+
+IMAGE_INSTANCE_SEGMENTATION_SCALAR_SET = {
+    MEAN_AVERAGE_PRECISION, RECALL, PRECISION
+}
+
+IMAGE_INSTANCE_SEGMENTATION_CLASSWISE_SET = {PER_LABEL_METRICS}
+
+IMAGE_INSTANCE_SEGMENTATION_SET = (IMAGE_INSTANCE_SEGMENTATION_SCALAR_SET | IMAGE_INSTANCE_SEGMENTATION_CLASSWISE_SET)
+
 # Text Classification Metrics
 
 TEXT_CLASSIFICATION_SET = {
     ACCURACY,
     AUC_WEIGHTED,
     PRECISION_MICRO,
     PRECISION_WEIGHTED,
@@ -737,45 +767,51 @@
 
 # All Metrics
 
 FULL_SET = (
     CLASSIFICATION_SET
     | REGRESSION_SET
     | IMAGE_OBJECT_DETECTION_SET
+    | IMAGE_INSTANCE_SEGMENTATION_SET
     | FORECASTING_NONSCALAR_SET
     | Metric.TRANSLATION_SET
     | Metric.SUMMARIZATION_SET
     | Metric.QA_SET
     | Metric.FILL_MASK_SET
     | Metric.TEXT_GENERATION_SET
 )
 
+FULL_CLASSWISE_SET = (
+    CLASSIFICATION_CLASSWISE_SET | IMAGE_OBJECT_DETECTION_CLASSWISE_SET | IMAGE_INSTANCE_SEGMENTATION_CLASSWISE_SET
+)
+
 FULL_NONSCALAR_SET = (
     CLASSIFICATION_NONSCALAR_SET
     | REGRESSION_NONSCALAR_SET
     | FILL_MASK_NONSCALAR_SET
     | FORECASTING_NONSCALAR_SET
+    | IMAGE_OBJECT_DETECTION_NONSCALAR_SET
 )
 
-
 FULL_SCALAR_SET = (
     CLASSIFICATION_SCALAR_SET
     | REGRESSION_SCALAR_SET
     | IMAGE_OBJECT_DETECTION_SCALAR_SET
+    | IMAGE_INSTANCE_SEGMENTATION_SCALAR_SET
 )
 
 METRICS_TASK_MAP = {
     CLASSIFICATION: CLASSIFICATION_SET,
     REGRESSION: REGRESSION_SET,
     FORECASTING: FORECASTING_SET,
     IMAGE_CLASSIFICATION: IMAGE_CLASSIFICATION_SET,
     IMAGE_CLASSIFICATION_MULTILABEL: IMAGE_CLASSIFICATION_MULTILABEL_CLASSIFICATION_SET,
     IMAGE_MULTI_LABEL_CLASSIFICATION: IMAGE_CLASSIFICATION_MULTILABEL_CLASSIFICATION_SET,
     IMAGE_OBJECT_DETECTION: IMAGE_OBJECT_DETECTION_SET,
-    IMAGE_INSTANCE_SEGMENTATION: IMAGE_OBJECT_DETECTION_SET,
+    IMAGE_INSTANCE_SEGMENTATION: IMAGE_INSTANCE_SEGMENTATION_SET,
     TEXT_CLASSIFICATION: TEXT_CLASSIFICATION_SET,
     TEXT_CLASSIFICATION_MULTILABEL: TEXT_CLASSIFICATION_MULTILABEL_SET,
     TEXT_NER: TEXT_NER_SET,
     TRANSLATION: Metric.TRANSLATION_SET,
     SUMMARIZATION: Metric.SUMMARIZATION_SET,
     QUESTION_ANSWERING: Metric.QA_SET,
     FILL_MASK: Metric.FILL_MASK_SET,
```

## azureml/metrics/utilities.py

```diff
@@ -210,29 +210,29 @@
         metric_name = metric_name[:-len(constants.MetricExtrasConstants.MetricExtrasSuffix)]
     if metric_name in constants.FULL_SCALAR_SET or \
             metric_name in constants.CLASSIFICATION_MULTILABEL_SET or \
             metric_name in constants.Metric.SCALAR_SEQ2SEQ_SET:
         return True
     elif metric_name in constants.FULL_NONSCALAR_SET:
         return False
-    elif metric_name in constants.CLASSIFICATION_CLASSWISE_SET:
+    elif metric_name in constants.FULL_CLASSWISE_SET:
         return False
     safe_message = "{} metric is not supported".format(metric_name)
     raise ClientException(safe_message, target="metric_name", reference_code="utilities.is_scalar",
                           safe_message=safe_message)
 
 
 def is_classwise(metric_name: str) -> bool:
     """
     Check whether a given metric is a classwise metric.
 
     :param metric_name: the name of the metric found in constants.py
     :return: boolean for if the metric is scalar
     """
-    if metric_name in constants.CLASSIFICATION_CLASSWISE_SET:
+    if metric_name in constants.FULL_CLASSWISE_SET:
         return True
     else:
         return False
     safe_message = "{} metric is not supported".format(metric_name)
     raise ClientException(safe_message, target="metric_name", reference_code="utilities.is_classwise",
                           safe_message=safe_message)
```

## Comparing `azureml_metrics-0.0.6.dist-info/LICENSE.txt` & `azureml_metrics-0.0.6.post1.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_metrics-0.0.6.dist-info/METADATA` & `azureml_metrics-0.0.6.post1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-metrics
-Version: 0.0.6
+Version: 0.0.6.post1
 Summary: Contains the ML and non-Azure specific common code associated     with AzureML metrics.
 Home-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py
 Author: Microsoft Corp
 License: https://aka.ms/azureml-sdk-license
 Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
```

## Comparing `azureml_metrics-0.0.6.dist-info/RECORD` & `azureml_metrics-0.0.6.post1.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -2,44 +2,45 @@
 azureml/metrics/NOTICE,sha256=fIZkNZdLtnzMQ56XPLfz1N9FPTv-CO4BHBD1xl3_gYk,260065
 azureml/metrics/__init__.py,sha256=3mDFscFE4vSB2MuBR96MTYQohN11sSFguJIL9prpowc,805
 azureml/metrics/_classification.py,sha256=AJozqv3Z4NouGAB3_dwl6l0nebivjoAOej8YM2I_RHk,49711
 azureml/metrics/_dataset_binning.py,sha256=wD5ea9LMolPEMYqpWxDAgCYOHu2-a1OKCnFMFnevTCQ,7829
 azureml/metrics/_forecasting.py,sha256=zqmdiLKN5f6XCmmKfkqQ3cFTmOsvK0SMbeXgPj3yyz8,24070
 azureml/metrics/_metric_base.py,sha256=jE3ZZIoNbHoLJJWyhcOloAO2aIB7ST3fhd3eBUtKjpA,5034
 azureml/metrics/_regression.py,sha256=wxIL2eh82d7ABsPG04SkBxYsIRiZhEhzBCRIZJw6P5I,25843
-azureml/metrics/_score.py,sha256=37sVtS4pT55FLn41LaFv5kIm3-GvVq-l38Cwb7a0Q48,27931
+azureml/metrics/_score.py,sha256=8Hv3NrJ6C7s9bp1MELXjADBAnmjAhGrFawU47H6s3ec,32574
 azureml/metrics/_scoring.py,sha256=e-s9PTYxMEnQakwgxBg8HaN3nJGpreidQOyCrta0kyM,32735
 azureml/metrics/_scoring_confidence.py,sha256=eTaG0ud_4U9O16zrw213-_J_iik4BqJuvIqTDi6eNMs,23940
 azureml/metrics/_scoring_utilities.py,sha256=JGGhsNxiVocZBhvXlPr9yCiQMHWv-BejZUSvkmd-sEM,34427
 azureml/metrics/_seq2seq_fill_mask.py,sha256=0nckgKC4h-TtCUBWWgsJGoOxIDEyYkReWETQFm3Du6s,2213
 azureml/metrics/_seq2seq_qa.py,sha256=BtsT8wqJhrgtaf1x91Opm2jnhXsBt2YVEcDzJNHEk48,5135
 azureml/metrics/_seq2seq_summarization.py,sha256=ZTXtotmF35KwyI3WDZ14A14I3oLQpItKa49pXjMAg38,2137
 azureml/metrics/_seq2seq_translation.py,sha256=UrkMnPgpQv8q_T6qzYWUpcespno4E-MTXvDjIacA5Ho,1968
 azureml/metrics/_token_classification.py,sha256=xZ9JJ4R8GfUNxKqQhVHDttj4AuA7U6H4jCiqCuH9nAc,5023
 azureml/metrics/_validation.py,sha256=HXWL0j1XDuI5mlhEhIVvnLWcyh04rTf-7dfRhUPuHY4,44604
-azureml/metrics/_version.py,sha256=zEK9qNwSOrrnJnVdRoBiTn9QiUMB122Z8Pf9WuceS2M,34
-azureml/metrics/azureml_classification_metrics.py,sha256=ZRmJToFBUBXAV7-8vnfZAeAswrpKw-mmzbxWMlruXuM,14181
+azureml/metrics/_version.py,sha256=uh0VWzHdEQl7gw9VpWtMeegnzWKSHzOpEVYk20d7uEs,40
+azureml/metrics/azureml_classification_metrics.py,sha256=YTNL92lBAIR5qvaUlI3nY_MQArfrlw4M3Poq_EKRt9c,14894
 azureml/metrics/azureml_fill_mask_metrics.py,sha256=vbuHl_j57p1vYZXUeyJ8gCrB3APrCk5fH0qTxrb4DJ4,3975
 azureml/metrics/azureml_forecasting_metrics.py,sha256=BhvVu6HJ8HVz2cXkf-39T5RjQJUqSlDxuAq8Vo8tmos,15561
 azureml/metrics/azureml_metrics.py,sha256=fqQREyFxQ_YlKXFoRTILm1XD3FaB6C66nFQJ6pyJArg,6621
+azureml/metrics/azureml_od_is_metrics.py,sha256=ch2gucXD5Z4dOZmWKkRQvgrT8vNkvewODNEu7qfKQaI,13109
 azureml/metrics/azureml_qa_metrics.py,sha256=8f3FI8CNmf8HSt_5ekZ5MjnFJLoSRdPANez7M7XhGtM,4523
 azureml/metrics/azureml_regression_metrics.py,sha256=nswYZ3QgzonObxedGvsVnBQ8-oD8Spyel38DnzrqeMQ,9157
 azureml/metrics/azureml_summarization_metrics.py,sha256=TQ9EDtkbuyDn7BKuEJULze_H5pzbfCIAnVDlo723MNA,3858
 azureml/metrics/azureml_text_classification_metrics.py,sha256=MMWj1IpiG2xUsreX43fWUjR1-NMDNBwYdqRkEQKuLRA,7329
 azureml/metrics/azureml_text_generation_metrics.py,sha256=bPhxT863PUYpy8jhF4dFOrZcqwt3aU7scwhUo8RnoN8,3985
 azureml/metrics/azureml_text_ner_metrics.py,sha256=GzWtzGMEOHqft4RloaIGqiKR86eqjcM3EM-P9F67t6M,4200
 azureml/metrics/azureml_translation_metrics.py,sha256=ausHWlkwuszJfi-KuqAnGZCRGZDSMntMVs-uRwtFwco,3646
 azureml/metrics/bleu.py,sha256=CCa6LCAm2bm1LtBk9dNXTo-0C8Ptws44WPksQp1PNRQ,4601
-azureml/metrics/constants.py,sha256=VNu_7SuJeIEMJXjwho0zAniRZ_9N8y6hatV5b4wK6U8,33784
+azureml/metrics/constants.py,sha256=1AnBgyB-iwOFxioqoAlEE1Os7zvdQWc0wVNEeRBxpck,35559
 azureml/metrics/contract.py,sha256=Ny1KJb35i4V4tCOE_qgqfIkEWKfX_PrjoFCyoo_WUoE,4820
 azureml/metrics/exceptions.py,sha256=cv4hLN-5skqXgUu_9tE-Wnv5azBH7A6CAAcDBMdywSw,6624
 azureml/metrics/reference_codes.py,sha256=r58ZAM2mWtzW9IJrO8RJd8vQx_KU3a2tUF4XZjp3ooI,47616
 azureml/metrics/scoring.py,sha256=BF5jicht3V4ZbeQQ_2Si2yVfu_1QQds1iHFOIdTNaVM,6424
-azureml/metrics/utilities.py,sha256=ijLmDXnkg3rXiDuc9aKyvoOTtfHSpxJbyzkoY4vqRy4,13269
+azureml/metrics/utilities.py,sha256=4oU6p74Eb1p3uu0CeTnofMWrtIosAgCEXsJMrV1t8S4,13249
 azureml/metrics/od_is_eval/__init__.py,sha256=VjnWZi7NjkYq6T61O4RMa6Pz5Nzqe8qoSpU7TlyFHQI,239
 azureml/metrics/od_is_eval/incremental_voc_evaluator.py,sha256=kXkNgBNN7t7aMG0MWA5fwQfVKZAacgTvyQJ2X5682TM,21196
 azureml/metrics/od_is_eval/metric_computation_utils.py,sha256=f7fpAiwlqI-MMM5q5eDDZN5kYwY4fy05zvApPJE5_Sk,17250
-azureml_metrics-0.0.6.dist-info/LICENSE.txt,sha256=GBoIyZ-6vJ4xjRc8U3wTw4EfkuaEdVTm_gbr1Nm8uDI,859
-azureml_metrics-0.0.6.dist-info/METADATA,sha256=UF-ciZdWrdt4cGZrVHyPpNPkIiORFZKnWxHiIwT6BcY,1800
-azureml_metrics-0.0.6.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_metrics-0.0.6.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
-azureml_metrics-0.0.6.dist-info/RECORD,,
+azureml_metrics-0.0.6.post1.dist-info/LICENSE.txt,sha256=GBoIyZ-6vJ4xjRc8U3wTw4EfkuaEdVTm_gbr1Nm8uDI,859
+azureml_metrics-0.0.6.post1.dist-info/METADATA,sha256=rHlRC9yZ-7VEXAoitNw7rqmPKoZE-EQ_k7mD-Atn4Zw,1806
+azureml_metrics-0.0.6.post1.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_metrics-0.0.6.post1.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
+azureml_metrics-0.0.6.post1.dist-info/RECORD,,
```

