# Comparing `tmp/scribe_data-2.2.2-py3-none-any.whl.zip` & `tmp/scribe_data-3.0.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,71 +1,75 @@
-Zip file size: 103197 bytes, number of entries: 69
+Zip file size: 107429 bytes, number of entries: 73
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-06 08:21 scribe_data/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:22 scribe_data/extract_transform/__init__.py
--rw-r--r--  2.0 unx    13956 b- defN 23-Apr-07 10:11 scribe_data/extract_transform/extract_wiki.py
--rw-r--r--  2.0 unx     6461 b- defN 23-Apr-07 11:06 scribe_data/extract_transform/process_unicode.py
--rw-r--r--  2.0 unx    12783 b- defN 23-Apr-07 10:12 scribe_data/extract_transform/process_wiki.py
+-rw-r--r--  2.0 unx     8252 b- defN 23-Apr-17 15:55 scribe_data/extract_transform/emoji_utils.py
+-rw-r--r--  2.0 unx    13956 b- defN 23-Apr-10 20:09 scribe_data/extract_transform/extract_wiki.py
+-rw-r--r--  2.0 unx     9447 b- defN 23-Apr-17 16:01 scribe_data/extract_transform/process_unicode.py
+-rw-r--r--  2.0 unx    12277 b- defN 23-Apr-10 21:27 scribe_data/extract_transform/process_wiki.py
+-rw-r--r--  2.0 unx    14322 b- defN 23-Apr-17 16:28 scribe_data/extract_transform/update_data.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:24 scribe_data/extract_transform/French/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:41 scribe_data/extract_transform/French/nouns/__init__.py
--rw-r--r--  2.0 unx     6162 b- defN 22-Oct-16 21:13 scribe_data/extract_transform/French/nouns/format_nouns.py
+-rw-r--r--  2.0 unx     4551 b- defN 23-Apr-16 20:53 scribe_data/extract_transform/French/nouns/format_nouns.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:43 scribe_data/extract_transform/French/translations/__init__.py
--rw-r--r--  2.0 unx     1297 b- defN 22-Oct-16 21:12 scribe_data/extract_transform/French/translations/format_translations.py
+-rw-r--r--  2.0 unx     1266 b- defN 23-Apr-10 20:12 scribe_data/extract_transform/French/translations/format_translations.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:43 scribe_data/extract_transform/French/verbs/__init__.py
--rw-r--r--  2.0 unx     3968 b- defN 22-Nov-04 22:50 scribe_data/extract_transform/French/verbs/format_verbs.py
+-rw-r--r--  2.0 unx     2271 b- defN 23-Apr-16 22:47 scribe_data/extract_transform/French/verbs/format_verbs.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/German/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/German/nouns/__init__.py
--rw-r--r--  2.0 unx     9499 b- defN 22-Oct-16 21:05 scribe_data/extract_transform/German/nouns/format_nouns.py
+-rw-r--r--  2.0 unx     7864 b- defN 23-Apr-16 20:53 scribe_data/extract_transform/German/nouns/format_nouns.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/German/prepositions/__init__.py
--rw-r--r--  2.0 unx     4779 b- defN 22-Oct-16 21:12 scribe_data/extract_transform/German/prepositions/format_prepositions.py
+-rw-r--r--  2.0 unx     3143 b- defN 23-Apr-16 20:54 scribe_data/extract_transform/German/prepositions/format_prepositions.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/German/translations/__init__.py
--rw-r--r--  2.0 unx     1424 b- defN 22-Oct-16 21:19 scribe_data/extract_transform/German/translations/format_translations.py
+-rw-r--r--  2.0 unx     1393 b- defN 23-Apr-10 20:11 scribe_data/extract_transform/German/translations/format_translations.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/German/verbs/__init__.py
--rw-r--r--  2.0 unx     6707 b- defN 22-Oct-16 21:12 scribe_data/extract_transform/German/verbs/format_verbs.py
+-rw-r--r--  2.0 unx     6537 b- defN 23-Apr-17 00:04 scribe_data/extract_transform/German/verbs/format_verbs.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Italian/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Italian/nouns/__init__.py
--rw-r--r--  2.0 unx     6218 b- defN 22-Oct-16 21:13 scribe_data/extract_transform/Italian/nouns/format_nouns.py
+-rw-r--r--  2.0 unx     4583 b- defN 23-Apr-16 20:54 scribe_data/extract_transform/Italian/nouns/format_nouns.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Italian/translations/__init__.py
--rw-r--r--  2.0 unx     1326 b- defN 22-Oct-16 21:12 scribe_data/extract_transform/Italian/translations/format_translations.py
+-rw-r--r--  2.0 unx     1294 b- defN 23-Apr-10 20:12 scribe_data/extract_transform/Italian/translations/format_translations.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Italian/verbs/__init__.py
--rw-r--r--  2.0 unx     3609 b- defN 22-Oct-16 21:13 scribe_data/extract_transform/Italian/verbs/format_verbs.py
+-rw-r--r--  2.0 unx     1938 b- defN 23-Apr-16 22:48 scribe_data/extract_transform/Italian/verbs/format_verbs.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Portuguese/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Portuguese/nouns/__init__.py
--rw-r--r--  2.0 unx     6221 b- defN 22-Oct-16 21:12 scribe_data/extract_transform/Portuguese/nouns/format_nouns.py
+-rw-r--r--  2.0 unx     4586 b- defN 23-Apr-16 20:54 scribe_data/extract_transform/Portuguese/nouns/format_nouns.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Portuguese/translations/__init__.py
--rw-r--r--  2.0 unx     1329 b- defN 22-Oct-16 21:13 scribe_data/extract_transform/Portuguese/translations/format_translations.py
+-rw-r--r--  2.0 unx     1294 b- defN 23-Apr-10 20:12 scribe_data/extract_transform/Portuguese/translations/format_translations.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Portuguese/verbs/__init__.py
--rw-r--r--  2.0 unx     3708 b- defN 22-Oct-16 21:12 scribe_data/extract_transform/Portuguese/verbs/format_verbs.py
+-rw-r--r--  2.0 unx     2037 b- defN 23-Apr-16 22:49 scribe_data/extract_transform/Portuguese/verbs/format_verbs.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Russian/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Russian/nouns/__init__.py
--rw-r--r--  2.0 unx     9524 b- defN 22-Oct-16 21:06 scribe_data/extract_transform/Russian/nouns/format_nouns.py
+-rw-r--r--  2.0 unx     7889 b- defN 23-Apr-16 20:55 scribe_data/extract_transform/Russian/nouns/format_nouns.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Russian/prepositions/__init__.py
--rw-r--r--  2.0 unx     4762 b- defN 22-Oct-16 21:13 scribe_data/extract_transform/Russian/prepositions/format_prepositions.py
+-rw-r--r--  2.0 unx     3126 b- defN 23-Apr-16 20:55 scribe_data/extract_transform/Russian/prepositions/format_prepositions.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Russian/translations/__init__.py
--rw-r--r--  2.0 unx     1298 b- defN 22-Oct-16 21:12 scribe_data/extract_transform/Russian/translations/format_translations.py
+-rw-r--r--  2.0 unx     1266 b- defN 23-Apr-10 20:13 scribe_data/extract_transform/Russian/translations/format_translations.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Russian/verbs/__init__.py
--rw-r--r--  2.0 unx     3513 b- defN 22-Oct-16 21:13 scribe_data/extract_transform/Russian/verbs/format_verbs.py
+-rw-r--r--  2.0 unx     1842 b- defN 23-Apr-16 22:49 scribe_data/extract_transform/Russian/verbs/format_verbs.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Spanish/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Spanish/nouns/__init__.py
--rw-r--r--  2.0 unx     6218 b- defN 22-Oct-16 21:27 scribe_data/extract_transform/Spanish/nouns/format_nouns.py
+-rw-r--r--  2.0 unx     4583 b- defN 23-Apr-16 20:55 scribe_data/extract_transform/Spanish/nouns/format_nouns.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Spanish/translations/__init__.py
--rw-r--r--  2.0 unx     1297 b- defN 22-Oct-16 21:13 scribe_data/extract_transform/Spanish/translations/format_translations.py
+-rw-r--r--  2.0 unx     1265 b- defN 23-Apr-10 20:13 scribe_data/extract_transform/Spanish/translations/format_translations.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Spanish/verbs/__init__.py
--rw-r--r--  2.0 unx     3609 b- defN 22-Oct-16 21:28 scribe_data/extract_transform/Spanish/verbs/format_verbs.py
+-rw-r--r--  2.0 unx     2188 b- defN 23-Apr-16 22:50 scribe_data/extract_transform/Spanish/verbs/format_verbs.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Swedish/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Swedish/nouns/__init__.py
--rw-r--r--  2.0 unx     9119 b- defN 22-Oct-16 21:14 scribe_data/extract_transform/Swedish/nouns/format_nouns.py
+-rw-r--r--  2.0 unx     7484 b- defN 23-Apr-16 20:55 scribe_data/extract_transform/Swedish/nouns/format_nouns.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Swedish/translations/__init__.py
--rw-r--r--  2.0 unx     1298 b- defN 22-Oct-16 21:14 scribe_data/extract_transform/Swedish/translations/format_translations.py
+-rw-r--r--  2.0 unx     1266 b- defN 23-Apr-10 20:13 scribe_data/extract_transform/Swedish/translations/format_translations.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-05 19:44 scribe_data/extract_transform/Swedish/verbs/__init__.py
--rw-r--r--  2.0 unx     3970 b- defN 22-Oct-16 21:14 scribe_data/extract_transform/Swedish/verbs/format_verbs.py
+-rw-r--r--  2.0 unx     2231 b- defN 23-Apr-16 22:53 scribe_data/extract_transform/Swedish/verbs/format_verbs.py
 -rw-r--r--  2.0 unx   102823 b- defN 22-Nov-23 22:13 scribe_data/extract_transform/_resources/2021_ranked.tsv
 -rw-r--r--  2.0 unx        0 b- defN 22-Nov-23 22:13 scribe_data/extract_transform/_resources/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Apr-06 08:23 scribe_data/load/__init__.py
+-rw-r--r--  2.0 unx     8655 b- defN 23-Apr-18 23:57 scribe_data/load/data_to_sqlite.py
+-rw-r--r--  2.0 unx     1185 b- defN 23-Apr-10 21:36 scribe_data/load/send_dbs_to_scribe.py
 -rw-r--r--  2.0 unx    12972 b- defN 22-Nov-04 23:06 scribe_data/load/update_data.py
--rw-r--r--  2.0 unx     6353 b- defN 22-Nov-21 16:40 scribe_data/load/update_utils.py
--rw-r--r--  2.0 unx      389 b- defN 22-Nov-23 22:45 scribe_data-2.2.2.data/data/requirements.txt
--rw-r--r--  2.0 unx    32472 b- defN 23-Apr-07 11:21 scribe_data-2.2.2.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx    13440 b- defN 23-Apr-07 11:21 scribe_data-2.2.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-07 11:21 scribe_data-2.2.2.dist-info/WHEEL
--rw-r--r--  2.0 unx       12 b- defN 23-Apr-07 11:21 scribe_data-2.2.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     7566 b- defN 23-Apr-07 11:21 scribe_data-2.2.2.dist-info/RECORD
-69 files, 310174 bytes uncompressed, 90357 bytes compressed:  70.9%
+-rw-r--r--  2.0 unx     6807 b- defN 23-Apr-10 09:30 scribe_data/load/update_utils.py
+-rw-r--r--  2.0 unx      420 b- defN 23-Apr-16 20:21 scribe_data-3.0.0.data/data/requirements.txt
+-rw-r--r--  2.0 unx    32472 b- defN 23-Apr-19 00:33 scribe_data-3.0.0.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx    15386 b- defN 23-Apr-19 00:33 scribe_data-3.0.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-19 00:33 scribe_data-3.0.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       12 b- defN 23-Apr-19 00:33 scribe_data-3.0.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     7955 b- defN 23-Apr-19 00:33 scribe_data-3.0.0.dist-info/RECORD
+73 files, 322930 bytes uncompressed, 93965 bytes compressed:  70.9%
```

## zipnote {}

```diff
@@ -1,22 +1,28 @@
 Filename: scribe_data/__init__.py
 Comment: 
 
 Filename: scribe_data/extract_transform/__init__.py
 Comment: 
 
+Filename: scribe_data/extract_transform/emoji_utils.py
+Comment: 
+
 Filename: scribe_data/extract_transform/extract_wiki.py
 Comment: 
 
 Filename: scribe_data/extract_transform/process_unicode.py
 Comment: 
 
 Filename: scribe_data/extract_transform/process_wiki.py
 Comment: 
 
+Filename: scribe_data/extract_transform/update_data.py
+Comment: 
+
 Filename: scribe_data/extract_transform/French/__init__.py
 Comment: 
 
 Filename: scribe_data/extract_transform/French/nouns/__init__.py
 Comment: 
 
 Filename: scribe_data/extract_transform/French/nouns/format_nouns.py
@@ -177,32 +183,38 @@
 
 Filename: scribe_data/extract_transform/_resources/__init__.py
 Comment: 
 
 Filename: scribe_data/load/__init__.py
 Comment: 
 
+Filename: scribe_data/load/data_to_sqlite.py
+Comment: 
+
+Filename: scribe_data/load/send_dbs_to_scribe.py
+Comment: 
+
 Filename: scribe_data/load/update_data.py
 Comment: 
 
 Filename: scribe_data/load/update_utils.py
 Comment: 
 
-Filename: scribe_data-2.2.2.data/data/requirements.txt
+Filename: scribe_data-3.0.0.data/data/requirements.txt
 Comment: 
 
-Filename: scribe_data-2.2.2.dist-info/LICENSE.txt
+Filename: scribe_data-3.0.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: scribe_data-2.2.2.dist-info/METADATA
+Filename: scribe_data-3.0.0.dist-info/METADATA
 Comment: 
 
-Filename: scribe_data-2.2.2.dist-info/WHEEL
+Filename: scribe_data-3.0.0.dist-info/WHEEL
 Comment: 
 
-Filename: scribe_data-2.2.2.dist-info/top_level.txt
+Filename: scribe_data-3.0.0.dist-info/top_level.txt
 Comment: 
 
-Filename: scribe_data-2.2.2.dist-info/RECORD
+Filename: scribe_data-3.0.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## scribe_data/extract_transform/process_unicode.py

```diff
@@ -1,41 +1,47 @@
 """
 Process Unicode
-------------
+---------------
 
 Module for processing Unicode based corpuses for autosuggestion and autocompletion generation.
 
 Contents:
-    gen_emoji_keywords
+    gen_emoji_lexicon
 """
 
+
 import csv
+import fileinput
 import json
+import re
 from importlib.resources import files
 
 import emoji
 from icu import Char, UProperty
 from tqdm.auto import tqdm
 
+from scribe_data.extract_transform.emoji_utils import get_emoji_codes_to_ignore
 from scribe_data.load.update_utils import (
+    add_num_commas,
     get_language_iso,
-    get_path_from_process_unicode,
-    get_path_from_update_data,
+    get_path_from_et_dir,
 )
 
 from . import _resources
 
+emoji_codes_to_ignore = get_emoji_codes_to_ignore()
+
 
-def gen_emoji_keywords(
+def gen_emoji_lexicon(
     language="English",
     num_emojis=None,
     emojis_per_keyword=None,
     ignore_keywords=None,
     export_base_rank=False,
-    update_scribe_apps=False,
+    update_local_data=False,
     verbose=True,
 ):
     """
     Generates a dictionary of keywords (keys) and emoji unicode(s) associated with them (values).
 
     Parameters
     ----------
@@ -50,16 +56,16 @@
 
         ignore_keywords : str or list (default=None)
             Keywords that should be ignored.
 
         export_base_rank : bool (default=False)
             Whether to export whether the emojis is a base character as well as its rank.
 
-        update_scribe_apps : bool (default=False)
-            Saves the created dictionaries as JSONs in Scribe app directories.
+        update_local_data : bool (default=False)
+            Saves the created dictionaries as JSONs in the local formatted_data directories.
 
         verbose : bool (default=True)
             Whether to show a tqdm progress bar for the process.
 
     Returns
     -------
         Keywords dictionary for emoji keywords-to-unicode are saved locally or uploaded to Scribe apps.
@@ -82,15 +88,20 @@
     popularity_dict = {}
 
     with files(_resources).joinpath("2021_ranked.tsv").open() as popularity_file:
         tsv_reader = csv.DictReader(popularity_file, delimiter="\t")
         for tsv_row in tsv_reader:
             popularity_dict[tsv_row["Emoji"]] = int(tsv_row["Rank"])
 
-    path_to_scribe_org = get_path_from_process_unicode()
+    # Pre-set up handling flags and tags (subdivision flags).
+    # emoji_flags = Char.getBinaryPropertySet(UProperty.RGI_EMOJI_FLAG_SEQUENCE)
+    # emoji_tags = Char.getBinaryPropertySet(UProperty.RGI_EMOJI_TAG_SEQUENCE)
+    # regexp_flag_keyword = re.compile(r".*\: (?P<flag_keyword>.*)")
+
+    path_to_scribe_org = get_path_from_et_dir()
     annotations_file_path = f"{path_to_scribe_org}/Scribe-Data/node_modules/cldr-annotations-full/annotations/{iso}/annotations.json"
     annotations_derived_file_path = f"{path_to_scribe_org}/Scribe-Data/node_modules/cldr-annotations-derived-full/annotationsDerived/{iso}/annotations.json"
 
     cldr_file_paths = {
         "annotations": annotations_file_path,
         "annotationsDerived": annotations_derived_file_path,
     }
@@ -103,16 +114,19 @@
 
         for cldr_char in tqdm(
             iterable=cldr_dict,
             desc=f"Characters processed from '{cldr_file_key}' CLDR file for {language}",
             unit="cldr characters",
             disable=not verbose,
         ):
-            # Filter CLDR data for emoji characters.
-            if cldr_char in emoji.EMOJI_DATA:
+            # Filter CLDR data for emoji characters while not including certain emojis.
+            if (
+                cldr_char in emoji.EMOJI_DATA
+                and cldr_char.encode("utf-8") not in emoji_codes_to_ignore
+            ):
                 emoji_rank = popularity_dict.get(cldr_char)
 
                 # If number limit specified, filter for the highest-ranked emojis.
                 if num_emojis and (emoji_rank is None or emoji_rank > num_emojis):
                     continue
 
                 # Process for emoji variants.
@@ -126,14 +140,28 @@
                 # See www.unicode.org/reports/tr51/#Emoji_Implementation_Notes.
                 if (
                     emoji.EMOJI_DATA[cldr_char]["status"]
                     == emoji.STATUS["fully_qualified"]
                 ):
                     emoji_annotations = cldr_dict[cldr_char]
 
+                    # # Process for flag keywords.
+                    # if cldr_char in emoji_flags or cldr_char in emoji_tags:
+                    #     flag_keyword_match = regexp_flag_keyword.match(
+                    #         emoji_annotations["tts"][0]
+                    #     )
+                    #     flag_keyword = flag_keyword_match.group("flag_keyword")
+                    #     keyword_dict.setdefault(flag_keyword, []).append(
+                    #         {
+                    #             "emoji": cldr_char,
+                    #             "is_base": has_modifier_base,
+                    #             "rank": emoji_rank,
+                    #         }
+                    #     )
+
                     for emoji_keyword in emoji_annotations["default"]:
                         emoji_keyword = emoji_keyword.lower()  # lower case the key
                         if (
                             # Use single-word annotations as keywords.
                             len(emoji_keyword.split()) == 1
                             and emoji_keyword not in keywords_to_ignore
                         ):
@@ -141,42 +169,89 @@
                                 {
                                     "emoji": cldr_char,
                                     "is_base": has_modifier_base,
                                     "rank": emoji_rank,
                                 }
                             )
 
+    # Check nouns files for plurals and update their data with the emojis for their singular forms.
+    with open(f"./{language}/formatted_data/nouns.json", encoding="utf-8") as f:
+        noun_data = json.load(f)
+
+    plurals_to_singulars_dict = {
+        noun_data[row]["plural"].lower(): row.lower()
+        for row in noun_data
+        if noun_data[row]["plural"] != "isPlural"
+    }
+
+    for plural, singular in plurals_to_singulars_dict.items():
+        if plural not in keyword_dict and singular in keyword_dict:
+            keyword_dict[plural] = keyword_dict[singular]
+
     # Sort by rank after all emojis already found per keyword.
-    for keywords in keyword_dict.values():
-        keywords.sort(
+    for emojis in keyword_dict.values():
+        emojis.sort(
             key=lambda suggestion: float("inf")
             if suggestion["rank"] is None
             else suggestion["rank"]
         )
 
         # If specified, enforce limit of emojis per keyword.
-        if emojis_per_keyword and len(keywords) > emojis_per_keyword:
-            keywords[:] = keywords[:emojis_per_keyword]
+        if emojis_per_keyword and len(emojis) > emojis_per_keyword:
+            emojis[:] = emojis[:emojis_per_keyword]
+
+    total_keywords = add_num_commas(num=len(keyword_dict))
 
     if verbose:
         print(
-            f"Number of emoji trigger keywords found for {language}: {len(keyword_dict)}"
+            f"Number of emoji trigger keywords found for {language}: {total_keywords}"
         )
 
     # Remove base status and rank if not needed.
     if not export_base_rank:
         keyword_dict = {
-            k: [{"emoji": emoji_options["emoji"]} for emoji_options in v]
+            k: [{"emoji": emoji_keys["emoji"]} for emoji_keys in v]
             for k, v in keyword_dict.items()
         }
 
-    if update_scribe_apps:
-        output_path = f"{get_path_from_update_data()}/Scribe-iOS/Keyboards/LanguageKeyboards/{language.capitalize()}/Data/emoji_keywords.json"
-    else:
-        output_path = f"{language.lower()}_emoji_keywords.json"
+    if update_local_data:
+        path_to_formatted_data = (
+            get_path_from_et_dir()
+            + f"/Scribe-Data/src/scribe_data/extract_transform/{language.capitalize()}/formatted_data/emoji_keywords.json"
+        )
+
+        with open(path_to_formatted_data, "w", encoding="utf-8") as file:
+            json.dump(keyword_dict, file, ensure_ascii=False, indent=0)
+
+        print(
+            f"Emoji keywords for {language} generated and saved to '{path_to_formatted_data}'."
+        )
+
+        path_to_data_table = (
+            get_path_from_et_dir()
+            + "/Scribe-Data/src/scribe_data/load/_update_files/data_table.txt"
+        )
+
+        for line in fileinput.input(path_to_data_table, inplace=True):
+            if line.split("|")[1].strip() == language.capitalize():
+                line = (
+                    "|".join(line.split("|")[:-2])
+                    + "|"
+                    + total_keywords.rjust(len(" Emoji Keywords") - 1, " ")
+                    + " |\n"
+                )
+
+            print(line, end="")
+
+        path_to_total_data = (
+            get_path_from_et_dir()
+            + "/Scribe-Data/src/scribe_data/load/_update_files/total_data.json"
+        )
 
-    with open(output_path, "w", encoding="utf-8") as file:
-        json.dump(keyword_dict, file, ensure_ascii=False, indent=0)
+        with open(path_to_total_data, encoding="utf-8") as f:
+            current_data = json.load(f)
 
-    print(f"Emoji keywords for {language} generated and saved to '{output_path}'.")
+        current_data[language.capitalize()]["emoji_keywords"] = len(keyword_dict)
+        with open(path_to_total_data, "w+", encoding="utf-8") as f:
+            json.dump(current_data, f, ensure_ascii=False, indent=0)
 
     return keyword_dict
```

## scribe_data/extract_transform/process_wiki.py

```diff
@@ -23,15 +23,15 @@
 
 from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
     add_num_commas,
     get_ios_data_path,
     get_language_qid,
     get_language_words_to_ignore,
     get_language_words_to_remove,
-    get_path_from_process_wiki,
+    get_path_from_et_dir,
 )
 
 warnings.filterwarnings("ignore", message=r"Passing", category=FutureWarning)
 
 # Set SPARQLWrapper query conditions.
 sparql = SPARQLWrapper("https://query.wikidata.org/sparql")
 sparql.setReturnFormat(JSON)
@@ -311,15 +311,15 @@
 
 
 def gen_autosuggestions(
     text_corpus,
     language="English",
     num_words=500,
     ignore_words=None,
-    update_scribe_apps=False,
+    update_local_data=False,
     verbose=True,
 ):
     """
     Generates a dictionary of common words (keys) and those that most commonly follow them (values).
 
     Parameters
     ----------
@@ -331,16 +331,16 @@
 
         num_words: int (default=500)
             The number of words that autosuggestions should be generated for.
 
         ignore_words : str or list (default=None)
             Strings that should be removed from the text body.
 
-        update_scribe_apps : bool (default=False)
-            Saves the created dictionaries as JSONs in Scribe app directories.
+        update_local_data : bool (default=False)
+            Saves the created dictionaries as JSONs in the local formatted_data directories.
 
         verbose : bool (default=True)
             Whether to show a tqdm progress bar for the process.
 
     Returns
     -------
         Autosuggestions dictionaries for common words are saved locally or uploaded to Scribe apps.
@@ -353,15 +353,15 @@
         words_to_ignore = [ignore_words]
     elif ignore_words is None:
         words_to_ignore = []
     words_to_ignore += get_language_words_to_ignore(language)
 
     print("Querying profanities to remove from suggestions.")
     # First format the lines into a multi-line string and then pass this to SPARQLWrapper.
-    with open("../extract_transform/query_profanity.sparql", encoding="utf-8") as file:
+    with open("./query_profanity.sparql", encoding="utf-8") as file:
         query_lines = file.readlines()
 
     query = "".join(query_lines).replace(
         "LANGUAGE_QID", get_language_qid(language=language)
     )
     sparql.setQuery(query)
 
@@ -418,29 +418,23 @@
                 break
 
         autosuggest_dict[w] = autosuggestions
 
     if not verbose:
         print(f"Autosuggestions for {language} generated.")
 
-    if update_scribe_apps:
-        # Get paths to load formatted data into.
-        path_to_scribe_org = get_path_from_process_wiki()
-        ios_data_dir_from_org = get_ios_data_path(language, "autosuggestions")
-        # android_data_dir_from_org = get_android_data_path(language, "autosuggestions")
-        # desktop_data_dir_from_org = get_desktop_data_path(language, "autosuggestions")
-
-        ios_output_path = f"{path_to_scribe_org}{ios_data_dir_from_org}"
-        # android_output_path = f"{path_to_scribe_org}{android_data_dir_from_org}"
-        # desktop_output_path = f"{path_to_scribe_org}{desktop_data_dir_from_org}"
-
-        all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-        for output_path in all_output_paths:
-            with open(output_path, "w", encoding="utf-8",) as file:
-                json.dump(autosuggest_dict, file, ensure_ascii=False, indent=0)
+    if update_local_data:
+        path_to_formatted_data = (
+            get_path_from_et_dir()
+            + f"/Scribe-Data/src/scribe_data/extract_transform/{language.capitalize()}/formatted_data/autosuggestions.json"
+        )
+
+        with open(path_to_formatted_data, "w", encoding="utf-8",) as file:
+            json.dump(autosuggest_dict, file, ensure_ascii=False, indent=0)
 
-        print(f"Autosuggestions for {language} saved to Scribe app directories.")
+        print(
+            f"Autosuggestions for {language} generated and saved to '{path_to_formatted_data}'."
+        )
 
         return autosuggest_dict
 
     return autosuggest_dict
```

## scribe_data/extract_transform/French/nouns/format_nouns.py

```diff
@@ -2,79 +2,39 @@
 Format Nouns
 ------------
 
 Formats the nouns queried from Wikidata using query_nouns.sparql.
 """
 
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "French"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/nouns/" not in file_path:
     with open("nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 
-# Get paths to load formatted data into.
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "nouns")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "nouns")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "nouns")
-
-path_from_file = get_path_from_format_file()
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    path_from_file = get_path_from_update_data()
-    ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 
 def map_genders(wikidata_gender):
     """
     Maps those genders from Wikidata to succinct versions.
     """
     if wikidata_gender in ["masculine", "Q499327"]:
         return "M"
@@ -160,12 +120,16 @@
             )
 
 for k in nouns_formatted:
     nouns_formatted[k]["form"] = order_annotations(nouns_formatted[k]["form"])
 
 nouns_formatted = collections.OrderedDict(sorted(nouns_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/nouns.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/nouns.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file nouns.json with {len(nouns_formatted)} nouns.")
```

## scribe_data/extract_transform/French/translations/format_translations.py

```diff
@@ -9,15 +9,15 @@
 
 import collections
 import json
 
 from tqdm.auto import tqdm
 from transformers import MarianMTModel, MarianTokenizer
 
-with open("../../translations_queried.json", encoding="utf-8") as f:
+with open("../../_translations/translations_queried.json", encoding="utf-8") as f:
     translations_list = json.load(f)
 
 words = [translation_vals["word"] for translation_vals in translations_list]
 words = list(set(words))
 
 translations_formatted = {}
 
@@ -29,15 +29,11 @@
     translated = model.generate(**tokenizer(w, return_tensors="pt", padding=True))
     translations_formatted[w] = tokenizer.decode(
         translated[0], skip_special_tokens=True
     )
 
 translations_formatted = collections.OrderedDict(sorted(translations_formatted.items()))
 
-with open(
-    "../../../Keyboards/LanguageKeyboards/French/Data/translations.json",
-    "w",
-    encoding="utf-8",
-) as f:
+with open("../formatted_data/translations.json", "w", encoding="utf-8",) as f:
     json.dump(translations_formatted, f, ensure_ascii=False, indent=0)
 
 print(f"Wrote file translations.json with {len(translations_formatted)} translations.")
```

## scribe_data/extract_transform/French/verbs/format_verbs.py

```diff
@@ -1,83 +1,42 @@
 """
 Format Verbs
 ------------
 
 Formats the verbs queried from Wikidata using query_verbs.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "French"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/verbs/" not in file_path:
     with open("verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "verbs")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "verbs")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "verbs")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 verbs_formatted = {}
 
-all_keys = [
-    "infinitive",
+all_conjugations = [
     "presFPS",
     "presSPS",
     "presTPS",
     "presFPP",
     "presSPP",
     "presTPP",
     "pretFPS",
@@ -100,25 +59,29 @@
     "futTPP",
 ]
 
 for verb_vals in verbs_list:
     if verb_vals["infinitive"] not in verbs_formatted:
         verbs_formatted[verb_vals["infinitive"]] = {}
 
-        for conj in [c for c in all_keys if c != "infinitive"]:
+        for conj in all_conjugations:
             if conj in verb_vals.keys():
                 verbs_formatted[verb_vals["infinitive"]][conj] = verb_vals[conj]
             else:
                 verbs_formatted[verb_vals["infinitive"]][conj] = ""
 
     else:
-        for conj in [c for c in all_keys if c != "infinitive"]:
+        for conj in all_conjugations:
             if conj in verb_vals.keys():
                 verbs_formatted[verb_vals["infinitive"]][conj] = verb_vals[conj]
 
 verbs_formatted = collections.OrderedDict(sorted(verbs_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/verbs.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/verbs.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file verbs.json with {len(verbs_formatted)} verbs.")
```

## scribe_data/extract_transform/German/nouns/format_nouns.py

```diff
@@ -1,79 +1,39 @@
 """
 Format Nouns
 ------------
 
 Formats the nouns queried from Wikidata using query_nouns.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "German"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/nouns/" not in file_path:
     with open("nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "nouns")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "nouns")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "nouns")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 
 def map_genders(wikidata_gender):
     """
     Maps those genders from Wikidata to succinct versions.
 
     Parameters
     ----------
@@ -228,12 +188,16 @@
                 nouns_formatted[noun_vals["plural"]]["form"] = "PL"
 
 for k in nouns_formatted:
     nouns_formatted[k]["form"] = order_annotations(nouns_formatted[k]["form"])
 
 nouns_formatted = collections.OrderedDict(sorted(nouns_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/nouns.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/nouns.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file nouns.json with {len(nouns_formatted)} nouns.")
```

## scribe_data/extract_transform/German/prepositions/format_prepositions.py

```diff
@@ -1,80 +1,41 @@
 """
 Format Prepositions
 -------------------
 
 Formats the prepositions queried from Wikidata using query_prepositions.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "German"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/prepositions/" not in file_path:
     with open("prepositions_queried.json", encoding="utf-8") as f:
         prepositions_list = json.load(f)
 else:
     update_data_in_use = True
     with open(
-        f"../extract_transform/{LANGUAGE}/prepositions/prepositions_queried.json",
-        encoding="utf-8",
+        f"./{LANGUAGE}/prepositions/prepositions_queried.json", encoding="utf-8",
     ) as f:
         prepositions_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "prepositions")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "prepositions")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "prepositions")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 
 def convert_cases(case):
     """
     Converts cases as found on Wikidata to more succinct versions.
     """
     case = case.split(" case")[0]
     if case in ["accusative", "Q146078"]:
@@ -126,12 +87,16 @@
             prepositions_formatted[prep_vals["preposition"]] = ""
 
 for k in prepositions_formatted:
     prepositions_formatted[k] = order_annotations(prepositions_formatted[k])
 
 prepositions_formatted = collections.OrderedDict(sorted(prepositions_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(prepositions_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/prepositions.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/prepositions.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(prepositions_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file prepositions.json with {len(prepositions_formatted)} prepositions.")
```

## scribe_data/extract_transform/German/translations/format_translations.py

```diff
@@ -9,15 +9,15 @@
 
 import collections
 import json
 
 from tqdm.auto import tqdm
 from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
 
-with open("../../translations_queried.json", encoding="utf-8") as f:
+with open("../../_translations/translations_queried.json", encoding="utf-8") as f:
     translations_list = json.load(f)
 
 words = [translation_vals["word"] for translation_vals in translations_list]
 words = list(set(words))
 
 translations_formatted = {}
 
@@ -29,15 +29,11 @@
     inputs = tokenizer.encode(f"translate English to German: {w}", return_tensors="pt")
     outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)
 
     translations_formatted[w] = tokenizer.decode(outputs[0], skip_special_tokens=True)
 
 translations_formatted = collections.OrderedDict(sorted(translations_formatted.items()))
 
-with open(
-    "../../../Keyboards/LanguageKeyboards/German/Data/translations.json",
-    "w",
-    encoding="utf-8",
-) as f:
+with open("../formatted_data/translations.json", "w", encoding="utf-8",) as f:
     json.dump(translations_formatted, f, ensure_ascii=False, indent=0)
 
 print(f"Wrote file translations.json with {len(translations_formatted)} translations.")
```

## scribe_data/extract_transform/German/verbs/format_verbs.py

```diff
@@ -1,175 +1,166 @@
 """
 Format Verbs
 ------------
 
 Formats the verbs queried from Wikidata using query_verbs.sparql.
+
+Attn: The formatting in the file is significantly more complex than for other verbs.
+    - We have two queries: query_verbs_1 and query_verbs_2.
+    - For the second query we could get two different auxiliary verbs (could be sein and haben).
+    - We thus need to get the results for the first and then check if we need to combine the second.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "German"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/verbs/" not in file_path:
     with open("verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "verbs")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "verbs")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "verbs")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 verbs_formatted = {}
 
-all_keys = [
-    "pastParticiple",
-    "auxiliaryVerb",
+# Note: The following are combined later: perfFPS, perfSPS, perfTPS, perfFPP, perfSPP, perfTPP
+all_query_1_conjugations = [
     "presFPS",
     "presSPS",
     "presTPS",
     "presFPP",
     "presSPP",
     "presTPP",
+]
+
+all_query_2_conjugations = [
+    "pastParticiple",
+    "auxiliaryVerb",
     "pretFPS",
     "pretSPS",
     "pretTPS",
     "pretFPP",
     "pretSPP",
     "pretTPP",
-    "perfFPS",
-    "perfSPS",
-    "perfTPS",
-    "perfFPP",
-    "perfSPP",
-    "perfTPP",
 ]
 
 
 def assign_past_participle(verb, tense):
     """
     Assigns the past participle after the auxiliary verb or by itself.
     """
-    if verbs_formatted[verb["infinitive"]][tense] not in ["", verb["pastParticiple"]]:
-        verbs_formatted[verb["infinitive"]][tense] += " " + verb["pastParticiple"]
+    if verbs_formatted[verb][tense] == "":
+        verbs_formatted[verb][tense] = verbs_formatted[verb]["pastParticiple"]
     else:
-        verbs_formatted[verb["infinitive"]][tense] = verb["pastParticiple"]
+        verbs_formatted[verb][tense] += f" {verbs_formatted[verb]['pastParticiple']}"
 
 
 for verb_vals in verbs_list:
-    if (
-        "infinitive" in verb_vals.keys()
-        and verb_vals["infinitive"] not in verbs_formatted
-    ):
-        non_infinitive_conjugations = {
-            k: v for k, v in verb_vals.items() if k != "infinitive"
-        }
-        verbs_formatted[verb_vals["infinitive"]] = non_infinitive_conjugations
+    if verb_vals["infinitive"] not in verbs_formatted.keys():
+        verbs_formatted[verb_vals["infinitive"]] = {}
 
-        for k in all_keys:
-            if k not in verbs_formatted[verb_vals["infinitive"]].keys():
+    # Note: query_verbs_1 result - we want all values.
+    if "auxiliaryVerb" not in verb_vals.keys():
+        for k in all_query_1_conjugations:
+            if k in verb_vals.keys():
+                verbs_formatted[verb_vals["infinitive"]][k] = verb_vals[k]
+            else:
                 verbs_formatted[verb_vals["infinitive"]][k] = ""
 
-        if "auxiliaryVerb" in verb_vals.keys():
-            # Sein
-            if verb_vals["auxiliaryVerb"] == "L1761":
-                verbs_formatted[verb_vals["infinitive"]]["auxiliaryVerb"] = "sein"
-
-                verbs_formatted[verb_vals["infinitive"]]["perfFPS"] += "bin"
-                verbs_formatted[verb_vals["infinitive"]]["perfSPS"] += "bist"
-                verbs_formatted[verb_vals["infinitive"]]["perfTPS"] += "ist"
-                verbs_formatted[verb_vals["infinitive"]]["perfFPP"] += "sind"
-                verbs_formatted[verb_vals["infinitive"]]["perfSPP"] += "seid"
-                verbs_formatted[verb_vals["infinitive"]]["perfTPP"] += "sind"
-
-            # Haben
-            elif verb_vals["auxiliaryVerb"] == "L4179":
-                verbs_formatted[verb_vals["infinitive"]]["auxiliaryVerb"] = "haben"
-
-                verbs_formatted[verb_vals["infinitive"]]["perfFPS"] += "habe"
-                verbs_formatted[verb_vals["infinitive"]]["perfSPS"] += "hast"
-                verbs_formatted[verb_vals["infinitive"]]["perfTPS"] += "hat"
-                verbs_formatted[verb_vals["infinitive"]]["perfFPP"] += "haben"
-                verbs_formatted[verb_vals["infinitive"]]["perfSPP"] += "habt"
-                verbs_formatted[verb_vals["infinitive"]]["perfTPP"] += "haben"
-
-    # The verb has two entries and thus has forms with both sein and haben.
+    # Note: query_verbs_2 first time seeing verb - we want all values.
     elif (
         "auxiliaryVerb" in verb_vals.keys()
-        and verbs_formatted[verb_vals["infinitive"]]["auxiliaryVerb"]
-        != verb_vals["auxiliaryVerb"]
+        and "auxiliaryVerb" not in verbs_formatted[verb_vals["infinitive"]].keys()
     ):
-        verbs_formatted[verb_vals["infinitive"]]["auxiliaryVerb"] = "sein/haben"
+        for k in all_query_2_conjugations:
+            if k in verb_vals.keys():
+                verbs_formatted[verb_vals["infinitive"]][k] = verb_vals[k]
+            else:
+                verbs_formatted[verb_vals["infinitive"]][k] = ""
 
-        verbs_formatted[verb_vals["infinitive"]]["perfFPS"] = "bin/habe"
-        verbs_formatted[verb_vals["infinitive"]]["perfSPS"] = "bist/hast"
-        verbs_formatted[verb_vals["infinitive"]]["perfTPS"] = "ist/hat"
-        verbs_formatted[verb_vals["infinitive"]]["perfFPP"] = "sind/haben"
-        verbs_formatted[verb_vals["infinitive"]]["perfSPP"] = "seid/habt"
-        verbs_formatted[verb_vals["infinitive"]]["perfTPP"] = "sind/haben"
-
-    if "pastParticiple" in verb_vals.keys():
-        assign_past_participle(verb=verb_vals, tense="perfFPS")
-        assign_past_participle(verb=verb_vals, tense="perfSPS")
-        assign_past_participle(verb=verb_vals, tense="perfTPS")
-        assign_past_participle(verb=verb_vals, tense="perfFPP")
-        assign_past_participle(verb=verb_vals, tense="perfSPP")
-        assign_past_participle(verb=verb_vals, tense="perfTPP")
+        # Note: Sein
+        if verb_vals["auxiliaryVerb"] == "L1761":
+            verbs_formatted[verb_vals["infinitive"]]["auxiliaryVerb"] = "sein"
+
+            verbs_formatted[verb_vals["infinitive"]]["perfFPS"] = "bin"
+            verbs_formatted[verb_vals["infinitive"]]["perfSPS"] = "bist"
+            verbs_formatted[verb_vals["infinitive"]]["perfTPS"] = "ist"
+            verbs_formatted[verb_vals["infinitive"]]["perfFPP"] = "sind"
+            verbs_formatted[verb_vals["infinitive"]]["perfSPP"] = "seid"
+            verbs_formatted[verb_vals["infinitive"]]["perfTPP"] = "sind"
+
+        # Note: Haben
+        elif verb_vals["auxiliaryVerb"] == "L4179":
+            verbs_formatted[verb_vals["infinitive"]]["auxiliaryVerb"] = "haben"
+
+            verbs_formatted[verb_vals["infinitive"]]["perfFPS"] = "habe"
+            verbs_formatted[verb_vals["infinitive"]]["perfSPS"] = "hast"
+            verbs_formatted[verb_vals["infinitive"]]["perfTPS"] = "hat"
+            verbs_formatted[verb_vals["infinitive"]]["perfFPP"] = "haben"
+            verbs_formatted[verb_vals["infinitive"]]["perfSPP"] = "habt"
+            verbs_formatted[verb_vals["infinitive"]]["perfTPP"] = "haben"
+
+        # Note: No auxiliaryVerb for this verb.
+        elif verb_vals["auxiliaryVerb"] == "":
+            verbs_formatted[verb_vals["infinitive"]]["perfFPS"] = ""
+            verbs_formatted[verb_vals["infinitive"]]["perfSPS"] = ""
+            verbs_formatted[verb_vals["infinitive"]]["perfTPS"] = ""
+            verbs_formatted[verb_vals["infinitive"]]["perfFPP"] = ""
+            verbs_formatted[verb_vals["infinitive"]]["perfSPP"] = ""
+            verbs_formatted[verb_vals["infinitive"]]["perfTPP"] = ""
+
+    # Note: query_verbs_2 second time seeing verb.
+    elif (
+        "auxiliaryVerb" in verb_vals.keys()
+        and "auxiliaryVerb" in verbs_formatted[verb_vals["infinitive"]].keys()
+    ):
+        # Note: Neither is "" and they're not the same, so we have the same verb with two different auxiliaries.
+        if (
+            verbs_formatted[verb_vals["infinitive"]]["auxiliaryVerb"] != ""
+            and verb_vals["auxiliaryVerb"] != ""
+        ) and (
+            verbs_formatted[verb_vals["infinitive"]]["auxiliaryVerb"]
+            != verb_vals["auxiliaryVerb"]
+        ):
+            verbs_formatted[verb_vals["infinitive"]]["auxiliaryVerb"] = "sein/haben"
+
+            verbs_formatted[verb_vals["infinitive"]]["perfFPS"] = "bin/habe"
+            verbs_formatted[verb_vals["infinitive"]]["perfSPS"] = "bist/hast"
+            verbs_formatted[verb_vals["infinitive"]]["perfTPS"] = "ist/hat"
+            verbs_formatted[verb_vals["infinitive"]]["perfFPP"] = "sind/haben"
+            verbs_formatted[verb_vals["infinitive"]]["perfSPP"] = "seid/habt"
+            verbs_formatted[verb_vals["infinitive"]]["perfTPP"] = "sind/haben"
+
+for k in verbs_formatted.keys():
+    assign_past_participle(verb=k, tense="perfFPS")
+    assign_past_participle(verb=k, tense="perfSPS")
+    assign_past_participle(verb=k, tense="perfTPS")
+    assign_past_participle(verb=k, tense="perfFPP")
+    assign_past_participle(verb=k, tense="perfSPP")
+    assign_past_participle(verb=k, tense="perfTPP")
 
 verbs_formatted = collections.OrderedDict(sorted(verbs_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/verbs.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/verbs.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file verbs.json with {len(verbs_formatted)} verbs.")
```

## scribe_data/extract_transform/Italian/nouns/format_nouns.py

```diff
@@ -1,79 +1,39 @@
 """
 Format Nouns
 ------------
 
 Formats the nouns queried from Wikidata using query_nouns.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Italian"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/nouns/" not in file_path:
     with open("nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "nouns")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "nouns")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "nouns")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 
 def map_genders(wikidata_gender):
     """
     Maps those genders from Wikidata to succinct versions.
     """
     if wikidata_gender in ["masculine", "Q499327"]:
         return "M"
@@ -160,12 +120,16 @@
                 )
 
 for k in nouns_formatted:
     nouns_formatted[k]["form"] = order_annotations(nouns_formatted[k]["form"])
 
 nouns_formatted = collections.OrderedDict(sorted(nouns_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/nouns.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/nouns.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file nouns.json with {len(nouns_formatted)} nouns.")
```

## scribe_data/extract_transform/Italian/translations/format_translations.py

```diff
@@ -9,15 +9,15 @@
 
 import collections
 import json
 
 from tqdm.auto import tqdm
 from transformers import MarianMTModel, MarianTokenizer
 
-with open("../../translations_queried.json", encoding="utf-8") as f:
+with open("../../_translations/translations_queried.json", encoding="utf-8") as f:
     translations_list = json.load(f)
 
 words = [translation_vals["word"] for translation_vals in translations_list]
 words = list(set(words))
 
 translations_formatted = {}
 
@@ -31,15 +31,11 @@
     )
     translations_formatted[w] = tokenizer.decode(
         translated[0], skip_special_tokens=True
     )
 
 translations_formatted = collections.OrderedDict(sorted(translations_formatted.items()))
 
-with open(
-    "../../../Keyboards/LanguageKeyboards/Italian/Data/translations.json",
-    "w",
-    encoding="utf-8",
-) as f:
+with open("../formatted_data/translations.json", "w", encoding="utf-8",) as f:
     json.dump(translations_formatted, f, ensure_ascii=False, indent=0)
 
 print(f"Wrote file translations.json with {len(translations_formatted)} translations.")
```

## scribe_data/extract_transform/Italian/verbs/format_verbs.py

```diff
@@ -1,83 +1,42 @@
 """
 Format Verbs
 ------------
 
 Formats the verbs queried from Wikidata using query_verbs.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Italian"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/verbs/" not in file_path:
     with open("verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "verbs")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "verbs")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "verbs")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 verbs_formatted = {}
 
-all_keys = [
-    "infinitive",
+all_conjugations = [
     "presFPS",
     "presSPS",
     "presTPS",
     "presFPP",
     "presSPP",
     "presTPP",
     "pretFPS",
@@ -93,20 +52,24 @@
     "impSPP",
     "impTPP",
 ]
 
 for verb_vals in verbs_list:
     verbs_formatted[verb_vals["infinitive"]] = {}
 
-    for conj in [c for c in all_keys if c != "infinitive"]:
+    for conj in all_conjugations:
         if conj in verb_vals.keys():
             verbs_formatted[verb_vals["infinitive"]][conj] = verb_vals[conj]
         else:
             verbs_formatted[verb_vals["infinitive"]][conj] = ""
 
 verbs_formatted = collections.OrderedDict(sorted(verbs_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/verbs.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/verbs.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file verbs.json with {len(verbs_formatted)} verbs.")
```

## scribe_data/extract_transform/Portuguese/nouns/format_nouns.py

```diff
@@ -1,79 +1,39 @@
 """
 Format Nouns
 ------------
 
 Formats the nouns queried from Wikidata using query_nouns.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Portuguese"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/nouns/" not in file_path:
     with open("nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "nouns")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "nouns")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "nouns")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 
 def map_genders(wikidata_gender):
     """
     Maps those genders from Wikidata to succinct versions.
     """
     if wikidata_gender in ["masculine", "Q499327"]:
         return "M"
@@ -160,12 +120,16 @@
                 )
 
 for k in nouns_formatted:
     nouns_formatted[k]["form"] = order_annotations(nouns_formatted[k]["form"])
 
 nouns_formatted = collections.OrderedDict(sorted(nouns_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/nouns.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/nouns.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file nouns.json with {len(nouns_formatted)} nouns.")
```

## scribe_data/extract_transform/Portuguese/translations/format_translations.py

```diff
@@ -9,15 +9,15 @@
 
 import collections
 import json
 
 from tqdm.auto import tqdm
 from transformers import MarianMTModel, MarianTokenizer
 
-with open("../../translations_queried.json", encoding="utf-8") as f:
+with open("../../_translations/translations_queried.json", encoding="utf-8") as f:
     translations_list = json.load(f)
 
 words = [translation_vals["word"] for translation_vals in translations_list]
 words = list(set(words))
 
 translations_formatted = {}
 
@@ -31,15 +31,11 @@
     )
     translations_formatted[w] = tokenizer.decode(
         translated[0], skip_special_tokens=True
     )
 
 translations_formatted = collections.OrderedDict(sorted(translations_formatted.items()))
 
-with open(
-    "../../../Keyboards/LanguageKeyboards/Portuguese/Data/translations.json",
-    "w",
-    encoding="utf-8",
-) as f:
+with open("../formatted_data/translations.json", "w", encoding="utf-8",) as f:
     json.dump(translations_formatted, f, ensure_ascii=False, indent=0)
 
 print(f"Wrote file translations.json with {len(translations_formatted)} translations.")
```

## scribe_data/extract_transform/Portuguese/verbs/format_verbs.py

```diff
@@ -1,83 +1,42 @@
 """
 Format Verbs
 ------------
 
 Formats the verbs queried from Wikidata using query_verbs.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Portuguese"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/verbs/" not in file_path:
     with open("verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "verbs")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "verbs")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "verbs")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 verbs_formatted = {}
 
-all_keys = [
-    "infinitive",
+all_conjugations = [
     "presFPS",
     "presSPS",
     "presTPS",
     "presFPP",
     "presSPP",
     "presTPP",
     "perfFPS",
@@ -99,20 +58,24 @@
     "fSimpSPP",
     "fSimpTPP",
 ]
 
 for verb_vals in verbs_list:
     verbs_formatted[verb_vals["infinitive"]] = {}
 
-    for conj in [c for c in all_keys if c != "infinitive"]:
+    for conj in all_conjugations:
         if conj in verb_vals.keys():
             verbs_formatted[verb_vals["infinitive"]][conj] = verb_vals[conj]
         else:
             verbs_formatted[verb_vals["infinitive"]][conj] = ""
 
 verbs_formatted = collections.OrderedDict(sorted(verbs_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/verbs.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/verbs.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file verbs.json with {len(verbs_formatted)} verbs.")
```

## scribe_data/extract_transform/Russian/nouns/format_nouns.py

```diff
@@ -1,79 +1,39 @@
 """
 Format Nouns
 ------------
 
 Formats the nouns queried from Wikidata using query_nouns.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Russian"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/nouns/" not in file_path:
     with open("nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "nouns")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "nouns")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "nouns")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 
 def map_genders(wikidata_gender):
     """
     Maps those genders from Wikidata to succinct versions.
 
     Parameters
     ----------
@@ -228,12 +188,16 @@
                 nouns_formatted[noun_vals["plural"]]["form"] = "PL"
 
 for k in nouns_formatted:
     nouns_formatted[k]["form"] = order_annotations(nouns_formatted[k]["form"])
 
 nouns_formatted = collections.OrderedDict(sorted(nouns_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/nouns.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/nouns.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file nouns.json with {len(nouns_formatted)} nouns.")
```

## scribe_data/extract_transform/Russian/prepositions/format_prepositions.py

```diff
@@ -1,80 +1,41 @@
 """
 Format Prepositions
 -------------------
 
 Formats the prepositions queried from Wikidata using query_prepositions.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Russian"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/prepositions/" not in file_path:
     with open("prepositions_queried.json", encoding="utf-8") as f:
         prepositions_list = json.load(f)
 else:
     update_data_in_use = True
     with open(
-        f"../extract_transform/{LANGUAGE}/prepositions/prepositions_queried.json",
-        encoding="utf-8",
+        f"./{LANGUAGE}/prepositions/prepositions_queried.json", encoding="utf-8",
     ) as f:
         prepositions_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "prepositions")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "prepositions")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "prepositions")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 
 def convert_cases(case):
     """
     Converts cases as found on Wikidata to more succinct versions.
     """
     case = case.split(" case")[0]
     if case in ["accusative", "Q146078"]:
@@ -126,12 +87,16 @@
             )
 
 for k in prepositions_formatted:
     prepositions_formatted[k] = order_annotations(prepositions_formatted[k])
 
 prepositions_formatted = collections.OrderedDict(sorted(prepositions_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(prepositions_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/prepositions.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/prepositions.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(prepositions_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file prepositions.json with {len(prepositions_formatted)} prepositions.")
```

## scribe_data/extract_transform/Russian/translations/format_translations.py

```diff
@@ -9,15 +9,15 @@
 
 import collections
 import json
 
 from tqdm.auto import tqdm
 from transformers import MarianMTModel, MarianTokenizer
 
-with open("../../translations_queried.json", encoding="utf-8") as f:
+with open("../../_translations/translations_queried.json", encoding="utf-8") as f:
     translations_list = json.load(f)
 
 words = [translation_vals["word"] for translation_vals in translations_list]
 words = list(set(words))
 
 translations_formatted = {}
 
@@ -29,15 +29,11 @@
     translated = model.generate(**tokenizer(w, return_tensors="pt", padding=True))
     translations_formatted[w] = tokenizer.decode(
         translated[0], skip_special_tokens=True
     )
 
 translations_formatted = collections.OrderedDict(sorted(translations_formatted.items()))
 
-with open(
-    "../../../Keyboards/LanguageKeyboards/Russian/Data/translations.json",
-    "w",
-    encoding="utf-8",
-) as f:
+with open("../formatted_data/translations.json", "w", encoding="utf-8",) as f:
     json.dump(translations_formatted, f, ensure_ascii=False, indent=0)
 
 print(f"Wrote file translations.json with {len(translations_formatted)} translations.")
```

## scribe_data/extract_transform/Russian/verbs/format_verbs.py

```diff
@@ -1,83 +1,42 @@
 """
 Format Verbs
 ------------
 
 Formats the verbs queried from Wikidata using query_verbs.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Russian"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/verbs/" not in file_path:
     with open("verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "verbs")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "verbs")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "verbs")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 verbs_formatted = {}
 
-all_keys = [
-    "infinitive",
+all_conjugations = [
     "presFPS",
     "presSPS",
     "presTPS",
     "presFPP",
     "presSPP",
     "presTPP",
     "pastFeminine",
@@ -85,20 +44,24 @@
     "pastNeutral",
     "pastPlural",
 ]
 
 for verb_vals in verbs_list:
     verbs_formatted[verb_vals["infinitive"]] = {}
 
-    for conj in [c for c in all_keys if c != "infinitive"]:
+    for conj in all_conjugations:
         if conj in verb_vals.keys():
             verbs_formatted[verb_vals["infinitive"]][conj] = verb_vals[conj]
         else:
             verbs_formatted[verb_vals["infinitive"]][conj] = ""
 
 verbs_formatted = collections.OrderedDict(sorted(verbs_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/verbs.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/verbs.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file verbs.json with {len(verbs_formatted)} verbs.")
```

## scribe_data/extract_transform/Spanish/nouns/format_nouns.py

```diff
@@ -1,79 +1,39 @@
 """
 Format Nouns
 ------------
 
 Formats the nouns queried from Wikidata using query_nouns.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Spanish"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/nouns/" not in file_path:
     with open("nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "nouns")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "nouns")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "nouns")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 
 def map_genders(wikidata_gender):
     """
     Maps those genders from Wikidata to succinct versions.
     """
     if wikidata_gender in ["masculine", "Q499327"]:
         return "M"
@@ -160,12 +120,16 @@
                 )
 
 for k in nouns_formatted:
     nouns_formatted[k]["form"] = order_annotations(nouns_formatted[k]["form"])
 
 nouns_formatted = collections.OrderedDict(sorted(nouns_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/nouns.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/nouns.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file nouns.json with {len(nouns_formatted)} nouns.")
```

## scribe_data/extract_transform/Spanish/translations/format_translations.py

```diff
@@ -9,15 +9,15 @@
 
 import collections
 import json
 
 from tqdm.auto import tqdm
 from transformers import MarianMTModel, MarianTokenizer
 
-with open("../../translationsQueried.json", encoding="utf-8") as f:
+with open("../../_translations/translationsQueried.json", encoding="utf-8") as f:
     translations_list = json.load(f)
 
 words = [translation_vals["word"] for translation_vals in translations_list]
 words = list(set(words))
 
 translations_formatted = {}
 
@@ -29,15 +29,11 @@
     translated = model.generate(**tokenizer(w, return_tensors="pt", padding=True))
     translations_formatted[w] = tokenizer.decode(
         translated[0], skip_special_tokens=True
     )
 
 translations_formatted = collections.OrderedDict(sorted(translations_formatted.items()))
 
-with open(
-    "../../../Keyboards/LanguageKeyboards/Spanish/Data/translations.json",
-    "w",
-    encoding="utf-8",
-) as f:
+with open("../formatted_data/translations.json", "w", encoding="utf-8",) as f:
     json.dump(translations_formatted, f, ensure_ascii=False, indent=0)
 
 print(f"Wrote file translations.json with {len(translations_formatted)} translations.")
```

## scribe_data/extract_transform/Spanish/verbs/format_verbs.py

```diff
@@ -1,83 +1,42 @@
 """
 Format Verbs
 ------------
 
 Formats the verbs queried from Wikidata using query_verbs.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Spanish"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/verbs/" not in file_path:
     with open("verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "verbs")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "verbs")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "verbs")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 verbs_formatted = {}
 
-all_keys = [
-    "infinitive",
+all_conjugations = [
     "presFPS",
     "presSPS",
     "presTPS",
     "presFPP",
     "presSPP",
     "presTPP",
     "pretFPS",
@@ -91,22 +50,32 @@
     "impTPS",
     "impFPP",
     "impSPP",
     "impTPP",
 ]
 
 for verb_vals in verbs_list:
-    verbs_formatted[verb_vals["infinitive"]] = {}
+    if verb_vals["infinitive"] not in verbs_formatted:
+        verbs_formatted[verb_vals["infinitive"]] = {}
 
-    for conj in [c for c in all_keys if c != "infinitive"]:
-        if conj in verb_vals.keys():
-            verbs_formatted[verb_vals["infinitive"]][conj] = verb_vals[conj]
-        else:
-            verbs_formatted[verb_vals["infinitive"]][conj] = ""
+        for conj in all_conjugations:
+            if conj in verb_vals.keys():
+                verbs_formatted[verb_vals["infinitive"]][conj] = verb_vals[conj]
+            else:
+                verbs_formatted[verb_vals["infinitive"]][conj] = ""
+
+    else:
+        for conj in all_conjugations:
+            if conj in verb_vals.keys():
+                verbs_formatted[verb_vals["infinitive"]][conj] = verb_vals[conj]
 
 verbs_formatted = collections.OrderedDict(sorted(verbs_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/verbs.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/verbs.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file verbs.json with {len(verbs_formatted)} verbs.")
```

## scribe_data/extract_transform/Swedish/nouns/format_nouns.py

```diff
@@ -1,79 +1,39 @@
 """
 Format Nouns
 ------------
 
 Formats the nouns queried from Wikidata using query_nouns.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Swedish"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path,
-    get_path_from_format_file,
-    get_path_from_update_data,
-)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/nouns/" not in file_path:
     with open("nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/nouns/nouns_queried.json", encoding="utf-8") as f:
         nouns_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "nouns")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "nouns")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "nouns")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 
 def map_genders(wikidata_gender):
     """
     Maps those genders from Wikidata to succinct versions.
     """
     if wikidata_gender in ["common gender", "Q1305037"]:
         return "C"
@@ -233,12 +193,16 @@
             )
 
 for k in nouns_formatted:
     nouns_formatted[k]["form"] = order_annotations(nouns_formatted[k]["form"])
 
 nouns_formatted = collections.OrderedDict(sorted(nouns_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/nouns.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/nouns.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(nouns_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file nouns.json with {len(nouns_formatted)} nouns.")
```

## scribe_data/extract_transform/Swedish/translations/format_translations.py

```diff
@@ -9,15 +9,15 @@
 
 import collections
 import json
 
 from tqdm.auto import tqdm
 from transformers import MarianMTModel, MarianTokenizer
 
-with open("../../translations_queried.json", encoding="utf-8") as f:
+with open("../../_translations/translations_queried.json", encoding="utf-8") as f:
     translations_list = json.load(f)
 
 words = [translation_vals["word"] for translation_vals in translations_list]
 words = list(set(words))
 
 translations_formatted = {}
 
@@ -29,15 +29,11 @@
     translated = model.generate(**tokenizer(w, return_tensors="pt", padding=True))
     translations_formatted[w] = tokenizer.decode(
         translated[0], skip_special_tokens=True
     )
 
 translations_formatted = collections.OrderedDict(sorted(translations_formatted.items()))
 
-with open(
-    "../../../Keyboards/LanguageKeyboards/Swedish/Data/translations.json",
-    "w",
-    encoding="utf-8",
-) as f:
+with open("../formatted_data/translations.json", "w", encoding="utf-8",) as f:
     json.dump(translations_formatted, f, ensure_ascii=False, indent=0)
 
 print(f"Wrote file translations.json with {len(translations_formatted)} translations.")
```

## scribe_data/extract_transform/Swedish/verbs/format_verbs.py

```diff
@@ -1,84 +1,45 @@
 """
 Format Verbs
 ------------
 
 Formats the verbs queried from Wikidata using query_verbs.sparql.
 """
 
-# pylint: disable=invalid-name, wrong-import-position
+# pylint: disable=invalid-name
 
 import collections
 import json
 import os
 import sys
 
 LANGUAGE = "Swedish"
 PATH_TO_SCRIBE_ORG = os.path.dirname(sys.path[0]).split("Scribe-Data")[0]
 PATH_TO_SCRIBE_DATA_SRC = f"{PATH_TO_SCRIBE_ORG}Scribe-Data/src"
 sys.path.insert(0, PATH_TO_SCRIBE_DATA_SRC)
 
-from scribe_data.load.update_utils import (  # get_android_data_path, get_desktop_data_path,
-    get_ios_data_path, get_path_from_format_file, get_path_from_update_data)
+from scribe_data.load.update_utils import get_path_from_et_dir
 
 file_path = sys.argv[0]
 
 update_data_in_use = False  # check if update_data.py is being used
 if f"{LANGUAGE}/verbs/" not in file_path:
     with open("verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 else:
     update_data_in_use = True
-    with open(
-        f"../extract_transform/{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8"
-    ) as f:
+    with open(f"./{LANGUAGE}/verbs/verbs_queried.json", encoding="utf-8") as f:
         verbs_list = json.load(f)
 
-# Get paths to load formatted data into.
-path_from_file = get_path_from_format_file()
-path_from_update_data = get_path_from_update_data()
-ios_data_dir_from_org = get_ios_data_path(LANGUAGE, "verbs")
-# android_data_dir_from_org = get_android_data_path(LANGUAGE, "verbs")
-# desktop_data_dir_from_org = get_desktop_data_path(LANGUAGE, "verbs")
-
-ios_output_path = f"{path_from_file}{ios_data_dir_from_org}"
-# android_output_path = f"{path_from_file}{android_data_dir_from_org}"
-# desktop_output_path = f"{path_from_file}{desktop_data_dir_from_org}"
-if update_data_in_use:
-    ios_output_path = f"{path_from_update_data}{ios_data_dir_from_org}"
-    # android_output_path = f"{path_from_update_data}{android_data_dir_from_org}"
-    # desktop_output_path = f"{path_from_update_data}{desktop_data_dir_from_org}"
-
-all_output_paths = [ios_output_path]  # android_output_path, desktop_output_path
-
-# Check to make sure that Scribe application directories are present for data updates.
-if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-iOS"):
-    all_output_paths = [p for p in all_output_paths if p != ios_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Android"):
-#     all_output_paths = [p for p in all_output_paths if p != android_output_path]
-
-# if not os.path.isdir(f"{PATH_TO_SCRIBE_ORG}Scribe-Desktop"):
-#     all_output_paths = [p for p in all_output_paths if p != desktop_output_path]
-
-if not all_output_paths:
-    raise OSError(
-        """No Scribe project directories have been found to update.
-        Scribe-Data should be in the same directory as applications that data should be updated for.
-        """
-    )
-
 verbs_formatted = {}
 
-# Currently there is a large problem with Swedish verbs not have needed features
-# See: https://www.wikidata.org/wiki/Lexeme:L38389
-# Any verbs occuring more than once will for now be deleted
+# Any verbs occurring more than once will for now be deleted.
 verbs_not_included = []
 
-all_keys = [
+all_conjugations = [
     "activeInfinitive",
     "imperative",
     "activeSupine",
     "activePresent",
     "activePreterite",
     "passiveInfinitive",
     "passiveSupine",
@@ -89,21 +50,25 @@
 for verb_vals in verbs_list:
     if (
         verb_vals["activeInfinitive"] not in verbs_formatted
         and verb_vals["activeInfinitive"] not in verbs_not_included
     ):
         verbs_formatted[verb_vals["activeInfinitive"]] = {
             conj: verb_vals[conj] if conj in verb_vals.keys() else ""
-            for conj in [c for c in all_keys if c != "activeInfinitive"]
+            for conj in [c for c in all_conjugations if c != "activeInfinitive"]
         }
 
     elif verb_vals["activeInfinitive"] in verbs_formatted:
         verbs_not_included.append(verb_vals["activeInfinitive"])
         del verbs_formatted[verb_vals["activeInfinitive"]]
 
 verbs_formatted = collections.OrderedDict(sorted(verbs_formatted.items()))
 
-for output_path in all_output_paths:
-    with open(output_path, "w", encoding="utf-8",) as file:
-        json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
+org_path = get_path_from_et_dir()
+export_path = "../formatted_data/verbs.json"
+if update_data_in_use:
+    export_path = f"{org_path}/Scribe-Data/src/scribe_data/extract_transform/{LANGUAGE}/formatted_data/verbs.json"
+
+with open(export_path, "w", encoding="utf-8",) as file:
+    json.dump(verbs_formatted, file, ensure_ascii=False, indent=0)
 
 print(f"Wrote file verbs.json with {len(verbs_formatted)} verbs.")
```

## scribe_data/load/update_utils.py

```diff
@@ -1,12 +1,26 @@
 """
 Update Utils
 ------------
 
 Utility functions for data updates.
+
+Contents:
+    get_language_qid,
+    get_language_iso,
+    get_language_from_iso,
+    get_language_words_to_remove,
+    get_language_words_to_ignore,
+    get_path_from_format_file,
+    get_path_from_load_dir,
+    get_path_from_et_dir,
+    get_ios_data_path,
+    get_android_data_path,
+    get_desktop_data_path,
+    add_num_commas
 """
 
 
 def get_language_qid(language):
     """
     Returns the QID of the given language.
 
@@ -62,20 +76,53 @@
         "russian": "ru",
         "spanish": "es",
         "swedish": "sv",
     }
 
     if language not in language_iso_dict:
         raise ValueError(
-            f"{language.upper()} is not currently not a supported language for ISO conversion."
+            f"{language.capitalize()} is not currently not a supported language for ISO conversion."
         )
 
     return language_iso_dict[language]
 
 
+def get_language_from_iso(iso):
+    """
+    Returns the language name for the given ISO.
+
+    Parameters
+    ----------
+        iso : str
+            The ISO the language name should be returned for.
+
+    Returns
+    -------
+        The name for the language as a value of a dictionary.
+    """
+    iso = iso.lower()
+
+    iso_language_dict = {
+        "fr": "French",
+        "de": "German",
+        "it": "Italian",
+        "pt": "Portuguese",
+        "ru": "Russian",
+        "es": "Spanish",
+        "sv": "Swedish",
+    }
+
+    if iso not in iso_language_dict:
+        raise ValueError(
+            f"{iso.upper()} is not currently not a supported ISO for language conversion."
+        )
+
+    return iso_language_dict[iso]
+
+
 def get_language_words_to_remove(language):
     """
     Returns the words that should not be included as autosuggestions for the given language.
 
     Parameters
     ----------
         language : str
@@ -129,90 +176,74 @@
 def get_path_from_format_file():
     """
     Returns the directory path from a data formatting file to scribe-org.
     """
     return "../../../../../.."
 
 
-def get_path_from_update_data():
-    """
-    Returns the directory path from update_data.py to scribe-org.
-    """
-    return "../../../.."
-
-
-def get_path_from_process_unicode():
+def get_path_from_load_dir():
     """
-    Returns the directory path from process_unicode.py to scribe-org.
+    Returns the directory path from the load directory to scribe-org.
     """
     return "../../../.."
 
 
-def get_path_from_process_wiki():
+def get_path_from_et_dir():
     """
-    Returns the directory path from process_wiki.py to scribe-org.
+    Returns the directory path from the extract_transform directory to scribe-org.
     """
     return "../../../.."
 
 
-def get_ios_data_path(language: str, word_type: str):
+def get_ios_data_path(language: str):
     """
-    Returns the path to the data json of the iOS app given a language and word type.
+    Returns the path to the data json of the iOS app given a language.
 
     Parameters
     ----------
         language : str
             The language the path should be returned for.
 
-        word_type : str
-            The type of word that should be accessed in the path.
-
     Returns
     -------
-        The path to the data json for the given language and word type.
+        The path to the data json for the given language.
     """
-    return f"/Scribe-iOS/Keyboards/LanguageKeyboards/{language}/Data/{word_type}.json"
+    return f"/Scribe-iOS/Keyboards/LanguageKeyboards/{language}"
 
 
-def get_android_data_path(language: str, word_type: str):
+def get_android_data_path(language: str):
     """
-    Returns the path to the data json of the Android app given a language and word type.
+    Returns the path to the data json of the Android app given a language.
 
     Parameters
     ----------
         language : str
             The language the path should be returned for.
 
-        word_type : str
-            The type of word that should be accessed in the path.
-
     Returns
     -------
-        The path to the data json for the given language and word type.
+        The path to the data json for the given language.
     """
-    return f"/Scribe-Android/app/src/main/LanguageKeyboards/{language}/Data/{word_type}.json"
+    return f"/Scribe-Android/app/src/main/LanguageKeyboards/{language}"
 
 
-def get_desktop_data_path(language: str, word_type: str):
+def get_desktop_data_path(language: str):
     """
-    Returns the path to the data json of the desktop app given a language and word type.
+    Returns the path to the data json of the desktop app given a language.
 
     Parameters
     ----------
         language : str
             The language the path should be returned for.
 
-        word_type : str
-            The type of word that should be accessed in the path.
-
     Returns
     -------
-        The path to the data json for the given language and word type.
+        The path to the data json for the given language.
     """
-    return f"/Scribe-Desktop/scribe/language_guis/{language}/data/{word_type}.json"
+    return f"/Scribe-Desktop/scribe/language_guis/{language}"
 
 
 def add_num_commas(num):
     """
     Adds commas to a numeric string for readability.
 
     Parameters
```

## Comparing `scribe_data-2.2.2.dist-info/LICENSE.txt` & `scribe_data-3.0.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `scribe_data-2.2.2.dist-info/METADATA` & `scribe_data-3.0.0.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: scribe-data
-Version: 2.2.2
+Version: 3.0.0
 Summary: Wikidata and Wikipedia data extraction for Scribe applications
 Home-page: https://github.com/scribe-org/Scribe-Data
 Author: Andrew Tavis McAllister
 Author-email: andrew.t.mcallister@gmail.com
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Education
@@ -19,19 +19,21 @@
 Description-Content-Type: text/markdown
 License-File: LICENSE.txt
 Requires-Dist: beautifulsoup4 (==4.9.3)
 Requires-Dist: black (>=19.10b0)
 Requires-Dist: certifi (>=2020.12.5)
 Requires-Dist: defusedxml (==0.7.1)
 Requires-Dist: emoji (>=2.2.0)
-Requires-Dist: mwparserfromhell (==0.6)
+Requires-Dist: mwparserfromhell (>=0.6)
 Requires-Dist: packaging (>=20.9)
+Requires-Dist: pandas (>=1.5.3)
 Requires-Dist: PyICU (>=2.10.2)
 Requires-Dist: pytest-cov (>=3.0.0)
 Requires-Dist: python-dateutil (>=2.8.2)
+Requires-Dist: regex (>=2023.3.23)
 Requires-Dist: sentencepiece (>=0.1.95)
 Requires-Dist: SPARQLWrapper (>=2.0.0)
 Requires-Dist: tabulate (>=0.8.9)
 Requires-Dist: tensorflow (>=2.5.1)
 Requires-Dist: tqdm (==4.56.1)
 Requires-Dist: transformers (>=4.12)
 
@@ -48,32 +50,38 @@
 [![coc](https://img.shields.io/badge/Contributor%20Covenant-ff69b4.svg)](https://github.com/scribe-org/Scribe-Data/blob/main/.github/CODE_OF_CONDUCT.md)
 [![twitter](https://img.shields.io/badge/Twitter-1DA1F2.svg?logo=twitter&logoColor=ffffff)](https://twitter.com/scribe_org)
 [![codestyle](https://img.shields.io/badge/black-000000.svg)](https://github.com/psf/black)
 [![matrix](https://img.shields.io/badge/Matrix-000000.svg?logo=matrix&logoColor=ffffff)](https://matrix.to/#/#scribe_community:matrix.org)
 
 ## Wikidata and Wikipedia data extraction for Scribe applications
 
-This repository contains the scripts for extracting and formatting data from [Wikidata](https://www.wikidata.org/) and [Wikipedia](https://www.wikipedia.org/) for Scribe applications. Updates to the language keyboard and interface data can be done using [scribe_data/load/update_data.py](https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load/update_data.py).
+**Scribe-Data** contains the scripts for extracting and formatting data from [Wikidata](https://www.wikidata.org/) and [Wikipedia](https://www.wikipedia.org/) for Scribe applications. Updates to the language keyboard and interface data can be done using [scribe_data/load/update_data.py](https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load/update_data.py) and the notebooks within the [scribe_data/load](https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load) directory.
+
+> **Note**: The [contributing](#contributing) section has information for those interested, with the articles and presentations in [featured by](#featured-by) also being good resources for learning more about Scribe.
+
+Scribe applications are available on [iOS](https://github.com/scribe-org/Scribe-iOS), [Android](https://github.com/scribe-org/Scribe-Android) (WIP) and [Desktop](https://github.com/scribe-org/Scribe-Desktop) (planned).
 
 <a id="contents"></a>
 
 # **Contents**
 
 - [Process](#process)
 - [Contributing](#contributing)
 - [Supported Languages](#supported-languages)
 - [Featured By](#featured-by)
 
 <a id="process"></a>
 
 # Process [`⇧`](#contents)
 
-[scribe_data/load/update_data.py](https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load/update_data.py) is used to update all data for [Scribe-iOS](https://github.com/scribe-org/Scribe-iOS), with this functionality later being expanded to update [Scribe-Android](https://github.com/scribe-org/Scribe-Android) and [Scribe-Desktop](https://github.com/scribe-org/Scribe-Desktop) when they're active. The autosuggestion process further derives popular words from [Wikipedia](https://www.wikipedia.org/) as well as those words that normally follow them for an effective baseline feature until natural language processing techniques are employed. Functions to generate autosuggestions are ran in [scribe_data/load/gen_autosuggestions.ipynb](https://github.com/scribe-org/Scribe-Data/blob/main/src/scribe_data/load/gen_autosuggestions.ipynb).
+[scribe_data/load/update_data.py](https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load/update_data.py) and the notebooks within the [scribe_data/load](https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load) directory are used to update all data for [Scribe-iOS](https://github.com/scribe-org/Scribe-iOS), with this functionality later being expanded to update [Scribe-Android](https://github.com/scribe-org/Scribe-Android) and [Scribe-Desktop](https://github.com/scribe-org/Scribe-Desktop) when they're active.
+
+The main data update process in [update_data.py](https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load/update_data.py) triggers [SPARQL queries](https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/extract_transform) to query language data from [Wikidata](https://www.wikidata.org/) using [SPARQLWrapper](https://github.com/RDFLib/sparqlwrapper) as a URI. The autosuggestion process derives popular words from [Wikipedia](https://www.wikipedia.org/) as well as those words that normally follow them for an effective baseline feature until natural language processing techniques are employed. Functions to generate autosuggestions are ran in [gen_autosuggestions.ipynb](https://github.com/scribe-org/Scribe-Data/blob/main/src/scribe_data/load/gen_autosuggestions.ipynb). Emojis are further sourced from [Unicode CLDR](https://github.com/unicode-org/cldr), with this process being ran in [gen_emoji_lexicon.ipynb](https://github.com/scribe-org/Scribe-Data/blob/main/src/scribe_data/load/gen_emoji_lexicon.ipynb).
 
-The ultimate goal is that this repository will house language packs that are periodically updated with new [Wikidata](https://www.wikidata.org/) lexicographical data, with these packs then being available to download by users of Scribe applications.
+The ultimate goal is that this repository will house language packs that are periodically updated with new [Wikidata](https://www.wikidata.org/) lexicographical data and data from other sources. These packs would then be available to download by users of Scribe applications.
 
 <a id="contributing"></a>
 
 # Contributing [`⇧`](#contents)
 
 <a href="https://matrix.to/#/#scribe_community:matrix.org"><img src="https://raw.githubusercontent.com/scribe-org/Organization/main/resources/images/logos/MatrixLogoGrey.png" height="50" alt="Public Matrix Chat" align="right"></a>
 
@@ -100,41 +108,45 @@
 
 <a id="supported-languages"></a>
 
 # Supported Languages [`⇧`](#contents)
 
 Scribe's goal is functional, feature-rich keyboards and interfaces for all languages. Check the [extract_transform](https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/extract_transform) directory for queries for currently supported languages and those that have substantial data on [Wikidata](https://www.wikidata.org/).
 
-The following table shows the supported languages and the amount of data available for each on [Wikidata](https://www.wikidata.org/):
+The following table shows the supported languages and the amount of data available for each on [Wikidata](https://www.wikidata.org/) and via [Unicode CLDR](https://github.com/unicode-org/cldr) for emojis:
 
-| Languages  |   Nouns | Verbs | Translations\* | Prepositions† |
-| :--------- | ------: | ----: | -------------: | ------------: |
-| French     |  16,815 | 5,450 |         67,652 |             - |
-| German     |  29,272 | 3,557 |         67,652 |           187 |
-| Italian    |   8,646 |    73 |         67,652 |             - |
-| Portuguese |   5,191 |   495 |         67,652 |             - |
-| Russian    | 194,419 |    11 |         67,652 |            13 |
-| Spanish    |  27,128 | 4,036 |         67,652 |             - |
-| Swedish    |  42,807 | 4,394 |         67,652 |             - |
+| Languages  |   Nouns | Verbs | Translations\* | Prepositions† | Emoji Keywords |
+| :--------- | ------: | ----: | -------------: | ------------: | -------------: |
+| French     |  17,070 | 6,572 |         67,652 |             - |          2,488 |
+| German     | 102,789 | 3,592 |         67,652 |           190 |          2,898 |
+| Italian    |   8,669 |    73 |         67,652 |             - |          2,457 |
+| Portuguese |   5,437 |   536 |         67,652 |             - |          2,327 |
+| Russian    | 194,448 |    12 |         67,652 |            15 |          3,827 |
+| Spanish    |  38,755 | 4,828 |         67,652 |             - |          3,134 |
+| Swedish    |  44,624 | 4,474 |         67,652 |             - |          2,913 |
 
 `*` Given the current **`beta`** status where words are machine translated.
 
 `†` Only for languages for which preposition annotation is needed.
 
 <a id="featured-by"></a>
 
 # Featured By [`⇧`](#contents)
 
-<details><summary><strong>Articles and Presentations on Scribe</strong></summary>
+<details open><summary><strong>Articles and Presentations on Scribe</strong></summary>
 <p>
 
+<strong>2023</strong>
+
+- [Presentation slides](https://docs.google.com/presentation/d/1W4ZkGi9UDDiTxM_silEij0gTE8YEubluHxe78xoqEP0/edit?usp=sharing) for a talk at [Berlin Hack and Tell](https://berlinhackandtell.rocks/) ([Hack of the month winner 🏆](https://berlinhackandtell.rocks/2023-03-28-no87-moore-hacks))
+
 <strong>2022</strong>
 
 - [Presentation slides](https://docs.google.com/presentation/d/12WNSt5xgNIAmSxPfvjno9-sBMGlvxG_xSaAxmHQDRNQ/edit?usp=sharing) for a session at the [2022 Wikimania Hackathon](https://wikimania.wikimedia.org/wiki/Hackathon)
-- [Presentation slides](https://docs.google.com/presentation/d/10Ai0-b8XUj5u9Hw4UgBtB7ufiPhvfFrb1vEUEyXYr5w/edit?usp=sharing) for a talk with [CocoaHeads Berlin](https://www.meetup.com/cocoaheads-berlin/)
+- [Presentation slides](https://docs.google.com/presentation/d/10Ai0-b8XUj5u9Hw4UgBtB7ufiPhvfFrb1vEUEyXYr5w/edit?usp=sharing) for a talk at [CocoaHeads Berlin](https://www.meetup.com/cocoaheads-berlin/)
 - [Video on Scribe](https://www.youtube.com/watch?v=4GpFN0gGmy4&list=PL66MRMNlLyR7p9wsYVfuqJOjKZpbuwp8U&index=6) for [Wikimedia Celtic Knot 2022](https://meta.wikimedia.org/wiki/Celtic_Knot_Conference_2022)
 - [Presentation slides](https://docs.google.com/presentation/d/1K2lj8PPgdx12I-xuhm--CBLrGm-Cz50NJmbp96zpGrk/edit?usp=sharing) for a talk with the [LD4 Wikidata Affinity Group](https://www.wikidata.org/wiki/Wikidata:WikiProject_LD4_Wikidata_Affinity_Group)
 - [Scribe](https://github.com/scribe-org) featured for new developers on [MediaWiki](https://www.mediawiki.org/wiki/New_Developers)
 - [Presentation slides](https://docs.google.com/presentation/d/1Cu3VwQ3lJUp5W84YDe0AFYS-6zfBxKsm0MI-OMl_IzY/edit?usp=sharing) for [Wikimedia Hackathon 2022](https://www.mediawiki.org/wiki/Wikimedia_Hackathon_2022)
 - [Blog post](https://tech-news.wikimedia.de/en/2022/03/18/lexicographical-data-for-language-learners-the-wikidata-based-app-scribe/) on [Scribe-iOS](https://github.com/scribe-org/Scribe-iOS) for [Wikimedia Tech News](https://tech-news.wikimedia.de/en/homepage/) ([DE](https://tech-news.wikimedia.de/2022/03/18/sprachenlernen-mit-lexikografische-daten-die-wikidata-basierte-app-scribe/) / [Tweet](https://twitter.com/wikidata/status/1507335538596106257?s=20&t=YGRGamftI-5B_VwQ_bFRhA))
 - [Presentation slides](https://docs.google.com/presentation/d/16ld_rCbwJCiAdRrfhF-Fq9Wm_ciHCbk_HCzGQs6TB1Q/edit?usp=sharing) for [Wikidata Data Reuse Days 2022](https://diff.wikimedia.org/event/wikidata-data-reuse-days-2022/)
```

### html2text {}

```diff
@@ -1,26 +1,28 @@
-Metadata-Version: 2.1 Name: scribe-data Version: 2.2.2 Summary: Wikidata and
+Metadata-Version: 2.1 Name: scribe-data Version: 3.0.0 Summary: Wikidata and
 Wikipedia data extraction for Scribe applications Home-page: https://
 github.com/scribe-org/Scribe-Data Author: Andrew Tavis McAllister Author-email:
 andrew.t.mcallister@gmail.com Classifier: Development Status :: 5 - Production/
 Stable Classifier: Intended Audience :: Developers Classifier: Intended
 Audience :: Education Classifier: License :: OSI Approved :: GNU General Public
 License v3 (GPLv3) Classifier: Programming Language :: Python Classifier:
 Programming Language :: Python :: 3 Classifier: Programming Language :: Python
 :: 3.9 Classifier: Programming Language :: Python :: 3.10 Classifier:
 Programming Language :: Python :: 3.11 Classifier: Operating System :: OS
 Independent Requires-Python: >=3.9 Description-Content-Type: text/markdown
 License-File: LICENSE.txt Requires-Dist: beautifulsoup4 (==4.9.3) Requires-
 Dist: black (>=19.10b0) Requires-Dist: certifi (>=2020.12.5) Requires-Dist:
 defusedxml (==0.7.1) Requires-Dist: emoji (>=2.2.0) Requires-Dist:
-mwparserfromhell (==0.6) Requires-Dist: packaging (>=20.9) Requires-Dist: PyICU
-(>=2.10.2) Requires-Dist: pytest-cov (>=3.0.0) Requires-Dist: python-dateutil
-(>=2.8.2) Requires-Dist: sentencepiece (>=0.1.95) Requires-Dist: SPARQLWrapper
-(>=2.0.0) Requires-Dist: tabulate (>=0.8.9) Requires-Dist: tensorflow (>=2.5.1)
-Requires-Dist: tqdm (==4.56.1) Requires-Dist: transformers (>=4.12)
+mwparserfromhell (>=0.6) Requires-Dist: packaging (>=20.9) Requires-Dist:
+pandas (>=1.5.3) Requires-Dist: PyICU (>=2.10.2) Requires-Dist: pytest-cov
+(>=3.0.0) Requires-Dist: python-dateutil (>=2.8.2) Requires-Dist: regex
+(>=2023.3.23) Requires-Dist: sentencepiece (>=0.1.95) Requires-Dist:
+SPARQLWrapper (>=2.0.0) Requires-Dist: tabulate (>=0.8.9) Requires-Dist:
+tensorflow (>=2.5.1) Requires-Dist: tqdm (==4.56.1) Requires-Dist: transformers
+(>=4.12)
                                  [Scribe_Logo]
 [![platforms](https://img.shields.io/badge/Wikidata-
 990000.svg?logo=wikidata&logoColor=ffffff)](https://github.com/scribe-org/
 Scribe-Data) [![issues](https://img.shields.io/github/issues/scribe-org/Scribe-
 Data?label=%20&logo=github)](https://github.com/scribe-org/Scribe-Data/issues)
 [![language](https://img.shields.io/badge/Python%203-
 306998.svg?logo=python&logoColor=ffffff)](https://github.com/scribe-org/Scribe-
@@ -33,49 +35,66 @@
 badge/Contributor%20Covenant-ff69b4.svg)](https://github.com/scribe-org/Scribe-
 Data/blob/main/.github/CODE_OF_CONDUCT.md) [![twitter](https://img.shields.io/
 badge/Twitter-1DA1F2.svg?logo=twitter&logoColor=ffffff)](https://twitter.com/
 scribe_org) [![codestyle](https://img.shields.io/badge/black-000000.svg)]
 (https://github.com/psf/black) [![matrix](https://img.shields.io/badge/Matrix-
 000000.svg?logo=matrix&logoColor=ffffff)](https://matrix.to/#/
 #scribe_community:matrix.org) ## Wikidata and Wikipedia data extraction for
-Scribe applications This repository contains the scripts for extracting and
+Scribe applications **Scribe-Data** contains the scripts for extracting and
 formatting data from [Wikidata](https://www.wikidata.org/) and [Wikipedia]
 (https://www.wikipedia.org/) for Scribe applications. Updates to the language
 keyboard and interface data can be done using [scribe_data/load/update_data.py]
 (https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load/
-update_data.py).  # **Contents** - [Process](#process) - [Contributing]
-(#contributing) - [Supported Languages](#supported-languages) - [Featured By]
-(#featured-by)  # Process [`â§`](#contents) [scribe_data/load/update_data.py]
-(https://github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load/
-update_data.py) is used to update all data for [Scribe-iOS](https://github.com/
-scribe-org/Scribe-iOS), with this functionality later being expanded to update
-[Scribe-Android](https://github.com/scribe-org/Scribe-Android) and [Scribe-
-Desktop](https://github.com/scribe-org/Scribe-Desktop) when they're active. The
-autosuggestion process further derives popular words from [Wikipedia](https://
-www.wikipedia.org/) as well as those words that normally follow them for an
-effective baseline feature until natural language processing techniques are
-employed. Functions to generate autosuggestions are ran in [scribe_data/load/
-gen_autosuggestions.ipynb](https://github.com/scribe-org/Scribe-Data/blob/main/
-src/scribe_data/load/gen_autosuggestions.ipynb). The ultimate goal is that this
-repository will house language packs that are periodically updated with new
-[Wikidata](https://www.wikidata.org/) lexicographical data, with these packs
-then being available to download by users of Scribe applications.  #
-Contributing [`â§`](#contents) [Public_Matrix_Chat] Scribe uses [Matrix]
-(https://matrix.org/) for communications. You're more than welcome to [join us
-in our public chat rooms](https://matrix.to/#/#scribe_community:matrix.org) to
-share ideas, ask questions or just say hi :) Please see the [contribution
-guidelines](https://github.com/scribe-org/Scribe-Data/blob/main/
-CONTRIBUTING.md) if you are interested in contributing to Scribe-Data. Work
-that is in progress or could be implemented is tracked in the [issues](https://
-github.com/scribe-org/Scribe-Data/issues) and [projects](https://github.com/
-scribe-org/Scribe-Data/projects). Also check the [`-priority-`](https://
-github.com/scribe-org/Scribe-Data/labels/-priority-) labels in the [issues]
-(https://github.com/scribe-org/Scribe-Data/issues) for those that are most
-important, as well as those marked [`good first issue`](https://github.com/
-scribe-org/Scribe-Data/
+update_data.py) and the notebooks within the [scribe_data/load](https://
+github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load) directory. >
+**Note**: The [contributing](#contributing) section has information for those
+interested, with the articles and presentations in [featured by](#featured-by)
+also being good resources for learning more about Scribe. Scribe applications
+are available on [iOS](https://github.com/scribe-org/Scribe-iOS), [Android]
+(https://github.com/scribe-org/Scribe-Android) (WIP) and [Desktop](https://
+github.com/scribe-org/Scribe-Desktop) (planned).  # **Contents** - [Process]
+(#process) - [Contributing](#contributing) - [Supported Languages](#supported-
+languages) - [Featured By](#featured-by)  # Process [`â§`](#contents)
+[scribe_data/load/update_data.py](https://github.com/scribe-org/Scribe-Data/
+tree/main/src/scribe_data/load/update_data.py) and the notebooks within the
+[scribe_data/load](https://github.com/scribe-org/Scribe-Data/tree/main/src/
+scribe_data/load) directory are used to update all data for [Scribe-iOS](https:
+//github.com/scribe-org/Scribe-iOS), with this functionality later being
+expanded to update [Scribe-Android](https://github.com/scribe-org/Scribe-
+Android) and [Scribe-Desktop](https://github.com/scribe-org/Scribe-Desktop)
+when they're active. The main data update process in [update_data.py](https://
+github.com/scribe-org/Scribe-Data/tree/main/src/scribe_data/load/
+update_data.py) triggers [SPARQL queries](https://github.com/scribe-org/Scribe-
+Data/tree/main/src/scribe_data/extract_transform) to query language data from
+[Wikidata](https://www.wikidata.org/) using [SPARQLWrapper](https://github.com/
+RDFLib/sparqlwrapper) as a URI. The autosuggestion process derives popular
+words from [Wikipedia](https://www.wikipedia.org/) as well as those words that
+normally follow them for an effective baseline feature until natural language
+processing techniques are employed. Functions to generate autosuggestions are
+ran in [gen_autosuggestions.ipynb](https://github.com/scribe-org/Scribe-Data/
+blob/main/src/scribe_data/load/gen_autosuggestions.ipynb). Emojis are further
+sourced from [Unicode CLDR](https://github.com/unicode-org/cldr), with this
+process being ran in [gen_emoji_lexicon.ipynb](https://github.com/scribe-org/
+Scribe-Data/blob/main/src/scribe_data/load/gen_emoji_lexicon.ipynb). The
+ultimate goal is that this repository will house language packs that are
+periodically updated with new [Wikidata](https://www.wikidata.org/
+) lexicographical data and data from other sources. These packs would then be
+available to download by users of Scribe applications.  # Contributing [`â§`]
+(#contents) [Public_Matrix_Chat] Scribe uses [Matrix](https://matrix.org/) for
+communications. You're more than welcome to [join us in our public chat rooms]
+(https://matrix.to/#/#scribe_community:matrix.org) to share ideas, ask
+questions or just say hi :) Please see the [contribution guidelines](https://
+github.com/scribe-org/Scribe-Data/blob/main/CONTRIBUTING.md) if you are
+interested in contributing to Scribe-Data. Work that is in progress or could be
+implemented is tracked in the [issues](https://github.com/scribe-org/Scribe-
+Data/issues) and [projects](https://github.com/scribe-org/Scribe-Data/
+projects). Also check the [`-priority-`](https://github.com/scribe-org/Scribe-
+Data/labels/-priority-) labels in the [issues](https://github.com/scribe-org/
+Scribe-Data/issues) for those that are most important, as well as those marked
+[`good first issue`](https://github.com/scribe-org/Scribe-Data/
 issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) that are tailored
 for first time contributors. After your first few pull requests organization
 members would be happy to discuss granting you further rights as a contributor,
 with a maintainer role then being possible after continued interest in the
 project. Scribe seeks to be an inclusive and supportive organization. We'd love
 to have you on the team! ### Ways to Help [`â§`](#contents) - [Reporting bugs]
 (https://github.com/scribe-org/Scribe-Data/issues/
@@ -100,29 +119,35 @@
 new?assignees=&labels=data&template=data_wikidata.yml) and we'll be happy to
 integrate them!  # Supported Languages [`â§`](#contents) Scribe's goal is
 functional, feature-rich keyboards and interfaces for all languages. Check the
 [extract_transform](https://github.com/scribe-org/Scribe-Data/tree/main/src/
 scribe_data/extract_transform) directory for queries for currently supported
 languages and those that have substantial data on [Wikidata](https://
 www.wikidata.org/). The following table shows the supported languages and the
-amount of data available for each on [Wikidata](https://www.wikidata.org/): |
-Languages | Nouns | Verbs | Translations\* | Prepositionsâ  | | :--------- | -
------: | ----: | -------------: | ------------: | | French | 16,815 | 5,450 |
-67,652 | - | | German | 29,272 | 3,557 | 67,652 | 187 | | Italian | 8,646 | 73
-| 67,652 | - | | Portuguese | 5,191 | 495 | 67,652 | - | | Russian | 194,419 |
-11 | 67,652 | 13 | | Spanish | 27,128 | 4,036 | 67,652 | - | | Swedish | 42,807
-| 4,394 | 67,652 | - | `*` Given the current **`beta`** status where words are
-machine translated. `â ` Only for languages for which preposition annotation
-is needed.  # Featured By [`â§`](#contents) Articles and Presentations on
-Scribe
-2022 - [Presentation slides](https://docs.google.com/presentation/d/
+amount of data available for each on [Wikidata](https://www.wikidata.org/) and
+via [Unicode CLDR](https://github.com/unicode-org/cldr) for emojis: | Languages
+| Nouns | Verbs | Translations\* | Prepositionsâ  | Emoji Keywords | | :------
+--- | ------: | ----: | -------------: | ------------: | -------------: | |
+French | 17,070 | 6,572 | 67,652 | - | 2,488 | | German | 102,789 | 3,592 |
+67,652 | 190 | 2,898 | | Italian | 8,669 | 73 | 67,652 | - | 2,457 | |
+Portuguese | 5,437 | 536 | 67,652 | - | 2,327 | | Russian | 194,448 | 12 |
+67,652 | 15 | 3,827 | | Spanish | 38,755 | 4,828 | 67,652 | - | 3,134 | |
+Swedish | 44,624 | 4,474 | 67,652 | - | 2,913 | `*` Given the current
+**`beta`** status where words are machine translated. `â ` Only for languages
+for which preposition annotation is needed.  # Featured By [`â§`](#contents)
+Articles and Presentations on Scribe
+2023 - [Presentation slides](https://docs.google.com/presentation/d/
+1W4ZkGi9UDDiTxM_silEij0gTE8YEubluHxe78xoqEP0/edit?usp=sharing) for a talk at
+[Berlin Hack and Tell](https://berlinhackandtell.rocks/) ([Hack of the month
+winner ð](https://berlinhackandtell.rocks/2023-03-28-no87-moore-hacks)) 2022
+- [Presentation slides](https://docs.google.com/presentation/d/
 12WNSt5xgNIAmSxPfvjno9-sBMGlvxG_xSaAxmHQDRNQ/edit?usp=sharing) for a session at
 the [2022 Wikimania Hackathon](https://wikimania.wikimedia.org/wiki/Hackathon)
 - [Presentation slides](https://docs.google.com/presentation/d/10Ai0-
-b8XUj5u9Hw4UgBtB7ufiPhvfFrb1vEUEyXYr5w/edit?usp=sharing) for a talk with
+b8XUj5u9Hw4UgBtB7ufiPhvfFrb1vEUEyXYr5w/edit?usp=sharing) for a talk at
 [CocoaHeads Berlin](https://www.meetup.com/cocoaheads-berlin/) - [Video on
 Scribe](https://www.youtube.com/
 watch?v=4GpFN0gGmy4&list=PL66MRMNlLyR7p9wsYVfuqJOjKZpbuwp8U&index=6) for
 [Wikimedia Celtic Knot 2022](https://meta.wikimedia.org/wiki/
 Celtic_Knot_Conference_2022) - [Presentation slides](https://docs.google.com/
 presentation/d/1K2lj8PPgdx12I-xuhm--CBLrGm-Cz50NJmbp96zpGrk/edit?usp=sharing)
 for a talk with the [LD4 Wikidata Affinity Group](https://www.wikidata.org/
```

## Comparing `scribe_data-2.2.2.dist-info/RECORD` & `scribe_data-3.0.0.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,69 +1,73 @@
 scribe_data/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scribe_data/extract_transform/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+scribe_data/extract_transform/emoji_utils.py,sha256=rf-bCIeW8M8XjqSFVRrs24EgN5wSMzTJFaIMYUG-ei0,8252
 scribe_data/extract_transform/extract_wiki.py,sha256=YlUAy_P2AbeVIAD1n1oiL282HdmYwEkuoXvDKjqDJ8w,13956
-scribe_data/extract_transform/process_unicode.py,sha256=Cp32tkLV0yf_8x57jmyrC4ExvnwOQDD-hGgvlv4Ub9Y,6461
-scribe_data/extract_transform/process_wiki.py,sha256=b5R24sRoAVIOalTIHC_c6vk_42lAQcxKmaxTp65_mn0,12783
+scribe_data/extract_transform/process_unicode.py,sha256=39v0PvG7b_Fcqoj2Orod2FRuLalPySf3KyfP3yTrtWg,9447
+scribe_data/extract_transform/process_wiki.py,sha256=OsZTafbkUqJ4lyDKiMXeYT7jl_TwHivAf4fUN5a7_Fw,12277
+scribe_data/extract_transform/update_data.py,sha256=mFctc5zDWCRl3bivmPh-2X_A9zhSHpJD8MQGBaRkpOs,14322
 scribe_data/extract_transform/French/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scribe_data/extract_transform/French/nouns/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/French/nouns/format_nouns.py,sha256=OOeZ6GP--vp58GFvZQFdoKDY6Gpj6yYn5FHrTeekZOg,6162
+scribe_data/extract_transform/French/nouns/format_nouns.py,sha256=xYBvSJ7uKyKES09YbeRKFlmBVKyQztw87ZesDut-CNc,4551
 scribe_data/extract_transform/French/translations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/French/translations/format_translations.py,sha256=1DUUd1ltSAgdim843aXzMrYTPQY4YsaQw6Qh4RPWDMg,1297
+scribe_data/extract_transform/French/translations/format_translations.py,sha256=oft9eeNlNewZZwr6zopqY0FQ23Klf9kT93jrqvS4a2Q,1266
 scribe_data/extract_transform/French/verbs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/French/verbs/format_verbs.py,sha256=ZfsPKHosPnv4iwrcSO_yTlbc8zmGtMfUaCJhw4hMeuI,3968
+scribe_data/extract_transform/French/verbs/format_verbs.py,sha256=3PjLAS6veFKE7wdOuBBkHwMfVc7YPjOiI1JrccbpKKo,2271
 scribe_data/extract_transform/German/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scribe_data/extract_transform/German/nouns/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/German/nouns/format_nouns.py,sha256=QUd_NaSbnvXl9mMwQm2DgnWnnhdvygub_hNo0z_GaAQ,9499
+scribe_data/extract_transform/German/nouns/format_nouns.py,sha256=pZRL4OBYn2C9UWuXb75N9IGKDcuWcOZbhPqXt60Y3Bs,7864
 scribe_data/extract_transform/German/prepositions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/German/prepositions/format_prepositions.py,sha256=lsrSxdWHBILiT29Zlj9FNAEmoFDRWL8JP5EvhQV_U1s,4779
+scribe_data/extract_transform/German/prepositions/format_prepositions.py,sha256=I3UfZ4eRWnrgoVs81jYBglQsrMtvFAnHT6ld0K1tHxw,3143
 scribe_data/extract_transform/German/translations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/German/translations/format_translations.py,sha256=qg7Ox92LyxNEIXrDm0TDor5GP-kqSy7u-0Q9nPS5XNo,1424
+scribe_data/extract_transform/German/translations/format_translations.py,sha256=yXvOTDKV6lKL5np6s6utmFY--HdEcZ9GL44gWXg3IOo,1393
 scribe_data/extract_transform/German/verbs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/German/verbs/format_verbs.py,sha256=H2b3uSbnag2nLtgOt2PdLhJz2un9DpfPIZcnznHDhzU,6707
+scribe_data/extract_transform/German/verbs/format_verbs.py,sha256=bJsvYosIqoas3n7zsmVW-EPtjtX1t_UnoiPhEDuHR3w,6537
 scribe_data/extract_transform/Italian/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scribe_data/extract_transform/Italian/nouns/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Italian/nouns/format_nouns.py,sha256=ATR1T8svycPHYhHi95qUYKswqGn_Mjt5EUJzmg99aDA,6218
+scribe_data/extract_transform/Italian/nouns/format_nouns.py,sha256=xjHUpeqSqGJcno1_w48gbetFkKA2vRPjWSCseYndGpE,4583
 scribe_data/extract_transform/Italian/translations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Italian/translations/format_translations.py,sha256=JmRdLHpGvXSnk6AAsAxUdMDoMBieqc-35viLrBTdZj8,1326
+scribe_data/extract_transform/Italian/translations/format_translations.py,sha256=u7tOuaGV7m9sb8pNp416AB54FrpteeIzaSMoW1XcUIE,1294
 scribe_data/extract_transform/Italian/verbs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Italian/verbs/format_verbs.py,sha256=ngnAoepUFx8baYn5_BrQRCyqOaxqQAe4f7Kqr8w-U5Q,3609
+scribe_data/extract_transform/Italian/verbs/format_verbs.py,sha256=UnDei8gsCgW_dw9SJESWGE5yMOazRf79hakT9gz9hAs,1938
 scribe_data/extract_transform/Portuguese/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scribe_data/extract_transform/Portuguese/nouns/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Portuguese/nouns/format_nouns.py,sha256=whw2A2jZ4Nz9r5gDZd6dDY3OEf3wqaizW_mXN5loqxo,6221
+scribe_data/extract_transform/Portuguese/nouns/format_nouns.py,sha256=G92UDZqx3fsj_nknxAdgE0Xicr7rCm9X2YxE1av2zc4,4586
 scribe_data/extract_transform/Portuguese/translations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Portuguese/translations/format_translations.py,sha256=5XzXg69V6zaEfEan6daJx_w1lb7P88vVU7gkuvu-uoA,1329
+scribe_data/extract_transform/Portuguese/translations/format_translations.py,sha256=YBC8cIsExltvkKu03TGtE-RGdfTdsRZe3G6nUvGbUe0,1294
 scribe_data/extract_transform/Portuguese/verbs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Portuguese/verbs/format_verbs.py,sha256=GVoYZYz-h7R6GhZx0NCMspA-jRIymnau5eJgPCh6SJQ,3708
+scribe_data/extract_transform/Portuguese/verbs/format_verbs.py,sha256=LYVrpAiYtPoJBcRGLupsQjDTtstIAkqUTthNJ4IVdtA,2037
 scribe_data/extract_transform/Russian/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scribe_data/extract_transform/Russian/nouns/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Russian/nouns/format_nouns.py,sha256=TM1IBFiihZcA2-OJo5hpgKpIicBKubDG-i-QtK1z4jA,9524
+scribe_data/extract_transform/Russian/nouns/format_nouns.py,sha256=u43JU-hYpHPgCBur2GIe5cVQ-ikFgWW42VOkb0ry_rE,7889
 scribe_data/extract_transform/Russian/prepositions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Russian/prepositions/format_prepositions.py,sha256=hSJ0qy2TJgx6PKyQgHAlsCgXn--jMc43r_9tZTRrYj8,4762
+scribe_data/extract_transform/Russian/prepositions/format_prepositions.py,sha256=eZ6NZpMlKAW1wx79lxjmWSwTpp0igz8bg69oxUz2K9k,3126
 scribe_data/extract_transform/Russian/translations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Russian/translations/format_translations.py,sha256=M682KSxBlH0fuLwG5f-fPQ5NDAUAfvg__HbtiwhodTI,1298
+scribe_data/extract_transform/Russian/translations/format_translations.py,sha256=AAQPaslRTM9FdLxs_fSdGK5by9oW_AkwIIuJGKr0598,1266
 scribe_data/extract_transform/Russian/verbs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Russian/verbs/format_verbs.py,sha256=hQlegbfniJGa_QKUsK07kV5CnWn0EEProZdXtJgHF3w,3513
+scribe_data/extract_transform/Russian/verbs/format_verbs.py,sha256=eztL_by0JcKJiq10xFhyCZUv6MeT-r7xL7Shdqe-i_o,1842
 scribe_data/extract_transform/Spanish/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scribe_data/extract_transform/Spanish/nouns/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Spanish/nouns/format_nouns.py,sha256=8QlFjKJHHQXmENuXSkzrQsRuUuBNGpKuQK4k_qdycFw,6218
+scribe_data/extract_transform/Spanish/nouns/format_nouns.py,sha256=idbEPn4SC7pI-gaVZunlpBpcgJPzkSHgZro35GVXAt0,4583
 scribe_data/extract_transform/Spanish/translations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Spanish/translations/format_translations.py,sha256=Px6HE5j9ftrtDpigB96fhL259PEAWYhiV46QWDq26JU,1297
+scribe_data/extract_transform/Spanish/translations/format_translations.py,sha256=X9mhED6UKFkznRU7dhp8CZ8ZnNxZEI46q_NJsdQUJxM,1265
 scribe_data/extract_transform/Spanish/verbs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Spanish/verbs/format_verbs.py,sha256=KYXwDri8Ircqk6GT4fehpx5czFqMMgeKEMrcghQe0SQ,3609
+scribe_data/extract_transform/Spanish/verbs/format_verbs.py,sha256=5JdO4cHnr766sW4pSdj-LQrRMBzBrMaMgiMJjqKiAgc,2188
 scribe_data/extract_transform/Swedish/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scribe_data/extract_transform/Swedish/nouns/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Swedish/nouns/format_nouns.py,sha256=RM4dv6osvyxR_McStGQdXor1PgSj3WaglWO3H26Y0gg,9119
+scribe_data/extract_transform/Swedish/nouns/format_nouns.py,sha256=fHDR0Mkd0soXhUyqRbDwYvkS0f__bSo6cHgNulcEOoU,7484
 scribe_data/extract_transform/Swedish/translations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Swedish/translations/format_translations.py,sha256=iijlMmOn6j_h3hawdM06VuVuvj3pZ9PyDJkdNlmRAsw,1298
+scribe_data/extract_transform/Swedish/translations/format_translations.py,sha256=CLyy0JvG3bUhoONzXExPEUL8dXRdsS6xRGm5GqK3ELw,1266
 scribe_data/extract_transform/Swedish/verbs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scribe_data/extract_transform/Swedish/verbs/format_verbs.py,sha256=OvnLKUjYfSmV5Y3ZxtEcRf3V1lKwAeMB4pODZ8JtDVs,3970
+scribe_data/extract_transform/Swedish/verbs/format_verbs.py,sha256=0WyNcV8e__QqiY9Mwl8ohR-64BAWJe9MsDqiHMY64gE,2231
 scribe_data/extract_transform/_resources/2021_ranked.tsv,sha256=1XXrIZetJHgCBkjmfbCLhuIbO8f4FPqBRLnQ2hH74Bc,102823
 scribe_data/extract_transform/_resources/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scribe_data/load/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+scribe_data/load/data_to_sqlite.py,sha256=jXXgbqQgukkwpGxEo-HlnbjvtK6cbr1dSwyVhYDE33Q,8655
+scribe_data/load/send_dbs_to_scribe.py,sha256=kA2gfrIEhOzW9OP5h17bzKCcHfJ5YeLqIQRO5MwZh3w,1185
 scribe_data/load/update_data.py,sha256=ouQ9WOk9sYL1kFJfQDkGos0-8jz_5th2TgYB57mtmOA,12972
-scribe_data/load/update_utils.py,sha256=WXHuMsfkFVK9cMjzfCttpOhFC118UWoZkbKTjx89hlY,6353
-scribe_data-2.2.2.data/data/requirements.txt,sha256=C13PXdDAYDT5VJjil2cnMkz4r6dMVXmI5usEZ_Khejk,389
-scribe_data-2.2.2.dist-info/LICENSE.txt,sha256=xh8S2nza1Sa9y-1HpMCmA-YNu_2vi2aTPNCI6RMsMD8,32472
-scribe_data-2.2.2.dist-info/METADATA,sha256=B2QlEsaHBJhiPhXPHhg8iinRoSbW6sl-49BHmFo5xMc,13440
-scribe_data-2.2.2.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-scribe_data-2.2.2.dist-info/top_level.txt,sha256=GZ2cJsBl_mJRjLt4ZRE21TpBtIqZMjB46rmPaxur-wo,12
-scribe_data-2.2.2.dist-info/RECORD,,
+scribe_data/load/update_utils.py,sha256=x_EW8OE2t5AwcCmYk1LJ72GpLVtxydry4MVWh_u0u7c,6807
+scribe_data-3.0.0.data/data/requirements.txt,sha256=D3SMCkZqONrtoRBNpV7-5nipZmaLZXfvVGAgfr6Cmhw,420
+scribe_data-3.0.0.dist-info/LICENSE.txt,sha256=xh8S2nza1Sa9y-1HpMCmA-YNu_2vi2aTPNCI6RMsMD8,32472
+scribe_data-3.0.0.dist-info/METADATA,sha256=RA1Fqq6ky1QE-6ldU2sjCQaSQkh1__8mXaSJIp66f_Y,15386
+scribe_data-3.0.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+scribe_data-3.0.0.dist-info/top_level.txt,sha256=GZ2cJsBl_mJRjLt4ZRE21TpBtIqZMjB46rmPaxur-wo,12
+scribe_data-3.0.0.dist-info/RECORD,,
```

