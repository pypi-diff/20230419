# Comparing `tmp/datarobot_model_metrics-0.1.3-py2.py3-none-any.whl.zip` & `tmp/datarobot_model_metrics-0.1.4-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,23 +1,23 @@
-Zip file size: 24202 bytes, number of entries: 21
--rw-r--r--  2.0 unx      385 b- defN 23-Apr-03 08:24 dmm/__init__.py
--rw-r--r--  2.0 unx      606 b- defN 23-Apr-03 08:24 dmm/constants.py
--rw-r--r--  2.0 unx     4733 b- defN 23-Apr-03 08:24 dmm/example_data_helper.py
--rw-r--r--  2.0 unx    13088 b- defN 23-Apr-03 08:24 dmm/metric_evaluator.py
--rw-r--r--  2.0 unx     3164 b- defN 23-Apr-03 08:24 dmm/time_bucket.py
--rw-r--r--  2.0 unx     1690 b- defN 23-Apr-03 08:24 dmm/utils.py
--rw-r--r--  2.0 unx      471 b- defN 23-Apr-03 08:24 dmm/data_source/__init__.py
--rw-r--r--  2.0 unx     2096 b- defN 23-Apr-03 08:24 dmm/data_source/data_source_base.py
--rw-r--r--  2.0 unx     2634 b- defN 23-Apr-03 08:24 dmm/data_source/dataframe_source.py
--rw-r--r--  2.0 unx    41610 b- defN 23-Apr-03 08:24 dmm/data_source/datarobot_source.py
--rw-r--r--  2.0 unx     3003 b- defN 23-Apr-03 08:24 dmm/data_source/generator_source.py
--rw-r--r--  2.0 unx      396 b- defN 23-Apr-03 08:24 dmm/metric/__init__.py
--rw-r--r--  2.0 unx     1620 b- defN 23-Apr-03 08:24 dmm/metric/asymmetric_error.py
--rw-r--r--  2.0 unx      320 b- defN 23-Apr-03 08:24 dmm/metric/median_absolute_error.py
--rw-r--r--  2.0 unx     3569 b- defN 23-Apr-03 08:24 dmm/metric/metric_base.py
--rw-r--r--  2.0 unx      957 b- defN 23-Apr-03 08:24 dmm/metric/missing_values.py
--rw-r--r--  2.0 unx     2014 b- defN 23-Apr-03 08:24 dmm/metric/sklearn_metric.py
--rw-r--r--  2.0 unx      886 b- defN 23-Apr-03 08:26 datarobot_model_metrics-0.1.3.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Apr-03 08:26 datarobot_model_metrics-0.1.3.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 23-Apr-03 08:26 datarobot_model_metrics-0.1.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1771 b- defN 23-Apr-03 08:26 datarobot_model_metrics-0.1.3.dist-info/RECORD
-21 files, 85127 bytes uncompressed, 21310 bytes compressed:  75.0%
+Zip file size: 24292 bytes, number of entries: 21
+-rw-r--r--  2.0 unx      385 b- defN 23-Apr-19 08:30 dmm/__init__.py
+-rw-r--r--  2.0 unx      606 b- defN 23-Apr-19 08:30 dmm/constants.py
+-rw-r--r--  2.0 unx     4733 b- defN 23-Apr-19 08:30 dmm/example_data_helper.py
+-rw-r--r--  2.0 unx    13088 b- defN 23-Apr-19 08:30 dmm/metric_evaluator.py
+-rw-r--r--  2.0 unx     3164 b- defN 23-Apr-19 08:30 dmm/time_bucket.py
+-rw-r--r--  2.0 unx     1690 b- defN 23-Apr-19 08:30 dmm/utils.py
+-rw-r--r--  2.0 unx      471 b- defN 23-Apr-19 08:30 dmm/data_source/__init__.py
+-rw-r--r--  2.0 unx     2096 b- defN 23-Apr-19 08:30 dmm/data_source/data_source_base.py
+-rw-r--r--  2.0 unx     2698 b- defN 23-Apr-19 08:30 dmm/data_source/dataframe_source.py
+-rw-r--r--  2.0 unx    41964 b- defN 23-Apr-19 08:30 dmm/data_source/datarobot_source.py
+-rw-r--r--  2.0 unx     3003 b- defN 23-Apr-19 08:30 dmm/data_source/generator_source.py
+-rw-r--r--  2.0 unx      396 b- defN 23-Apr-19 08:30 dmm/metric/__init__.py
+-rw-r--r--  2.0 unx     1620 b- defN 23-Apr-19 08:30 dmm/metric/asymmetric_error.py
+-rw-r--r--  2.0 unx      320 b- defN 23-Apr-19 08:30 dmm/metric/median_absolute_error.py
+-rw-r--r--  2.0 unx     3569 b- defN 23-Apr-19 08:30 dmm/metric/metric_base.py
+-rw-r--r--  2.0 unx      957 b- defN 23-Apr-19 08:30 dmm/metric/missing_values.py
+-rw-r--r--  2.0 unx     2014 b- defN 23-Apr-19 08:30 dmm/metric/sklearn_metric.py
+-rw-r--r--  2.0 unx      886 b- defN 23-Apr-19 08:31 datarobot_model_metrics-0.1.4.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-Apr-19 08:31 datarobot_model_metrics-0.1.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 23-Apr-19 08:31 datarobot_model_metrics-0.1.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1771 b- defN 23-Apr-19 08:31 datarobot_model_metrics-0.1.4.dist-info/RECORD
+21 files, 85545 bytes uncompressed, 21400 bytes compressed:  75.0%
```

## zipnote {}

```diff
@@ -45,20 +45,20 @@
 
 Filename: dmm/metric/missing_values.py
 Comment: 
 
 Filename: dmm/metric/sklearn_metric.py
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.3.dist-info/METADATA
+Filename: datarobot_model_metrics-0.1.4.dist-info/METADATA
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.3.dist-info/WHEEL
+Filename: datarobot_model_metrics-0.1.4.dist-info/WHEEL
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.3.dist-info/top_level.txt
+Filename: datarobot_model_metrics-0.1.4.dist-info/top_level.txt
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.3.dist-info/RECORD
+Filename: datarobot_model_metrics-0.1.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dmm/data_source/dataframe_source.py

```diff
@@ -1,11 +1,11 @@
 import pandas as pd
 from dateutil.parser import parse
 
-from dmm.constants import ColumnName, TimeBucket
+from dmm.constants import ColumnName
 from dmm.data_source.data_source_base import DataSourceBase
 from dmm.time_bucket import check_if_in_same_time_bucket
 
 
 class DataFrameSource(DataSourceBase):
     def __init__(
         self,
@@ -37,15 +37,15 @@
         # In case we are already done with this DF
         if self._current_row >= self._df.shape[0]:
             return None, -1
 
         self._df.reset_index()
         chunk_df = pd.DataFrame(columns=self._df.columns)
         # We can always take at least 1 row into the chunk
-        chunk_df = chunk_df.append(self._df.iloc[self._current_row])
+        chunk_df = pd.concat([chunk_df, self._df.iloc[self._current_row].to_frame().T])
         self._current_row += 1
         chunk_start_datetime = parse(chunk_df.iloc[0][self._timestamp_col])
 
         if self._prev_chunk_datetime:
             # This is for the case where the boundaries of chunks are aligned with the max rows
             if not check_if_in_same_time_bucket(
                 self._prev_chunk_datetime, chunk_start_datetime, self._time_bucket
@@ -62,13 +62,15 @@
         chunk_id_to_return = self._current_chunk_id
         for loc in range(search_start_row, search_end_row):
             loc_datetime = parse(self._df.iloc[loc][self._timestamp_col])
 
             if check_if_in_same_time_bucket(
                 chunk_start_datetime, loc_datetime, self._time_bucket
             ):
-                chunk_df = chunk_df.append(self._df.iloc[self._current_row])
+                chunk_df = pd.concat(
+                    [chunk_df, self._df.iloc[self._current_row].to_frame().T]
+                )
                 self._current_row += 1
             else:
                 self._current_chunk_id += 1
                 break
         return chunk_df, chunk_id_to_return
```

## dmm/data_source/datarobot_source.py

```diff
@@ -1,13 +1,14 @@
 from __future__ import annotations
 
 import datetime
 import errno
 import json
 import logging
+import os
 import tempfile
 import time
 from enum import Enum
 from typing import Callable, Generator, Iterator, List, Optional, Union
 
 import datarobot as dr
 import pandas as pd
@@ -626,29 +627,33 @@
 
         status_url = response.headers["Location"]
         export_id = status_url.split("/")[-2]
 
         dataset_id = self._api.get_training_export_dataset_id(status_url)
 
         dataset = self._api.fetch_dataset_sync(dataset_id)
-        with tempfile.NamedTemporaryFile() as f:
+        tempfile_path = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())
+        with open(tempfile_path, "wb") as f:
             try:
-                dataset.get_file(filelike=f)
+                dataset.get_file(file_path=tempfile_path)
             except OSError as e:
                 if e.errno == errno.ENOSPC:
                     raise Exception(
                         f"We ran out of disk space for training data export: {export_id}"
                     )
                 raise e
             try:
                 result = pd.read_csv(f.name)
             except MemoryError:
+                os.remove(tempfile_path)
                 raise Exception(
                     f"We ran out of memory for training data export: {export_id}"
                 )
+        os.remove(tempfile_path)
+
         return result
 
 
 class DeploymentType(Enum):
     """
     Supported deployment types (based on deployment.model.targetType)
     """
@@ -1034,31 +1039,35 @@
                 fail_info = f"failed to export data for {start_dt} - {start_dt + interval} interval: {str(e)}"
             logger.info(fail_info)
             return pd.DataFrame(), interval
 
         result = pd.DataFrame()
         for dataset_id in self._get_export_dataset_ids(self._deployment_id, export_id):
             dataset = self._api.fetch_dataset_sync(dataset_id)
-            with tempfile.NamedTemporaryFile() as f:
+            tempfile_path = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())
+            with open(tempfile_path, "wb") as f:
                 try:
-                    dataset.get_file(filelike=f)
+                    dataset.get_file(file_path=tempfile_path)
                 except OSError as e:
                     if e.errno == errno.ENOSPC:
                         raise Exception(
                             f"We ran out of disk space for interval {start_dt} - {start_dt + interval}. "
                             f"Consider fetching smaller chunks of data"
                         )
                     raise e
                 try:
                     result = pd.concat([result, pd.read_csv(f.name)])
                 except MemoryError:
+                    os.remove(tempfile_path)
                     raise Exception(
                         f"We ran out of memory for interval {start_dt} - {start_dt + interval}. "
                         f"Consider fetching smaller chunks of data"
                     )
+            os.remove(tempfile_path)
+
         return result, interval
 
     def _get_export_response(
         self, from_dt: datetime.datetime, suggested_interval: datetime.timedelta
     ) -> (requests.Response, datetime.timedelta):
         # we try to increase interval to mitigate too aggressive shrinking
         interval = min(self._end_dt - from_dt, 2 * suggested_interval)
```

## Comparing `datarobot_model_metrics-0.1.3.dist-info/METADATA` & `datarobot_model_metrics-0.1.4.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: datarobot-model-metrics
-Version: 0.1.3
+Version: 0.1.4
 Summary: datarobot-model-metrics is a framework to compute model ML metrics
 Home-page: https://github.com/datarobot/datarobot-model-metrics
 Author: DataRobot
 Author-email: info@datarobot.com
 License: DataRobot
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

## Comparing `datarobot_model_metrics-0.1.3.dist-info/RECORD` & `datarobot_model_metrics-0.1.4.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -2,20 +2,20 @@
 dmm/constants.py,sha256=t3UUKkKgWpbUX0TB3B9S1bVlcDUB_lheArpddCPkWZQ,606
 dmm/example_data_helper.py,sha256=lUSfuzt5DskwccJu3OQLOAzuTJ2nQUuVvtTBKFcjNi4,4733
 dmm/metric_evaluator.py,sha256=e5iUfGPYGTn4XjxImRSO_8WPK2KWExdjAFsXmb6MAlU,13088
 dmm/time_bucket.py,sha256=cg42R1U_o7is6nZf5qUuvdc-2HBH9UKuACKjKe3dLto,3164
 dmm/utils.py,sha256=kaZ7erHZofSwqZL1ddd8GXK3wrZfvYZ7I0ziz2CgbL0,1690
 dmm/data_source/__init__.py,sha256=U9h2pJ89u834vGnBJ_5jjTkQsQQhfCgvAtccXb-Pk7c,471
 dmm/data_source/data_source_base.py,sha256=5zJ9SZYs1ufbku11GYa4PQRY1lwOOsXbx8kDkiG1VMk,2096
-dmm/data_source/dataframe_source.py,sha256=Tzct_ykO62_-quCup_5qfEvCt5cq_9Xj-hYvGMTrtKM,2634
-dmm/data_source/datarobot_source.py,sha256=tkSlMbZmM96vKCFpCjNrblfKvjc-KVGemOSAv5vIuJU,41610
+dmm/data_source/dataframe_source.py,sha256=mKEGXyNFVAsqiocAVKQlhbGlUlwjx77sl5KAlXT6nx4,2698
+dmm/data_source/datarobot_source.py,sha256=ueVPbLGaIK8iOd-Nu-_jgO8UKR7EAK1Et4TrkZLwbTE,41964
 dmm/data_source/generator_source.py,sha256=O-GsreX8mQognj3u6LQxHkZ3yWj3sOgx_dojvObVdlI,3003
 dmm/metric/__init__.py,sha256=nIX41kZJKfQztTw93CXVLRbgTt5W1qhdK1uakWxL9CE,396
 dmm/metric/asymmetric_error.py,sha256=h4J7nzXw9BurtGrkoe4oe7QBI2l69XeB9CiGfpcKUxc,1620
 dmm/metric/median_absolute_error.py,sha256=qV0NpJdF6daTzJOaGpd8KAPG3gZWPS43FZxOb-rR0kE,320
 dmm/metric/metric_base.py,sha256=Rzsju5eZhoyafxo7ErFmO76sDcsvkb3SyODDwcNudy4,3569
 dmm/metric/missing_values.py,sha256=i9ujXCuOWEPrUteFXTCDGX6SM8RVd7cQoI6byQPga4E,957
 dmm/metric/sklearn_metric.py,sha256=Bv4ukOSZyOKjXK-_4b7KKmlkOVrYnwSX5GST_Hf-qpc,2014
-datarobot_model_metrics-0.1.3.dist-info/METADATA,sha256=8wlIo9tXxxpLjbuxdjlOFUZKXjnmBt-8PT20Lx0xmD8,886
-datarobot_model_metrics-0.1.3.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-datarobot_model_metrics-0.1.3.dist-info/top_level.txt,sha256=69FbTyYFh17OyfaIppCUhlu4QG-prAaQ6ovJ_X0SNG8,4
-datarobot_model_metrics-0.1.3.dist-info/RECORD,,
+datarobot_model_metrics-0.1.4.dist-info/METADATA,sha256=7ydRmSHTDyGIDzKxSjP0azdmuqHuq6e_iJYxN0RpgbA,886
+datarobot_model_metrics-0.1.4.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+datarobot_model_metrics-0.1.4.dist-info/top_level.txt,sha256=69FbTyYFh17OyfaIppCUhlu4QG-prAaQ6ovJ_X0SNG8,4
+datarobot_model_metrics-0.1.4.dist-info/RECORD,,
```

