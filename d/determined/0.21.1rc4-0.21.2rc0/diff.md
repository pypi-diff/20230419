# Comparing `tmp/determined-0.21.1rc4-py3-none-any.whl.zip` & `tmp/determined-0.21.2rc0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,252 +1,251 @@
-Zip file size: 563714 bytes, number of entries: 250
--rw-r--r--  2.0 unx     1131 b- defN 23-Apr-11 23:18 determined/__init__.py
--rw-r--r--  2.0 unx       27 b- defN 23-Apr-11 23:18 determined/__version__.py
--rw-r--r--  2.0 unx     1664 b- defN 23-Apr-11 23:18 determined/_env_context.py
--rw-r--r--  2.0 unx     8804 b- defN 23-Apr-11 23:18 determined/_execution.py
--rw-r--r--  2.0 unx     2779 b- defN 23-Apr-11 23:18 determined/_experiment_config.py
--rw-r--r--  2.0 unx     4781 b- defN 23-Apr-11 23:18 determined/_import.py
--rw-r--r--  2.0 unx    14901 b- defN 23-Apr-11 23:18 determined/_info.py
--rw-r--r--  2.0 unx     1272 b- defN 23-Apr-11 23:18 determined/_tf_rng.py
--rw-r--r--  2.0 unx     1214 b- defN 23-Apr-11 23:18 determined/_trial.py
--rw-r--r--  2.0 unx     4726 b- defN 23-Apr-11 23:18 determined/_trial_context.py
--rw-r--r--  2.0 unx     3356 b- defN 23-Apr-11 23:18 determined/_trial_controller.py
--rw-r--r--  2.0 unx     2417 b- defN 23-Apr-11 23:18 determined/constants.py
--rw-r--r--  2.0 unx     2734 b- defN 23-Apr-11 23:18 determined/errors.py
--rw-r--r--  2.0 unx     5431 b- defN 23-Apr-11 23:18 determined/gpu.py
--rw-r--r--  2.0 unx     6182 b- defN 23-Apr-11 23:18 determined/horovod.py
--rw-r--r--  2.0 unx    19374 b- defN 23-Apr-11 23:18 determined/ipc.py
--rw-r--r--  2.0 unx     3065 b- defN 23-Apr-11 23:18 determined/load.py
--rw-r--r--  2.0 unx      737 b- defN 23-Apr-11 23:18 determined/monkey_patch.py
--rw-r--r--  2.0 unx    39670 b- defN 23-Apr-11 23:18 determined/profiler.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/py.typed
--rw-r--r--  2.0 unx    15913 b- defN 23-Apr-11 23:18 determined/util.py
--rw-r--r--  2.0 unx     6578 b- defN 23-Apr-11 23:18 determined/workload.py
--rw-r--r--  2.0 unx      476 b- defN 23-Apr-11 23:18 determined/cli/__init__.py
--rw-r--r--  2.0 unx       75 b- defN 23-Apr-11 23:18 determined/cli/__main__.py
--rw-r--r--  2.0 unx     3423 b- defN 23-Apr-11 23:18 determined/cli/_util.py
--rw-r--r--  2.0 unx     9636 b- defN 23-Apr-11 23:18 determined/cli/agent.py
--rw-r--r--  2.0 unx     6425 b- defN 23-Apr-11 23:18 determined/cli/checkpoint.py
--rw-r--r--  2.0 unx    12433 b- defN 23-Apr-11 23:18 determined/cli/cli.py
--rw-r--r--  2.0 unx    13446 b- defN 23-Apr-11 23:18 determined/cli/command.py
--rw-r--r--  2.0 unx     2354 b- defN 23-Apr-11 23:18 determined/cli/dev.py
--rw-r--r--  2.0 unx      739 b- defN 23-Apr-11 23:18 determined/cli/errors.py
--rw-r--r--  2.0 unx    49043 b- defN 23-Apr-11 23:18 determined/cli/experiment.py
--rw-r--r--  2.0 unx     8386 b- defN 23-Apr-11 23:18 determined/cli/job.py
--rw-r--r--  2.0 unx     2110 b- defN 23-Apr-11 23:18 determined/cli/master.py
--rw-r--r--  2.0 unx     8693 b- defN 23-Apr-11 23:18 determined/cli/model.py
--rw-r--r--  2.0 unx     5650 b- defN 23-Apr-11 23:18 determined/cli/notebook.py
--rw-r--r--  2.0 unx     1761 b- defN 23-Apr-11 23:18 determined/cli/oauth.py
--rw-r--r--  2.0 unx    12212 b- defN 23-Apr-11 23:18 determined/cli/project.py
--rw-r--r--  2.0 unx     2826 b- defN 23-Apr-11 23:18 determined/cli/proxy.py
--rw-r--r--  2.0 unx    18745 b- defN 23-Apr-11 23:18 determined/cli/rbac.py
--rw-r--r--  2.0 unx     3527 b- defN 23-Apr-11 23:18 determined/cli/remote.py
--rw-r--r--  2.0 unx     4484 b- defN 23-Apr-11 23:18 determined/cli/render.py
--rw-r--r--  2.0 unx     2386 b- defN 23-Apr-11 23:18 determined/cli/resources.py
--rw-r--r--  2.0 unx     9719 b- defN 23-Apr-11 23:18 determined/cli/shell.py
--rw-r--r--  2.0 unx     5105 b- defN 23-Apr-11 23:18 determined/cli/sso.py
--rw-r--r--  2.0 unx     5591 b- defN 23-Apr-11 23:18 determined/cli/task.py
--rw-r--r--  2.0 unx     2716 b- defN 23-Apr-11 23:18 determined/cli/template.py
--rw-r--r--  2.0 unx     6733 b- defN 23-Apr-11 23:18 determined/cli/tensorboard.py
--rw-r--r--  2.0 unx      135 b- defN 23-Apr-11 23:18 determined/cli/top_arg_descriptions.py
--rw-r--r--  2.0 unx    14756 b- defN 23-Apr-11 23:18 determined/cli/trial.py
--rw-r--r--  2.0 unx     8899 b- defN 23-Apr-11 23:18 determined/cli/tunnel.py
--rw-r--r--  2.0 unx     6560 b- defN 23-Apr-11 23:18 determined/cli/user.py
--rw-r--r--  2.0 unx     9727 b- defN 23-Apr-11 23:18 determined/cli/user_groups.py
--rw-r--r--  2.0 unx     2557 b- defN 23-Apr-11 23:18 determined/cli/version.py
--rw-r--r--  2.0 unx    15211 b- defN 23-Apr-11 23:18 determined/cli/workspace.py
--rw-r--r--  2.0 unx      352 b- defN 23-Apr-11 23:18 determined/common/__init__.py
--rw-r--r--  2.0 unx      498 b- defN 23-Apr-11 23:18 determined/common/_logging.py
--rw-r--r--  2.0 unx     8593 b- defN 23-Apr-11 23:18 determined/common/check.py
--rw-r--r--  2.0 unx     2255 b- defN 23-Apr-11 23:18 determined/common/constants.py
--rw-r--r--  2.0 unx     8213 b- defN 23-Apr-11 23:18 determined/common/context.py
--rw-r--r--  2.0 unx     7359 b- defN 23-Apr-11 23:18 determined/common/declarative_argparse.py
--rw-r--r--  2.0 unx     1662 b- defN 23-Apr-11 23:18 determined/common/requests.py
--rw-r--r--  2.0 unx     6349 b- defN 23-Apr-11 23:18 determined/common/util.py
--rw-r--r--  2.0 unx      806 b- defN 23-Apr-11 23:18 determined/common/api/__init__.py
--rw-r--r--  2.0 unx     3383 b- defN 23-Apr-11 23:18 determined/common/api/_session.py
--rw-r--r--  2.0 unx     2855 b- defN 23-Apr-11 23:18 determined/common/api/_util.py
--rw-r--r--  2.0 unx     1450 b- defN 23-Apr-11 23:18 determined/common/api/analytics.py
--rw-r--r--  2.0 unx    15676 b- defN 23-Apr-11 23:18 determined/common/api/authentication.py
--rw-r--r--  2.0 unx   597398 b- defN 23-Apr-11 23:18 determined/common/api/bindings.py
--rw-r--r--  2.0 unx     7415 b- defN 23-Apr-11 23:18 determined/common/api/certs.py
--rw-r--r--  2.0 unx     2555 b- defN 23-Apr-11 23:18 determined/common/api/errors.py
--rw-r--r--  2.0 unx     3970 b- defN 23-Apr-11 23:18 determined/common/api/logs.py
--rw-r--r--  2.0 unx      247 b- defN 23-Apr-11 23:18 determined/common/api/metric.py
--rw-r--r--  2.0 unx     2620 b- defN 23-Apr-11 23:18 determined/common/api/profiler.py
--rw-r--r--  2.0 unx    10384 b- defN 23-Apr-11 23:18 determined/common/api/request.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/common/api/checkpoint/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/common/api/checkpoint/torch_load.py
--rw-r--r--  2.0 unx      502 b- defN 23-Apr-11 23:18 determined/common/experimental/__init__.py
--rw-r--r--  2.0 unx    15863 b- defN 23-Apr-11 23:18 determined/common/experimental/determined.py
--rw-r--r--  2.0 unx    11536 b- defN 23-Apr-11 23:18 determined/common/experimental/experiment.py
--rw-r--r--  2.0 unx    12748 b- defN 23-Apr-11 23:18 determined/common/experimental/model.py
--rw-r--r--  2.0 unx      307 b- defN 23-Apr-11 23:18 determined/common/experimental/oauth2_scim_client.py
--rw-r--r--  2.0 unx      466 b- defN 23-Apr-11 23:18 determined/common/experimental/session.py
--rw-r--r--  2.0 unx    15977 b- defN 23-Apr-11 23:18 determined/common/experimental/trial.py
--rw-r--r--  2.0 unx     3503 b- defN 23-Apr-11 23:18 determined/common/experimental/user.py
--rw-r--r--  2.0 unx      125 b- defN 23-Apr-11 23:18 determined/common/experimental/checkpoint/__init__.py
--rw-r--r--  2.0 unx    13791 b- defN 23-Apr-11 23:18 determined/common/experimental/checkpoint/_checkpoint.py
--rw-r--r--  2.0 unx     3868 b- defN 23-Apr-11 23:18 determined/common/storage/__init__.py
--rw-r--r--  2.0 unx     3822 b- defN 23-Apr-11 23:18 determined/common/storage/azure.py
--rw-r--r--  2.0 unx     3514 b- defN 23-Apr-11 23:18 determined/common/storage/azure_client.py
--rw-r--r--  2.0 unx     7152 b- defN 23-Apr-11 23:18 determined/common/storage/base.py
--rw-r--r--  2.0 unx     4447 b- defN 23-Apr-11 23:18 determined/common/storage/boto3_credential_manager.py
--rw-r--r--  2.0 unx     1014 b- defN 23-Apr-11 23:18 determined/common/storage/cloud.py
--rw-r--r--  2.0 unx     5789 b- defN 23-Apr-11 23:18 determined/common/storage/gcs.py
--rw-r--r--  2.0 unx     2070 b- defN 23-Apr-11 23:18 determined/common/storage/hdfs.py
--rw-r--r--  2.0 unx     6195 b- defN 23-Apr-11 23:18 determined/common/storage/s3.py
--rw-r--r--  2.0 unx     7816 b- defN 23-Apr-11 23:18 determined/common/storage/shared.py
--rw-r--r--  2.0 unx      747 b- defN 23-Apr-11 23:18 determined/core/__init__.py
--rw-r--r--  2.0 unx    29477 b- defN 23-Apr-11 23:18 determined/core/_checkpoint.py
--rw-r--r--  2.0 unx    10130 b- defN 23-Apr-11 23:18 determined/core/_context.py
--rw-r--r--  2.0 unx    16194 b- defN 23-Apr-11 23:18 determined/core/_distributed.py
--rw-r--r--  2.0 unx    12738 b- defN 23-Apr-11 23:18 determined/core/_preempt.py
--rw-r--r--  2.0 unx    15041 b- defN 23-Apr-11 23:18 determined/core/_searcher.py
--rw-r--r--  2.0 unx      932 b- defN 23-Apr-11 23:18 determined/core/_tensorboard_mode.py
--rw-r--r--  2.0 unx    10462 b- defN 23-Apr-11 23:18 determined/core/_train.py
--rw-r--r--  2.0 unx       40 b- defN 23-Apr-11 23:18 determined/deploy/__init__.py
--rw-r--r--  2.0 unx     1057 b- defN 23-Apr-11 23:18 determined/deploy/cli.py
--rw-r--r--  2.0 unx       94 b- defN 23-Apr-11 23:18 determined/deploy/errors.py
--rw-r--r--  2.0 unx     1472 b- defN 23-Apr-11 23:18 determined/deploy/healthcheck.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/deploy/aws/__init__.py
--rw-r--r--  2.0 unx    19016 b- defN 23-Apr-11 23:18 determined/deploy/aws/aws.py
--rw-r--r--  2.0 unx    24035 b- defN 23-Apr-11 23:18 determined/deploy/aws/cli.py
--rw-r--r--  2.0 unx     2998 b- defN 23-Apr-11 23:18 determined/deploy/aws/constants.py
--rw-r--r--  2.0 unx     1396 b- defN 23-Apr-11 23:18 determined/deploy/aws/gen_vcpu_mapping.py
--rw-r--r--  2.0 unx     2201 b- defN 23-Apr-11 23:18 determined/deploy/aws/master_config_inject.py
--rw-r--r--  2.0 unx     4933 b- defN 23-Apr-11 23:18 determined/deploy/aws/preflight.py
--rw-r--r--  2.0 unx    17324 b- defN 23-Apr-11 23:18 determined/deploy/aws/vcpu_mapping.yaml
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/deploy/aws/deployment_types/__init__.py
--rw-r--r--  2.0 unx     5624 b- defN 23-Apr-11 23:18 determined/deploy/aws/deployment_types/base.py
--rw-r--r--  2.0 unx     1593 b- defN 23-Apr-11 23:18 determined/deploy/aws/deployment_types/govcloud.py
--rw-r--r--  2.0 unx     3094 b- defN 23-Apr-11 23:18 determined/deploy/aws/deployment_types/secure.py
--rw-r--r--  2.0 unx     1349 b- defN 23-Apr-11 23:18 determined/deploy/aws/deployment_types/simple.py
--rw-r--r--  2.0 unx     1400 b- defN 23-Apr-11 23:18 determined/deploy/aws/deployment_types/vpc.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/deploy/aws/templates/__init__.py
--rw-r--r--  2.0 unx    29259 b- defN 23-Apr-11 23:18 determined/deploy/aws/templates/efs.yaml
--rw-r--r--  2.0 unx    29969 b- defN 23-Apr-11 23:18 determined/deploy/aws/templates/fsx.yaml
--rw-r--r--  2.0 unx    23518 b- defN 23-Apr-11 23:18 determined/deploy/aws/templates/govcloud.yaml
--rw-r--r--  2.0 unx    29153 b- defN 23-Apr-11 23:18 determined/deploy/aws/templates/secure.yaml
--rw-r--r--  2.0 unx    24926 b- defN 23-Apr-11 23:18 determined/deploy/aws/templates/simple.yaml
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/deploy/gcp/__init__.py
--rw-r--r--  2.0 unx    19015 b- defN 23-Apr-11 23:18 determined/deploy/gcp/cli.py
--rw-r--r--  2.0 unx      773 b- defN 23-Apr-11 23:18 determined/deploy/gcp/constants.py
--rw-r--r--  2.0 unx    12420 b- defN 23-Apr-11 23:18 determined/deploy/gcp/gcp.py
--rw-r--r--  2.0 unx     2380 b- defN 23-Apr-11 23:18 determined/deploy/gcp/preflight.py
--rw-r--r--  2.0 unx     5376 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/main.tf
--rw-r--r--  2.0 unx     1569 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/master.yaml.tmpl
--rw-r--r--  2.0 unx     1487 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/outputs.tf
--rw-r--r--  2.0 unx     3767 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/variables.tf
--rw-r--r--  2.0 unx     6105 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/compute/main.tf
--rw-r--r--  2.0 unx      424 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/compute/outputs.tf
--rw-r--r--  2.0 unx     1574 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/compute/variables.tf
--rw-r--r--  2.0 unx     1222 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/database/main.tf
--rw-r--r--  2.0 unx      283 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/database/outputs.tf
--rw-r--r--  2.0 unx      320 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/database/variables.tf
--rw-r--r--  2.0 unx      522 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/filestore/main.tf
--rw-r--r--  2.0 unx      184 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/filestore/outputs.tf
--rw-r--r--  2.0 unx      125 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/filestore/variables.tf
--rw-r--r--  2.0 unx     1026 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/firewall/main.tf
--rw-r--r--  2.0 unx      195 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/firewall/outputs.tf
--rw-r--r--  2.0 unx       75 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/firewall/variables.tf
--rw-r--r--  2.0 unx      637 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/gcs/main.tf
--rw-r--r--  2.0 unx       51 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/gcs/outputs.tf
--rw-r--r--  2.0 unx      161 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/gcs/variables.tf
--rw-r--r--  2.0 unx      161 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/ip/main.tf
--rw-r--r--  2.0 unx      122 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/ip/outputs.tf
--rw-r--r--  2.0 unx       88 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/ip/variables.tf
--rw-r--r--  2.0 unx     1112 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/network/main.tf
--rw-r--r--  2.0 unx      275 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/network/outputs.tf
--rw-r--r--  2.0 unx      207 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/network/variables.tf
--rw-r--r--  2.0 unx      962 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/service_account/main.tf
--rw-r--r--  2.0 unx       73 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/service_account/outputs.tf
--rw-r--r--  2.0 unx      138 b- defN 23-Apr-11 23:18 determined/deploy/gcp/terraform/modules/service_account/variables.tf
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/deploy/gke/__init__.py
--rw-r--r--  2.0 unx    19568 b- defN 23-Apr-11 23:18 determined/deploy/gke/cli.py
--rw-r--r--  2.0 unx      408 b- defN 23-Apr-11 23:18 determined/deploy/gke/constants.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/deploy/local/__init__.py
--rw-r--r--  2.0 unx    12007 b- defN 23-Apr-11 23:18 determined/deploy/local/cli.py
--rw-r--r--  2.0 unx    12390 b- defN 23-Apr-11 23:18 determined/deploy/local/cluster_utils.py
--rw-r--r--  2.0 unx      870 b- defN 23-Apr-11 23:18 determined/deploy/local/docker-compose.yaml
--rw-r--r--  2.0 unx     1134 b- defN 23-Apr-11 23:18 determined/deploy/local/preflight.py
--rw-r--r--  2.0 unx      697 b- defN 23-Apr-11 23:18 determined/estimator/__init__.py
--rw-r--r--  2.0 unx     1288 b- defN 23-Apr-11 23:18 determined/estimator/_callback.py
--rw-r--r--  2.0 unx     7375 b- defN 23-Apr-11 23:18 determined/estimator/_estimator_context.py
--rw-r--r--  2.0 unx    40174 b- defN 23-Apr-11 23:18 determined/estimator/_estimator_trial.py
--rw-r--r--  2.0 unx     3086 b- defN 23-Apr-11 23:18 determined/estimator/_load.py
--rw-r--r--  2.0 unx    13742 b- defN 23-Apr-11 23:18 determined/estimator/_reducer.py
--rw-r--r--  2.0 unx     9600 b- defN 23-Apr-11 23:18 determined/estimator/_util.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/exec/__init__.py
--rw-r--r--  2.0 unx     3836 b- defN 23-Apr-11 23:18 determined/exec/gc_checkpoints.py
--rw-r--r--  2.0 unx     8857 b- defN 23-Apr-11 23:18 determined/exec/harness.py
--rw-r--r--  2.0 unx     4233 b- defN 23-Apr-11 23:18 determined/exec/launch.py
--rw-r--r--  2.0 unx      389 b- defN 23-Apr-11 23:18 determined/exec/pid_client.py
--rw-r--r--  2.0 unx     1476 b- defN 23-Apr-11 23:18 determined/exec/pid_server.py
--rw-r--r--  2.0 unx    13479 b- defN 23-Apr-11 23:18 determined/exec/prep_container.py
--rw-r--r--  2.0 unx    10994 b- defN 23-Apr-11 23:18 determined/exec/tensorboard.py
--rw-r--r--  2.0 unx      264 b- defN 23-Apr-11 23:18 determined/experimental/__init__.py
--rw-r--r--  2.0 unx     3473 b- defN 23-Apr-11 23:18 determined/experimental/_native.py
--rw-r--r--  2.0 unx    15160 b- defN 23-Apr-11 23:18 determined/experimental/client.py
--rw-r--r--  2.0 unx      744 b- defN 23-Apr-11 23:18 determined/keras/__init__.py
--rw-r--r--  2.0 unx     7579 b- defN 23-Apr-11 23:18 determined/keras/_data.py
--rw-r--r--  2.0 unx    11491 b- defN 23-Apr-11 23:18 determined/keras/_enqueuer.py
--rw-r--r--  2.0 unx     4020 b- defN 23-Apr-11 23:18 determined/keras/_load.py
--rw-r--r--  2.0 unx      486 b- defN 23-Apr-11 23:18 determined/keras/_tensorboard_callback.py
--rw-r--r--  2.0 unx    19043 b- defN 23-Apr-11 23:18 determined/keras/_tf_keras_context.py
--rw-r--r--  2.0 unx     1204 b- defN 23-Apr-11 23:18 determined/keras/_tf_keras_multi_gpu.py
--rw-r--r--  2.0 unx    47717 b- defN 23-Apr-11 23:18 determined/keras/_tf_keras_trial.py
--rw-r--r--  2.0 unx    28556 b- defN 23-Apr-11 23:18 determined/keras/callbacks.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-11 23:18 determined/launch/__init__.py
--rw-r--r--  2.0 unx    13722 b- defN 23-Apr-11 23:18 determined/launch/deepspeed.py
--rw-r--r--  2.0 unx    10813 b- defN 23-Apr-11 23:18 determined/launch/horovod.py
--rw-r--r--  2.0 unx     4574 b- defN 23-Apr-11 23:18 determined/launch/torch_distributed.py
--rw-r--r--  2.0 unx     4502 b- defN 23-Apr-11 23:18 determined/launch/wrap_rank.py
--rw-r--r--  2.0 unx       98 b- defN 23-Apr-11 23:18 determined/layers/__init__.py
--rw-r--r--  2.0 unx    19401 b- defN 23-Apr-11 23:18 determined/layers/_workload_sequencer.py
--rw-r--r--  2.0 unx     1134 b- defN 23-Apr-11 23:18 determined/pytorch/__init__.py
--rw-r--r--  2.0 unx     5459 b- defN 23-Apr-11 23:18 determined/pytorch/_callback.py
--rw-r--r--  2.0 unx    16282 b- defN 23-Apr-11 23:18 determined/pytorch/_data.py
--rw-r--r--  2.0 unx     3670 b- defN 23-Apr-11 23:18 determined/pytorch/_experimental.py
--rw-r--r--  2.0 unx    12510 b- defN 23-Apr-11 23:18 determined/pytorch/_load.py
--rw-r--r--  2.0 unx     3286 b- defN 23-Apr-11 23:18 determined/pytorch/_lr_scheduler.py
--rw-r--r--  2.0 unx     8276 b- defN 23-Apr-11 23:18 determined/pytorch/_metric_utils.py
--rw-r--r--  2.0 unx    45114 b- defN 23-Apr-11 23:18 determined/pytorch/_pytorch_context.py
--rw-r--r--  2.0 unx    67783 b- defN 23-Apr-11 23:18 determined/pytorch/_pytorch_trial.py
--rw-r--r--  2.0 unx    21404 b- defN 23-Apr-11 23:18 determined/pytorch/_reducer.py
--rw-r--r--  2.0 unx    13510 b- defN 23-Apr-11 23:18 determined/pytorch/_trainer.py
--rw-r--r--  2.0 unx     9561 b- defN 23-Apr-11 23:18 determined/pytorch/samplers.py
--rw-r--r--  2.0 unx      347 b- defN 23-Apr-11 23:18 determined/pytorch/deepspeed/__init__.py
--rw-r--r--  2.0 unx    17524 b- defN 23-Apr-11 23:18 determined/pytorch/deepspeed/_deepspeed_context.py
--rw-r--r--  2.0 unx    42777 b- defN 23-Apr-11 23:18 determined/pytorch/deepspeed/_deepspeed_trial.py
--rw-r--r--  2.0 unx     1987 b- defN 23-Apr-11 23:18 determined/pytorch/deepspeed/_mpu.py
--rw-r--r--  2.0 unx       67 b- defN 23-Apr-11 23:18 determined/pytorch/lightning/__init__.py
--rw-r--r--  2.0 unx    18157 b- defN 23-Apr-11 23:18 determined/pytorch/lightning/_adapter.py
--rw-r--r--  2.0 unx      343 b- defN 23-Apr-11 23:18 determined/searcher/__init__.py
--rw-r--r--  2.0 unx     3563 b- defN 23-Apr-11 23:18 determined/searcher/_remote_search_runner.py
--rw-r--r--  2.0 unx    10064 b- defN 23-Apr-11 23:18 determined/searcher/_search_method.py
--rw-r--r--  2.0 unx    14093 b- defN 23-Apr-11 23:18 determined/searcher/_search_runner.py
--rw-r--r--  2.0 unx      512 b- defN 23-Apr-11 23:18 determined/tensorboard/__init__.py
--rw-r--r--  2.0 unx     1573 b- defN 23-Apr-11 23:18 determined/tensorboard/azure.py
--rw-r--r--  2.0 unx     5797 b- defN 23-Apr-11 23:18 determined/tensorboard/base.py
--rw-r--r--  2.0 unx     4534 b- defN 23-Apr-11 23:18 determined/tensorboard/build.py
--rw-r--r--  2.0 unx     2305 b- defN 23-Apr-11 23:18 determined/tensorboard/gcs.py
--rw-r--r--  2.0 unx     1333 b- defN 23-Apr-11 23:18 determined/tensorboard/hdfs.py
--rw-r--r--  2.0 unx     1892 b- defN 23-Apr-11 23:18 determined/tensorboard/s3.py
--rw-r--r--  2.0 unx     1747 b- defN 23-Apr-11 23:18 determined/tensorboard/shared.py
--rw-r--r--  2.0 unx     3734 b- defN 23-Apr-11 23:18 determined/tensorboard/util.py
--rw-r--r--  2.0 unx      780 b- defN 23-Apr-11 23:18 determined/tensorboard/fetchers/__init__.py
--rw-r--r--  2.0 unx     2534 b- defN 23-Apr-11 23:18 determined/tensorboard/fetchers/azure.py
--rw-r--r--  2.0 unx     1670 b- defN 23-Apr-11 23:18 determined/tensorboard/fetchers/base.py
--rw-r--r--  2.0 unx     1733 b- defN 23-Apr-11 23:18 determined/tensorboard/fetchers/gcs.py
--rw-r--r--  2.0 unx     2476 b- defN 23-Apr-11 23:18 determined/tensorboard/fetchers/s3.py
--rw-r--r--  2.0 unx     1634 b- defN 23-Apr-11 23:18 determined/tensorboard/fetchers/shared.py
--rw-r--r--  2.0 unx       91 b- defN 23-Apr-11 23:18 determined/tensorboard/metric_writers/__init__.py
--rw-r--r--  2.0 unx     2113 b- defN 23-Apr-11 23:18 determined/tensorboard/metric_writers/callback.py
--rw-r--r--  2.0 unx     2936 b- defN 23-Apr-11 23:18 determined/tensorboard/metric_writers/pytorch.py
--rw-r--r--  2.0 unx     4385 b- defN 23-Apr-11 23:18 determined/tensorboard/metric_writers/tensorflow.py
--rw-r--r--  2.0 unx     1363 b- defN 23-Apr-11 23:18 determined-0.21.1rc4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-11 23:18 determined-0.21.1rc4.dist-info/WHEEL
--rw-r--r--  2.0 unx       53 b- defN 23-Apr-11 23:18 determined-0.21.1rc4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       11 b- defN 23-Apr-11 23:18 determined-0.21.1rc4.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    22941 b- defN 23-Apr-11 23:18 determined-0.21.1rc4.dist-info/RECORD
-250 files, 2334441 bytes uncompressed, 527122 bytes compressed:  77.4%
+Zip file size: 566304 bytes, number of entries: 249
+-rw-r--r--  2.0 unx     1131 b- defN 23-Apr-19 15:01 determined/__init__.py
+-rw-r--r--  2.0 unx       27 b- defN 23-Apr-19 15:01 determined/__version__.py
+-rw-r--r--  2.0 unx     1664 b- defN 23-Apr-19 15:01 determined/_env_context.py
+-rw-r--r--  2.0 unx     8804 b- defN 23-Apr-19 15:01 determined/_execution.py
+-rw-r--r--  2.0 unx     2779 b- defN 23-Apr-19 15:01 determined/_experiment_config.py
+-rw-r--r--  2.0 unx     4781 b- defN 23-Apr-19 15:01 determined/_import.py
+-rw-r--r--  2.0 unx    14901 b- defN 23-Apr-19 15:01 determined/_info.py
+-rw-r--r--  2.0 unx     1272 b- defN 23-Apr-19 15:01 determined/_tf_rng.py
+-rw-r--r--  2.0 unx     1214 b- defN 23-Apr-19 15:01 determined/_trial.py
+-rw-r--r--  2.0 unx     4726 b- defN 23-Apr-19 15:01 determined/_trial_context.py
+-rw-r--r--  2.0 unx     3356 b- defN 23-Apr-19 15:01 determined/_trial_controller.py
+-rw-r--r--  2.0 unx     2417 b- defN 23-Apr-19 15:01 determined/constants.py
+-rw-r--r--  2.0 unx     2734 b- defN 23-Apr-19 15:01 determined/errors.py
+-rw-r--r--  2.0 unx     5431 b- defN 23-Apr-19 15:01 determined/gpu.py
+-rw-r--r--  2.0 unx     6182 b- defN 23-Apr-19 15:01 determined/horovod.py
+-rw-r--r--  2.0 unx    19374 b- defN 23-Apr-19 15:01 determined/ipc.py
+-rw-r--r--  2.0 unx     3065 b- defN 23-Apr-19 15:01 determined/load.py
+-rw-r--r--  2.0 unx      737 b- defN 23-Apr-19 15:01 determined/monkey_patch.py
+-rw-r--r--  2.0 unx    39670 b- defN 23-Apr-19 15:01 determined/profiler.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/py.typed
+-rw-r--r--  2.0 unx    15909 b- defN 23-Apr-19 15:01 determined/util.py
+-rw-r--r--  2.0 unx     6578 b- defN 23-Apr-19 15:01 determined/workload.py
+-rw-r--r--  2.0 unx      476 b- defN 23-Apr-19 15:01 determined/cli/__init__.py
+-rw-r--r--  2.0 unx       75 b- defN 23-Apr-19 15:01 determined/cli/__main__.py
+-rw-r--r--  2.0 unx     3423 b- defN 23-Apr-19 15:01 determined/cli/_util.py
+-rw-r--r--  2.0 unx     9783 b- defN 23-Apr-19 15:01 determined/cli/agent.py
+-rw-r--r--  2.0 unx     6409 b- defN 23-Apr-19 15:01 determined/cli/checkpoint.py
+-rw-r--r--  2.0 unx    12433 b- defN 23-Apr-19 15:01 determined/cli/cli.py
+-rw-r--r--  2.0 unx    13446 b- defN 23-Apr-19 15:01 determined/cli/command.py
+-rw-r--r--  2.0 unx     2354 b- defN 23-Apr-19 15:01 determined/cli/dev.py
+-rw-r--r--  2.0 unx      739 b- defN 23-Apr-19 15:01 determined/cli/errors.py
+-rw-r--r--  2.0 unx    49929 b- defN 23-Apr-19 15:01 determined/cli/experiment.py
+-rw-r--r--  2.0 unx     8326 b- defN 23-Apr-19 15:01 determined/cli/job.py
+-rw-r--r--  2.0 unx     2110 b- defN 23-Apr-19 15:01 determined/cli/master.py
+-rw-r--r--  2.0 unx     8693 b- defN 23-Apr-19 15:01 determined/cli/model.py
+-rw-r--r--  2.0 unx     5635 b- defN 23-Apr-19 15:01 determined/cli/notebook.py
+-rw-r--r--  2.0 unx     1761 b- defN 23-Apr-19 15:01 determined/cli/oauth.py
+-rw-r--r--  2.0 unx    12180 b- defN 23-Apr-19 15:01 determined/cli/project.py
+-rw-r--r--  2.0 unx     2808 b- defN 23-Apr-19 15:01 determined/cli/proxy.py
+-rw-r--r--  2.0 unx    18745 b- defN 23-Apr-19 15:01 determined/cli/rbac.py
+-rw-r--r--  2.0 unx     3783 b- defN 23-Apr-19 15:01 determined/cli/remote.py
+-rw-r--r--  2.0 unx     4484 b- defN 23-Apr-19 15:01 determined/cli/render.py
+-rw-r--r--  2.0 unx     2386 b- defN 23-Apr-19 15:01 determined/cli/resources.py
+-rw-r--r--  2.0 unx     9718 b- defN 23-Apr-19 15:01 determined/cli/shell.py
+-rw-r--r--  2.0 unx     5105 b- defN 23-Apr-19 15:01 determined/cli/sso.py
+-rw-r--r--  2.0 unx     6363 b- defN 23-Apr-19 15:01 determined/cli/task.py
+-rw-r--r--  2.0 unx     2716 b- defN 23-Apr-19 15:01 determined/cli/template.py
+-rw-r--r--  2.0 unx     6718 b- defN 23-Apr-19 15:01 determined/cli/tensorboard.py
+-rw-r--r--  2.0 unx      135 b- defN 23-Apr-19 15:01 determined/cli/top_arg_descriptions.py
+-rw-r--r--  2.0 unx    15065 b- defN 23-Apr-19 15:01 determined/cli/trial.py
+-rw-r--r--  2.0 unx     8899 b- defN 23-Apr-19 15:01 determined/cli/tunnel.py
+-rw-r--r--  2.0 unx     6560 b- defN 23-Apr-19 15:01 determined/cli/user.py
+-rw-r--r--  2.0 unx     9727 b- defN 23-Apr-19 15:01 determined/cli/user_groups.py
+-rw-r--r--  2.0 unx     2557 b- defN 23-Apr-19 15:01 determined/cli/version.py
+-rw-r--r--  2.0 unx    15179 b- defN 23-Apr-19 15:01 determined/cli/workspace.py
+-rw-r--r--  2.0 unx      352 b- defN 23-Apr-19 15:01 determined/common/__init__.py
+-rw-r--r--  2.0 unx      498 b- defN 23-Apr-19 15:01 determined/common/_logging.py
+-rw-r--r--  2.0 unx     8593 b- defN 23-Apr-19 15:01 determined/common/check.py
+-rw-r--r--  2.0 unx     2255 b- defN 23-Apr-19 15:01 determined/common/constants.py
+-rw-r--r--  2.0 unx     8213 b- defN 23-Apr-19 15:01 determined/common/context.py
+-rw-r--r--  2.0 unx     7359 b- defN 23-Apr-19 15:01 determined/common/declarative_argparse.py
+-rw-r--r--  2.0 unx     1662 b- defN 23-Apr-19 15:01 determined/common/requests.py
+-rw-r--r--  2.0 unx     6349 b- defN 23-Apr-19 15:01 determined/common/util.py
+-rw-r--r--  2.0 unx      799 b- defN 23-Apr-19 15:01 determined/common/api/__init__.py
+-rw-r--r--  2.0 unx     3383 b- defN 23-Apr-19 15:01 determined/common/api/_session.py
+-rw-r--r--  2.0 unx     2840 b- defN 23-Apr-19 15:01 determined/common/api/_util.py
+-rw-r--r--  2.0 unx     1450 b- defN 23-Apr-19 15:01 determined/common/api/analytics.py
+-rw-r--r--  2.0 unx    15676 b- defN 23-Apr-19 15:01 determined/common/api/authentication.py
+-rw-r--r--  2.0 unx   611406 b- defN 23-Apr-19 15:01 determined/common/api/bindings.py
+-rw-r--r--  2.0 unx     7415 b- defN 23-Apr-19 15:01 determined/common/api/certs.py
+-rw-r--r--  2.0 unx     2555 b- defN 23-Apr-19 15:01 determined/common/api/errors.py
+-rw-r--r--  2.0 unx     3438 b- defN 23-Apr-19 15:01 determined/common/api/logs.py
+-rw-r--r--  2.0 unx      247 b- defN 23-Apr-19 15:01 determined/common/api/metric.py
+-rw-r--r--  2.0 unx     2620 b- defN 23-Apr-19 15:01 determined/common/api/profiler.py
+-rw-r--r--  2.0 unx    10384 b- defN 23-Apr-19 15:01 determined/common/api/request.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/common/api/checkpoint/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/common/api/checkpoint/torch_load.py
+-rw-r--r--  2.0 unx      502 b- defN 23-Apr-19 15:01 determined/common/experimental/__init__.py
+-rw-r--r--  2.0 unx    15864 b- defN 23-Apr-19 15:01 determined/common/experimental/determined.py
+-rw-r--r--  2.0 unx    11398 b- defN 23-Apr-19 15:01 determined/common/experimental/experiment.py
+-rw-r--r--  2.0 unx    12639 b- defN 23-Apr-19 15:01 determined/common/experimental/model.py
+-rw-r--r--  2.0 unx      306 b- defN 23-Apr-19 15:01 determined/common/experimental/oauth2_scim_client.py
+-rw-r--r--  2.0 unx      466 b- defN 23-Apr-19 15:01 determined/common/experimental/session.py
+-rw-r--r--  2.0 unx    15743 b- defN 23-Apr-19 15:01 determined/common/experimental/trial.py
+-rw-r--r--  2.0 unx     3503 b- defN 23-Apr-19 15:01 determined/common/experimental/user.py
+-rw-r--r--  2.0 unx      125 b- defN 23-Apr-19 15:01 determined/common/experimental/checkpoint/__init__.py
+-rw-r--r--  2.0 unx    13755 b- defN 23-Apr-19 15:01 determined/common/experimental/checkpoint/_checkpoint.py
+-rw-r--r--  2.0 unx     3868 b- defN 23-Apr-19 15:01 determined/common/storage/__init__.py
+-rw-r--r--  2.0 unx     3822 b- defN 23-Apr-19 15:01 determined/common/storage/azure.py
+-rw-r--r--  2.0 unx     3514 b- defN 23-Apr-19 15:01 determined/common/storage/azure_client.py
+-rw-r--r--  2.0 unx     7152 b- defN 23-Apr-19 15:01 determined/common/storage/base.py
+-rw-r--r--  2.0 unx     4447 b- defN 23-Apr-19 15:01 determined/common/storage/boto3_credential_manager.py
+-rw-r--r--  2.0 unx     1014 b- defN 23-Apr-19 15:01 determined/common/storage/cloud.py
+-rw-r--r--  2.0 unx     5789 b- defN 23-Apr-19 15:01 determined/common/storage/gcs.py
+-rw-r--r--  2.0 unx     2070 b- defN 23-Apr-19 15:01 determined/common/storage/hdfs.py
+-rw-r--r--  2.0 unx     6195 b- defN 23-Apr-19 15:01 determined/common/storage/s3.py
+-rw-r--r--  2.0 unx     7816 b- defN 23-Apr-19 15:01 determined/common/storage/shared.py
+-rw-r--r--  2.0 unx      747 b- defN 23-Apr-19 15:01 determined/core/__init__.py
+-rw-r--r--  2.0 unx    29468 b- defN 23-Apr-19 15:01 determined/core/_checkpoint.py
+-rw-r--r--  2.0 unx    10130 b- defN 23-Apr-19 15:01 determined/core/_context.py
+-rw-r--r--  2.0 unx    16194 b- defN 23-Apr-19 15:01 determined/core/_distributed.py
+-rw-r--r--  2.0 unx    12738 b- defN 23-Apr-19 15:01 determined/core/_preempt.py
+-rw-r--r--  2.0 unx    15041 b- defN 23-Apr-19 15:01 determined/core/_searcher.py
+-rw-r--r--  2.0 unx      932 b- defN 23-Apr-19 15:01 determined/core/_tensorboard_mode.py
+-rw-r--r--  2.0 unx    10462 b- defN 23-Apr-19 15:01 determined/core/_train.py
+-rw-r--r--  2.0 unx       40 b- defN 23-Apr-19 15:01 determined/deploy/__init__.py
+-rw-r--r--  2.0 unx     1057 b- defN 23-Apr-19 15:01 determined/deploy/cli.py
+-rw-r--r--  2.0 unx       94 b- defN 23-Apr-19 15:01 determined/deploy/errors.py
+-rw-r--r--  2.0 unx     1472 b- defN 23-Apr-19 15:01 determined/deploy/healthcheck.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/deploy/aws/__init__.py
+-rw-r--r--  2.0 unx    19015 b- defN 23-Apr-19 15:01 determined/deploy/aws/aws.py
+-rw-r--r--  2.0 unx    24035 b- defN 23-Apr-19 15:01 determined/deploy/aws/cli.py
+-rw-r--r--  2.0 unx     2998 b- defN 23-Apr-19 15:01 determined/deploy/aws/constants.py
+-rw-r--r--  2.0 unx     1420 b- defN 23-Apr-19 15:01 determined/deploy/aws/gen_vcpu_mapping.py
+-rw-r--r--  2.0 unx     2201 b- defN 23-Apr-19 15:01 determined/deploy/aws/master_config_inject.py
+-rw-r--r--  2.0 unx     4911 b- defN 23-Apr-19 15:01 determined/deploy/aws/preflight.py
+-rw-r--r--  2.0 unx    17324 b- defN 23-Apr-19 15:01 determined/deploy/aws/vcpu_mapping.yaml
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/deploy/aws/deployment_types/__init__.py
+-rw-r--r--  2.0 unx     5624 b- defN 23-Apr-19 15:01 determined/deploy/aws/deployment_types/base.py
+-rw-r--r--  2.0 unx     1593 b- defN 23-Apr-19 15:01 determined/deploy/aws/deployment_types/govcloud.py
+-rw-r--r--  2.0 unx     3094 b- defN 23-Apr-19 15:01 determined/deploy/aws/deployment_types/secure.py
+-rw-r--r--  2.0 unx     1349 b- defN 23-Apr-19 15:01 determined/deploy/aws/deployment_types/simple.py
+-rw-r--r--  2.0 unx     1400 b- defN 23-Apr-19 15:01 determined/deploy/aws/deployment_types/vpc.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/deploy/aws/templates/__init__.py
+-rw-r--r--  2.0 unx    29259 b- defN 23-Apr-19 15:01 determined/deploy/aws/templates/efs.yaml
+-rw-r--r--  2.0 unx    29969 b- defN 23-Apr-19 15:01 determined/deploy/aws/templates/fsx.yaml
+-rw-r--r--  2.0 unx    23518 b- defN 23-Apr-19 15:01 determined/deploy/aws/templates/govcloud.yaml
+-rw-r--r--  2.0 unx    29153 b- defN 23-Apr-19 15:01 determined/deploy/aws/templates/secure.yaml
+-rw-r--r--  2.0 unx    24926 b- defN 23-Apr-19 15:01 determined/deploy/aws/templates/simple.yaml
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/deploy/gcp/__init__.py
+-rw-r--r--  2.0 unx    19015 b- defN 23-Apr-19 15:01 determined/deploy/gcp/cli.py
+-rw-r--r--  2.0 unx      772 b- defN 23-Apr-19 15:01 determined/deploy/gcp/constants.py
+-rw-r--r--  2.0 unx    12420 b- defN 23-Apr-19 15:01 determined/deploy/gcp/gcp.py
+-rw-r--r--  2.0 unx     2380 b- defN 23-Apr-19 15:01 determined/deploy/gcp/preflight.py
+-rw-r--r--  2.0 unx     5376 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/main.tf
+-rw-r--r--  2.0 unx     1569 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/master.yaml.tmpl
+-rw-r--r--  2.0 unx     1487 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/outputs.tf
+-rw-r--r--  2.0 unx     3767 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/variables.tf
+-rw-r--r--  2.0 unx     6105 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/compute/main.tf
+-rw-r--r--  2.0 unx      424 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/compute/outputs.tf
+-rw-r--r--  2.0 unx     1574 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/compute/variables.tf
+-rw-r--r--  2.0 unx     1222 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/database/main.tf
+-rw-r--r--  2.0 unx      283 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/database/outputs.tf
+-rw-r--r--  2.0 unx      320 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/database/variables.tf
+-rw-r--r--  2.0 unx      522 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/filestore/main.tf
+-rw-r--r--  2.0 unx      184 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/filestore/outputs.tf
+-rw-r--r--  2.0 unx      125 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/filestore/variables.tf
+-rw-r--r--  2.0 unx     1026 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/firewall/main.tf
+-rw-r--r--  2.0 unx      195 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/firewall/outputs.tf
+-rw-r--r--  2.0 unx       75 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/firewall/variables.tf
+-rw-r--r--  2.0 unx      637 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/gcs/main.tf
+-rw-r--r--  2.0 unx       51 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/gcs/outputs.tf
+-rw-r--r--  2.0 unx      161 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/gcs/variables.tf
+-rw-r--r--  2.0 unx      161 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/ip/main.tf
+-rw-r--r--  2.0 unx      122 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/ip/outputs.tf
+-rw-r--r--  2.0 unx       88 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/ip/variables.tf
+-rw-r--r--  2.0 unx     1112 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/network/main.tf
+-rw-r--r--  2.0 unx      275 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/network/outputs.tf
+-rw-r--r--  2.0 unx      207 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/network/variables.tf
+-rw-r--r--  2.0 unx      962 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/service_account/main.tf
+-rw-r--r--  2.0 unx       73 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/service_account/outputs.tf
+-rw-r--r--  2.0 unx      138 b- defN 23-Apr-19 15:01 determined/deploy/gcp/terraform/modules/service_account/variables.tf
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/deploy/gke/__init__.py
+-rw-r--r--  2.0 unx    19568 b- defN 23-Apr-19 15:01 determined/deploy/gke/cli.py
+-rw-r--r--  2.0 unx      408 b- defN 23-Apr-19 15:01 determined/deploy/gke/constants.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/deploy/local/__init__.py
+-rw-r--r--  2.0 unx    12829 b- defN 23-Apr-19 15:01 determined/deploy/local/cli.py
+-rw-r--r--  2.0 unx    17140 b- defN 23-Apr-19 15:01 determined/deploy/local/cluster_utils.py
+-rw-r--r--  2.0 unx     1134 b- defN 23-Apr-19 15:01 determined/deploy/local/preflight.py
+-rw-r--r--  2.0 unx      697 b- defN 23-Apr-19 15:01 determined/estimator/__init__.py
+-rw-r--r--  2.0 unx     1288 b- defN 23-Apr-19 15:01 determined/estimator/_callback.py
+-rw-r--r--  2.0 unx     7375 b- defN 23-Apr-19 15:01 determined/estimator/_estimator_context.py
+-rw-r--r--  2.0 unx    40174 b- defN 23-Apr-19 15:01 determined/estimator/_estimator_trial.py
+-rw-r--r--  2.0 unx     3086 b- defN 23-Apr-19 15:01 determined/estimator/_load.py
+-rw-r--r--  2.0 unx    13742 b- defN 23-Apr-19 15:01 determined/estimator/_reducer.py
+-rw-r--r--  2.0 unx     9600 b- defN 23-Apr-19 15:01 determined/estimator/_util.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/exec/__init__.py
+-rw-r--r--  2.0 unx     3836 b- defN 23-Apr-19 15:01 determined/exec/gc_checkpoints.py
+-rw-r--r--  2.0 unx     8857 b- defN 23-Apr-19 15:01 determined/exec/harness.py
+-rw-r--r--  2.0 unx     4233 b- defN 23-Apr-19 15:01 determined/exec/launch.py
+-rw-r--r--  2.0 unx      389 b- defN 23-Apr-19 15:01 determined/exec/pid_client.py
+-rw-r--r--  2.0 unx     1476 b- defN 23-Apr-19 15:01 determined/exec/pid_server.py
+-rw-r--r--  2.0 unx    13479 b- defN 23-Apr-19 15:01 determined/exec/prep_container.py
+-rw-r--r--  2.0 unx    10994 b- defN 23-Apr-19 15:01 determined/exec/tensorboard.py
+-rw-r--r--  2.0 unx      264 b- defN 23-Apr-19 15:01 determined/experimental/__init__.py
+-rw-r--r--  2.0 unx     3473 b- defN 23-Apr-19 15:01 determined/experimental/_native.py
+-rw-r--r--  2.0 unx    15160 b- defN 23-Apr-19 15:01 determined/experimental/client.py
+-rw-r--r--  2.0 unx      744 b- defN 23-Apr-19 15:01 determined/keras/__init__.py
+-rw-r--r--  2.0 unx     7579 b- defN 23-Apr-19 15:01 determined/keras/_data.py
+-rw-r--r--  2.0 unx    11491 b- defN 23-Apr-19 15:01 determined/keras/_enqueuer.py
+-rw-r--r--  2.0 unx     4020 b- defN 23-Apr-19 15:01 determined/keras/_load.py
+-rw-r--r--  2.0 unx      486 b- defN 23-Apr-19 15:01 determined/keras/_tensorboard_callback.py
+-rw-r--r--  2.0 unx    19042 b- defN 23-Apr-19 15:01 determined/keras/_tf_keras_context.py
+-rw-r--r--  2.0 unx     1204 b- defN 23-Apr-19 15:01 determined/keras/_tf_keras_multi_gpu.py
+-rw-r--r--  2.0 unx    48040 b- defN 23-Apr-19 15:01 determined/keras/_tf_keras_trial.py
+-rw-r--r--  2.0 unx    28556 b- defN 23-Apr-19 15:01 determined/keras/callbacks.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-19 15:01 determined/launch/__init__.py
+-rw-r--r--  2.0 unx    13720 b- defN 23-Apr-19 15:01 determined/launch/deepspeed.py
+-rw-r--r--  2.0 unx    10813 b- defN 23-Apr-19 15:01 determined/launch/horovod.py
+-rw-r--r--  2.0 unx     4574 b- defN 23-Apr-19 15:01 determined/launch/torch_distributed.py
+-rw-r--r--  2.0 unx     4502 b- defN 23-Apr-19 15:01 determined/launch/wrap_rank.py
+-rw-r--r--  2.0 unx       98 b- defN 23-Apr-19 15:01 determined/layers/__init__.py
+-rw-r--r--  2.0 unx    19401 b- defN 23-Apr-19 15:01 determined/layers/_workload_sequencer.py
+-rw-r--r--  2.0 unx     1134 b- defN 23-Apr-19 15:01 determined/pytorch/__init__.py
+-rw-r--r--  2.0 unx     5459 b- defN 23-Apr-19 15:01 determined/pytorch/_callback.py
+-rw-r--r--  2.0 unx    16294 b- defN 23-Apr-19 15:01 determined/pytorch/_data.py
+-rw-r--r--  2.0 unx     3670 b- defN 23-Apr-19 15:01 determined/pytorch/_experimental.py
+-rw-r--r--  2.0 unx    12510 b- defN 23-Apr-19 15:01 determined/pytorch/_load.py
+-rw-r--r--  2.0 unx     3286 b- defN 23-Apr-19 15:01 determined/pytorch/_lr_scheduler.py
+-rw-r--r--  2.0 unx     8275 b- defN 23-Apr-19 15:01 determined/pytorch/_metric_utils.py
+-rw-r--r--  2.0 unx    45136 b- defN 23-Apr-19 15:01 determined/pytorch/_pytorch_context.py
+-rw-r--r--  2.0 unx    67813 b- defN 23-Apr-19 15:01 determined/pytorch/_pytorch_trial.py
+-rw-r--r--  2.0 unx    21406 b- defN 23-Apr-19 15:01 determined/pytorch/_reducer.py
+-rw-r--r--  2.0 unx    13510 b- defN 23-Apr-19 15:01 determined/pytorch/_trainer.py
+-rw-r--r--  2.0 unx     9561 b- defN 23-Apr-19 15:01 determined/pytorch/samplers.py
+-rw-r--r--  2.0 unx      347 b- defN 23-Apr-19 15:01 determined/pytorch/deepspeed/__init__.py
+-rw-r--r--  2.0 unx    17524 b- defN 23-Apr-19 15:01 determined/pytorch/deepspeed/_deepspeed_context.py
+-rw-r--r--  2.0 unx    42777 b- defN 23-Apr-19 15:01 determined/pytorch/deepspeed/_deepspeed_trial.py
+-rw-r--r--  2.0 unx     1987 b- defN 23-Apr-19 15:01 determined/pytorch/deepspeed/_mpu.py
+-rw-r--r--  2.0 unx       67 b- defN 23-Apr-19 15:01 determined/pytorch/lightning/__init__.py
+-rw-r--r--  2.0 unx    18157 b- defN 23-Apr-19 15:01 determined/pytorch/lightning/_adapter.py
+-rw-r--r--  2.0 unx      343 b- defN 23-Apr-19 15:01 determined/searcher/__init__.py
+-rw-r--r--  2.0 unx     3563 b- defN 23-Apr-19 15:01 determined/searcher/_remote_search_runner.py
+-rw-r--r--  2.0 unx     9914 b- defN 23-Apr-19 15:01 determined/searcher/_search_method.py
+-rw-r--r--  2.0 unx    14053 b- defN 23-Apr-19 15:01 determined/searcher/_search_runner.py
+-rw-r--r--  2.0 unx      512 b- defN 23-Apr-19 15:01 determined/tensorboard/__init__.py
+-rw-r--r--  2.0 unx     1573 b- defN 23-Apr-19 15:01 determined/tensorboard/azure.py
+-rw-r--r--  2.0 unx     5797 b- defN 23-Apr-19 15:01 determined/tensorboard/base.py
+-rw-r--r--  2.0 unx     4534 b- defN 23-Apr-19 15:01 determined/tensorboard/build.py
+-rw-r--r--  2.0 unx     2305 b- defN 23-Apr-19 15:01 determined/tensorboard/gcs.py
+-rw-r--r--  2.0 unx     1333 b- defN 23-Apr-19 15:01 determined/tensorboard/hdfs.py
+-rw-r--r--  2.0 unx     1892 b- defN 23-Apr-19 15:01 determined/tensorboard/s3.py
+-rw-r--r--  2.0 unx     1747 b- defN 23-Apr-19 15:01 determined/tensorboard/shared.py
+-rw-r--r--  2.0 unx     3734 b- defN 23-Apr-19 15:01 determined/tensorboard/util.py
+-rw-r--r--  2.0 unx      780 b- defN 23-Apr-19 15:01 determined/tensorboard/fetchers/__init__.py
+-rw-r--r--  2.0 unx     2534 b- defN 23-Apr-19 15:01 determined/tensorboard/fetchers/azure.py
+-rw-r--r--  2.0 unx     1670 b- defN 23-Apr-19 15:01 determined/tensorboard/fetchers/base.py
+-rw-r--r--  2.0 unx     1733 b- defN 23-Apr-19 15:01 determined/tensorboard/fetchers/gcs.py
+-rw-r--r--  2.0 unx     2476 b- defN 23-Apr-19 15:01 determined/tensorboard/fetchers/s3.py
+-rw-r--r--  2.0 unx     1634 b- defN 23-Apr-19 15:01 determined/tensorboard/fetchers/shared.py
+-rw-r--r--  2.0 unx       91 b- defN 23-Apr-19 15:01 determined/tensorboard/metric_writers/__init__.py
+-rw-r--r--  2.0 unx     2113 b- defN 23-Apr-19 15:01 determined/tensorboard/metric_writers/callback.py
+-rw-r--r--  2.0 unx     2936 b- defN 23-Apr-19 15:01 determined/tensorboard/metric_writers/pytorch.py
+-rw-r--r--  2.0 unx     4385 b- defN 23-Apr-19 15:01 determined/tensorboard/metric_writers/tensorflow.py
+-rw-r--r--  2.0 unx     1285 b- defN 23-Apr-19 15:01 determined-0.21.2rc0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-19 15:01 determined-0.21.2rc0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       53 b- defN 23-Apr-19 15:01 determined-0.21.2rc0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       11 b- defN 23-Apr-19 15:01 determined-0.21.2rc0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    22841 b- defN 23-Apr-19 15:01 determined-0.21.2rc0.dist-info/RECORD
+249 files, 2354265 bytes uncompressed, 529874 bytes compressed:  77.5%
```

## zipnote {}

```diff
@@ -498,17 +498,14 @@
 
 Filename: determined/deploy/local/cli.py
 Comment: 
 
 Filename: determined/deploy/local/cluster_utils.py
 Comment: 
 
-Filename: determined/deploy/local/docker-compose.yaml
-Comment: 
-
 Filename: determined/deploy/local/preflight.py
 Comment: 
 
 Filename: determined/estimator/__init__.py
 Comment: 
 
 Filename: determined/estimator/_callback.py
@@ -729,23 +726,23 @@
 
 Filename: determined/tensorboard/metric_writers/pytorch.py
 Comment: 
 
 Filename: determined/tensorboard/metric_writers/tensorflow.py
 Comment: 
 
-Filename: determined-0.21.1rc4.dist-info/METADATA
+Filename: determined-0.21.2rc0.dist-info/METADATA
 Comment: 
 
-Filename: determined-0.21.1rc4.dist-info/WHEEL
+Filename: determined-0.21.2rc0.dist-info/WHEEL
 Comment: 
 
-Filename: determined-0.21.1rc4.dist-info/entry_points.txt
+Filename: determined-0.21.2rc0.dist-info/entry_points.txt
 Comment: 
 
-Filename: determined-0.21.1rc4.dist-info/top_level.txt
+Filename: determined-0.21.2rc0.dist-info/top_level.txt
 Comment: 
 
-Filename: determined-0.21.1rc4.dist-info/RECORD
+Filename: determined-0.21.2rc0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## determined/__version__.py

```diff
@@ -1 +1 @@
-__version__ = "0.21.1-rc4"
+__version__ = "0.21.2-rc0"
```

## determined/util.py

```diff
@@ -53,30 +53,30 @@
             return shutil.rmtree(path, ignore_errors=ignore_errors, onerror=onerror)
         except OSError as e:
             if e.errno != errno.ENOTEMPTY or tries == max_tries:
                 raise
             # This should not be possible, since rmtree empties directories before rmdir'ing them.
             logging.debug(f"rmtree() failed ({e}), is this NFS?  Retrying (tries={tries})...")
             # All 5 tries should take on the order of half a second.
-            time.sleep(2 ** tries / 100)
+            time.sleep(2**tries / 100)
 
 
 @util.preserve_random_state
 def download_gcs_blob_with_backoff(blob: Any, n_retries: int = 32, max_backoff: int = 32) -> Any:
     from google.cloud import storage
 
     if not (isinstance(blob, storage.Blob)):
         raise Exception(
             f"Called download_gcs_blob_with_backoff with object of type {type(blob).__name__}"
         )
     for n in range(n_retries):
         try:
             return blob.download_as_string()
         except Exception:
-            time.sleep(min(2 ** n + random.random(), max_backoff))
+            time.sleep(min(2**n + random.random(), max_backoff))
     raise Exception("Max retries exceeded for downloading blob.")
 
 
 def is_overridden(full_method: Any, parent_class: Any) -> bool:
     """Check if a function is overridden over the given parent class.
 
     Note that full_method should always be the name of a method, but users may override
```

## determined/cli/agent.py

```diff
@@ -77,19 +77,19 @@
         r["container_id"]: {"name": a["name"], "allocation_id": a["allocation_id"]}
         for a in allocations.values()
         for r in a["resources"]
         if r["container_id"]
     }
 
     def device_type_string(deviceType: typing.Optional[devicev1Type]) -> str:
-        if deviceType == devicev1Type.TYPE_CUDA:
+        if deviceType == devicev1Type.CUDA:
             return "cuda"
-        if deviceType == devicev1Type.TYPE_ROCM:
+        if deviceType == devicev1Type.ROCM:
             return "rocm"
-        if deviceType == devicev1Type.TYPE_CPU:
+        if deviceType == devicev1Type.CPU:
             return "cpu"
         return "unknown"
 
     def get_task_name(containers: Dict[str, Any], slot: v1Slot) -> str:
         if not slot.container:
             return "FREE"
 
@@ -185,19 +185,23 @@
             api.post(args.master, path, payload)
             status = "Disabled" if not enabled else "Enabled"
             print(f"{status} agent {agent_id}.", file=sys.stderr)
 
         # When draining, check if there're any tasks currently running on
         # these slots, and list them.
         if drain_mode:
-            rsp = api.get(args.master, "tasks")
+            rsp = bindings.get_GetTasks(cli.setup_session(args))
             tasks_data = {
                 k: t
-                for (k, t) in rsp.json().items()
-                if any(a in agent_ids for r in t.get("resources", []) for a in r["agent_devices"])
+                for (k, t) in (
+                    rsp.allocationIdToSummary.items()
+                    if rsp.allocationIdToSummary is not None
+                    else {}
+                )
+                if any(a in agent_ids for r in (t.resources or []) for a in (r.agentDevices or {}))
             }
 
             if not (args.json or args.csv):
                 if tasks_data:
                     print("Tasks still in progress on draining nodes.")
                 else:
                     print("No tasks in progress on draining nodes.")
```

## determined/cli/checkpoint.py

```diff
@@ -39,17 +39,17 @@
 
     render.tabulate_or_csv(headers, [values], False)
 
 
 @authentication.required
 def list_checkpoints(args: Namespace) -> None:
     if args.best:
-        sorter = bindings.v1GetExperimentCheckpointsRequestSortBy.SORT_BY_SEARCHER_METRIC
+        sorter = bindings.v1GetExperimentCheckpointsRequestSortBy.SEARCHER_METRIC
     else:
-        sorter = bindings.v1GetExperimentCheckpointsRequestSortBy.SORT_BY_END_TIME
+        sorter = bindings.v1GetExperimentCheckpointsRequestSortBy.END_TIME
     r = bindings.get_GetExperimentCheckpoints(
         cli.setup_session(args),
         id=args.experiment_id,
         limit=args.best,
         sortBy=sorter,
     )
     checkpoints = r.checkpoints
```

## determined/cli/experiment.py

```diff
@@ -136,16 +136,25 @@
         if len(trials) > 0:
             break
         else:
             time.sleep(0.1)
 
     first_trial_id = sorted(t_id.id for t_id in trials)[0]
     print(f"Following first trial with ID {first_trial_id}")
-    tlogs = logs.trial_logs(sess, first_trial_id, follow=True)
-    logs.pprint_trial_logs(first_trial_id, tlogs)
+    try:
+        tlogs = logs.trial_logs(sess, first_trial_id, follow=True)
+        logs.pprint_logs(tlogs)
+    finally:
+        print(
+            termcolor.colored(
+                "Trial log stream ended. To reopen log stream, run: "
+                "det trial logs -f {}".format(first_trial_id),
+                "green",
+            )
+        )
 
 
 def _follow_test_experiment_logs(sess: api.Session, exp_id: int) -> None:
     def print_progress(active_stage: int, ended: bool) -> None:
         # There are four sequential stages of verification. Track the
         # current stage with an index into this list.
         stages = [
@@ -177,46 +186,55 @@
         exp = bindings.get_GetExperiment(sess, experimentId=exp_id).experiment
         trials = bindings.get_GetExperimentTrials(sess, experimentId=exp_id).trials
 
         # Wait for experiment to start and initialize a trial.
         runner_state = trials[0].runnerState if trials else None
 
         # Update the active_stage by examining the experiment state and trial runner state.
-        if exp.state == bindings.experimentv1State.STATE_COMPLETED:
+        if exp.state == bindings.experimentv1State.COMPLETED:
             active_stage = 4
         elif runner_state == "checkpointing":
             active_stage = 3
         elif runner_state == "validating":
             active_stage = 2
         elif runner_state in (None, "UNSPECIFIED", "training"):
             active_stage = 1
         else:
             active_stage = 0
 
         # If the experiment is in a terminal state, output the appropriate
         # message and exit. Otherwise, sleep and repeat.
-        if exp.state == bindings.experimentv1State.STATE_COMPLETED:
+        if exp.state == bindings.experimentv1State.COMPLETED:
             print_progress(active_stage, ended=True)
             print(termcolor.colored("Model definition test succeeded! ", "green"))
             return
-        elif exp.state == bindings.experimentv1State.STATE_CANCELED:
+        elif exp.state == bindings.experimentv1State.CANCELED:
             print_progress(active_stage, ended=True)
             print(
                 termcolor.colored(
                     f"Model definition test (ID: {exp_id}) canceled before "
                     "model test could complete. Please re-run the command.",
                     "yellow",
                 )
             )
             sys.exit(1)
-        elif exp.state == bindings.experimentv1State.STATE_ERROR:
+        elif exp.state == bindings.experimentv1State.ERROR:
             print_progress(active_stage, ended=True)
             trial_id = trials[0].id
-            tlogs = logs.trial_logs(sess, trial_id)
-            logs.pprint_trial_logs(trial_id, tlogs)
+            try:
+                tlogs = logs.trial_logs(sess, trial_id)
+                logs.pprint_logs(tlogs)
+            finally:
+                print(
+                    termcolor.colored(
+                        "Trial log stream ended. To reopen log stream, run: "
+                        "det trial logs -f {}".format(trial_id),
+                        "green",
+                    )
+                )
             sys.exit(1)
         else:
             print_progress(active_stage, ended=False)
             time.sleep(0.2)
 
 
 @authentication.required
@@ -578,31 +596,42 @@
     if len(trials) == 0:
         print(
             f"No trials found for experiment {args.experiment_id}. "
             "Try again after the experiment has a trial running."
         )
         return
     first_trial_id = sorted(t_id.id for t_id in trials)[0]
-
-    logs = api.trial_logs(
-        cli.setup_session(args),
-        first_trial_id,
-        head=args.head,
-        tail=args.tail,
-        follow=args.follow,
-        agent_ids=args.agent_ids,
-        container_ids=args.container_ids,
-        rank_ids=args.rank_ids,
-        sources=args.sources,
-        stdtypes=args.stdtypes,
-        min_level=args.level,
-        timestamp_before=args.timestamp_before,
-        timestamp_after=args.timestamp_after,
-    )
-    api.pprint_trial_logs(first_trial_id, logs)
+    try:
+        logs = api.trial_logs(
+            cli.setup_session(args),
+            first_trial_id,
+            head=args.head,
+            tail=args.tail,
+            follow=args.follow,
+            agent_ids=args.agent_ids,
+            container_ids=args.container_ids,
+            rank_ids=args.rank_ids,
+            sources=args.sources,
+            stdtypes=args.stdtypes,
+            min_level=args.level,
+            timestamp_before=args.timestamp_before,
+            timestamp_after=args.timestamp_after,
+        )
+        if args.json:
+            api.print_json_logs(logs)
+        else:
+            api.pprint_logs(logs)
+    finally:
+        print(
+            termcolor.colored(
+                "Trial log stream ended. To reopen log stream, run: "
+                "det trial logs -f {}".format(first_trial_id),
+                "green",
+            )
+        )
 
 
 @authentication.required
 def config(args: Namespace) -> None:
     result = bindings.get_GetExperiment(
         cli.setup_session(args), experimentId=args.experiment_id
     ).experiment.config
@@ -959,26 +988,27 @@
             "describe",
             describe,
             "describe experiment",
             [
                 Arg("experiment_ids", help="comma-separated list of experiment IDs to describe"),
                 Arg("--metrics", action="store_true", help="display full metrics"),
                 Group(
-                    Arg("--csv", action="store_true", help="print as CSV"),
-                    Arg("--json", action="store_true", help="print as JSON"),
+                    cli.output_format_args["csv"],
+                    cli.output_format_args["json"],
                     Arg("--outdir", type=Path, help="directory to save output"),
                 ),
             ],
         ),
         Cmd(
             "logs",
             experiment_logs,
             "fetch logs of the first trial of an experiment",
             [
                 experiment_id_arg("experiment ID"),
+                cli.output_format_args["json"],
             ]
             + logs_args_description,
         ),
         Cmd(
             "download-model-def",
             download_model_def,
             "download model definition",
```

## determined/cli/job.py

```diff
@@ -1,31 +1,28 @@
 import json
 from argparse import ONE_OR_MORE, Namespace
 from datetime import datetime
 from typing import Any, List
 
 import pytz
-import yaml
 
 from determined import cli
 from determined.cli import render
-from determined.common import api
+from determined.common import api, yaml
 from determined.common.api import authentication, bindings
 from determined.common.declarative_argparse import Arg, Cmd, Group
 
 
 @authentication.required
 def ls(args: Namespace) -> None:
     session = cli.setup_session(args)
     pools = bindings.get_GetResourcePools(cli.setup_session(args))
     is_priority = check_is_priority(pools, args.resource_pool)
 
-    order_by = (
-        bindings.v1OrderBy.ORDER_BY_ASC if not args.reverse else bindings.v1OrderBy.ORDER_BY_DESC
-    )
+    order_by = bindings.v1OrderBy.ASC if not args.reverse else bindings.v1OrderBy.DESC
 
     def get_with_offset(offset: int) -> bindings.v1GetJobsResponse:
         return bindings.get_GetJobs(
             session,
             resourcePool=args.resource_pool,
             offset=offset,
             limit=args.limit,
@@ -54,15 +51,15 @@
         "Submitted",
         "Slots (acquired/needed)",
         "State",
         "User",
     ]
 
     def computed_job_name(job: bindings.v1Job) -> str:
-        if job.type == bindings.jobv1Type.TYPE_EXPERIMENT:
+        if job.type == bindings.jobv1Type.EXPERIMENT:
             return f"{job.name} ({job.entityId})"
         else:
             return job.name
 
     values = [
         [
             j.summary.jobsAhead if j.summary is not None and j.summary.jobsAhead > -1 else "N/A",
@@ -127,15 +124,15 @@
 
 def check_is_priority(pools: bindings.v1GetResourcePoolsResponse, resource_pool: str) -> bool:
     if pools.resourcePools is None:
         raise ValueError(f"No resource pools found checking scheduler type of {resource_pool}")
 
     for pool in pools.resourcePools:
         if (resource_pool is None and pool.defaultComputePool) or resource_pool == pool.name:
-            return pool.schedulerType == bindings.v1SchedulerType.SCHEDULER_TYPE_PRIORITY
+            return pool.schedulerType == bindings.v1SchedulerType.PRIORITY
     raise ValueError(f"Pool {resource_pool} not found")
 
 
 def validate_operation_args(operation: str) -> dict:
     valid_cmds = ("priority", "weight", "resource_pool", "ahead_of", "behind_of")
     replacements = {
         "resource-pool": "resource_pool",
```

## determined/cli/notebook.py

```diff
@@ -39,15 +39,15 @@
     if args.detach:
         print(nb.id)
         return
 
     if resp.warnings:
         cli.print_warnings(resp.warnings)
     currentSlotsExceeded = (resp.warnings is not None) and (
-        bindings.v1LaunchWarning.LAUNCH_WARNING_CURRENT_SLOTS_EXCEEDED in resp.warnings
+        bindings.v1LaunchWarning.CURRENT_SLOTS_EXCEEDED in resp.warnings
     )
 
     with api.ws(args.master, "notebooks/{}/events".format(nb.id)) as ws:
         for msg in ws:
             if msg["service_ready_event"] and nb.serviceAddress and not args.no_browser:
                 url = api.browser_open(
                     args.master,
```

## determined/cli/project.py

```diff
@@ -145,17 +145,17 @@
             print(f"Successfully deleted project {args.project_name}.")
         else:
             print(f"Started deletion of project {args.project_name}...")
             while True:
                 sleep(2)
                 try:
                     p = bindings.get_GetProject(sess, id=p.id).project
-                    if p.state == bindings.v1WorkspaceState.WORKSPACE_STATE_DELETE_FAILED:
+                    if p.state == bindings.v1WorkspaceState.DELETE_FAILED:
                         raise errors.DeleteFailedException(p.errorMessage)
-                    elif p.state == bindings.v1WorkspaceState.WORKSPACE_STATE_DELETING:
+                    elif p.state == bindings.v1WorkspaceState.DELETING:
                         print(f"Remaining experiment count: {p.numExperiments}")
                 except errors.NotFoundException:
                     print("Project deleted successfully.")
                     break
     else:
         print("Aborting project deletion.")
```

## determined/cli/proxy.py

```diff
@@ -36,17 +36,17 @@
     # TODO(DET-9000): perhaps the tunnel should be able to probe master for service status,
     # instead of us explicitly polling for task/trial status.
     while True:
         resp = bindings.get_GetTrial(sess, trialId=trial_id)
         trial = resp.trial
 
         terminal_states = [
-            bindings.experimentv1State.STATE_COMPLETED,
-            bindings.experimentv1State.STATE_CANCELED,
-            bindings.experimentv1State.STATE_ERROR,
+            bindings.experimentv1State.COMPLETED,
+            bindings.experimentv1State.CANCELED,
+            bindings.experimentv1State.ERROR,
         ]
         if trial.state in terminal_states:
             raise ValueError("Can't tunnel a trial in terminal state")
 
         task_id = trial.taskId
         if task_id is not None:
             break
```

## determined/cli/remote.py

```diff
@@ -1,12 +1,14 @@
 from argparse import ONE_OR_MORE, REMAINDER, FileType, Namespace
 from functools import partial
 from pathlib import Path
 from typing import Any, List
 
+from termcolor import colored
+
 from determined import cli
 from determined.cli import command, task
 from determined.common import api
 from determined.common.api import authentication
 from determined.common.declarative_argparse import Arg, Cmd, Group
 
 
@@ -24,16 +26,25 @@
         workspace_id=workspace_id,
     )["command"]
 
     if args.detach:
         print(resp["id"])
         return
 
-    logs = api.task_logs(cli.setup_session(args), resp["id"], follow=True)
-    api.pprint_task_logs(resp["id"], logs)
+    try:
+        logs = api.task_logs(cli.setup_session(args), resp["id"], follow=True)
+        api.pprint_logs(logs)
+    finally:
+        print(
+            colored(
+                "Task log stream ended. To reopen log stream, run: "
+                "det task logs -f {}".format(resp["id"]),
+                "green",
+            )
+        )
 
 
 # fmt: off
 
 args_description = [
     Cmd("command cmd", None, "manage commands", [
         Cmd("list ls", partial(command.list_tasks), "list commands", [
```

## determined/cli/shell.py

```diff
@@ -91,15 +91,14 @@
         key_path = retention_dir / "key"
         keyfile = key_path.open("w")
         key_path.chmod(0o600)
 
         return keyfile, str(key_path)
 
     else:
-
         # Avoid using tempfile.NamedTemporaryFile, which does not produce a file that can be opened
         # by name on Windows, which prevents the ssh process from reading it.
         fd, path = tempfile.mkstemp(text=True)
         f = open(fd, "w")
 
         @contextlib.contextmanager
         def file_closer() -> Iterator[IO]:
```

## determined/cli/task.py

```diff
@@ -1,32 +1,40 @@
 import json
 from argparse import Namespace
 from functools import partial
 from typing import Any, Dict, List, Union, cast
 
+from termcolor import colored
+
 from determined import cli
 from determined.cli import command, render
 from determined.common import api
-from determined.common.api import authentication
+from determined.common.api import authentication, bindings
+from determined.common.api.bindings import v1AllocationSummary
 from determined.common.declarative_argparse import Arg, Cmd, Group
 
 
-def render_tasks(args: Namespace, tasks: Dict[str, Dict[str, Any]]) -> None:
-    def agent_info(t: Dict[str, Any]) -> Union[str, List[str]]:
-        resources = t.get("resources", [])
-        if not resources:
+def render_tasks(args: Namespace, tasks: Dict[str, v1AllocationSummary]) -> None:
+    """Render tasks for JSON, tabulate or csv output.
+
+    The tasks parameter requires a map from allocation IDs to v1AllocationSummary
+    describing individual tasks.
+    """
+
+    def agent_info(t: v1AllocationSummary) -> Union[str, List[str]]:
+        if t.resources is None:
             return "unassigned"
-        agents = [a for r in resources for a in r["agent_devices"]]
+        agents = [a for r in t.resources for a in (r.agentDevices or {})]
         if len(agents) == 1:
             agent = agents[0]  # type: str
             return agent
         return agents
 
     if args.json:
-        print(json.dumps(tasks, indent=4))
+        print(json.dumps({a: t.to_json() for (a, t) in tasks.items()}, indent=4))
         return
 
     headers = [
         "Task ID",
         "Allocation ID",
         "Name",
         "Slots Needed",
@@ -34,59 +42,76 @@
         "Agent",
         "Priority",
         "Resource Pool",
         "Ports",
     ]
     values = [
         [
-            task["task_id"],
-            task["allocation_id"],
-            task["name"],
-            task["slots_needed"],
-            render.format_time(task["registered_time"]),
+            task.taskId,
+            task.allocationId,
+            task.name,
+            task.slotsNeeded,
+            render.format_time(task.registeredTime),
             agent_info(task),
-            task["priority"] if task["scheduler_type"] == "priority" else "N/A",
-            task["resource_pool"],
-            ",".join(map(str, sorted(pp["port"] for pp in task.get("proxy_ports", [])))),
+            task.priority if task.schedulerType == "priority" else "N/A",
+            task.resourcePool,
+            ",".join(
+                map(
+                    str,
+                    sorted([pp.port for pp in (task.proxyPorts or []) if pp.port is not None]),
+                )
+            ),
         ]
         for task_id, task in sorted(
             tasks.items(),
-            key=lambda tup: (render.format_time(tup[1]["registered_time"]),),
+            key=lambda tup: (render.format_time(tup[1].registeredTime),),
         )
     ]
 
     render.tabulate_or_csv(headers, values, args.csv)
 
 
 @authentication.required
 def list_tasks(args: Namespace) -> None:
-    r = api.get(args.master, "tasks")
-    tasks = r.json()
+    r = bindings.get_GetTasks(cli.setup_session(args))
+    tasks = r.allocationIdToSummary or {}
     render_tasks(args, tasks)
 
 
 @authentication.required
 def logs(args: Namespace) -> None:
     task_id = cast(str, command.expand_uuid_prefixes(args, args.task_id))
-    logs = api.task_logs(
-        cli.setup_session(args),
-        task_id,
-        head=args.head,
-        tail=args.tail,
-        follow=args.follow,
-        agent_ids=args.agent_ids,
-        container_ids=args.container_ids,
-        rank_ids=args.rank_ids,
-        sources=args.sources,
-        stdtypes=args.stdtypes,
-        min_level=args.level,
-        timestamp_before=args.timestamp_before,
-        timestamp_after=args.timestamp_after,
-    )
-    api.pprint_task_logs(task_id, logs)
+    try:
+        logs = api.task_logs(
+            cli.setup_session(args),
+            task_id,
+            head=args.head,
+            tail=args.tail,
+            follow=args.follow,
+            agent_ids=args.agent_ids,
+            container_ids=args.container_ids,
+            rank_ids=args.rank_ids,
+            sources=args.sources,
+            stdtypes=args.stdtypes,
+            min_level=args.level,
+            timestamp_before=args.timestamp_before,
+            timestamp_after=args.timestamp_after,
+        )
+        if args.json:
+            api.print_json_logs(logs)
+        else:
+            api.pprint_logs(logs)
+    finally:
+        print(
+            colored(
+                "Task log stream ended. To reopen log stream, run: "
+                "det task logs -f {}".format(task_id),
+                "green",
+            )
+        )
 
 
 common_log_options: List[Any] = [
     Arg(
         "-f",
         "--follow",
         action="store_true",
@@ -167,28 +192,29 @@
         [
             Cmd(
                 "list ls",
                 list_tasks,
                 "list tasks in cluster",
                 [
                     Group(
-                        Arg("--csv", action="store_true", help="print as CSV"),
-                        Arg("--json", action="store_true", help="print as JSON"),
+                        cli.output_format_args["csv"],
+                        cli.output_format_args["json"],
                     )
                 ],
                 is_default=True,
             ),
             Cmd(
                 "logs",
                 # Since declarative argparse tries to attach the help_str to the func itself:
                 # ./harness/determined/common/declarative_argparse.py#L57
                 # Each func must be unique.
                 partial(logs),
                 "fetch task logs",
                 [
                     Arg("task_id", help="task ID"),
+                    cli.output_format_args["json"],
                     *common_log_options,
                 ],
             ),
         ],
     ),
 ]
```

## determined/cli/tensorboard.py

```diff
@@ -35,15 +35,15 @@
     if args.detach:
         print(resp.tensorboard.id)
         return
 
     if resp.warnings:
         cli.print_warnings(resp.warnings)
     currentSlotsExceeded = (resp.warnings is not None) and (
-        bindings.v1LaunchWarning.LAUNCH_WARNING_CURRENT_SLOTS_EXCEEDED in resp.warnings
+        bindings.v1LaunchWarning.CURRENT_SLOTS_EXCEEDED in resp.warnings
     )
     url = "tensorboard/{}/events".format(resp.tensorboard.id)
     with api.ws(args.master, url) as ws:
         for msg in ws:
             if msg["log_event"] is not None:
                 # TensorBoard will print a url by default. The URL is incorrect since
                 # TensorBoard is not aware of the master proxy address it is assigned.
```

## determined/cli/trial.py

```diff
@@ -3,14 +3,16 @@
 import os
 import tarfile
 import tempfile
 from argparse import Namespace
 from datetime import datetime
 from typing import Any, List, Optional, Sequence, Tuple, Union
 
+from termcolor import colored
+
 from determined import cli
 from determined.cli import render
 from determined.cli.master import format_log_entry
 from determined.common import api, constants
 from determined.common.api import authentication, bindings
 from determined.common.declarative_argparse import Arg, Cmd, Group
 from determined.common.experimental import Determined
@@ -72,18 +74,16 @@
 
     values = []
 
     for w in workloads:
         w_unpacked = _workload_container_unpack(w)
 
         row_metrics = []
-        if metrics:
-            metrics_workload = w.training or w.validation
-            if metrics_workload:
-                row_metrics = [json.dumps(metrics_workload.metrics.to_json(), indent=4)]
+        if metrics and w.training:
+            row_metrics = [json.dumps(w.training.metrics.to_json(), indent=4)]
 
         values.append(
             [
                 w_unpacked.totalBatches,
                 render.format_time(w_unpacked.endTime),
                 *_format_checkpoint(w.checkpoint),
                 _format_validation(w.validation),
@@ -170,30 +170,42 @@
 def kill_trial(args: Namespace) -> None:
     api.post(args.master, "/api/v1/trials/{}/kill".format(args.trial_id))
     print("Killed trial {}".format(args.trial_id))
 
 
 @authentication.required
 def trial_logs(args: Namespace) -> None:
-    logs = api.trial_logs(
-        cli.setup_session(args),
-        args.trial_id,
-        head=args.head,
-        tail=args.tail,
-        follow=args.follow,
-        agent_ids=args.agent_ids,
-        container_ids=args.container_ids,
-        rank_ids=args.rank_ids,
-        sources=args.sources,
-        stdtypes=args.stdtypes,
-        min_level=args.level,
-        timestamp_before=args.timestamp_before,
-        timestamp_after=args.timestamp_after,
-    )
-    api.pprint_trial_logs(args.trial_id, logs)
+    try:
+        logs = api.trial_logs(
+            cli.setup_session(args),
+            args.trial_id,
+            head=args.head,
+            tail=args.tail,
+            follow=args.follow,
+            agent_ids=args.agent_ids,
+            container_ids=args.container_ids,
+            rank_ids=args.rank_ids,
+            sources=args.sources,
+            stdtypes=args.stdtypes,
+            min_level=args.level,
+            timestamp_before=args.timestamp_before,
+            timestamp_after=args.timestamp_after,
+        )
+        if args.json:
+            api.print_json_logs(logs)
+        else:
+            api.pprint_logs(logs)
+    finally:
+        print(
+            colored(
+                "Trial log stream ended. To reopen log stream, run: "
+                "det trial logs -f {}".format(args.trial_id),
+                "green",
+            )
+        )
 
 
 @authentication.required
 def generate_support_bundle(args: Namespace) -> None:
     try:
         output_dir = args.output_dir
         if output_dir is None:
@@ -351,16 +363,16 @@
                     Arg("trial_id", type=int, help="trial ID"),
                     Arg(
                         "--metrics",
                         action="store_true",
                         help="display full metrics, such as batch metrics",
                     ),
                     Group(
-                        Arg("--csv", action="store_true", help="print as CSV"),
-                        Arg("--json", action="store_true", help="print JSON"),
+                        cli.output_format_args["csv"],
+                        cli.output_format_args["json"],
                     ),
                     *cli.make_pagination_args(limit=1000),
                 ],
             ),
             Cmd(
                 "download",
                 download,
@@ -434,14 +446,15 @@
             ),
             Cmd(
                 "logs",
                 trial_logs,
                 "fetch trial logs",
                 [
                     Arg("trial_id", type=int, help="trial ID"),
+                    cli.output_format_args["json"],
                 ]
                 + logs_args_description,
             ),
             Cmd(
                 "kill", kill_trial, "forcibly terminate a trial", [Arg("trial_id", help="trial ID")]
             ),
         ],
```

## determined/cli/workspace.py

```diff
@@ -231,17 +231,17 @@
             print(f"Successfully deleted workspace {args.workspace_name}.")
         else:
             print(f"Started deletion of workspace {args.workspace_name}...")
             while True:
                 sleep(2)
                 try:
                     w = bindings.get_GetWorkspace(sess, id=w.id).workspace
-                    if w.state == bindings.v1WorkspaceState.WORKSPACE_STATE_DELETE_FAILED:
+                    if w.state == bindings.v1WorkspaceState.DELETE_FAILED:
                         raise errors.DeleteFailedException(w.errorMessage)
-                    elif w.state == bindings.v1WorkspaceState.WORKSPACE_STATE_DELETING:
+                    elif w.state == bindings.v1WorkspaceState.DELETING:
                         print(f"Remaining project count: {w.numProjects}")
                 except errors.NotFoundException:
                     print("Workspace deleted successfully.")
                     break
     else:
         print("Aborting workspace deletion.")
```

## determined/common/api/__init__.py

```diff
@@ -1,15 +1,15 @@
 from determined.common.api import authentication, errors, metric, request
 from determined.common.api._session import Session
 from determined.common.api import bindings
 from determined.common.api._util import PageOpts, read_paginated, WARNING_MESSAGE_MAP
 from determined.common.api.authentication import Authentication, salt_and_hash
 from determined.common.api.logs import (
-    pprint_trial_logs,
-    pprint_task_logs,
+    pprint_logs,
+    print_json_logs,
     trial_logs,
     task_logs,
 )
 from determined.common.api.request import (
     WebSocket,
     delete,
     do_request,
```

## determined/common/api/_util.py

```diff
@@ -12,15 +12,15 @@
 
 # Not that read_paginated requires the output of get_with_offset to be a Paginated type to work.
 # The Paginated union type is generated based on response objects with a .pagination attribute.
 T = TypeVar("T", bound=bindings.Paginated)
 
 # Map of launch warnings to the warning message shown to users.
 WARNING_MESSAGE_MAP = {
-    bindings.v1LaunchWarning.LAUNCH_WARNING_CURRENT_SLOTS_EXCEEDED: (
+    bindings.v1LaunchWarning.CURRENT_SLOTS_EXCEEDED: (
         "Warning: The requested job requires more slots than currently available. "
         "You may need to increase cluster resources in order for the job to run."
     )
 }
 
 
 def read_paginated(
```

## determined/common/api/bindings.py

```diff
@@ -51,82 +51,50 @@
             f"Stream Error during {operation_name}: {error.message}"
         )
 
     def __str__(self) -> str:
         return self.message
 
 
-class GetHPImportanceResponseMetricHPImportance:
-    error: "typing.Optional[str]" = None
-    experimentProgress: "typing.Optional[float]" = None
-    hpImportance: "typing.Optional[typing.Dict[str, float]]" = None
-    inProgress: "typing.Optional[bool]" = None
-    pending: "typing.Optional[bool]" = None
+class GetMasterResponseProduct(enum.Enum):
+    UNSPECIFIED = "PRODUCT_UNSPECIFIED"
+    COMMUNITY = "PRODUCT_COMMUNITY"
+
+class GetTrialWorkloadsRequestFilterOption(enum.Enum):
+    UNSPECIFIED = "FILTER_OPTION_UNSPECIFIED"
+    CHECKPOINT = "FILTER_OPTION_CHECKPOINT"
+    VALIDATION = "FILTER_OPTION_VALIDATION"
+    CHECKPOINT_OR_VALIDATION = "FILTER_OPTION_CHECKPOINT_OR_VALIDATION"
+
+class ResourcesSummaryDevices:
+    devices: "typing.Optional[typing.Sequence[v1Device]]" = None
 
     def __init__(
         self,
         *,
-        error: "typing.Union[str, None, Unset]" = _unset,
-        experimentProgress: "typing.Union[float, None, Unset]" = _unset,
-        hpImportance: "typing.Union[typing.Dict[str, float], None, Unset]" = _unset,
-        inProgress: "typing.Union[bool, None, Unset]" = _unset,
-        pending: "typing.Union[bool, None, Unset]" = _unset,
+        devices: "typing.Union[typing.Sequence[v1Device], None, Unset]" = _unset,
     ):
-        if not isinstance(error, Unset):
-            self.error = error
-        if not isinstance(experimentProgress, Unset):
-            self.experimentProgress = experimentProgress
-        if not isinstance(hpImportance, Unset):
-            self.hpImportance = hpImportance
-        if not isinstance(inProgress, Unset):
-            self.inProgress = inProgress
-        if not isinstance(pending, Unset):
-            self.pending = pending
+        if not isinstance(devices, Unset):
+            self.devices = devices
 
     @classmethod
-    def from_json(cls, obj: Json) -> "GetHPImportanceResponseMetricHPImportance":
+    def from_json(cls, obj: Json) -> "ResourcesSummaryDevices":
         kwargs: "typing.Dict[str, typing.Any]" = {
         }
-        if "error" in obj:
-            kwargs["error"] = obj["error"]
-        if "experimentProgress" in obj:
-            kwargs["experimentProgress"] = float(obj["experimentProgress"]) if obj["experimentProgress"] is not None else None
-        if "hpImportance" in obj:
-            kwargs["hpImportance"] = {k: float(v) for k, v in obj["hpImportance"].items()} if obj["hpImportance"] is not None else None
-        if "inProgress" in obj:
-            kwargs["inProgress"] = obj["inProgress"]
-        if "pending" in obj:
-            kwargs["pending"] = obj["pending"]
+        if "devices" in obj:
+            kwargs["devices"] = [v1Device.from_json(x) for x in obj["devices"]] if obj["devices"] is not None else None
         return cls(**kwargs)
 
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
         }
-        if not omit_unset or "error" in vars(self):
-            out["error"] = self.error
-        if not omit_unset or "experimentProgress" in vars(self):
-            out["experimentProgress"] = None if self.experimentProgress is None else dump_float(self.experimentProgress)
-        if not omit_unset or "hpImportance" in vars(self):
-            out["hpImportance"] = None if self.hpImportance is None else {k: dump_float(v) for k, v in self.hpImportance.items()}
-        if not omit_unset or "inProgress" in vars(self):
-            out["inProgress"] = self.inProgress
-        if not omit_unset or "pending" in vars(self):
-            out["pending"] = self.pending
+        if not omit_unset or "devices" in vars(self):
+            out["devices"] = None if self.devices is None else [x.to_json(omit_unset) for x in self.devices]
         return out
 
-class GetMasterResponseProduct(enum.Enum):
-    PRODUCT_UNSPECIFIED = "PRODUCT_UNSPECIFIED"
-    PRODUCT_COMMUNITY = "PRODUCT_COMMUNITY"
-
-class GetTrialWorkloadsRequestFilterOption(enum.Enum):
-    FILTER_OPTION_UNSPECIFIED = "FILTER_OPTION_UNSPECIFIED"
-    FILTER_OPTION_CHECKPOINT = "FILTER_OPTION_CHECKPOINT"
-    FILTER_OPTION_VALIDATION = "FILTER_OPTION_VALIDATION"
-    FILTER_OPTION_CHECKPOINT_OR_VALIDATION = "FILTER_OPTION_CHECKPOINT_OR_VALIDATION"
-
 class TrialFiltersRankWithinExp:
     rank: "typing.Optional[int]" = None
     sorter: "typing.Optional[v1TrialSorter]" = None
 
     def __init__(
         self,
         *,
@@ -154,24 +122,24 @@
         if not omit_unset or "rank" in vars(self):
             out["rank"] = self.rank
         if not omit_unset or "sorter" in vars(self):
             out["sorter"] = None if self.sorter is None else self.sorter.to_json(omit_unset)
         return out
 
 class TrialProfilerMetricLabelsProfilerMetricType(enum.Enum):
-    PROFILER_METRIC_TYPE_UNSPECIFIED = "PROFILER_METRIC_TYPE_UNSPECIFIED"
-    PROFILER_METRIC_TYPE_SYSTEM = "PROFILER_METRIC_TYPE_SYSTEM"
-    PROFILER_METRIC_TYPE_TIMING = "PROFILER_METRIC_TYPE_TIMING"
-    PROFILER_METRIC_TYPE_MISC = "PROFILER_METRIC_TYPE_MISC"
+    UNSPECIFIED = "PROFILER_METRIC_TYPE_UNSPECIFIED"
+    SYSTEM = "PROFILER_METRIC_TYPE_SYSTEM"
+    TIMING = "PROFILER_METRIC_TYPE_TIMING"
+    MISC = "PROFILER_METRIC_TYPE_MISC"
 
 class TrialSorterNamespace(enum.Enum):
-    NAMESPACE_UNSPECIFIED = "NAMESPACE_UNSPECIFIED"
-    NAMESPACE_HPARAMS = "NAMESPACE_HPARAMS"
-    NAMESPACE_TRAINING_METRICS = "NAMESPACE_TRAINING_METRICS"
-    NAMESPACE_VALIDATION_METRICS = "NAMESPACE_VALIDATION_METRICS"
+    UNSPECIFIED = "NAMESPACE_UNSPECIFIED"
+    HPARAMS = "NAMESPACE_HPARAMS"
+    TRAINING_METRICS = "NAMESPACE_TRAINING_METRICS"
+    VALIDATION_METRICS = "NAMESPACE_VALIDATION_METRICS"
 
 class UpdateTrialTagsRequestIds:
     ids: "typing.Optional[typing.Sequence[int]]" = None
 
     def __init__(
         self,
         *,
@@ -192,67 +160,67 @@
         out: "typing.Dict[str, typing.Any]" = {
         }
         if not omit_unset or "ids" in vars(self):
             out["ids"] = self.ids
         return out
 
 class checkpointv1State(enum.Enum):
-    STATE_UNSPECIFIED = "STATE_UNSPECIFIED"
-    STATE_ACTIVE = "STATE_ACTIVE"
-    STATE_COMPLETED = "STATE_COMPLETED"
-    STATE_ERROR = "STATE_ERROR"
-    STATE_DELETED = "STATE_DELETED"
+    UNSPECIFIED = "STATE_UNSPECIFIED"
+    ACTIVE = "STATE_ACTIVE"
+    COMPLETED = "STATE_COMPLETED"
+    ERROR = "STATE_ERROR"
+    DELETED = "STATE_DELETED"
 
 class containerv1State(enum.Enum):
-    STATE_UNSPECIFIED = "STATE_UNSPECIFIED"
-    STATE_ASSIGNED = "STATE_ASSIGNED"
-    STATE_PULLING = "STATE_PULLING"
-    STATE_STARTING = "STATE_STARTING"
-    STATE_RUNNING = "STATE_RUNNING"
-    STATE_TERMINATED = "STATE_TERMINATED"
+    UNSPECIFIED = "STATE_UNSPECIFIED"
+    ASSIGNED = "STATE_ASSIGNED"
+    PULLING = "STATE_PULLING"
+    STARTING = "STATE_STARTING"
+    RUNNING = "STATE_RUNNING"
+    TERMINATED = "STATE_TERMINATED"
 
 class devicev1Type(enum.Enum):
-    TYPE_UNSPECIFIED = "TYPE_UNSPECIFIED"
-    TYPE_CPU = "TYPE_CPU"
-    TYPE_CUDA = "TYPE_CUDA"
-    TYPE_ROCM = "TYPE_ROCM"
+    UNSPECIFIED = "TYPE_UNSPECIFIED"
+    CPU = "TYPE_CPU"
+    CUDA = "TYPE_CUDA"
+    ROCM = "TYPE_ROCM"
 
 class experimentv1State(enum.Enum):
-    STATE_UNSPECIFIED = "STATE_UNSPECIFIED"
-    STATE_ACTIVE = "STATE_ACTIVE"
-    STATE_PAUSED = "STATE_PAUSED"
-    STATE_STOPPING_COMPLETED = "STATE_STOPPING_COMPLETED"
-    STATE_STOPPING_CANCELED = "STATE_STOPPING_CANCELED"
-    STATE_STOPPING_ERROR = "STATE_STOPPING_ERROR"
-    STATE_COMPLETED = "STATE_COMPLETED"
-    STATE_CANCELED = "STATE_CANCELED"
-    STATE_ERROR = "STATE_ERROR"
-    STATE_DELETED = "STATE_DELETED"
-    STATE_DELETING = "STATE_DELETING"
-    STATE_DELETE_FAILED = "STATE_DELETE_FAILED"
-    STATE_STOPPING_KILLED = "STATE_STOPPING_KILLED"
-    STATE_QUEUED = "STATE_QUEUED"
-    STATE_PULLING = "STATE_PULLING"
-    STATE_STARTING = "STATE_STARTING"
-    STATE_RUNNING = "STATE_RUNNING"
+    UNSPECIFIED = "STATE_UNSPECIFIED"
+    ACTIVE = "STATE_ACTIVE"
+    PAUSED = "STATE_PAUSED"
+    STOPPING_COMPLETED = "STATE_STOPPING_COMPLETED"
+    STOPPING_CANCELED = "STATE_STOPPING_CANCELED"
+    STOPPING_ERROR = "STATE_STOPPING_ERROR"
+    COMPLETED = "STATE_COMPLETED"
+    CANCELED = "STATE_CANCELED"
+    ERROR = "STATE_ERROR"
+    DELETED = "STATE_DELETED"
+    DELETING = "STATE_DELETING"
+    DELETE_FAILED = "STATE_DELETE_FAILED"
+    STOPPING_KILLED = "STATE_STOPPING_KILLED"
+    QUEUED = "STATE_QUEUED"
+    PULLING = "STATE_PULLING"
+    STARTING = "STATE_STARTING"
+    RUNNING = "STATE_RUNNING"
 
 class jobv1State(enum.Enum):
-    STATE_UNSPECIFIED = "STATE_UNSPECIFIED"
-    STATE_QUEUED = "STATE_QUEUED"
-    STATE_SCHEDULED = "STATE_SCHEDULED"
-    STATE_SCHEDULED_BACKFILLED = "STATE_SCHEDULED_BACKFILLED"
+    UNSPECIFIED = "STATE_UNSPECIFIED"
+    QUEUED = "STATE_QUEUED"
+    SCHEDULED = "STATE_SCHEDULED"
+    SCHEDULED_BACKFILLED = "STATE_SCHEDULED_BACKFILLED"
 
 class jobv1Type(enum.Enum):
-    TYPE_UNSPECIFIED = "TYPE_UNSPECIFIED"
-    TYPE_EXPERIMENT = "TYPE_EXPERIMENT"
-    TYPE_NOTEBOOK = "TYPE_NOTEBOOK"
-    TYPE_TENSORBOARD = "TYPE_TENSORBOARD"
-    TYPE_SHELL = "TYPE_SHELL"
-    TYPE_COMMAND = "TYPE_COMMAND"
-    TYPE_CHECKPOINT_GC = "TYPE_CHECKPOINT_GC"
+    UNSPECIFIED = "TYPE_UNSPECIFIED"
+    EXPERIMENT = "TYPE_EXPERIMENT"
+    NOTEBOOK = "TYPE_NOTEBOOK"
+    TENSORBOARD = "TYPE_TENSORBOARD"
+    SHELL = "TYPE_SHELL"
+    COMMAND = "TYPE_COMMAND"
+    CHECKPOINT_GC = "TYPE_CHECKPOINT_GC"
 
 class protobufAny:
     typeUrl: "typing.Optional[str]" = None
     value: "typing.Optional[str]" = None
 
     def __init__(
         self,
@@ -392,34 +360,34 @@
         if not omit_unset or "httpStatus" in vars(self):
             out["httpStatus"] = self.httpStatus
         if not omit_unset or "message" in vars(self):
             out["message"] = self.message
         return out
 
 class taskv1State(enum.Enum):
-    STATE_UNSPECIFIED = "STATE_UNSPECIFIED"
-    STATE_PULLING = "STATE_PULLING"
-    STATE_STARTING = "STATE_STARTING"
-    STATE_RUNNING = "STATE_RUNNING"
-    STATE_TERMINATED = "STATE_TERMINATED"
-    STATE_TERMINATING = "STATE_TERMINATING"
-    STATE_WAITING = "STATE_WAITING"
-    STATE_QUEUED = "STATE_QUEUED"
+    UNSPECIFIED = "STATE_UNSPECIFIED"
+    PULLING = "STATE_PULLING"
+    STARTING = "STATE_STARTING"
+    RUNNING = "STATE_RUNNING"
+    TERMINATED = "STATE_TERMINATED"
+    TERMINATING = "STATE_TERMINATING"
+    WAITING = "STATE_WAITING"
+    QUEUED = "STATE_QUEUED"
 
 class trialv1State(enum.Enum):
-    STATE_UNSPECIFIED = "STATE_UNSPECIFIED"
-    STATE_ACTIVE = "STATE_ACTIVE"
-    STATE_PAUSED = "STATE_PAUSED"
-    STATE_STOPPING_CANCELED = "STATE_STOPPING_CANCELED"
-    STATE_STOPPING_KILLED = "STATE_STOPPING_KILLED"
-    STATE_STOPPING_COMPLETED = "STATE_STOPPING_COMPLETED"
-    STATE_STOPPING_ERROR = "STATE_STOPPING_ERROR"
-    STATE_CANCELED = "STATE_CANCELED"
-    STATE_COMPLETED = "STATE_COMPLETED"
-    STATE_ERROR = "STATE_ERROR"
+    UNSPECIFIED = "STATE_UNSPECIFIED"
+    ACTIVE = "STATE_ACTIVE"
+    PAUSED = "STATE_PAUSED"
+    STOPPING_CANCELED = "STATE_STOPPING_CANCELED"
+    STOPPING_KILLED = "STATE_STOPPING_KILLED"
+    STOPPING_COMPLETED = "STATE_STOPPING_COMPLETED"
+    STOPPING_ERROR = "STATE_STOPPING_ERROR"
+    CANCELED = "STATE_CANCELED"
+    COMPLETED = "STATE_COMPLETED"
+    ERROR = "STATE_ERROR"
 
 class trialv1Trial:
     bestCheckpoint: "typing.Optional[v1CheckpointWorkload]" = None
     bestValidation: "typing.Optional[v1MetricsWorkload]" = None
     checkpointCount: "typing.Optional[int]" = None
     endTime: "typing.Optional[str]" = None
     latestValidation: "typing.Optional[v1MetricsWorkload]" = None
@@ -614,16 +582,16 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "results": [x.to_json(omit_unset) for x in self.results],
         }
         return out
 
 class v1ActivityType(enum.Enum):
-    ACTIVITY_TYPE_UNSPECIFIED = "ACTIVITY_TYPE_UNSPECIFIED"
-    ACTIVITY_TYPE_GET = "ACTIVITY_TYPE_GET"
+    UNSPECIFIED = "ACTIVITY_TYPE_UNSPECIFIED"
+    GET = "ACTIVITY_TYPE_GET"
 
 class v1AddProjectNoteResponse:
 
     def __init__(
         self,
         *,
         notes: "typing.Sequence[v1Note]",
@@ -639,14 +607,64 @@
 
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "notes": [x.to_json(omit_unset) for x in self.notes],
         }
         return out
 
+class v1Address:
+    containerIp: "typing.Optional[str]" = None
+    containerPort: "typing.Optional[int]" = None
+    hostIp: "typing.Optional[str]" = None
+    hostPort: "typing.Optional[int]" = None
+
+    def __init__(
+        self,
+        *,
+        containerIp: "typing.Union[str, None, Unset]" = _unset,
+        containerPort: "typing.Union[int, None, Unset]" = _unset,
+        hostIp: "typing.Union[str, None, Unset]" = _unset,
+        hostPort: "typing.Union[int, None, Unset]" = _unset,
+    ):
+        if not isinstance(containerIp, Unset):
+            self.containerIp = containerIp
+        if not isinstance(containerPort, Unset):
+            self.containerPort = containerPort
+        if not isinstance(hostIp, Unset):
+            self.hostIp = hostIp
+        if not isinstance(hostPort, Unset):
+            self.hostPort = hostPort
+
+    @classmethod
+    def from_json(cls, obj: Json) -> "v1Address":
+        kwargs: "typing.Dict[str, typing.Any]" = {
+        }
+        if "containerIp" in obj:
+            kwargs["containerIp"] = obj["containerIp"]
+        if "containerPort" in obj:
+            kwargs["containerPort"] = obj["containerPort"]
+        if "hostIp" in obj:
+            kwargs["hostIp"] = obj["hostIp"]
+        if "hostPort" in obj:
+            kwargs["hostPort"] = obj["hostPort"]
+        return cls(**kwargs)
+
+    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
+        out: "typing.Dict[str, typing.Any]" = {
+        }
+        if not omit_unset or "containerIp" in vars(self):
+            out["containerIp"] = self.containerIp
+        if not omit_unset or "containerPort" in vars(self):
+            out["containerPort"] = self.containerPort
+        if not omit_unset or "hostIp" in vars(self):
+            out["hostIp"] = self.hostIp
+        if not omit_unset or "hostPort" in vars(self):
+            out["hostPort"] = self.hostPort
+        return out
+
 class v1Agent:
     addresses: "typing.Optional[typing.Sequence[str]]" = None
     containers: "typing.Optional[typing.Dict[str, v1Container]]" = None
     draining: "typing.Optional[bool]" = None
     enabled: "typing.Optional[bool]" = None
     label: "typing.Optional[str]" = None
     registeredTime: "typing.Optional[str]" = None
@@ -1035,14 +1053,112 @@
 
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "rendezvousInfo": self.rendezvousInfo.to_json(omit_unset),
         }
         return out
 
+class v1AllocationSummary:
+    allocationId: "typing.Optional[str]" = None
+    name: "typing.Optional[str]" = None
+    priority: "typing.Optional[int]" = None
+    proxyPorts: "typing.Optional[typing.Sequence[v1ProxyPortConfig]]" = None
+    registeredTime: "typing.Optional[str]" = None
+    resourcePool: "typing.Optional[str]" = None
+    resources: "typing.Optional[typing.Sequence[v1ResourcesSummary]]" = None
+    schedulerType: "typing.Optional[str]" = None
+    slotsNeeded: "typing.Optional[int]" = None
+    taskId: "typing.Optional[str]" = None
+
+    def __init__(
+        self,
+        *,
+        allocationId: "typing.Union[str, None, Unset]" = _unset,
+        name: "typing.Union[str, None, Unset]" = _unset,
+        priority: "typing.Union[int, None, Unset]" = _unset,
+        proxyPorts: "typing.Union[typing.Sequence[v1ProxyPortConfig], None, Unset]" = _unset,
+        registeredTime: "typing.Union[str, None, Unset]" = _unset,
+        resourcePool: "typing.Union[str, None, Unset]" = _unset,
+        resources: "typing.Union[typing.Sequence[v1ResourcesSummary], None, Unset]" = _unset,
+        schedulerType: "typing.Union[str, None, Unset]" = _unset,
+        slotsNeeded: "typing.Union[int, None, Unset]" = _unset,
+        taskId: "typing.Union[str, None, Unset]" = _unset,
+    ):
+        if not isinstance(allocationId, Unset):
+            self.allocationId = allocationId
+        if not isinstance(name, Unset):
+            self.name = name
+        if not isinstance(priority, Unset):
+            self.priority = priority
+        if not isinstance(proxyPorts, Unset):
+            self.proxyPorts = proxyPorts
+        if not isinstance(registeredTime, Unset):
+            self.registeredTime = registeredTime
+        if not isinstance(resourcePool, Unset):
+            self.resourcePool = resourcePool
+        if not isinstance(resources, Unset):
+            self.resources = resources
+        if not isinstance(schedulerType, Unset):
+            self.schedulerType = schedulerType
+        if not isinstance(slotsNeeded, Unset):
+            self.slotsNeeded = slotsNeeded
+        if not isinstance(taskId, Unset):
+            self.taskId = taskId
+
+    @classmethod
+    def from_json(cls, obj: Json) -> "v1AllocationSummary":
+        kwargs: "typing.Dict[str, typing.Any]" = {
+        }
+        if "allocationId" in obj:
+            kwargs["allocationId"] = obj["allocationId"]
+        if "name" in obj:
+            kwargs["name"] = obj["name"]
+        if "priority" in obj:
+            kwargs["priority"] = obj["priority"]
+        if "proxyPorts" in obj:
+            kwargs["proxyPorts"] = [v1ProxyPortConfig.from_json(x) for x in obj["proxyPorts"]] if obj["proxyPorts"] is not None else None
+        if "registeredTime" in obj:
+            kwargs["registeredTime"] = obj["registeredTime"]
+        if "resourcePool" in obj:
+            kwargs["resourcePool"] = obj["resourcePool"]
+        if "resources" in obj:
+            kwargs["resources"] = [v1ResourcesSummary.from_json(x) for x in obj["resources"]] if obj["resources"] is not None else None
+        if "schedulerType" in obj:
+            kwargs["schedulerType"] = obj["schedulerType"]
+        if "slotsNeeded" in obj:
+            kwargs["slotsNeeded"] = obj["slotsNeeded"]
+        if "taskId" in obj:
+            kwargs["taskId"] = obj["taskId"]
+        return cls(**kwargs)
+
+    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
+        out: "typing.Dict[str, typing.Any]" = {
+        }
+        if not omit_unset or "allocationId" in vars(self):
+            out["allocationId"] = self.allocationId
+        if not omit_unset or "name" in vars(self):
+            out["name"] = self.name
+        if not omit_unset or "priority" in vars(self):
+            out["priority"] = self.priority
+        if not omit_unset or "proxyPorts" in vars(self):
+            out["proxyPorts"] = None if self.proxyPorts is None else [x.to_json(omit_unset) for x in self.proxyPorts]
+        if not omit_unset or "registeredTime" in vars(self):
+            out["registeredTime"] = self.registeredTime
+        if not omit_unset or "resourcePool" in vars(self):
+            out["resourcePool"] = self.resourcePool
+        if not omit_unset or "resources" in vars(self):
+            out["resources"] = None if self.resources is None else [x.to_json(omit_unset) for x in self.resources]
+        if not omit_unset or "schedulerType" in vars(self):
+            out["schedulerType"] = self.schedulerType
+        if not omit_unset or "slotsNeeded" in vars(self):
+            out["slotsNeeded"] = self.slotsNeeded
+        if not omit_unset or "taskId" in vars(self):
+            out["taskId"] = self.taskId
+        return out
+
 class v1AllocationWaitingRequest:
     allocationId: "typing.Optional[str]" = None
 
     def __init__(
         self,
         *,
         allocationId: "typing.Union[str, None, Unset]" = _unset,
@@ -2245,14 +2361,66 @@
 
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "checkpointUuids": self.checkpointUuids,
         }
         return out
 
+class v1DeleteExperimentsRequest:
+    filters: "typing.Optional[v1BulkExperimentFilters]" = None
+
+    def __init__(
+        self,
+        *,
+        experimentIds: "typing.Sequence[int]",
+        filters: "typing.Union[v1BulkExperimentFilters, None, Unset]" = _unset,
+    ):
+        self.experimentIds = experimentIds
+        if not isinstance(filters, Unset):
+            self.filters = filters
+
+    @classmethod
+    def from_json(cls, obj: Json) -> "v1DeleteExperimentsRequest":
+        kwargs: "typing.Dict[str, typing.Any]" = {
+            "experimentIds": obj["experimentIds"],
+        }
+        if "filters" in obj:
+            kwargs["filters"] = v1BulkExperimentFilters.from_json(obj["filters"]) if obj["filters"] is not None else None
+        return cls(**kwargs)
+
+    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
+        out: "typing.Dict[str, typing.Any]" = {
+            "experimentIds": self.experimentIds,
+        }
+        if not omit_unset or "filters" in vars(self):
+            out["filters"] = None if self.filters is None else self.filters.to_json(omit_unset)
+        return out
+
+class v1DeleteExperimentsResponse:
+
+    def __init__(
+        self,
+        *,
+        results: "typing.Sequence[v1ExperimentActionResult]",
+    ):
+        self.results = results
+
+    @classmethod
+    def from_json(cls, obj: Json) -> "v1DeleteExperimentsResponse":
+        kwargs: "typing.Dict[str, typing.Any]" = {
+            "results": [v1ExperimentActionResult.from_json(x) for x in obj["results"]],
+        }
+        return cls(**kwargs)
+
+    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
+        out: "typing.Dict[str, typing.Any]" = {
+            "results": [x.to_json(omit_unset) for x in self.results],
+        }
+        return out
+
 class v1DeleteProjectResponse:
 
     def __init__(
         self,
         *,
         completed: bool,
     ):
@@ -2570,18 +2738,19 @@
         out: "typing.Dict[str, typing.Any]" = {
         }
         if not omit_unset or "slot" in vars(self):
             out["slot"] = None if self.slot is None else self.slot.to_json(omit_unset)
         return out
 
 class v1EntityType(enum.Enum):
-    ENTITY_TYPE_UNSPECIFIED = "ENTITY_TYPE_UNSPECIFIED"
-    ENTITY_TYPE_PROJECT = "ENTITY_TYPE_PROJECT"
+    UNSPECIFIED = "ENTITY_TYPE_UNSPECIFIED"
+    PROJECT = "ENTITY_TYPE_PROJECT"
 
 class v1Experiment:
+    bestTrialId: "typing.Optional[int]" = None
     bestTrialSearcherMetric: "typing.Optional[float]" = None
     checkpointCount: "typing.Optional[int]" = None
     checkpointSize: "typing.Optional[str]" = None
     description: "typing.Optional[str]" = None
     displayName: "typing.Optional[str]" = None
     endTime: "typing.Optional[str]" = None
     forkedFrom: "typing.Optional[int]" = None
@@ -2608,14 +2777,15 @@
         originalConfig: str,
         projectId: int,
         projectOwnerId: int,
         searcherType: str,
         startTime: str,
         state: "experimentv1State",
         username: str,
+        bestTrialId: "typing.Union[int, None, Unset]" = _unset,
         bestTrialSearcherMetric: "typing.Union[float, None, Unset]" = _unset,
         checkpointCount: "typing.Union[int, None, Unset]" = _unset,
         checkpointSize: "typing.Union[str, None, Unset]" = _unset,
         description: "typing.Union[str, None, Unset]" = _unset,
         displayName: "typing.Union[str, None, Unset]" = _unset,
         endTime: "typing.Union[str, None, Unset]" = _unset,
         forkedFrom: "typing.Union[int, None, Unset]" = _unset,
@@ -2639,14 +2809,16 @@
         self.originalConfig = originalConfig
         self.projectId = projectId
         self.projectOwnerId = projectOwnerId
         self.searcherType = searcherType
         self.startTime = startTime
         self.state = state
         self.username = username
+        if not isinstance(bestTrialId, Unset):
+            self.bestTrialId = bestTrialId
         if not isinstance(bestTrialSearcherMetric, Unset):
             self.bestTrialSearcherMetric = bestTrialSearcherMetric
         if not isinstance(checkpointCount, Unset):
             self.checkpointCount = checkpointCount
         if not isinstance(checkpointSize, Unset):
             self.checkpointSize = checkpointSize
         if not isinstance(description, Unset):
@@ -2691,14 +2863,16 @@
             "projectId": obj["projectId"],
             "projectOwnerId": obj["projectOwnerId"],
             "searcherType": obj["searcherType"],
             "startTime": obj["startTime"],
             "state": experimentv1State(obj["state"]),
             "username": obj["username"],
         }
+        if "bestTrialId" in obj:
+            kwargs["bestTrialId"] = obj["bestTrialId"]
         if "bestTrialSearcherMetric" in obj:
             kwargs["bestTrialSearcherMetric"] = float(obj["bestTrialSearcherMetric"]) if obj["bestTrialSearcherMetric"] is not None else None
         if "checkpointCount" in obj:
             kwargs["checkpointCount"] = obj["checkpointCount"]
         if "checkpointSize" in obj:
             kwargs["checkpointSize"] = obj["checkpointSize"]
         if "description" in obj:
@@ -2743,14 +2917,16 @@
             "projectId": self.projectId,
             "projectOwnerId": self.projectOwnerId,
             "searcherType": self.searcherType,
             "startTime": self.startTime,
             "state": self.state.value,
             "username": self.username,
         }
+        if not omit_unset or "bestTrialId" in vars(self):
+            out["bestTrialId"] = self.bestTrialId
         if not omit_unset or "bestTrialSearcherMetric" in vars(self):
             out["bestTrialSearcherMetric"] = None if self.bestTrialSearcherMetric is None else dump_float(self.bestTrialSearcherMetric)
         if not omit_unset or "checkpointCount" in vars(self):
             out["checkpointCount"] = self.checkpointCount
         if not omit_unset or "checkpointSize" in vars(self):
             out["checkpointSize"] = self.checkpointSize
         if not omit_unset or "description" in vars(self):
@@ -2869,14 +3045,26 @@
             out["config"] = self.config
         if not omit_unset or "seed" in vars(self):
             out["seed"] = self.seed
         if not omit_unset or "trials" in vars(self):
             out["trials"] = None if self.trials is None else [x.to_json(omit_unset) for x in self.trials]
         return out
 
+class v1FailureType(enum.Enum):
+    UNSPECIFIED = "FAILURE_TYPE_UNSPECIFIED"
+    RESOURCES_FAILED = "FAILURE_TYPE_RESOURCES_FAILED"
+    RESOURCES_ABORTED = "FAILURE_TYPE_RESOURCES_ABORTED"
+    RESOURCES_MISSING = "FAILURE_TYPE_RESOURCES_MISSING"
+    TASK_ABORTED = "FAILURE_TYPE_TASK_ABORTED"
+    TASK_ERROR = "FAILURE_TYPE_TASK_ERROR"
+    AGENT_FAILED = "FAILURE_TYPE_AGENT_FAILED"
+    AGENT_ERROR = "FAILURE_TYPE_AGENT_ERROR"
+    RESTORE_ERROR = "FAILURE_TYPE_RESTORE_ERROR"
+    UNKNOWN_ERROR = "FAILURE_TYPE_UNKNOWN_ERROR"
+
 class v1File:
 
     def __init__(
         self,
         *,
         content: str,
         gid: int,
@@ -2990,38 +3178,38 @@
         if not omit_unset or "name" in vars(self):
             out["name"] = self.name
         if not omit_unset or "path" in vars(self):
             out["path"] = self.path
         return out
 
 class v1FittingPolicy(enum.Enum):
-    FITTING_POLICY_UNSPECIFIED = "FITTING_POLICY_UNSPECIFIED"
-    FITTING_POLICY_BEST = "FITTING_POLICY_BEST"
-    FITTING_POLICY_WORST = "FITTING_POLICY_WORST"
-    FITTING_POLICY_KUBERNETES = "FITTING_POLICY_KUBERNETES"
-    FITTING_POLICY_SLURM = "FITTING_POLICY_SLURM"
-    FITTING_POLICY_PBS = "FITTING_POLICY_PBS"
+    UNSPECIFIED = "FITTING_POLICY_UNSPECIFIED"
+    BEST = "FITTING_POLICY_BEST"
+    WORST = "FITTING_POLICY_WORST"
+    KUBERNETES = "FITTING_POLICY_KUBERNETES"
+    SLURM = "FITTING_POLICY_SLURM"
+    PBS = "FITTING_POLICY_PBS"
 
 class v1GeneralColumn(enum.Enum):
-    GENERAL_COLUMN_UNSPECIFIED = "GENERAL_COLUMN_UNSPECIFIED"
-    GENERAL_COLUMN_ID = "GENERAL_COLUMN_ID"
-    GENERAL_COLUMN_NAME = "GENERAL_COLUMN_NAME"
-    GENERAL_COLUMN_DESCRIPTION = "GENERAL_COLUMN_DESCRIPTION"
-    GENERAL_COLUMN_TAGS = "GENERAL_COLUMN_TAGS"
-    GENERAL_COLUMN_FORKED = "GENERAL_COLUMN_FORKED"
-    GENERAL_COLUMN_STARTTIME = "GENERAL_COLUMN_STARTTIME"
-    GENERAL_COLUMN_DURATION = "GENERAL_COLUMN_DURATION"
-    GENERAL_COLUMN_COUNT = "GENERAL_COLUMN_COUNT"
-    GENERAL_COLUMN_STATE = "GENERAL_COLUMN_STATE"
-    GENERAL_COLUMN_SEARCHER_TYPE = "GENERAL_COLUMN_SEARCHER_TYPE"
-    GENERAL_COLUMN_RESOURSE_POOL = "GENERAL_COLUMN_RESOURSE_POOL"
-    GENERAL_COLUMN_PROGRESS = "GENERAL_COLUMN_PROGRESS"
-    GENERAL_COLUMN_CHECKPOINT_SIZE = "GENERAL_COLUMN_CHECKPOINT_SIZE"
-    GENERAL_COLUMN_CHECKPOINT_COUNT = "GENERAL_COLUMN_CHECKPOINT_COUNT"
-    GENERAL_COLUMN_USER = "GENERAL_COLUMN_USER"
+    UNSPECIFIED = "GENERAL_COLUMN_UNSPECIFIED"
+    ID = "GENERAL_COLUMN_ID"
+    NAME = "GENERAL_COLUMN_NAME"
+    DESCRIPTION = "GENERAL_COLUMN_DESCRIPTION"
+    TAGS = "GENERAL_COLUMN_TAGS"
+    FORKED = "GENERAL_COLUMN_FORKED"
+    STARTTIME = "GENERAL_COLUMN_STARTTIME"
+    DURATION = "GENERAL_COLUMN_DURATION"
+    COUNT = "GENERAL_COLUMN_COUNT"
+    STATE = "GENERAL_COLUMN_STATE"
+    SEARCHER_TYPE = "GENERAL_COLUMN_SEARCHER_TYPE"
+    RESOURSE_POOL = "GENERAL_COLUMN_RESOURSE_POOL"
+    PROGRESS = "GENERAL_COLUMN_PROGRESS"
+    CHECKPOINT_SIZE = "GENERAL_COLUMN_CHECKPOINT_SIZE"
+    CHECKPOINT_COUNT = "GENERAL_COLUMN_CHECKPOINT_COUNT"
+    USER = "GENERAL_COLUMN_USER"
 
 class v1GetActiveTasksCountResponse:
 
     def __init__(
         self,
         *,
         commands: int,
@@ -3072,17 +3260,17 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "agent": self.agent.to_json(omit_unset),
         }
         return out
 
 class v1GetAgentsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_ID = "SORT_BY_ID"
-    SORT_BY_TIME = "SORT_BY_TIME"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    ID = "SORT_BY_ID"
+    TIME = "SORT_BY_TIME"
 
 class v1GetAgentsResponse:
     pagination: "typing.Optional[v1Pagination]" = None
 
     def __init__(
         self,
         *,
@@ -3181,19 +3369,19 @@
         out: "typing.Dict[str, typing.Any]" = {
             "command": self.command.to_json(omit_unset),
             "config": self.config,
         }
         return out
 
 class v1GetCommandsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_ID = "SORT_BY_ID"
-    SORT_BY_DESCRIPTION = "SORT_BY_DESCRIPTION"
-    SORT_BY_START_TIME = "SORT_BY_START_TIME"
-    SORT_BY_WORKSPACE_ID = "SORT_BY_WORKSPACE_ID"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    ID = "SORT_BY_ID"
+    DESCRIPTION = "SORT_BY_DESCRIPTION"
+    START_TIME = "SORT_BY_START_TIME"
+    WORKSPACE_ID = "SORT_BY_WORKSPACE_ID"
 
 class v1GetCommandsResponse:
     pagination: "typing.Optional[v1Pagination]" = None
 
     def __init__(
         self,
         *,
@@ -3252,21 +3440,21 @@
         if not omit_unset or "completed" in vars(self):
             out["completed"] = self.completed
         if not omit_unset or "op" in vars(self):
             out["op"] = None if self.op is None else self.op.to_json(omit_unset)
         return out
 
 class v1GetExperimentCheckpointsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_UUID = "SORT_BY_UUID"
-    SORT_BY_TRIAL_ID = "SORT_BY_TRIAL_ID"
-    SORT_BY_BATCH_NUMBER = "SORT_BY_BATCH_NUMBER"
-    SORT_BY_END_TIME = "SORT_BY_END_TIME"
-    SORT_BY_STATE = "SORT_BY_STATE"
-    SORT_BY_SEARCHER_METRIC = "SORT_BY_SEARCHER_METRIC"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    UUID = "SORT_BY_UUID"
+    TRIAL_ID = "SORT_BY_TRIAL_ID"
+    BATCH_NUMBER = "SORT_BY_BATCH_NUMBER"
+    END_TIME = "SORT_BY_END_TIME"
+    STATE = "SORT_BY_STATE"
+    SEARCHER_METRIC = "SORT_BY_SEARCHER_METRIC"
 
 class v1GetExperimentCheckpointsResponse:
 
     def __init__(
         self,
         *,
         checkpoints: "typing.Sequence[v1Checkpoint]",
@@ -3343,25 +3531,25 @@
             "experiment": self.experiment.to_json(omit_unset),
         }
         if not omit_unset or "jobSummary" in vars(self):
             out["jobSummary"] = None if self.jobSummary is None else self.jobSummary.to_json(omit_unset)
         return out
 
 class v1GetExperimentTrialsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_ID = "SORT_BY_ID"
-    SORT_BY_START_TIME = "SORT_BY_START_TIME"
-    SORT_BY_END_TIME = "SORT_BY_END_TIME"
-    SORT_BY_STATE = "SORT_BY_STATE"
-    SORT_BY_BEST_VALIDATION_METRIC = "SORT_BY_BEST_VALIDATION_METRIC"
-    SORT_BY_LATEST_VALIDATION_METRIC = "SORT_BY_LATEST_VALIDATION_METRIC"
-    SORT_BY_BATCHES_PROCESSED = "SORT_BY_BATCHES_PROCESSED"
-    SORT_BY_DURATION = "SORT_BY_DURATION"
-    SORT_BY_RESTARTS = "SORT_BY_RESTARTS"
-    SORT_BY_CHECKPOINT_SIZE = "SORT_BY_CHECKPOINT_SIZE"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    ID = "SORT_BY_ID"
+    START_TIME = "SORT_BY_START_TIME"
+    END_TIME = "SORT_BY_END_TIME"
+    STATE = "SORT_BY_STATE"
+    BEST_VALIDATION_METRIC = "SORT_BY_BEST_VALIDATION_METRIC"
+    LATEST_VALIDATION_METRIC = "SORT_BY_LATEST_VALIDATION_METRIC"
+    BATCHES_PROCESSED = "SORT_BY_BATCHES_PROCESSED"
+    DURATION = "SORT_BY_DURATION"
+    RESTARTS = "SORT_BY_RESTARTS"
+    CHECKPOINT_SIZE = "SORT_BY_CHECKPOINT_SIZE"
 
 class v1GetExperimentTrialsResponse:
 
     def __init__(
         self,
         *,
         pagination: "v1Pagination",
@@ -3408,30 +3596,30 @@
         out: "typing.Dict[str, typing.Any]" = {
         }
         if not omit_unset or "validationHistory" in vars(self):
             out["validationHistory"] = None if self.validationHistory is None else [x.to_json(omit_unset) for x in self.validationHistory]
         return out
 
 class v1GetExperimentsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_ID = "SORT_BY_ID"
-    SORT_BY_DESCRIPTION = "SORT_BY_DESCRIPTION"
-    SORT_BY_START_TIME = "SORT_BY_START_TIME"
-    SORT_BY_END_TIME = "SORT_BY_END_TIME"
-    SORT_BY_STATE = "SORT_BY_STATE"
-    SORT_BY_NUM_TRIALS = "SORT_BY_NUM_TRIALS"
-    SORT_BY_PROGRESS = "SORT_BY_PROGRESS"
-    SORT_BY_USER = "SORT_BY_USER"
-    SORT_BY_NAME = "SORT_BY_NAME"
-    SORT_BY_FORKED_FROM = "SORT_BY_FORKED_FROM"
-    SORT_BY_RESOURCE_POOL = "SORT_BY_RESOURCE_POOL"
-    SORT_BY_PROJECT_ID = "SORT_BY_PROJECT_ID"
-    SORT_BY_CHECKPOINT_SIZE = "SORT_BY_CHECKPOINT_SIZE"
-    SORT_BY_CHECKPOINT_COUNT = "SORT_BY_CHECKPOINT_COUNT"
-    SORT_BY_SEARCHER_METRIC_VAL = "SORT_BY_SEARCHER_METRIC_VAL"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    ID = "SORT_BY_ID"
+    DESCRIPTION = "SORT_BY_DESCRIPTION"
+    START_TIME = "SORT_BY_START_TIME"
+    END_TIME = "SORT_BY_END_TIME"
+    STATE = "SORT_BY_STATE"
+    NUM_TRIALS = "SORT_BY_NUM_TRIALS"
+    PROGRESS = "SORT_BY_PROGRESS"
+    USER = "SORT_BY_USER"
+    NAME = "SORT_BY_NAME"
+    FORKED_FROM = "SORT_BY_FORKED_FROM"
+    RESOURCE_POOL = "SORT_BY_RESOURCE_POOL"
+    PROJECT_ID = "SORT_BY_PROJECT_ID"
+    CHECKPOINT_SIZE = "SORT_BY_CHECKPOINT_SIZE"
+    CHECKPOINT_COUNT = "SORT_BY_CHECKPOINT_COUNT"
+    SEARCHER_METRIC_VAL = "SORT_BY_SEARCHER_METRIC_VAL"
 
 class v1GetExperimentsResponse:
 
     def __init__(
         self,
         *,
         experiments: "typing.Sequence[v1Experiment]",
@@ -3583,40 +3771,14 @@
         }
         if not omit_unset or "groups" in vars(self):
             out["groups"] = None if self.groups is None else [x.to_json(omit_unset) for x in self.groups]
         if not omit_unset or "pagination" in vars(self):
             out["pagination"] = None if self.pagination is None else self.pagination.to_json(omit_unset)
         return out
 
-class v1GetHPImportanceResponse:
-
-    def __init__(
-        self,
-        *,
-        trainingMetrics: "typing.Dict[str, GetHPImportanceResponseMetricHPImportance]",
-        validationMetrics: "typing.Dict[str, GetHPImportanceResponseMetricHPImportance]",
-    ):
-        self.trainingMetrics = trainingMetrics
-        self.validationMetrics = validationMetrics
-
-    @classmethod
-    def from_json(cls, obj: Json) -> "v1GetHPImportanceResponse":
-        kwargs: "typing.Dict[str, typing.Any]" = {
-            "trainingMetrics": {k: GetHPImportanceResponseMetricHPImportance.from_json(v) for k, v in obj["trainingMetrics"].items()},
-            "validationMetrics": {k: GetHPImportanceResponseMetricHPImportance.from_json(v) for k, v in obj["validationMetrics"].items()},
-        }
-        return cls(**kwargs)
-
-    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
-        out: "typing.Dict[str, typing.Any]" = {
-            "trainingMetrics": {k: v.to_json(omit_unset) for k, v in self.trainingMetrics.items()},
-            "validationMetrics": {k: v.to_json(omit_unset) for k, v in self.validationMetrics.items()},
-        }
-        return out
-
 class v1GetJobQueueStatsResponse:
 
     def __init__(
         self,
         *,
         results: "typing.Sequence[v1RPQueueStat]",
     ):
@@ -3982,17 +4144,17 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "modelVersion": self.modelVersion.to_json(omit_unset),
         }
         return out
 
 class v1GetModelVersionsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_VERSION = "SORT_BY_VERSION"
-    SORT_BY_CREATION_TIME = "SORT_BY_CREATION_TIME"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    VERSION = "SORT_BY_VERSION"
+    CREATION_TIME = "SORT_BY_CREATION_TIME"
 
 class v1GetModelVersionsResponse:
 
     def __init__(
         self,
         *,
         model: "v1Model",
@@ -4017,21 +4179,21 @@
             "model": self.model.to_json(omit_unset),
             "modelVersions": [x.to_json(omit_unset) for x in self.modelVersions],
             "pagination": self.pagination.to_json(omit_unset),
         }
         return out
 
 class v1GetModelsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_NAME = "SORT_BY_NAME"
-    SORT_BY_DESCRIPTION = "SORT_BY_DESCRIPTION"
-    SORT_BY_CREATION_TIME = "SORT_BY_CREATION_TIME"
-    SORT_BY_LAST_UPDATED_TIME = "SORT_BY_LAST_UPDATED_TIME"
-    SORT_BY_NUM_VERSIONS = "SORT_BY_NUM_VERSIONS"
-    SORT_BY_WORKSPACE = "SORT_BY_WORKSPACE"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    NAME = "SORT_BY_NAME"
+    DESCRIPTION = "SORT_BY_DESCRIPTION"
+    CREATION_TIME = "SORT_BY_CREATION_TIME"
+    LAST_UPDATED_TIME = "SORT_BY_LAST_UPDATED_TIME"
+    NUM_VERSIONS = "SORT_BY_NUM_VERSIONS"
+    WORKSPACE = "SORT_BY_WORKSPACE"
 
 class v1GetModelsResponse:
 
     def __init__(
         self,
         *,
         models: "typing.Sequence[v1Model]",
@@ -4078,19 +4240,19 @@
         out: "typing.Dict[str, typing.Any]" = {
             "config": self.config,
             "notebook": self.notebook.to_json(omit_unset),
         }
         return out
 
 class v1GetNotebooksRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_ID = "SORT_BY_ID"
-    SORT_BY_DESCRIPTION = "SORT_BY_DESCRIPTION"
-    SORT_BY_START_TIME = "SORT_BY_START_TIME"
-    SORT_BY_WORKSPACE_ID = "SORT_BY_WORKSPACE_ID"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    ID = "SORT_BY_ID"
+    DESCRIPTION = "SORT_BY_DESCRIPTION"
+    START_TIME = "SORT_BY_START_TIME"
+    WORKSPACE_ID = "SORT_BY_WORKSPACE_ID"
 
 class v1GetNotebooksResponse:
     pagination: "typing.Optional[v1Pagination]" = None
 
     def __init__(
         self,
         *,
@@ -4405,19 +4567,19 @@
         out: "typing.Dict[str, typing.Any]" = {
             "config": self.config,
             "shell": self.shell.to_json(omit_unset),
         }
         return out
 
 class v1GetShellsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_ID = "SORT_BY_ID"
-    SORT_BY_DESCRIPTION = "SORT_BY_DESCRIPTION"
-    SORT_BY_START_TIME = "SORT_BY_START_TIME"
-    SORT_BY_WORKSPACE_ID = "SORT_BY_WORKSPACE_ID"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    ID = "SORT_BY_ID"
+    DESCRIPTION = "SORT_BY_DESCRIPTION"
+    START_TIME = "SORT_BY_START_TIME"
+    WORKSPACE_ID = "SORT_BY_WORKSPACE_ID"
 
 class v1GetShellsResponse:
     pagination: "typing.Optional[v1Pagination]" = None
 
     def __init__(
         self,
         *,
@@ -4519,14 +4681,40 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
         }
         if not omit_unset or "task" in vars(self):
             out["task"] = None if self.task is None else self.task.to_json(omit_unset)
         return out
 
+class v1GetTasksResponse:
+    allocationIdToSummary: "typing.Optional[typing.Dict[str, v1AllocationSummary]]" = None
+
+    def __init__(
+        self,
+        *,
+        allocationIdToSummary: "typing.Union[typing.Dict[str, v1AllocationSummary], None, Unset]" = _unset,
+    ):
+        if not isinstance(allocationIdToSummary, Unset):
+            self.allocationIdToSummary = allocationIdToSummary
+
+    @classmethod
+    def from_json(cls, obj: Json) -> "v1GetTasksResponse":
+        kwargs: "typing.Dict[str, typing.Any]" = {
+        }
+        if "allocationIdToSummary" in obj:
+            kwargs["allocationIdToSummary"] = {k: v1AllocationSummary.from_json(v) for k, v in obj["allocationIdToSummary"].items()} if obj["allocationIdToSummary"] is not None else None
+        return cls(**kwargs)
+
+    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
+        out: "typing.Dict[str, typing.Any]" = {
+        }
+        if not omit_unset or "allocationIdToSummary" in vars(self):
+            out["allocationIdToSummary"] = None if self.allocationIdToSummary is None else {k: v.to_json(omit_unset) for k, v in self.allocationIdToSummary.items()}
+        return out
+
 class v1GetTelemetryResponse:
     segmentKey: "typing.Optional[str]" = None
 
     def __init__(
         self,
         *,
         enabled: bool,
@@ -4576,16 +4764,16 @@
         out: "typing.Dict[str, typing.Any]" = {
         }
         if not omit_unset or "template" in vars(self):
             out["template"] = None if self.template is None else self.template.to_json(omit_unset)
         return out
 
 class v1GetTemplatesRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_NAME = "SORT_BY_NAME"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    NAME = "SORT_BY_NAME"
 
 class v1GetTemplatesResponse:
     pagination: "typing.Optional[v1Pagination]" = None
     templates: "typing.Optional[typing.Sequence[v1Template]]" = None
 
     def __init__(
         self,
@@ -4640,19 +4828,19 @@
         out: "typing.Dict[str, typing.Any]" = {
             "config": self.config,
             "tensorboard": self.tensorboard.to_json(omit_unset),
         }
         return out
 
 class v1GetTensorboardsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_ID = "SORT_BY_ID"
-    SORT_BY_DESCRIPTION = "SORT_BY_DESCRIPTION"
-    SORT_BY_START_TIME = "SORT_BY_START_TIME"
-    SORT_BY_WORKSPACE_ID = "SORT_BY_WORKSPACE_ID"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    ID = "SORT_BY_ID"
+    DESCRIPTION = "SORT_BY_DESCRIPTION"
+    START_TIME = "SORT_BY_START_TIME"
+    WORKSPACE_ID = "SORT_BY_WORKSPACE_ID"
 
 class v1GetTensorboardsResponse:
     pagination: "typing.Optional[v1Pagination]" = None
 
     def __init__(
         self,
         *,
@@ -4699,19 +4887,19 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "metrics": [x.to_json(omit_unset) for x in self.metrics],
         }
         return out
 
 class v1GetTrialCheckpointsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_UUID = "SORT_BY_UUID"
-    SORT_BY_BATCH_NUMBER = "SORT_BY_BATCH_NUMBER"
-    SORT_BY_END_TIME = "SORT_BY_END_TIME"
-    SORT_BY_STATE = "SORT_BY_STATE"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    UUID = "SORT_BY_UUID"
+    BATCH_NUMBER = "SORT_BY_BATCH_NUMBER"
+    END_TIME = "SORT_BY_END_TIME"
+    STATE = "SORT_BY_STATE"
 
 class v1GetTrialCheckpointsResponse:
 
     def __init__(
         self,
         *,
         checkpoints: "typing.Sequence[v1Checkpoint]",
@@ -4916,21 +5104,21 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "settings": [x.to_json(omit_unset) for x in self.settings],
         }
         return out
 
 class v1GetUsersRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_DISPLAY_NAME = "SORT_BY_DISPLAY_NAME"
-    SORT_BY_USER_NAME = "SORT_BY_USER_NAME"
-    SORT_BY_ADMIN = "SORT_BY_ADMIN"
-    SORT_BY_ACTIVE = "SORT_BY_ACTIVE"
-    SORT_BY_MODIFIED_TIME = "SORT_BY_MODIFIED_TIME"
-    SORT_BY_NAME = "SORT_BY_NAME"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    DISPLAY_NAME = "SORT_BY_DISPLAY_NAME"
+    USER_NAME = "SORT_BY_USER_NAME"
+    ADMIN = "SORT_BY_ADMIN"
+    ACTIVE = "SORT_BY_ACTIVE"
+    MODIFIED_TIME = "SORT_BY_MODIFIED_TIME"
+    NAME = "SORT_BY_NAME"
 
 class v1GetUsersResponse:
     pagination: "typing.Optional[v1Pagination]" = None
     users: "typing.Optional[typing.Sequence[v1User]]" = None
 
     def __init__(
         self,
@@ -5003,20 +5191,20 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "webhooks": [x.to_json(omit_unset) for x in self.webhooks],
         }
         return out
 
 class v1GetWorkspaceProjectsRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_CREATION_TIME = "SORT_BY_CREATION_TIME"
-    SORT_BY_LAST_EXPERIMENT_START_TIME = "SORT_BY_LAST_EXPERIMENT_START_TIME"
-    SORT_BY_NAME = "SORT_BY_NAME"
-    SORT_BY_DESCRIPTION = "SORT_BY_DESCRIPTION"
-    SORT_BY_ID = "SORT_BY_ID"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    CREATION_TIME = "SORT_BY_CREATION_TIME"
+    LAST_EXPERIMENT_START_TIME = "SORT_BY_LAST_EXPERIMENT_START_TIME"
+    NAME = "SORT_BY_NAME"
+    DESCRIPTION = "SORT_BY_DESCRIPTION"
+    ID = "SORT_BY_ID"
 
 class v1GetWorkspaceProjectsResponse:
 
     def __init__(
         self,
         *,
         pagination: "v1Pagination",
@@ -5059,17 +5247,17 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "workspace": self.workspace.to_json(omit_unset),
         }
         return out
 
 class v1GetWorkspacesRequestSortBy(enum.Enum):
-    SORT_BY_UNSPECIFIED = "SORT_BY_UNSPECIFIED"
-    SORT_BY_ID = "SORT_BY_ID"
-    SORT_BY_NAME = "SORT_BY_NAME"
+    UNSPECIFIED = "SORT_BY_UNSPECIFIED"
+    ID = "SORT_BY_ID"
+    NAME = "SORT_BY_NAME"
 
 class v1GetWorkspacesResponse:
 
     def __init__(
         self,
         *,
         pagination: "v1Pagination",
@@ -5937,34 +6125,38 @@
             out["warnings"] = None if self.warnings is None else [x.value for x in self.warnings]
         return out
 
 class v1LaunchTensorboardRequest:
     config: "typing.Optional[typing.Dict[str, typing.Any]]" = None
     experimentIds: "typing.Optional[typing.Sequence[int]]" = None
     files: "typing.Optional[typing.Sequence[v1File]]" = None
+    filters: "typing.Optional[v1BulkExperimentFilters]" = None
     templateName: "typing.Optional[str]" = None
     trialIds: "typing.Optional[typing.Sequence[int]]" = None
     workspaceId: "typing.Optional[int]" = None
 
     def __init__(
         self,
         *,
         config: "typing.Union[typing.Dict[str, typing.Any], None, Unset]" = _unset,
         experimentIds: "typing.Union[typing.Sequence[int], None, Unset]" = _unset,
         files: "typing.Union[typing.Sequence[v1File], None, Unset]" = _unset,
+        filters: "typing.Union[v1BulkExperimentFilters, None, Unset]" = _unset,
         templateName: "typing.Union[str, None, Unset]" = _unset,
         trialIds: "typing.Union[typing.Sequence[int], None, Unset]" = _unset,
         workspaceId: "typing.Union[int, None, Unset]" = _unset,
     ):
         if not isinstance(config, Unset):
             self.config = config
         if not isinstance(experimentIds, Unset):
             self.experimentIds = experimentIds
         if not isinstance(files, Unset):
             self.files = files
+        if not isinstance(filters, Unset):
+            self.filters = filters
         if not isinstance(templateName, Unset):
             self.templateName = templateName
         if not isinstance(trialIds, Unset):
             self.trialIds = trialIds
         if not isinstance(workspaceId, Unset):
             self.workspaceId = workspaceId
 
@@ -5974,14 +6166,16 @@
         }
         if "config" in obj:
             kwargs["config"] = obj["config"]
         if "experimentIds" in obj:
             kwargs["experimentIds"] = obj["experimentIds"]
         if "files" in obj:
             kwargs["files"] = [v1File.from_json(x) for x in obj["files"]] if obj["files"] is not None else None
+        if "filters" in obj:
+            kwargs["filters"] = v1BulkExperimentFilters.from_json(obj["filters"]) if obj["filters"] is not None else None
         if "templateName" in obj:
             kwargs["templateName"] = obj["templateName"]
         if "trialIds" in obj:
             kwargs["trialIds"] = obj["trialIds"]
         if "workspaceId" in obj:
             kwargs["workspaceId"] = obj["workspaceId"]
         return cls(**kwargs)
@@ -5991,14 +6185,16 @@
         }
         if not omit_unset or "config" in vars(self):
             out["config"] = self.config
         if not omit_unset or "experimentIds" in vars(self):
             out["experimentIds"] = self.experimentIds
         if not omit_unset or "files" in vars(self):
             out["files"] = None if self.files is None else [x.to_json(omit_unset) for x in self.files]
+        if not omit_unset or "filters" in vars(self):
+            out["filters"] = None if self.filters is None else self.filters.to_json(omit_unset)
         if not omit_unset or "templateName" in vars(self):
             out["templateName"] = self.templateName
         if not omit_unset or "trialIds" in vars(self):
             out["trialIds"] = self.trialIds
         if not omit_unset or "workspaceId" in vars(self):
             out["workspaceId"] = self.workspaceId
         return out
@@ -6034,16 +6230,16 @@
             "tensorboard": self.tensorboard.to_json(omit_unset),
         }
         if not omit_unset or "warnings" in vars(self):
             out["warnings"] = None if self.warnings is None else [x.value for x in self.warnings]
         return out
 
 class v1LaunchWarning(enum.Enum):
-    LAUNCH_WARNING_UNSPECIFIED = "LAUNCH_WARNING_UNSPECIFIED"
-    LAUNCH_WARNING_CURRENT_SLOTS_EXCEEDED = "LAUNCH_WARNING_CURRENT_SLOTS_EXCEEDED"
+    UNSPECIFIED = "LAUNCH_WARNING_UNSPECIFIED"
+    CURRENT_SLOTS_EXCEEDED = "LAUNCH_WARNING_CURRENT_SLOTS_EXCEEDED"
 
 class v1ListRolesRequest:
     offset: "typing.Optional[int]" = None
 
     def __init__(
         self,
         *,
@@ -6128,21 +6324,21 @@
             "level": self.level.value,
             "message": self.message,
             "timestamp": self.timestamp,
         }
         return out
 
 class v1LogLevel(enum.Enum):
-    LOG_LEVEL_UNSPECIFIED = "LOG_LEVEL_UNSPECIFIED"
-    LOG_LEVEL_TRACE = "LOG_LEVEL_TRACE"
-    LOG_LEVEL_DEBUG = "LOG_LEVEL_DEBUG"
-    LOG_LEVEL_INFO = "LOG_LEVEL_INFO"
-    LOG_LEVEL_WARNING = "LOG_LEVEL_WARNING"
-    LOG_LEVEL_ERROR = "LOG_LEVEL_ERROR"
-    LOG_LEVEL_CRITICAL = "LOG_LEVEL_CRITICAL"
+    UNSPECIFIED = "LOG_LEVEL_UNSPECIFIED"
+    TRACE = "LOG_LEVEL_TRACE"
+    DEBUG = "LOG_LEVEL_DEBUG"
+    INFO = "LOG_LEVEL_INFO"
+    WARNING = "LOG_LEVEL_WARNING"
+    ERROR = "LOG_LEVEL_ERROR"
+    CRITICAL = "LOG_LEVEL_CRITICAL"
 
 class v1LoginRequest:
     isHashed: "typing.Optional[bool]" = None
 
     def __init__(
         self,
         *,
@@ -6317,17 +6513,17 @@
         if not omit_unset or "trainingMetrics" in vars(self):
             out["trainingMetrics"] = self.trainingMetrics
         if not omit_unset or "validationMetrics" in vars(self):
             out["validationMetrics"] = self.validationMetrics
         return out
 
 class v1MetricType(enum.Enum):
-    METRIC_TYPE_UNSPECIFIED = "METRIC_TYPE_UNSPECIFIED"
-    METRIC_TYPE_TRAINING = "METRIC_TYPE_TRAINING"
-    METRIC_TYPE_VALIDATION = "METRIC_TYPE_VALIDATION"
+    UNSPECIFIED = "METRIC_TYPE_UNSPECIFIED"
+    TRAINING = "METRIC_TYPE_TRAINING"
+    VALIDATION = "METRIC_TYPE_VALIDATION"
 
 class v1Metrics:
     batchMetrics: "typing.Optional[typing.Sequence[typing.Dict[str, typing.Any]]]" = None
 
     def __init__(
         self,
         *,
@@ -6946,17 +7142,17 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "data": self.data,
         }
         return out
 
 class v1OrderBy(enum.Enum):
-    ORDER_BY_UNSPECIFIED = "ORDER_BY_UNSPECIFIED"
-    ORDER_BY_ASC = "ORDER_BY_ASC"
-    ORDER_BY_DESC = "ORDER_BY_DESC"
+    UNSPECIFIED = "ORDER_BY_UNSPECIFIED"
+    ASC = "ORDER_BY_ASC"
+    DESC = "ORDER_BY_DESC"
 
 class v1Pagination:
     endIndex: "typing.Optional[int]" = None
     limit: "typing.Optional[int]" = None
     offset: "typing.Optional[int]" = None
     startIndex: "typing.Optional[int]" = None
     total: "typing.Optional[int]" = None
@@ -7659,44 +7855,46 @@
         if not omit_unset or "name" in vars(self):
             out["name"] = self.name
         if not omit_unset or "scopeTypeMask" in vars(self):
             out["scopeTypeMask"] = None if self.scopeTypeMask is None else self.scopeTypeMask.to_json(omit_unset)
         return out
 
 class v1PermissionType(enum.Enum):
-    PERMISSION_TYPE_UNSPECIFIED = "PERMISSION_TYPE_UNSPECIFIED"
-    PERMISSION_TYPE_ADMINISTRATE_USER = "PERMISSION_TYPE_ADMINISTRATE_USER"
-    PERMISSION_TYPE_CREATE_EXPERIMENT = "PERMISSION_TYPE_CREATE_EXPERIMENT"
-    PERMISSION_TYPE_VIEW_EXPERIMENT_ARTIFACTS = "PERMISSION_TYPE_VIEW_EXPERIMENT_ARTIFACTS"
-    PERMISSION_TYPE_VIEW_EXPERIMENT_METADATA = "PERMISSION_TYPE_VIEW_EXPERIMENT_METADATA"
-    PERMISSION_TYPE_UPDATE_EXPERIMENT = "PERMISSION_TYPE_UPDATE_EXPERIMENT"
-    PERMISSION_TYPE_UPDATE_EXPERIMENT_METADATA = "PERMISSION_TYPE_UPDATE_EXPERIMENT_METADATA"
-    PERMISSION_TYPE_DELETE_EXPERIMENT = "PERMISSION_TYPE_DELETE_EXPERIMENT"
-    PERMISSION_TYPE_CREATE_NSC = "PERMISSION_TYPE_CREATE_NSC"
-    PERMISSION_TYPE_VIEW_NSC = "PERMISSION_TYPE_VIEW_NSC"
-    PERMISSION_TYPE_UPDATE_NSC = "PERMISSION_TYPE_UPDATE_NSC"
-    PERMISSION_TYPE_UPDATE_GROUP = "PERMISSION_TYPE_UPDATE_GROUP"
-    PERMISSION_TYPE_CREATE_WORKSPACE = "PERMISSION_TYPE_CREATE_WORKSPACE"
-    PERMISSION_TYPE_VIEW_WORKSPACE = "PERMISSION_TYPE_VIEW_WORKSPACE"
-    PERMISSION_TYPE_UPDATE_WORKSPACE = "PERMISSION_TYPE_UPDATE_WORKSPACE"
-    PERMISSION_TYPE_DELETE_WORKSPACE = "PERMISSION_TYPE_DELETE_WORKSPACE"
-    PERMISSION_TYPE_SET_WORKSPACE_AGENT_USER_GROUP = "PERMISSION_TYPE_SET_WORKSPACE_AGENT_USER_GROUP"
-    PERMISSION_TYPE_SET_WORKSPACE_CHECKPOINT_STORAGE_CONFIG = "PERMISSION_TYPE_SET_WORKSPACE_CHECKPOINT_STORAGE_CONFIG"
-    PERMISSION_TYPE_CREATE_PROJECT = "PERMISSION_TYPE_CREATE_PROJECT"
-    PERMISSION_TYPE_VIEW_PROJECT = "PERMISSION_TYPE_VIEW_PROJECT"
-    PERMISSION_TYPE_UPDATE_PROJECT = "PERMISSION_TYPE_UPDATE_PROJECT"
-    PERMISSION_TYPE_DELETE_PROJECT = "PERMISSION_TYPE_DELETE_PROJECT"
-    PERMISSION_TYPE_ASSIGN_ROLES = "PERMISSION_TYPE_ASSIGN_ROLES"
-    PERMISSION_TYPE_VIEW_MODEL_REGISTRY = "PERMISSION_TYPE_VIEW_MODEL_REGISTRY"
-    PERMISSION_TYPE_EDIT_MODEL_REGISTRY = "PERMISSION_TYPE_EDIT_MODEL_REGISTRY"
-    PERMISSION_TYPE_CREATE_MODEL_REGISTRY = "PERMISSION_TYPE_CREATE_MODEL_REGISTRY"
-    PERMISSION_TYPE_DELETE_MODEL_REGISTRY = "PERMISSION_TYPE_DELETE_MODEL_REGISTRY"
-    PERMISSION_TYPE_UPDATE_AGENTS = "PERMISSION_TYPE_UPDATE_AGENTS"
-    PERMISSION_TYPE_UPDATE_ROLES = "PERMISSION_TYPE_UPDATE_ROLES"
-    PERMISSION_TYPE_EDIT_WEBHOOKS = "PERMISSION_TYPE_EDIT_WEBHOOKS"
+    UNSPECIFIED = "PERMISSION_TYPE_UNSPECIFIED"
+    ADMINISTRATE_USER = "PERMISSION_TYPE_ADMINISTRATE_USER"
+    CREATE_EXPERIMENT = "PERMISSION_TYPE_CREATE_EXPERIMENT"
+    VIEW_EXPERIMENT_ARTIFACTS = "PERMISSION_TYPE_VIEW_EXPERIMENT_ARTIFACTS"
+    VIEW_EXPERIMENT_METADATA = "PERMISSION_TYPE_VIEW_EXPERIMENT_METADATA"
+    UPDATE_EXPERIMENT = "PERMISSION_TYPE_UPDATE_EXPERIMENT"
+    UPDATE_EXPERIMENT_METADATA = "PERMISSION_TYPE_UPDATE_EXPERIMENT_METADATA"
+    DELETE_EXPERIMENT = "PERMISSION_TYPE_DELETE_EXPERIMENT"
+    CREATE_NSC = "PERMISSION_TYPE_CREATE_NSC"
+    VIEW_NSC = "PERMISSION_TYPE_VIEW_NSC"
+    UPDATE_NSC = "PERMISSION_TYPE_UPDATE_NSC"
+    UPDATE_GROUP = "PERMISSION_TYPE_UPDATE_GROUP"
+    CREATE_WORKSPACE = "PERMISSION_TYPE_CREATE_WORKSPACE"
+    VIEW_WORKSPACE = "PERMISSION_TYPE_VIEW_WORKSPACE"
+    UPDATE_WORKSPACE = "PERMISSION_TYPE_UPDATE_WORKSPACE"
+    DELETE_WORKSPACE = "PERMISSION_TYPE_DELETE_WORKSPACE"
+    SET_WORKSPACE_AGENT_USER_GROUP = "PERMISSION_TYPE_SET_WORKSPACE_AGENT_USER_GROUP"
+    SET_WORKSPACE_CHECKPOINT_STORAGE_CONFIG = "PERMISSION_TYPE_SET_WORKSPACE_CHECKPOINT_STORAGE_CONFIG"
+    CREATE_PROJECT = "PERMISSION_TYPE_CREATE_PROJECT"
+    VIEW_PROJECT = "PERMISSION_TYPE_VIEW_PROJECT"
+    UPDATE_PROJECT = "PERMISSION_TYPE_UPDATE_PROJECT"
+    DELETE_PROJECT = "PERMISSION_TYPE_DELETE_PROJECT"
+    ASSIGN_ROLES = "PERMISSION_TYPE_ASSIGN_ROLES"
+    VIEW_MODEL_REGISTRY = "PERMISSION_TYPE_VIEW_MODEL_REGISTRY"
+    EDIT_MODEL_REGISTRY = "PERMISSION_TYPE_EDIT_MODEL_REGISTRY"
+    CREATE_MODEL_REGISTRY = "PERMISSION_TYPE_CREATE_MODEL_REGISTRY"
+    DELETE_MODEL_REGISTRY = "PERMISSION_TYPE_DELETE_MODEL_REGISTRY"
+    VIEW_MASTER_LOGS = "PERMISSION_TYPE_VIEW_MASTER_LOGS"
+    VIEW_CLUSTER_USAGE = "PERMISSION_TYPE_VIEW_CLUSTER_USAGE"
+    UPDATE_AGENTS = "PERMISSION_TYPE_UPDATE_AGENTS"
+    UPDATE_ROLES = "PERMISSION_TYPE_UPDATE_ROLES"
+    EDIT_WEBHOOKS = "PERMISSION_TYPE_EDIT_WEBHOOKS"
 
 class v1PolymorphicFilter:
     doubleRange: "typing.Optional[v1DoubleFieldFilter]" = None
     integerRange: "typing.Optional[v1Int32FieldFilter]" = None
     name: "typing.Optional[str]" = None
     timeRange: "typing.Optional[v1TimestampFieldFilter]" = None
 
@@ -8486,14 +8684,64 @@
             out["description"] = self.description
         if not omit_unset or "lastExperimentStartedAt" in vars(self):
             out["lastExperimentStartedAt"] = self.lastExperimentStartedAt
         if not omit_unset or "workspaceName" in vars(self):
             out["workspaceName"] = self.workspaceName
         return out
 
+class v1ProxyPortConfig:
+    port: "typing.Optional[int]" = None
+    proxyTcp: "typing.Optional[bool]" = None
+    serviceId: "typing.Optional[str]" = None
+    unauthenticated: "typing.Optional[bool]" = None
+
+    def __init__(
+        self,
+        *,
+        port: "typing.Union[int, None, Unset]" = _unset,
+        proxyTcp: "typing.Union[bool, None, Unset]" = _unset,
+        serviceId: "typing.Union[str, None, Unset]" = _unset,
+        unauthenticated: "typing.Union[bool, None, Unset]" = _unset,
+    ):
+        if not isinstance(port, Unset):
+            self.port = port
+        if not isinstance(proxyTcp, Unset):
+            self.proxyTcp = proxyTcp
+        if not isinstance(serviceId, Unset):
+            self.serviceId = serviceId
+        if not isinstance(unauthenticated, Unset):
+            self.unauthenticated = unauthenticated
+
+    @classmethod
+    def from_json(cls, obj: Json) -> "v1ProxyPortConfig":
+        kwargs: "typing.Dict[str, typing.Any]" = {
+        }
+        if "port" in obj:
+            kwargs["port"] = obj["port"]
+        if "proxyTcp" in obj:
+            kwargs["proxyTcp"] = obj["proxyTcp"]
+        if "serviceId" in obj:
+            kwargs["serviceId"] = obj["serviceId"]
+        if "unauthenticated" in obj:
+            kwargs["unauthenticated"] = obj["unauthenticated"]
+        return cls(**kwargs)
+
+    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
+        out: "typing.Dict[str, typing.Any]" = {
+        }
+        if not omit_unset or "port" in vars(self):
+            out["port"] = self.port
+        if not omit_unset or "proxyTcp" in vars(self):
+            out["proxyTcp"] = self.proxyTcp
+        if not omit_unset or "serviceId" in vars(self):
+            out["serviceId"] = self.serviceId
+        if not omit_unset or "unauthenticated" in vars(self):
+            out["unauthenticated"] = self.unauthenticated
+        return out
+
 class v1PutProjectNotesRequest:
 
     def __init__(
         self,
         *,
         notes: "typing.Sequence[v1Note]",
         projectId: int,
@@ -8883,17 +9131,17 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "resourceEntries": [x.to_json(omit_unset) for x in self.resourceEntries],
         }
         return out
 
 class v1ResourceAllocationAggregationPeriod(enum.Enum):
-    RESOURCE_ALLOCATION_AGGREGATION_PERIOD_UNSPECIFIED = "RESOURCE_ALLOCATION_AGGREGATION_PERIOD_UNSPECIFIED"
-    RESOURCE_ALLOCATION_AGGREGATION_PERIOD_DAILY = "RESOURCE_ALLOCATION_AGGREGATION_PERIOD_DAILY"
-    RESOURCE_ALLOCATION_AGGREGATION_PERIOD_MONTHLY = "RESOURCE_ALLOCATION_AGGREGATION_PERIOD_MONTHLY"
+    UNSPECIFIED = "RESOURCE_ALLOCATION_AGGREGATION_PERIOD_UNSPECIFIED"
+    DAILY = "RESOURCE_ALLOCATION_AGGREGATION_PERIOD_DAILY"
+    MONTHLY = "RESOURCE_ALLOCATION_AGGREGATION_PERIOD_MONTHLY"
 
 class v1ResourceAllocationRawEntry:
     endTime: "typing.Optional[str]" = None
     experimentId: "typing.Optional[int]" = None
     kind: "typing.Optional[str]" = None
     labels: "typing.Optional[typing.Sequence[str]]" = None
     seconds: "typing.Optional[float]" = None
@@ -9454,19 +9702,195 @@
             "preemption": self.preemption,
         }
         if not omit_unset or "k8Priorities" in vars(self):
             out["k8Priorities"] = None if self.k8Priorities is None else [x.to_json(omit_unset) for x in self.k8Priorities]
         return out
 
 class v1ResourcePoolType(enum.Enum):
-    RESOURCE_POOL_TYPE_UNSPECIFIED = "RESOURCE_POOL_TYPE_UNSPECIFIED"
-    RESOURCE_POOL_TYPE_AWS = "RESOURCE_POOL_TYPE_AWS"
-    RESOURCE_POOL_TYPE_GCP = "RESOURCE_POOL_TYPE_GCP"
-    RESOURCE_POOL_TYPE_STATIC = "RESOURCE_POOL_TYPE_STATIC"
-    RESOURCE_POOL_TYPE_K8S = "RESOURCE_POOL_TYPE_K8S"
+    UNSPECIFIED = "RESOURCE_POOL_TYPE_UNSPECIFIED"
+    AWS = "RESOURCE_POOL_TYPE_AWS"
+    GCP = "RESOURCE_POOL_TYPE_GCP"
+    STATIC = "RESOURCE_POOL_TYPE_STATIC"
+    K8S = "RESOURCE_POOL_TYPE_K8S"
+
+class v1ResourcesFailure:
+    errMsg: "typing.Optional[str]" = None
+    exitCode: "typing.Optional[int]" = None
+    failureType: "typing.Optional[v1FailureType]" = None
+
+    def __init__(
+        self,
+        *,
+        errMsg: "typing.Union[str, None, Unset]" = _unset,
+        exitCode: "typing.Union[int, None, Unset]" = _unset,
+        failureType: "typing.Union[v1FailureType, None, Unset]" = _unset,
+    ):
+        if not isinstance(errMsg, Unset):
+            self.errMsg = errMsg
+        if not isinstance(exitCode, Unset):
+            self.exitCode = exitCode
+        if not isinstance(failureType, Unset):
+            self.failureType = failureType
+
+    @classmethod
+    def from_json(cls, obj: Json) -> "v1ResourcesFailure":
+        kwargs: "typing.Dict[str, typing.Any]" = {
+        }
+        if "errMsg" in obj:
+            kwargs["errMsg"] = obj["errMsg"]
+        if "exitCode" in obj:
+            kwargs["exitCode"] = obj["exitCode"]
+        if "failureType" in obj:
+            kwargs["failureType"] = v1FailureType(obj["failureType"]) if obj["failureType"] is not None else None
+        return cls(**kwargs)
+
+    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
+        out: "typing.Dict[str, typing.Any]" = {
+        }
+        if not omit_unset or "errMsg" in vars(self):
+            out["errMsg"] = self.errMsg
+        if not omit_unset or "exitCode" in vars(self):
+            out["exitCode"] = self.exitCode
+        if not omit_unset or "failureType" in vars(self):
+            out["failureType"] = None if self.failureType is None else self.failureType.value
+        return out
+
+class v1ResourcesStarted:
+    addresses: "typing.Optional[typing.Sequence[v1Address]]" = None
+    nativeResourcesId: "typing.Optional[str]" = None
+
+    def __init__(
+        self,
+        *,
+        addresses: "typing.Union[typing.Sequence[v1Address], None, Unset]" = _unset,
+        nativeResourcesId: "typing.Union[str, None, Unset]" = _unset,
+    ):
+        if not isinstance(addresses, Unset):
+            self.addresses = addresses
+        if not isinstance(nativeResourcesId, Unset):
+            self.nativeResourcesId = nativeResourcesId
+
+    @classmethod
+    def from_json(cls, obj: Json) -> "v1ResourcesStarted":
+        kwargs: "typing.Dict[str, typing.Any]" = {
+        }
+        if "addresses" in obj:
+            kwargs["addresses"] = [v1Address.from_json(x) for x in obj["addresses"]] if obj["addresses"] is not None else None
+        if "nativeResourcesId" in obj:
+            kwargs["nativeResourcesId"] = obj["nativeResourcesId"]
+        return cls(**kwargs)
+
+    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
+        out: "typing.Dict[str, typing.Any]" = {
+        }
+        if not omit_unset or "addresses" in vars(self):
+            out["addresses"] = None if self.addresses is None else [x.to_json(omit_unset) for x in self.addresses]
+        if not omit_unset or "nativeResourcesId" in vars(self):
+            out["nativeResourcesId"] = self.nativeResourcesId
+        return out
+
+class v1ResourcesStopped:
+    failure: "typing.Optional[v1ResourcesFailure]" = None
+
+    def __init__(
+        self,
+        *,
+        failure: "typing.Union[v1ResourcesFailure, None, Unset]" = _unset,
+    ):
+        if not isinstance(failure, Unset):
+            self.failure = failure
+
+    @classmethod
+    def from_json(cls, obj: Json) -> "v1ResourcesStopped":
+        kwargs: "typing.Dict[str, typing.Any]" = {
+        }
+        if "failure" in obj:
+            kwargs["failure"] = v1ResourcesFailure.from_json(obj["failure"]) if obj["failure"] is not None else None
+        return cls(**kwargs)
+
+    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
+        out: "typing.Dict[str, typing.Any]" = {
+        }
+        if not omit_unset or "failure" in vars(self):
+            out["failure"] = None if self.failure is None else self.failure.to_json(omit_unset)
+        return out
+
+class v1ResourcesSummary:
+    agentDevices: "typing.Optional[typing.Dict[str, ResourcesSummaryDevices]]" = None
+    allocationId: "typing.Optional[str]" = None
+    containerId: "typing.Optional[str]" = None
+    exited: "typing.Optional[v1ResourcesStopped]" = None
+    resourcesId: "typing.Optional[str]" = None
+    resourcesType: "typing.Optional[str]" = None
+    started: "typing.Optional[v1ResourcesStarted]" = None
+
+    def __init__(
+        self,
+        *,
+        agentDevices: "typing.Union[typing.Dict[str, ResourcesSummaryDevices], None, Unset]" = _unset,
+        allocationId: "typing.Union[str, None, Unset]" = _unset,
+        containerId: "typing.Union[str, None, Unset]" = _unset,
+        exited: "typing.Union[v1ResourcesStopped, None, Unset]" = _unset,
+        resourcesId: "typing.Union[str, None, Unset]" = _unset,
+        resourcesType: "typing.Union[str, None, Unset]" = _unset,
+        started: "typing.Union[v1ResourcesStarted, None, Unset]" = _unset,
+    ):
+        if not isinstance(agentDevices, Unset):
+            self.agentDevices = agentDevices
+        if not isinstance(allocationId, Unset):
+            self.allocationId = allocationId
+        if not isinstance(containerId, Unset):
+            self.containerId = containerId
+        if not isinstance(exited, Unset):
+            self.exited = exited
+        if not isinstance(resourcesId, Unset):
+            self.resourcesId = resourcesId
+        if not isinstance(resourcesType, Unset):
+            self.resourcesType = resourcesType
+        if not isinstance(started, Unset):
+            self.started = started
+
+    @classmethod
+    def from_json(cls, obj: Json) -> "v1ResourcesSummary":
+        kwargs: "typing.Dict[str, typing.Any]" = {
+        }
+        if "agentDevices" in obj:
+            kwargs["agentDevices"] = {k: ResourcesSummaryDevices.from_json(v) for k, v in obj["agentDevices"].items()} if obj["agentDevices"] is not None else None
+        if "allocationId" in obj:
+            kwargs["allocationId"] = obj["allocationId"]
+        if "containerId" in obj:
+            kwargs["containerId"] = obj["containerId"]
+        if "exited" in obj:
+            kwargs["exited"] = v1ResourcesStopped.from_json(obj["exited"]) if obj["exited"] is not None else None
+        if "resourcesId" in obj:
+            kwargs["resourcesId"] = obj["resourcesId"]
+        if "resourcesType" in obj:
+            kwargs["resourcesType"] = obj["resourcesType"]
+        if "started" in obj:
+            kwargs["started"] = v1ResourcesStarted.from_json(obj["started"]) if obj["started"] is not None else None
+        return cls(**kwargs)
+
+    def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
+        out: "typing.Dict[str, typing.Any]" = {
+        }
+        if not omit_unset or "agentDevices" in vars(self):
+            out["agentDevices"] = None if self.agentDevices is None else {k: v.to_json(omit_unset) for k, v in self.agentDevices.items()}
+        if not omit_unset or "allocationId" in vars(self):
+            out["allocationId"] = self.allocationId
+        if not omit_unset or "containerId" in vars(self):
+            out["containerId"] = self.containerId
+        if not omit_unset or "exited" in vars(self):
+            out["exited"] = None if self.exited is None else self.exited.to_json(omit_unset)
+        if not omit_unset or "resourcesId" in vars(self):
+            out["resourcesId"] = self.resourcesId
+        if not omit_unset or "resourcesType" in vars(self):
+            out["resourcesType"] = self.resourcesType
+        if not omit_unset or "started" in vars(self):
+            out["started"] = None if self.started is None else self.started.to_json(omit_unset)
+        return out
 
 class v1Role:
     name: "typing.Optional[str]" = None
     permissions: "typing.Optional[typing.Sequence[v1Permission]]" = None
     scopeTypeMask: "typing.Optional[v1ScopeTypeMask]" = None
 
     def __init__(
@@ -9659,17 +10083,17 @@
         if not omit_unset or "length" in vars(self):
             out["length"] = self.length
         if not omit_unset or "type" in vars(self):
             out["type"] = None if self.type is None else self.type.value
         return out
 
 class v1RunnableType(enum.Enum):
-    RUNNABLE_TYPE_UNSPECIFIED = "RUNNABLE_TYPE_UNSPECIFIED"
-    RUNNABLE_TYPE_TRAIN = "RUNNABLE_TYPE_TRAIN"
-    RUNNABLE_TYPE_VALIDATE = "RUNNABLE_TYPE_VALIDATE"
+    UNSPECIFIED = "RUNNABLE_TYPE_UNSPECIFIED"
+    TRAIN = "RUNNABLE_TYPE_TRAIN"
+    VALIDATE = "RUNNABLE_TYPE_VALIDATE"
 
 class v1SSOProvider:
 
     def __init__(
         self,
         *,
         name: str,
@@ -9690,26 +10114,26 @@
         out: "typing.Dict[str, typing.Any]" = {
             "name": self.name,
             "ssoUrl": self.ssoUrl,
         }
         return out
 
 class v1Scale(enum.Enum):
-    SCALE_UNSPECIFIED = "SCALE_UNSPECIFIED"
-    SCALE_LINEAR = "SCALE_LINEAR"
-    SCALE_LOG = "SCALE_LOG"
+    UNSPECIFIED = "SCALE_UNSPECIFIED"
+    LINEAR = "SCALE_LINEAR"
+    LOG = "SCALE_LOG"
 
 class v1SchedulerType(enum.Enum):
-    SCHEDULER_TYPE_UNSPECIFIED = "SCHEDULER_TYPE_UNSPECIFIED"
-    SCHEDULER_TYPE_PRIORITY = "SCHEDULER_TYPE_PRIORITY"
-    SCHEDULER_TYPE_FAIR_SHARE = "SCHEDULER_TYPE_FAIR_SHARE"
-    SCHEDULER_TYPE_ROUND_ROBIN = "SCHEDULER_TYPE_ROUND_ROBIN"
-    SCHEDULER_TYPE_KUBERNETES = "SCHEDULER_TYPE_KUBERNETES"
-    SCHEDULER_TYPE_SLURM = "SCHEDULER_TYPE_SLURM"
-    SCHEDULER_TYPE_PBS = "SCHEDULER_TYPE_PBS"
+    UNSPECIFIED = "SCHEDULER_TYPE_UNSPECIFIED"
+    PRIORITY = "SCHEDULER_TYPE_PRIORITY"
+    FAIR_SHARE = "SCHEDULER_TYPE_FAIR_SHARE"
+    ROUND_ROBIN = "SCHEDULER_TYPE_ROUND_ROBIN"
+    KUBERNETES = "SCHEDULER_TYPE_KUBERNETES"
+    SLURM = "SCHEDULER_TYPE_SLURM"
+    PBS = "SCHEDULER_TYPE_PBS"
 
 class v1ScopeTypeMask:
     cluster: "typing.Optional[bool]" = None
     workspace: "typing.Optional[bool]" = None
 
     def __init__(
         self,
@@ -11020,17 +11444,17 @@
     def to_json(self, omit_unset: bool = False) -> typing.Dict[str, typing.Any]:
         out: "typing.Dict[str, typing.Any]" = {
             "reason": self.reason.value,
         }
         return out
 
 class v1TrialEarlyExitExitedReason(enum.Enum):
-    EXITED_REASON_UNSPECIFIED = "EXITED_REASON_UNSPECIFIED"
-    EXITED_REASON_INVALID_HP = "EXITED_REASON_INVALID_HP"
-    EXITED_REASON_INIT_INVALID_HP = "EXITED_REASON_INIT_INVALID_HP"
+    UNSPECIFIED = "EXITED_REASON_UNSPECIFIED"
+    INVALID_HP = "EXITED_REASON_INVALID_HP"
+    INIT_INVALID_HP = "EXITED_REASON_INIT_INVALID_HP"
 
 class v1TrialExitedEarly:
 
     def __init__(
         self,
         *,
         exitedReason: "v1TrialExitedEarlyExitedReason",
@@ -11051,18 +11475,18 @@
         out: "typing.Dict[str, typing.Any]" = {
             "exitedReason": self.exitedReason.value,
             "requestId": self.requestId,
         }
         return out
 
 class v1TrialExitedEarlyExitedReason(enum.Enum):
-    EXITED_REASON_UNSPECIFIED = "EXITED_REASON_UNSPECIFIED"
-    EXITED_REASON_INVALID_HP = "EXITED_REASON_INVALID_HP"
-    EXITED_REASON_USER_REQUESTED_STOP = "EXITED_REASON_USER_REQUESTED_STOP"
-    EXITED_REASON_USER_CANCELED = "EXITED_REASON_USER_CANCELED"
+    UNSPECIFIED = "EXITED_REASON_UNSPECIFIED"
+    INVALID_HP = "EXITED_REASON_INVALID_HP"
+    USER_REQUESTED_STOP = "EXITED_REASON_USER_REQUESTED_STOP"
+    USER_CANCELED = "EXITED_REASON_USER_CANCELED"
 
 class v1TrialFilters:
     endTime: "typing.Optional[v1TimestampFieldFilter]" = None
     experimentIds: "typing.Optional[typing.Sequence[int]]" = None
     hparams: "typing.Optional[typing.Sequence[v1ColumnFilter]]" = None
     projectIds: "typing.Optional[typing.Sequence[int]]" = None
     rankWithinExp: "typing.Optional[TrialFiltersRankWithinExp]" = None
@@ -11871,17 +12295,17 @@
         if not omit_unset or "triggerType" in vars(self):
             out["triggerType"] = None if self.triggerType is None else self.triggerType.value
         if not omit_unset or "webhookId" in vars(self):
             out["webhookId"] = self.webhookId
         return out
 
 class v1TriggerType(enum.Enum):
-    TRIGGER_TYPE_UNSPECIFIED = "TRIGGER_TYPE_UNSPECIFIED"
-    TRIGGER_TYPE_EXPERIMENT_STATE_CHANGE = "TRIGGER_TYPE_EXPERIMENT_STATE_CHANGE"
-    TRIGGER_TYPE_METRIC_THRESHOLD_EXCEEDED = "TRIGGER_TYPE_METRIC_THRESHOLD_EXCEEDED"
+    UNSPECIFIED = "TRIGGER_TYPE_UNSPECIFIED"
+    EXPERIMENT_STATE_CHANGE = "TRIGGER_TYPE_EXPERIMENT_STATE_CHANGE"
+    METRIC_THRESHOLD_EXCEEDED = "TRIGGER_TYPE_METRIC_THRESHOLD_EXCEEDED"
 
 class v1UnarchiveExperimentsRequest:
     filters: "typing.Optional[v1BulkExperimentFilters]" = None
 
     def __init__(
         self,
         *,
@@ -12352,17 +12776,17 @@
         if not omit_unset or "id" in vars(self):
             out["id"] = self.id
         if not omit_unset or "triggers" in vars(self):
             out["triggers"] = None if self.triggers is None else [x.to_json(omit_unset) for x in self.triggers]
         return out
 
 class v1WebhookType(enum.Enum):
-    WEBHOOK_TYPE_UNSPECIFIED = "WEBHOOK_TYPE_UNSPECIFIED"
-    WEBHOOK_TYPE_DEFAULT = "WEBHOOK_TYPE_DEFAULT"
-    WEBHOOK_TYPE_SLACK = "WEBHOOK_TYPE_SLACK"
+    UNSPECIFIED = "WEBHOOK_TYPE_UNSPECIFIED"
+    DEFAULT = "WEBHOOK_TYPE_DEFAULT"
+    SLACK = "WEBHOOK_TYPE_SLACK"
 
 class v1WorkloadContainer:
     checkpoint: "typing.Optional[v1CheckpointWorkload]" = None
     training: "typing.Optional[v1MetricsWorkload]" = None
     validation: "typing.Optional[v1MetricsWorkload]" = None
 
     def __init__(
@@ -12485,24 +12909,18 @@
         if not omit_unset or "checkpointStorageConfig" in vars(self):
             out["checkpointStorageConfig"] = self.checkpointStorageConfig
         if not omit_unset or "pinnedAt" in vars(self):
             out["pinnedAt"] = self.pinnedAt
         return out
 
 class v1WorkspaceState(enum.Enum):
-    WORKSPACE_STATE_UNSPECIFIED = "WORKSPACE_STATE_UNSPECIFIED"
-    WORKSPACE_STATE_DELETING = "WORKSPACE_STATE_DELETING"
-    WORKSPACE_STATE_DELETE_FAILED = "WORKSPACE_STATE_DELETE_FAILED"
-    WORKSPACE_STATE_DELETED = "WORKSPACE_STATE_DELETED"
-
-class v1XAxis(enum.Enum):
-    X_AXIS_UNSPECIFIED = "X_AXIS_UNSPECIFIED"
-    X_AXIS_BATCH = "X_AXIS_BATCH"
-    X_AXIS_TIME = "X_AXIS_TIME"
-    X_AXIS_EPOCH = "X_AXIS_EPOCH"
+    UNSPECIFIED = "WORKSPACE_STATE_UNSPECIFIED"
+    DELETING = "WORKSPACE_STATE_DELETING"
+    DELETE_FAILED = "WORKSPACE_STATE_DELETE_FAILED"
+    DELETED = "WORKSPACE_STATE_DELETED"
 
 def post_AckAllocationPreemptionSignal(
     session: "api.Session",
     *,
     allocationId: str,
     body: "v1AckAllocationPreemptionSignalRequest",
 ) -> None:
@@ -12892,15 +13310,14 @@
     timeSeriesFilter_integerRange_notIn: "typing.Optional[typing.Sequence[int]]" = None,
     timeSeriesFilter_name: "typing.Optional[str]" = None,
     timeSeriesFilter_timeRange_gt: "typing.Optional[str]" = None,
     timeSeriesFilter_timeRange_gte: "typing.Optional[str]" = None,
     timeSeriesFilter_timeRange_lt: "typing.Optional[str]" = None,
     timeSeriesFilter_timeRange_lte: "typing.Optional[str]" = None,
     trialIds: "typing.Optional[typing.Sequence[int]]" = None,
-    xAxis: "typing.Optional[v1XAxis]" = None,
 ) -> "v1CompareTrialsResponse":
     _params = {
         "endBatches": endBatches,
         "maxDatapoints": maxDatapoints,
         "metricIds": metricIds,
         "metricNames": metricNames,
         "metricType": metricType.value if metricType is not None else None,
@@ -12918,19 +13335,18 @@
         "timeSeriesFilter.integerRange.notIn": timeSeriesFilter_integerRange_notIn,
         "timeSeriesFilter.name": timeSeriesFilter_name,
         "timeSeriesFilter.timeRange.gt": timeSeriesFilter_timeRange_gt,
         "timeSeriesFilter.timeRange.gte": timeSeriesFilter_timeRange_gte,
         "timeSeriesFilter.timeRange.lt": timeSeriesFilter_timeRange_lt,
         "timeSeriesFilter.timeRange.lte": timeSeriesFilter_timeRange_lte,
         "trialIds": trialIds,
-        "xAxis": xAxis.value if xAxis is not None else None,
     }
     _resp = session._do_request(
         method="GET",
-        path="/api/v1/trials/compare",
+        path="/api/v1/trials/time-series",
         params=_params,
         json=None,
         data=None,
         headers=None,
         timeout=None,
         stream=False,
     )
@@ -12955,34 +13371,14 @@
         timeout=None,
         stream=False,
     )
     if _resp.status_code == 200:
         return
     raise APIHttpError("post_CompleteTrialSearcherValidation", _resp)
 
-def post_ComputeHPImportance(
-    session: "api.Session",
-    *,
-    experimentId: int,
-) -> None:
-    _params = None
-    _resp = session._do_request(
-        method="POST",
-        path=f"/api/v1/experiments/{experimentId}/hyperparameter-importance",
-        params=_params,
-        json=None,
-        data=None,
-        headers=None,
-        timeout=None,
-        stream=False,
-    )
-    if _resp.status_code == 200:
-        return
-    raise APIHttpError("post_ComputeHPImportance", _resp)
-
 def post_CreateExperiment(
     session: "api.Session",
     *,
     body: "v1CreateExperimentRequest",
 ) -> "v1CreateExperimentResponse":
     _params = None
     _resp = session._do_request(
@@ -13093,14 +13489,34 @@
         timeout=None,
         stream=False,
     )
     if _resp.status_code == 200:
         return
     raise APIHttpError("delete_DeleteExperiment", _resp)
 
+def delete_DeleteExperiments(
+    session: "api.Session",
+    *,
+    body: "v1DeleteExperimentsRequest",
+) -> "v1DeleteExperimentsResponse":
+    _params = None
+    _resp = session._do_request(
+        method="DELETE",
+        path="/api/v1/experiments/delete",
+        params=_params,
+        json=body.to_json(True),
+        data=None,
+        headers=None,
+        timeout=None,
+        stream=False,
+    )
+    if _resp.status_code == 200:
+        return v1DeleteExperimentsResponse.from_json(_resp.json())
+    raise APIHttpError("delete_DeleteExperiments", _resp)
+
 def delete_DeleteGroup(
     session: "api.Session",
     *,
     groupId: int,
 ) -> None:
     _params = None
     _resp = session._do_request(
@@ -13767,48 +14183,14 @@
         timeout=None,
         stream=False,
     )
     if _resp.status_code == 200:
         return v1GetGroupsAndUsersAssignedToWorkspaceResponse.from_json(_resp.json())
     raise APIHttpError("get_GetGroupsAndUsersAssignedToWorkspace", _resp)
 
-def get_GetHPImportance(
-    session: "api.Session",
-    *,
-    experimentId: int,
-    periodSeconds: "typing.Optional[int]" = None,
-) -> "typing.Iterable[v1GetHPImportanceResponse]":
-    _params = {
-        "periodSeconds": periodSeconds,
-    }
-    _resp = session._do_request(
-        method="GET",
-        path=f"/api/v1/experiments/{experimentId}/hyperparameter-importance",
-        params=_params,
-        json=None,
-        data=None,
-        headers=None,
-        timeout=None,
-        stream=True,
-    )
-    if _resp.status_code == 200:
-        try:
-            for _line in _resp.iter_lines(chunk_size=1024 * 1024):
-                _j = json.loads(_line)
-                if "error" in _j:
-                    raise APIHttpStreamError(
-                        "get_GetHPImportance",
-                        runtimeStreamError.from_json(_j["error"])
-                )
-                yield v1GetHPImportanceResponse.from_json(_j["result"])
-        except requests.exceptions.ChunkedEncodingError:
-            raise APIHttpStreamError("get_GetHPImportance", runtimeStreamError(message="ChunkedEncodingError"))
-        return
-    raise APIHttpError("get_GetHPImportance", _resp)
-
 def get_GetJobQueueStats(
     session: "api.Session",
     *,
     resourcePools: "typing.Optional[typing.Sequence[str]]" = None,
 ) -> "v1GetJobQueueStatsResponse":
     _params = {
         "resourcePools": resourcePools,
@@ -14459,14 +14841,32 @@
         timeout=None,
         stream=False,
     )
     if _resp.status_code == 200:
         return v1GetTaskResponse.from_json(_resp.json())
     raise APIHttpError("get_GetTask", _resp)
 
+def get_GetTasks(
+    session: "api.Session",
+) -> "v1GetTasksResponse":
+    _params = None
+    _resp = session._do_request(
+        method="GET",
+        path="/api/v1/tasks",
+        params=_params,
+        json=None,
+        data=None,
+        headers=None,
+        timeout=None,
+        stream=False,
+    )
+    if _resp.status_code == 200:
+        return v1GetTasksResponse.from_json(_resp.json())
+    raise APIHttpError("get_GetTasks", _resp)
+
 def get_GetTelemetry(
     session: "api.Session",
 ) -> "v1GetTelemetryResponse":
     _params = None
     _resp = session._do_request(
         method="GET",
         path="/api/v1/master/telemetry",
@@ -16319,19 +16719,21 @@
 
 def get_SearchExperiments(
     session: "api.Session",
     *,
     limit: "typing.Optional[int]" = None,
     offset: "typing.Optional[int]" = None,
     projectId: "typing.Optional[int]" = None,
+    sort: "typing.Optional[str]" = None,
 ) -> "v1SearchExperimentsResponse":
     _params = {
         "limit": limit,
         "offset": offset,
         "projectId": projectId,
+        "sort": sort,
     }
     _resp = session._do_request(
         method="GET",
         path="/api/v1/experiments-search",
         params=_params,
         json=None,
         data=None,
```

## determined/common/api/logs.py

```diff
@@ -1,45 +1,26 @@
-from typing import Iterable, List, Optional
-
-from termcolor import colored
+import json
+from typing import Iterable, List, Optional, Union
 
 from determined.common import api
 from determined.common.api import bindings
 
 
-def pprint_task_logs(task_id: str, logs: Iterable[bindings.v1TaskLogsResponse]) -> None:
-    try:
-        for log in logs:
-            print(log.message, end="")
-    except KeyboardInterrupt:
-        pass
-    finally:
-        print(
-            colored(
-                "Task log stream ended. To reopen log stream, run: "
-                "det task logs -f {}".format(task_id),
-                "green",
-            )
-        )
-
-
-def pprint_trial_logs(trial_id: int, logs: Iterable[bindings.v1TrialLogsResponse]) -> None:
-    try:
-        for log in logs:
-            print(log.message, end="")
-    except KeyboardInterrupt:
-        pass
-    finally:
-        print(
-            colored(
-                "Trial log stream ended. To reopen log stream, run: "
-                "det trial logs -f {}".format(trial_id),
-                "green",
-            )
-        )
+def pprint_logs(
+    logs: Iterable[Union[bindings.v1TaskLogsResponse, bindings.v1TrialLogsResponse]]
+) -> None:
+    for log in logs:
+        print(log.message, end="")
+
+
+def print_json_logs(
+    logs: Iterable[Union[bindings.v1TaskLogsResponse, bindings.v1TrialLogsResponse]]
+) -> None:
+    for log in logs:
+        print(json.dumps(log.to_json(), indent=4))
 
 
 def trial_logs(
     session: api.Session,
     trial_id: int,
     head: Optional[int] = None,
     tail: Optional[int] = None,
@@ -59,15 +40,15 @@
         session,
         trialId=trial_id,
         agentIds=agent_ids,
         containerIds=container_ids,
         follow=follow,
         levels=levels_at_or_above(min_level),
         limit=head or tail,
-        orderBy=tail is not None and bindings.v1OrderBy.ORDER_BY_DESC or None,
+        orderBy=tail is not None and bindings.v1OrderBy.DESC or None,
         rankIds=rank_ids,
         searchText=None,
         sources=sources,
         stdtypes=stdtypes,
         timestampBefore=timestamp_before,
         timestampAfter=timestamp_after,
     )
@@ -97,15 +78,15 @@
         taskId=task_id,
         agentIds=agent_ids,
         allocationIds=allocation_ids,
         containerIds=container_ids,
         follow=follow,
         levels=levels_at_or_above(min_level),
         limit=head or tail,
-        orderBy=tail is not None and bindings.v1OrderBy.ORDER_BY_DESC or None,
+        orderBy=tail is not None and bindings.v1OrderBy.DESC or None,
         rankIds=rank_ids,
         searchText=None,
         sources=sources,
         stdtypes=stdtypes,
         timestampBefore=timestamp_before,
         timestampAfter=timestamp_after,
     )
```

## determined/common/experimental/determined.py

```diff
@@ -305,14 +305,15 @@
             name: If this parameter is set, models will be filtered to only
                 include models with names matching this parameter.
             description: If this parameter is set, models will be filtered to
                 only include models with descriptions matching this parameter.
             model_id: If this paramter is set, models will be filtered to
                 only include the model with this unique numeric id.
         """
+
         # TODO: more parameters?
         #   - archived
         #   - labels
         #   - userIds
         #   - users
         def get_with_offset(offset: int) -> bindings.v1GetModelsResponse:
             return bindings.get_GetModels(
```

## determined/common/experimental/experiment.py

```diff
@@ -8,31 +8,31 @@
 from determined.common import api
 from determined.common.api import bindings
 from determined.common.experimental import checkpoint, trial
 
 
 # Wrap the autogenerated bindings in something a little more ergonomic.
 class ExperimentState(enum.Enum):
-    UNSPECIFIED = bindings.experimentv1State.STATE_UNSPECIFIED.value
-    ACTIVE = bindings.experimentv1State.STATE_ACTIVE.value
-    PAUSED = bindings.experimentv1State.STATE_PAUSED.value
-    STOPPING_COMPLETED = bindings.experimentv1State.STATE_STOPPING_COMPLETED.value
-    STOPPING_CANCELED = bindings.experimentv1State.STATE_STOPPING_CANCELED.value
-    STOPPING_ERROR = bindings.experimentv1State.STATE_STOPPING_ERROR.value
-    COMPLETED = bindings.experimentv1State.STATE_COMPLETED.value
-    CANCELED = bindings.experimentv1State.STATE_CANCELED.value
-    ERROR = bindings.experimentv1State.STATE_ERROR.value
-    DELETED = bindings.experimentv1State.STATE_DELETED.value
-    DELETING = bindings.experimentv1State.STATE_DELETING.value
-    DELETE_FAILED = bindings.experimentv1State.STATE_DELETE_FAILED.value
-    STOPPING_KILLED = bindings.experimentv1State.STATE_STOPPING_KILLED.value
-    QUEUED = bindings.experimentv1State.STATE_QUEUED.value
-    PULLING = bindings.experimentv1State.STATE_PULLING.value
-    STARTING = bindings.experimentv1State.STATE_STARTING.value
-    RUNNING = bindings.experimentv1State.STATE_RUNNING.value
+    UNSPECIFIED = bindings.experimentv1State.UNSPECIFIED.value
+    ACTIVE = bindings.experimentv1State.ACTIVE.value
+    PAUSED = bindings.experimentv1State.PAUSED.value
+    STOPPING_COMPLETED = bindings.experimentv1State.STOPPING_COMPLETED.value
+    STOPPING_CANCELED = bindings.experimentv1State.STOPPING_CANCELED.value
+    STOPPING_ERROR = bindings.experimentv1State.STOPPING_ERROR.value
+    COMPLETED = bindings.experimentv1State.COMPLETED.value
+    CANCELED = bindings.experimentv1State.CANCELED.value
+    ERROR = bindings.experimentv1State.ERROR.value
+    DELETED = bindings.experimentv1State.DELETED.value
+    DELETING = bindings.experimentv1State.DELETING.value
+    DELETE_FAILED = bindings.experimentv1State.DELETE_FAILED.value
+    STOPPING_KILLED = bindings.experimentv1State.STOPPING_KILLED.value
+    QUEUED = bindings.experimentv1State.QUEUED.value
+    PULLING = bindings.experimentv1State.PULLING.value
+    STARTING = bindings.experimentv1State.STARTING.value
+    RUNNING = bindings.experimentv1State.RUNNING.value
 
     def _to_bindings(self) -> bindings.experimentv1State:
         return bindings.experimentv1State(self.value)
 
 
 class ExperimentReference:
     """
@@ -147,21 +147,21 @@
         )
         sess = self._session.with_retry(retry)
 
         elapsed_time = 0.0
         while True:
             exp = bindings.get_GetExperiment(sess, experimentId=self._id).experiment
             if exp.state in (
-                bindings.experimentv1State.STATE_COMPLETED,
-                bindings.experimentv1State.STATE_CANCELED,
-                bindings.experimentv1State.STATE_DELETED,
-                bindings.experimentv1State.STATE_ERROR,
+                bindings.experimentv1State.COMPLETED,
+                bindings.experimentv1State.CANCELED,
+                bindings.experimentv1State.DELETED,
+                bindings.experimentv1State.ERROR,
             ):
                 return ExperimentState(exp.state.value)
-            elif exp.state == bindings.experimentv1State.STATE_PAUSED:
+            elif exp.state == bindings.experimentv1State.PAUSED:
                 raise ValueError(
                     f"Experiment {self.id} is in paused state. Make sure the experiment is active."
                 )
             else:
                 # ACTIVE, STOPPING_COMPLETED, etc.
                 time.sleep(interval)
                 elapsed_time += interval
@@ -229,15 +229,15 @@
         """
 
         def get_with_offset(offset: int) -> bindings.v1GetExperimentCheckpointsResponse:
             return bindings.get_GetExperimentCheckpoints(
                 self._session,
                 id=self._id,
                 offset=offset,
-                states=[bindings.checkpointv1State.STATE_COMPLETED],
+                states=[bindings.checkpointv1State.COMPLETED],
             )
 
         resps = api.read_paginated(get_with_offset)
 
         checkpoints = [
             checkpoint.Checkpoint._from_bindings(c, self._session)
             for r in resps
```

## determined/common/experimental/model.py

```diff
@@ -99,36 +99,36 @@
         NAME
         DESCRIPTION
         CREATION_TIME
         LAST_UPDATED_TIME
         WORKSPACE
     """
 
-    UNSPECIFIED = bindings.v1GetModelsRequestSortBy.SORT_BY_UNSPECIFIED.value
-    NAME = bindings.v1GetModelsRequestSortBy.SORT_BY_NAME.value
-    DESCRIPTION = bindings.v1GetModelsRequestSortBy.SORT_BY_DESCRIPTION.value
-    CREATION_TIME = bindings.v1GetModelsRequestSortBy.SORT_BY_CREATION_TIME.value
-    LAST_UPDATED_TIME = bindings.v1GetModelsRequestSortBy.SORT_BY_LAST_UPDATED_TIME.value
-    NUM_VERSIONS = bindings.v1GetModelsRequestSortBy.SORT_BY_NUM_VERSIONS.value
-    WORKSPACE = bindings.v1GetModelsRequestSortBy.SORT_BY_WORKSPACE.value
+    UNSPECIFIED = bindings.v1GetModelsRequestSortBy.UNSPECIFIED.value
+    NAME = bindings.v1GetModelsRequestSortBy.NAME.value
+    DESCRIPTION = bindings.v1GetModelsRequestSortBy.DESCRIPTION.value
+    CREATION_TIME = bindings.v1GetModelsRequestSortBy.CREATION_TIME.value
+    LAST_UPDATED_TIME = bindings.v1GetModelsRequestSortBy.LAST_UPDATED_TIME.value
+    NUM_VERSIONS = bindings.v1GetModelsRequestSortBy.NUM_VERSIONS.value
+    WORKSPACE = bindings.v1GetModelsRequestSortBy.WORKSPACE.value
 
     def _to_bindings(self) -> bindings.v1GetModelsRequestSortBy:
         return bindings.v1GetModelsRequestSortBy(self.value)
 
 
 class ModelOrderBy(enum.Enum):
     """
     Specifies whether a sorted list of models should be in ascending or
     descending order.
     """
 
-    ASCENDING = bindings.v1OrderBy.ORDER_BY_ASC.value
-    ASC = bindings.v1OrderBy.ORDER_BY_ASC.value
-    DESCENDING = bindings.v1OrderBy.ORDER_BY_DESC.value
-    DESC = bindings.v1OrderBy.ORDER_BY_DESC.value
+    ASCENDING = bindings.v1OrderBy.ASC.value
+    ASC = bindings.v1OrderBy.ASC.value
+    DESCENDING = bindings.v1OrderBy.DESC.value
+    DESC = bindings.v1OrderBy.DESC.value
 
     def _to_bindings(self) -> bindings.v1OrderBy:
         return bindings.v1OrderBy(self.value)
 
 
 class Model:
     """
@@ -191,16 +191,16 @@
             version (int, optional): The model version ID requested.
         """
         if version == -1:
             resp = bindings.get_GetModelVersions(
                 self._session,
                 modelName=self.name,
                 limit=1,
-                sortBy=bindings.v1GetModelVersionsRequestSortBy.SORT_BY_VERSION,
-                orderBy=bindings.v1OrderBy.ORDER_BY_DESC,
+                sortBy=bindings.v1GetModelVersionsRequestSortBy.VERSION,
+                orderBy=bindings.v1OrderBy.DESC,
             )
             if not resp.modelVersions:
                 return None
             return ModelVersion._from_bindings(resp.modelVersions[0], self._session)
 
         r = bindings.get_GetModelVersion(
             self._session, modelName=self.name, modelVersionNum=version
```

## determined/common/experimental/oauth2_scim_client.py

```diff
@@ -5,12 +5,11 @@
     def __init__(
         self,
         client_id: str,
         domain: str,
         name: str,
         secret: Optional[str] = None,
     ):
-
         self.id = client_id
         self.secret = secret
         self.domain = domain
         self.name = name
```

## determined/common/experimental/trial.py

```diff
@@ -5,50 +5,50 @@
 
 from determined.common import api, util
 from determined.common.api import bindings, logs
 from determined.common.experimental import checkpoint
 
 
 class LogLevel(enum.Enum):
-    TRACE = bindings.v1LogLevel.LOG_LEVEL_TRACE.value
-    DEBUG = bindings.v1LogLevel.LOG_LEVEL_DEBUG.value
-    INFO = bindings.v1LogLevel.LOG_LEVEL_INFO.value
-    WARNING = bindings.v1LogLevel.LOG_LEVEL_WARNING.value
-    ERROR = bindings.v1LogLevel.LOG_LEVEL_ERROR.value
-    CRITICAL = bindings.v1LogLevel.LOG_LEVEL_CRITICAL.value
+    TRACE = bindings.v1LogLevel.TRACE.value
+    DEBUG = bindings.v1LogLevel.DEBUG.value
+    INFO = bindings.v1LogLevel.INFO.value
+    WARNING = bindings.v1LogLevel.WARNING.value
+    ERROR = bindings.v1LogLevel.ERROR.value
+    CRITICAL = bindings.v1LogLevel.CRITICAL.value
 
     def _to_bindings(self) -> bindings.v1LogLevel:
         return bindings.v1LogLevel(self.value)
 
 
 _csb = bindings.v1GetTrialCheckpointsRequestSortBy
 
 
 class CheckpointSortBy(enum.Enum):
     """
     Specifies the field to sort a list of checkpoints on.
     """
 
-    END_TIME = _csb.SORT_BY_END_TIME.value
-    STATE = _csb.SORT_BY_STATE.value
-    UUID = _csb.SORT_BY_UUID.value
-    BATCH_NUMBER = _csb.SORT_BY_BATCH_NUMBER.value
+    END_TIME = _csb.END_TIME.value
+    STATE = _csb.STATE.value
+    UUID = _csb.UUID.value
+    BATCH_NUMBER = _csb.BATCH_NUMBER.value
 
     def _to_bindings(self) -> bindings.v1GetTrialCheckpointsRequestSortBy:
         return _csb(self.value)
 
 
 class CheckpointOrderBy(enum.Enum):
     """
     Specifies whether a sorted list of checkpoints should be in ascending or
     descending order.
     """
 
-    ASC = bindings.v1OrderBy.ORDER_BY_ASC.value
-    DESC = bindings.v1OrderBy.ORDER_BY_DESC.value
+    ASC = bindings.v1OrderBy.ASC.value
+    DESC = bindings.v1OrderBy.DESC.value
 
     def _to_bindings(self) -> bindings.v1OrderBy:
         return bindings.v1OrderBy(self.value)
 
 
 @dataclasses.dataclass
 class TrainingMetrics:
@@ -373,40 +373,40 @@
 
 
 class TrialSortBy(enum.Enum):
     """
     Specifies the field to sort a list of trials on.
     """
 
-    UNSPECIFIED = _tsb.SORT_BY_UNSPECIFIED.value
-    ID = _tsb.SORT_BY_ID.value
-    START_TIME = _tsb.SORT_BY_START_TIME.value
-    END_TIME = _tsb.SORT_BY_END_TIME.value
-    STATE = _tsb.SORT_BY_STATE.value
-    BEST_VALIDATION_METRIC = _tsb.SORT_BY_BEST_VALIDATION_METRIC.value
-    LATEST_VALIDATION_METRIC = _tsb.SORT_BY_LATEST_VALIDATION_METRIC.value
-    BATCHES_PROCESSED = _tsb.SORT_BY_BATCHES_PROCESSED.value
-    DURATION = _tsb.SORT_BY_DURATION.value
-    RESTARTS = _tsb.SORT_BY_RESTARTS.value
-    CHECKPOINT_SIZE = _tsb.SORT_BY_CHECKPOINT_SIZE.value
+    UNSPECIFIED = _tsb.UNSPECIFIED.value
+    ID = _tsb.ID.value
+    START_TIME = _tsb.START_TIME.value
+    END_TIME = _tsb.END_TIME.value
+    STATE = _tsb.STATE.value
+    BEST_VALIDATION_METRIC = _tsb.BEST_VALIDATION_METRIC.value
+    LATEST_VALIDATION_METRIC = _tsb.LATEST_VALIDATION_METRIC.value
+    BATCHES_PROCESSED = _tsb.BATCHES_PROCESSED.value
+    DURATION = _tsb.DURATION.value
+    RESTARTS = _tsb.RESTARTS.value
+    CHECKPOINT_SIZE = _tsb.CHECKPOINT_SIZE.value
 
     def _to_bindings(self) -> bindings.v1GetExperimentTrialsRequestSortBy:
         return _tsb(self.value)
 
 
 class TrialOrderBy(enum.Enum):
     """
     Specifies whether a sorted list of trials should be in ascending or
     descending order.
     """
 
-    ASCENDING = bindings.v1OrderBy.ORDER_BY_ASC.value
-    ASC = bindings.v1OrderBy.ORDER_BY_ASC.value
-    DESCENDING = bindings.v1OrderBy.ORDER_BY_DESC.value
-    DESC = bindings.v1OrderBy.ORDER_BY_DESC.value
+    ASCENDING = bindings.v1OrderBy.ASC.value
+    ASC = bindings.v1OrderBy.ASC.value
+    DESCENDING = bindings.v1OrderBy.DESC.value
+    DESC = bindings.v1OrderBy.DESC.value
 
     def _to_bindings(self) -> bindings.v1OrderBy:
         return bindings.v1OrderBy(self.value)
 
 
 def _stream_training_metrics(
     session: api.Session, trial_ids: List[int]
```

## determined/common/experimental/checkpoint/_checkpoint.py

```diff
@@ -26,19 +26,19 @@
 
 class ModelFramework(enum.Enum):
     PYTORCH = 1
     TENSORFLOW = 2
 
 
 class CheckpointState(enum.Enum):
-    UNSPECIFIED = bindings.checkpointv1State.STATE_UNSPECIFIED.value
-    ACTIVE = bindings.checkpointv1State.STATE_ACTIVE.value
-    COMPLETED = bindings.checkpointv1State.STATE_COMPLETED.value
-    ERROR = bindings.checkpointv1State.STATE_ERROR.value
-    DELETED = bindings.checkpointv1State.STATE_DELETED.value
+    UNSPECIFIED = bindings.checkpointv1State.UNSPECIFIED.value
+    ACTIVE = bindings.checkpointv1State.ACTIVE.value
+    COMPLETED = bindings.checkpointv1State.COMPLETED.value
+    ERROR = bindings.checkpointv1State.ERROR.value
+    DELETED = bindings.checkpointv1State.DELETED.value
 
 
 @dataclasses.dataclass
 class CheckpointTrainingMetadata:
     experiment_config: Dict[str, Any]
     experiment_id: int
     trial_id: int
@@ -272,15 +272,15 @@
         # TODO: in a future version of this REST API, an entire, well-formed Checkpoint object.
         req = bindings.v1PostCheckpointMetadataRequest(
             checkpoint=bindings.v1Checkpoint(
                 uuid=self.uuid,
                 metadata=self.metadata,
                 resources={},
                 training=bindings.v1CheckpointTrainingMetadata(),
-                state=bindings.checkpointv1State.STATE_UNSPECIFIED,
+                state=bindings.checkpointv1State.UNSPECIFIED,
             ),
         )
         bindings.post_PostCheckpointMetadata(self._session, body=req, checkpoint_uuid=self.uuid)
 
     def add_metadata(self, metadata: Dict[str, Any]) -> None:
         """
         Adds user-defined metadata to the checkpoint. The ``metadata`` argument must be a
```

## determined/core/_checkpoint.py

```diff
@@ -39,17 +39,15 @@
     merged: Dict[str, Any],
     rank_metadata: Dict[str, Any],
     rank: int,
     key_ranks: Dict[str, Any],
     key_conflicts: Dict[str, List[int]],
     prev_key: str,
 ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
-
     for key, metadata in rank_metadata.items():
-
         # First, update key_ranks. We will use it later
         if key not in key_ranks:
             key_ranks[key] = {"_ranks_": []}
         key_ranks[key]["_ranks_"].append(rank)
 
         full_key = f"{prev_key}/{key}"
 
@@ -135,15 +133,14 @@
       - a dict mapping conflicting files to ranks that would upload them
 
     """
     files: Set[str] = set()
     uploaders: Dict[str, List] = {}
     merged: Dict[str, int] = {}
     for rank, rscs in enumerate(all_resources):
-
         for name in rscs:
             size = rscs[name]
             if name.endswith(os.sep):
                 # Dir name.
                 stripped = name.rstrip(os.sep)
                 uploaders.setdefault(stripped, []).append(rank)
             else:
@@ -700,15 +697,15 @@
             allocationId=self._allocation_id,
             metadata=metadata,
             resources={k: str(v) for k, v in resources.items()},
             taskId=self._task_id,
             training=bindings.v1CheckpointTrainingMetadata(),
             uuid=storage_id,
             reportTime=datetime.now(timezone.utc).isoformat(),
-            state=bindings.checkpointv1State.STATE_COMPLETED,
+            state=bindings.checkpointv1State.COMPLETED,
         )
         bindings.post_ReportCheckpoint(self._session, body=ckpt)
         logger.info(f"Reported checkpoint to master {storage_id}")
 
         # Also sync tensorboard.
         if self._tensorboard_mode == core.TensorboardMode.AUTO:
             self._tensorboard_manager.sync()
```

## determined/deploy/aws/aws.py

```diff
@@ -506,15 +506,14 @@
 
     return instances_to_del
 
 
 def clean_up_spot(
     stack_name: str, boto3_session: boto3.session.Session, disable_tqdm: bool = False
 ) -> None:
-
     # The spot API is eventually consistent and the only way to guarantee
     # that we don't leave any spot requests alive (that may eventually be
     # fulfilled and lead to running EC2 instances) is to wait a long enough
     # period that any created spot requests will have shown up in the API.
     # 60 seconds seems like a relatively safe amount of time.
     SPOT_WAIT_SECONDS = 60
```

## determined/deploy/aws/gen_vcpu_mapping.py

```diff
@@ -1,14 +1,15 @@
 import argparse
 import json
 from pathlib import Path
 from typing import Dict, Iterable, List, Tuple
 
 import boto3
-import yaml
+
+from determined.common import yaml
 
 
 def _fetch_vcpu_mapping() -> Iterable[Tuple[str, Dict]]:
     # Price List api is only available in us-east-1 and ap-southeast-1.
     client = boto3.client("pricing", region_name="us-east-1")
     for page in client.get_paginator("get_products").paginate(ServiceCode="AmazonEC2"):
         for sku_str in page["PriceList"]:
```

## determined/deploy/aws/preflight.py

```diff
@@ -1,18 +1,17 @@
 import re
 import sys
 from typing import Any, Dict
 
 import boto3
 import pkg_resources
-import yaml
 from botocore.exceptions import ClientError
 from termcolor import colored
 
-from determined.common import util
+from determined.common import util, yaml
 from determined.deploy.errors import PreflightFailure
 
 from . import constants
 from .deployment_types.base import DeterminedDeployment
 
 # There's no reliable way to map instance type to its quota category via an API.
 # Lookup quota codes in AWS console at:
@@ -63,15 +62,15 @@
 # since we only make use of the default parameters section,
 # and it doesn't contain any such function calls.
 class LoaderIgnoreUnknown(yaml.SafeLoader):
     def ignore_unknown(self, node: Any) -> None:
         return None
 
 
-LoaderIgnoreUnknown.add_constructor(None, LoaderIgnoreUnknown.ignore_unknown)  # type: ignore
+LoaderIgnoreUnknown.add_constructor(None, LoaderIgnoreUnknown.ignore_unknown)
 
 
 def get_default_cf_parameter(deployment_object: DeterminedDeployment, parameter: str) -> Any:
     with open(deployment_object.template_path) as fin:
         data = yaml.load(fin, Loader=LoaderIgnoreUnknown)
 
     return data["Parameters"][parameter]["Default"]
```

## determined/deploy/aws/templates/efs.yaml

```diff
@@ -1,41 +1,41 @@
 Description:  This template deploys a VPC, with a public and private subnet, and FSx filesystem.
 Mappings:
   RegionMap:
     ap-northeast-1:
       Master: ami-00910ef9457f0df47
-      Agent: ami-095dc4fb79f2496da
+      Agent: ami-0bb9d169eea880f0c
     # TODO(DET-4258) Uncomment these when we fully support all P3 regions.
     # ap-northeast-2:
     #   Master: ami-035e3e44dc41db6a2
-    #   Agent: ami-078bb019c9a133349
+    #   Agent: ami-04b2a73ef7923efe6
     # ap-southeast-1:
     #   Master: ami-0fd1ee6c8b656f020
-    #   Agent: ami-0d926ac4d57af0fe2
+    #   Agent: ami-024d675978237fa80
     # ap-southeast-2:
     #   Master: ami-0b62ecd3babd1c548
-    #   Agent: ami-04cc5e46f80b88805
+    #   Agent: ami-0238276397b254a83
     eu-central-1:
       Master: ami-0abbe417ed83c0b29
-      Agent: ami-0337795faa3fa47e8
+      Agent: ami-09506c0a6df6626bc
     eu-west-1:
       Master: ami-0e3f7dd2dc743e48a
-      Agent: ami-01005388142fe709e
+      Agent: ami-0e2b716415874cb07
     # eu-west-2:
     #   Master: ami-0d78429fb6af30994
-    #   Agent: ami-0c961f624e82afc9e
+    #   Agent: ami-0c8a8a8c7a2721ee7
     us-east-1:
       Master: ami-0172070f66a8ebe63
-      Agent: ami-0d66a70e16e8b5cf0
+      Agent: ami-0b18bcecbd428ae4c
     us-east-2:
       Master: ami-0bafa3699418551cd
-      Agent: ami-007bd0d8cecde0ad0
+      Agent: ami-00e86d39577d39acf
     us-west-2:
       Master: ami-0ceeab680f529cc36
-      Agent: ami-0eb117edc93f5a736
+      Agent: ami-0c841a5d48a137ae2
 
 Parameters:
   VpcCIDR:
     Description: Please enter the IP range (CIDR notation) for this VPC
     Type: String
     Default: 10.192.0.0/16
 
@@ -97,15 +97,15 @@
     Type: String
     Description: Docker password to pull images that need authentication
     Default: ""
 
   Version:
     Type: String
     Description: Determined version or commit for master image
-    Default: 0.21.1-rc4
+    Default: 0.21.2-rc0
 
   DBPassword:
     Type: String
     Description: Password for database
     NoEcho: true
 
   MaxAuxContainersPerAgent:
```

## determined/deploy/aws/templates/fsx.yaml

```diff
@@ -1,41 +1,41 @@
 Description:  This template deploys a VPC, with a public and private subnet, and FSx filesystem.
 Mappings:
   RegionMap:
     ap-northeast-1:
       Master: ami-00910ef9457f0df47
-      Agent: ami-095dc4fb79f2496da
+      Agent: ami-0bb9d169eea880f0c
     # TODO(DET-4258) Uncomment these when we fully support all P3 regions.
     # ap-northeast-2:
     #   Master: ami-035e3e44dc41db6a2
-    #   Agent: ami-078bb019c9a133349
+    #   Agent: ami-04b2a73ef7923efe6
     # ap-southeast-1:
     #   Master: ami-0fd1ee6c8b656f020
-    #   Agent: ami-0d926ac4d57af0fe2
+    #   Agent: ami-024d675978237fa80
     # ap-southeast-2:
     #   Master: ami-0b62ecd3babd1c548
-    #   Agent: ami-04cc5e46f80b88805
+    #   Agent: ami-0238276397b254a83
     eu-central-1:
       Master: ami-0abbe417ed83c0b29
-      Agent: ami-0337795faa3fa47e8
+      Agent: ami-09506c0a6df6626bc
     eu-west-1:
       Master: ami-0e3f7dd2dc743e48a
-      Agent: ami-01005388142fe709e
+      Agent: ami-0e2b716415874cb07
     # eu-west-2:
     #   Master: ami-0d78429fb6af30994
-    #   Agent: ami-0c961f624e82afc9e
+    #   Agent: ami-0c8a8a8c7a2721ee7
     us-east-1:
       Master: ami-0172070f66a8ebe63
-      Agent: ami-0d66a70e16e8b5cf0
+      Agent: ami-0b18bcecbd428ae4c
     us-east-2:
       Master: ami-0bafa3699418551cd
-      Agent: ami-007bd0d8cecde0ad0
+      Agent: ami-00e86d39577d39acf
     us-west-2:
       Master: ami-0ceeab680f529cc36
-      Agent: ami-0eb117edc93f5a736
+      Agent: ami-0c841a5d48a137ae2
 
 Parameters:
   VpcCIDR:
     Description: Please enter the IP range (CIDR notation) for this VPC
     Type: String
     Default: 10.192.0.0/16
 
@@ -97,15 +97,15 @@
     Type: String
     Description: Docker password to pull images that need authentication
     Default: ""
 
   Version:
     Type: String
     Description: Determined version or commit for master image
-    Default: 0.21.1-rc4
+    Default: 0.21.2-rc0
 
   DBPassword:
     Type: String
     Description: Password for database
     NoEcho: true
 
   MaxAuxContainersPerAgent:
```

## determined/deploy/aws/templates/govcloud.yaml

```diff
@@ -63,15 +63,15 @@
     Type: String
     Description: Docker password to pull images that need authentication
     Default: ""
 
   Version:
     Type: String
     Description: Determined version or commit for master docker image
-    Default: 0.21.1-rc4
+    Default: 0.21.2-rc0
 
   DBPassword:
     Type: String
     Description: Password for database (eg. "postgres")
     NoEcho: true
 
   MaxAuxContainersPerAgent:
```

## determined/deploy/aws/templates/secure.yaml

```diff
@@ -1,51 +1,51 @@
 Description:  This template deploys a VPC, with a public and private subnet. It deploys an internet gateway,
   with a default route on the public subnet with a NAT gateway, and default routes for them in the private subnet.
 Mappings:
   RegionMap:
     ap-northeast-1:
       Master: ami-00910ef9457f0df47
-      Agent: ami-095dc4fb79f2496da
+      Agent: ami-0bb9d169eea880f0c
       Bastion: ami-00910ef9457f0df47
     # TODO(DET-4258) Uncomment these when we fully support all P3 regions.
     # ap-northeast-2:
     #   Master: ami-035e3e44dc41db6a2
-    #   Agent: ami-078bb019c9a133349
+    #   Agent: ami-04b2a73ef7923efe6
     #   Bastion: ami-035e3e44dc41db6a2
     # ap-southeast-1:
     #   Master: ami-0fd1ee6c8b656f020
-    #   Agent: ami-0d926ac4d57af0fe2
+    #   Agent: ami-024d675978237fa80
     #   Bastion: ami-0fd1ee6c8b656f020
     # ap-southeast-2:
     #   Master: ami-0b62ecd3babd1c548
-    #   Agent: ami-04cc5e46f80b88805
+    #   Agent: ami-0238276397b254a83
     #   Bastion: ami-0b62ecd3babd1c548
     eu-central-1:
       Master: ami-0abbe417ed83c0b29
-      Agent: ami-0337795faa3fa47e8
+      Agent: ami-09506c0a6df6626bc
       Bastion: ami-0abbe417ed83c0b29
     eu-west-1:
       Master: ami-0e3f7dd2dc743e48a
-      Agent: ami-01005388142fe709e
+      Agent: ami-0e2b716415874cb07
       Bastion: ami-0e3f7dd2dc743e48a
     # eu-west-2:
     #   Master: ami-0d78429fb6af30994
-    #   Agent: ami-0c961f624e82afc9e
+    #   Agent: ami-0c8a8a8c7a2721ee7
     #   Bastion: ami-0d78429fb6af30994
     us-east-1:
       Master: ami-0172070f66a8ebe63
-      Agent: ami-0d66a70e16e8b5cf0
+      Agent: ami-0b18bcecbd428ae4c
       Bastion: ami-0172070f66a8ebe63
     us-east-2:
       Master: ami-0bafa3699418551cd
-      Agent: ami-007bd0d8cecde0ad0
+      Agent: ami-00e86d39577d39acf
       Bastion: ami-0bafa3699418551cd
     us-west-2:
       Master: ami-0ceeab680f529cc36
-      Agent: ami-0eb117edc93f5a736
+      Agent: ami-0c841a5d48a137ae2
       Bastion: ami-0ceeab680f529cc36
 
 Parameters:
   VpcCIDR:
     Description: Please enter the IP range (CIDR notation) for this VPC
     Type: String
     Default: 10.192.0.0/16
@@ -118,15 +118,15 @@
     Type: String
     Description: Docker password to pull images that need authentication
     Default: ""
 
   Version:
     Type: String
     Description: Determined version or commit for master image
-    Default: 0.21.1-rc4
+    Default: 0.21.2-rc0
 
   DBPassword:
     Type: String
     Description: Password for database
     NoEcho: true
 
   MaxAuxContainersPerAgent:
```

## determined/deploy/aws/templates/simple.yaml

```diff
@@ -1,43 +1,43 @@
 ---
 AWSTemplateFormatVersion: 2010-09-09
 Description: Determined Template
 Mappings:
   RegionMap:
     ap-northeast-1:
       Master: ami-00910ef9457f0df47
-      Agent: ami-095dc4fb79f2496da
+      Agent: ami-0bb9d169eea880f0c
     # TODO(DET-4258) Uncomment these when we fully support all P3 regions.
     # ap-northeast-2:
     #   Master: ami-035e3e44dc41db6a2
-    #   Agent: ami-078bb019c9a133349
+    #   Agent: ami-04b2a73ef7923efe6
     # ap-southeast-1:
     #   Master: ami-0fd1ee6c8b656f020
-    #   Agent: ami-0d926ac4d57af0fe2
+    #   Agent: ami-024d675978237fa80
     # ap-southeast-2:
     #   Master: ami-0b62ecd3babd1c548
-    #   Agent: ami-04cc5e46f80b88805
+    #   Agent: ami-0238276397b254a83
     eu-central-1:
       Master: ami-0abbe417ed83c0b29
-      Agent: ami-0337795faa3fa47e8
+      Agent: ami-09506c0a6df6626bc
     eu-west-1:
       Master: ami-0e3f7dd2dc743e48a
-      Agent: ami-01005388142fe709e
+      Agent: ami-0e2b716415874cb07
     # eu-west-2:
     #   Master: ami-0d78429fb6af30994
-    #   Agent: ami-0c961f624e82afc9e
+    #   Agent: ami-0c8a8a8c7a2721ee7
     us-east-1:
       Master: ami-0172070f66a8ebe63
-      Agent: ami-0d66a70e16e8b5cf0
+      Agent: ami-0b18bcecbd428ae4c
     us-east-2:
       Master: ami-0bafa3699418551cd
-      Agent: ami-007bd0d8cecde0ad0
+      Agent: ami-00e86d39577d39acf
     us-west-2:
       Master: ami-0ceeab680f529cc36
-      Agent: ami-0eb117edc93f5a736
+      Agent: ami-0c841a5d48a137ae2
 
 Parameters:
   Keypair:
     Type: AWS::EC2::KeyPair::KeyName
     Description: Keypair to SSH
 
   MasterInstanceType:
@@ -89,15 +89,15 @@
     Type: String
     Description: Docker password to pull images that need authentication
     Default: ""
 
   Version:
     Type: String
     Description: Determined version or commit for master docker image
-    Default: 0.21.1-rc4
+    Default: 0.21.2-rc0
 
   DBPassword:
     Type: String
     Description: Password for database (eg. "postgres")
     NoEcho: true
 
   MaxAuxContainersPerAgent:
```

## determined/deploy/gcp/constants.py

```diff
@@ -1,15 +1,14 @@
 class defaults:
-
     AUX_AGENT_INSTANCE_TYPE = "n1-standard-4"
     COMPUTE_AGENT_INSTANCE_TYPE = "n1-standard-32"
     DB_PASSWORD = "postgres"
     BOOT_DISK_SIZE = 200
     BOOT_DISK_TYPE = "pd-standard"
-    ENVIRONMENT_IMAGE = "det-environments-9d07809"
+    ENVIRONMENT_IMAGE = "det-environments-9b5db1b"
     GPU_NUM = 4
     GPU_TYPE = "nvidia-tesla-t4"
     MASTER_INSTANCE_TYPE = "n1-standard-2"
     MAX_AUX_CONTAINERS_PER_AGENT = 100
     MAX_IDLE_AGENT_PERIOD = "10m"
     MAX_AGENT_STARTING_PERIOD = "20m"
     OPERATION_TIMEOUT_PERIOD = "5m"
```

## determined/deploy/local/cli.py

```diff
@@ -1,12 +1,13 @@
 import argparse
 import sys
 from pathlib import Path
 from typing import Callable, Dict
 
+import determined
 from determined.common.declarative_argparse import Arg, BoolOptArg, Cmd, Group
 
 from . import cluster_utils
 from .preflight import check_docker_install
 
 
 def handle_cluster_up(args: argparse.Namespace) -> None:
@@ -30,15 +31,15 @@
 
 
 def handle_cluster_down(args: argparse.Namespace) -> None:
     cluster_utils.cluster_down(cluster_name=args.cluster_name, delete_db=args.delete_db)
 
 
 def handle_logs(args: argparse.Namespace) -> None:
-    cluster_utils.logs(cluster_name=args.cluster_name, no_follow=args.no_follow)
+    cluster_utils.logs(cluster_name=args.cluster_name, follow=not args.no_follow)
 
 
 def handle_master_up(args: argparse.Namespace) -> None:
     cluster_utils.master_up(
         port=args.master_port,
         master_config_path=args.master_config_path,
         storage_host_path=args.storage_host_path,
@@ -50,15 +51,17 @@
         autorestart=(not args.no_autorestart),
         cluster_name=args.cluster_name,
         auto_work_dir=args.auto_work_dir,
     )
 
 
 def handle_master_down(args: argparse.Namespace) -> None:
-    cluster_utils.master_down(master_name=args.master_name, delete_db=args.delete_db)
+    cluster_utils.master_down(
+        master_name=args.master_name, delete_db=args.delete_db, cluster_name=args.cluster_name
+    )
 
 
 def handle_agent_up(args: argparse.Namespace) -> None:
     cluster_utils.agent_up(
         master_host=args.master_host,
         master_port=args.master_port,
         agent_config_path=args.agent_config_path,
@@ -131,15 +134,20 @@
                 ),
                 Arg(
                     "--cluster-name",
                     type=str,
                     default="determined",
                     help="name for the cluster resources",
                 ),
-                Arg("--det-version", type=str, default=None, help="version or commit to use"),
+                Arg(
+                    "--det-version",
+                    type=str,
+                    default=determined.__version__,
+                    help="version or commit to use",
+                ),
                 Arg(
                     "--db-password",
                     type=str,
                     default="postgres",
                     help="password for master database",
                 ),
                 Arg(
@@ -210,18 +218,23 @@
                     type=int,
                     default=cluster_utils.MASTER_PORT_DEFAULT,
                     help="port to expose master on",
                 ),
                 Arg(
                     "--master-name",
                     type=str,
-                    default="determined",
-                    help="name for the cluster resources",
+                    default=None,
+                    help="name for the master instance",
+                ),
+                Arg(
+                    "--det-version",
+                    type=str,
+                    default=determined.__version__,
+                    help="version or commit to use",
                 ),
-                Arg("--det-version", type=str, default=None, help="version or commit to use"),
                 Arg(
                     "--db-password",
                     type=str,
                     default="postgres",
                     help="password for master database",
                 ),
                 Arg(
@@ -237,14 +250,20 @@
                 Arg(
                     "--cluster-name",
                     type=str,
                     default="determined",
                     help="name for the cluster resources",
                 ),
                 Arg(
+                    "--image-repo-prefix",
+                    type=str,
+                    default="determinedai",
+                    help="prefix for the master image",
+                ),
+                Arg(
                     "--auto-work-dir",
                     type=Path,
                     default=None,
                     help="the default work dir, used for interactive jobs",
                 ),
             ],
         ),
@@ -252,16 +271,16 @@
             "master-down",
             handle_master_down,
             "Stop a Determined master",
             [
                 Arg(
                     "--master-name",
                     type=str,
-                    default="determined",
-                    help="name for the cluster resources",
+                    default=None,
+                    help="name for the master instance",
                 ),
                 Arg(
                     "--delete-db",
                     action="store_true",
                     help="remove current master database",
                 ),
                 Arg(
@@ -300,15 +319,20 @@
                 ),
                 Arg(
                     "--agent-config-path",
                     type=Path,
                     default=None,
                     help="path to agent configuration",
                 ),
-                Arg("--det-version", type=str, default=None, help="version or commit to use"),
+                Arg(
+                    "--det-version",
+                    type=str,
+                    default=determined.__version__,
+                    help="version or commit to use",
+                ),
                 Arg(
                     "--agent-name",
                     type=str,
                     default=cluster_utils.AGENT_NAME_DEFAULT,
                     help="agent name",
                 ),
                 Arg("--agent-resource-pool", type=str, default=None, help="agent resource pool"),
@@ -327,14 +351,20 @@
                 ),
                 Arg(
                     "--cluster-name",
                     type=str,
                     default="determined",
                     help="name for the cluster resources",
                 ),
+                Arg(
+                    "--image-repo-prefix",
+                    type=str,
+                    default="determinedai",
+                    help="prefix for the master image",
+                ),
             ],
         ),
         Cmd(
             "agent-down",
             handle_agent_down,
             "Stop a Determined agent",
             [
```

## determined/deploy/local/cluster_utils.py

```diff
@@ -1,32 +1,67 @@
+import contextlib
 import os
+import pathlib
 import re
 import socket
 import subprocess
 import sys
 import tempfile
-from pathlib import Path
-from typing import Any, Dict, List, Optional, Sequence
+import threading
+import time
+from typing import Any, Callable, Dict, Generator, List, Optional, Sequence, Type
 
 import appdirs
 import docker
 
-import determined
-import determined.deploy
 from determined.common import yaml
 from determined.deploy.errors import MasterTimeoutExpired
 from determined.deploy.healthcheck import wait_for_master
 
 AGENT_NAME_DEFAULT = f"det-agent-{socket.gethostname()}"
 MASTER_PORT_DEFAULT = 8080
 
+# Default names are consistent with previous docker-compose names to support migration.
+DB_NAME = "determined-db_1"
+NETWORK_NAME = "determined_default"
+VOLUME_NAME = "determined-db-volume"
+MASTER_NAME = "determined-master_1"
+
 # This object, when included in the host config in a container creation request, tells Docker to
 # expose all host GPUs inside a container.
 GPU_DEVICE_REQUEST = {"Driver": "nvidia", "Count": -1, "Capabilities": [["gpu", "utility"]]}
 
+Container = Type[docker.models.containers.Container]
+
+# These defaults come from master/packaging/master.yaml (except for host_path).
+MASTER_CONF_DEFAULT = {
+    "db": {
+        "user": "postgres",
+        "host": "determined-db",
+        "port": 5432,
+        "name": "determined",
+    },
+    "checkpoint_storage": {
+        "type": "shared_fs",
+        "host_path": appdirs.user_data_dir("determined"),
+        "save_experiment_best": 0,
+        "save_trial_best": 1,
+        "save_trial_latest": 1,
+    },
+}
+
+_docker_client = None
+
+
+def docker_client() -> docker.client:
+    global _docker_client
+    if not _docker_client:
+        _docker_client = docker.from_env()
+    return _docker_client
+
 
 # Patch the Docker library to support device requests, since it has yet to support them natively
 # (see https://github.com/docker/docker-py/issues/2395).
 def _patch_docker_for_device_requests() -> None:
     _old_create_container_args = docker.models.containers._create_container_args
 
     def _create_container_args(kwargs: Any) -> Any:
@@ -71,166 +106,256 @@
         if matches is not None:
             groups: Sequence[str] = matches.groups()
             if len(groups) != 0:
                 return groups[0]
         return ""
 
 
-def docker_compose(
-    args: List[str],
-    cluster_name: str,
-    env: Optional[Dict] = None,
-    extra_files: Optional[List[str]] = None,
-) -> None:
-    path = Path(__file__).parent.joinpath("docker-compose.yaml")
-    # Start with the user's environment to ensure that Docker and Docker Compose work correctly.
-    process_env = dict(os.environ)
-    if env is not None:
-        process_env.update(env)
-    process_env["INTEGRATIONS_PROXY_ADDR"] = get_proxy_addr()
-    base_command = ["docker-compose", "-f", str(path), "-p", cluster_name]
-    if extra_files is not None:
-        for extra_file in extra_files:
-            base_command += ["-f", extra_file]
-    args = base_command + args
-    subprocess.check_call(args, env=process_env)
-
-
 def _wait_for_master(master_host: str, master_port: int, cluster_name: str) -> None:
     try:
         wait_for_master(master_host, master_port, timeout=100)
         return
     except MasterTimeoutExpired:
         print("Timed out connecting to master, but attempting to dump logs from cluster...")
-        docker_compose(["logs"], cluster_name)
+        logs(cluster_name=cluster_name, follow=False)
         raise ConnectionError("Timed out connecting to master")
 
 
+def _wait_for_container(container_name: str, timeout: int = 100) -> None:
+    """
+    Waits for a Docker container's healthcheck (specified when the container is run) to reach a
+    "healthy" status.
+    """
+    print(f"Waiting for {container_name}...")
+    client = docker_client()
+    container = client.containers.get(container_name)
+    start_time = time.time()
+    while time.time() - start_time < timeout:
+        inspect = client.api.inspect_container(container.name)
+        if inspect["State"]["Health"]["Status"] == "healthy":
+            return
+        time.sleep(1)
+    raise TimeoutError
+
+
 def master_up(
     port: int,
-    master_config_path: Optional[Path],
-    storage_host_path: Path,
-    master_name: str,
-    image_repo_prefix: Optional[str],
-    version: Optional[str],
+    master_config_path: Optional[pathlib.Path],
+    storage_host_path: pathlib.Path,
+    master_name: Optional[str],
+    image_repo_prefix: str,
+    version: str,
     db_password: str,
     delete_db: bool,
     autorestart: bool,
     cluster_name: str,
-    auto_work_dir: Optional[Path],
+    auto_work_dir: Optional[pathlib.Path],
 ) -> None:
-    command = ["up", "-d"]
-    if image_repo_prefix is None:
-        image_repo_prefix = "determinedai"
-    if version is None:
-        version = determined.__version__
-    if autorestart:
-        restart_policy = "unless-stopped"
-    else:
-        restart_policy = "no"
-
-    env = {
-        "INTEGRATIONS_HOST_PORT": str(port),
-        "DET_DB_PASSWORD": db_password,
-        "IMAGE_REPO_PREFIX": image_repo_prefix,
-        "DET_VERSION": version,
-        "DET_RESTART_POLICY": restart_policy,
-    }
-
     # Some cli flags for det deploy local will cause us to write a temporary master.yaml.
-    master_conf = {}
     make_temp_conf = False
 
+    if master_name is None:
+        master_name = f"{cluster_name}_{MASTER_NAME}"
+
     if master_config_path is not None:
         with master_config_path.open() as f:
             master_conf = yaml.safe_load(f)
     else:
-        # These defaults come from master/packaging/master.yaml (except for host_path).
-        master_conf = {
-            "db": {
-                "user": "postgres",
-                "host": "determined-db",
-                "port": 5432,
-                "name": "determined",
-            },
-            "checkpoint_storage": {
-                "type": "shared_fs",
-                "host_path": appdirs.user_data_dir("determined"),
-                "save_experiment_best": 0,
-                "save_trial_best": 1,
-                "save_trial_latest": 1,
-            },
-        }
+        master_conf = MASTER_CONF_DEFAULT
         make_temp_conf = True
 
     if storage_host_path is not None:
         master_conf["checkpoint_storage"] = {
             "type": "shared_fs",
             "host_path": str(storage_host_path.resolve()),
         }
         make_temp_conf = True
 
+    # Ensure checkpoint storage directory exists.
+    final_storage_host_path = master_conf.get("checkpoint_storage", {}).get("host_path")
+    if final_storage_host_path is not None:
+        final_storage_host_path = pathlib.Path(final_storage_host_path)
+        if not final_storage_host_path.exists():
+            final_storage_host_path.mkdir(parents=True)
+
     if auto_work_dir is not None:
         work_dir = str(auto_work_dir.resolve())
         master_conf.setdefault("task_container_defaults", {})["work_dir"] = work_dir
         master_conf["task_container_defaults"].setdefault("bind_mounts", []).append(
             {"host_path": work_dir, "container_path": work_dir}
         )
         make_temp_conf = True
 
-    # Ensure checkpoint storage directory exists.
-    final_storage_host_path = master_conf.get("checkpoint_storage", {}).get("host_path")
-    if final_storage_host_path is not None:
-        final_storage_host_path = Path(final_storage_host_path)
-        if not final_storage_host_path.exists():
-            final_storage_host_path.mkdir(parents=True)
-
     if make_temp_conf:
         fd, temp_path = tempfile.mkstemp(prefix="det-deploy-local-master-config-")
         with open(fd, "w") as f:
             yaml.dump(master_conf, f)
-        master_config_path = Path(temp_path)
+        master_config_path = pathlib.Path(temp_path)
 
     # This is always true by now, but mypy needs help.
     assert master_config_path is not None
+    restart_policy = "unless-stopped" if autorestart else "no"
+    env = {
+        "DET_MASTER_HTTP_PORT": str(port),
+        "DET_DB_PASSWORD": db_password,
+        "DET_LOG_INFO": "info",
+    }
 
-    env["DET_MASTER_CONFIG"] = str(master_config_path.resolve())
+    # Kill existing master container if exists.
+    master_down(master_name=master_name, delete_db=delete_db, cluster_name=cluster_name)
 
-    master_down(master_name, delete_db)
-    docker_compose(command, master_name, env)
-    _wait_for_master("localhost", port, cluster_name)
+    # Create network.
+    client = docker_client()
 
+    # Start db and wait for healthcheck.
+    db_name = f"{cluster_name}_{DB_NAME}"
+    volume_name = f"{cluster_name}_{VOLUME_NAME}"
+
+    @contextlib.contextmanager
+    def defer_cleanup(fn: Callable[[], None]) -> Generator:
+        """
+        Defer cleanup tasks for each resource if Exceptions are caught.
+        """
+        try:
+            yield
+        except Exception as ex:
+            fn()
+            raise ex
+
+    with contextlib.ExitStack() as exit_stack:
+        # Create network used by DB and master.
+        print(f"Creating network {NETWORK_NAME}...")
+        exit_stack.enter_context(defer_cleanup(lambda: remove_network(NETWORK_NAME)))
+        client.networks.create(name=NETWORK_NAME, attachable=True)
+
+        # Start up db.
+        exit_stack.enter_context(defer_cleanup(lambda: db_down(db_name, VOLUME_NAME, delete_db)))
+        db_up(
+            name=db_name,
+            password=db_password,
+            network_name=NETWORK_NAME,
+            cluster_name=cluster_name,
+            volume_name=volume_name,
+        )
 
-def master_down(master_name: str, delete_db: bool) -> None:
-    if delete_db:
-        docker_compose(["down", "--volumes", "-t", "1"], master_name)
-    else:
-        docker_compose(["down", "-t", "1"], master_name)
+        # Wait for db to reach a healthy state.
+        _wait_for_container(db_name, timeout=5)
+
+        # Remove cleanup methods from ExitStack after DB successfully starts.
+        exit_stack.pop_all()
+
+        # Start master instance.
+        print(f"Creating {master_name}...")
+        exit_stack.enter_context(
+            defer_cleanup(lambda: master_down(master_name, delete_db, cluster_name))  # type: ignore
+        )
+        volumes = [f"{os.path.abspath(master_config_path)}:/etc/determined/master.yaml"]
+        client.containers.run(
+            image=f"{image_repo_prefix}/determined-master:{version}",
+            environment=env,
+            init=True,
+            mounts=[],
+            volumes=volumes,
+            name=master_name,
+            detach=True,
+            labels={},
+            restart_policy={"Name": restart_policy},
+            device_requests=None,
+            ports={"8080": f"{port}"},
+        )
+
+        # Connect to the network separately to set alias.
+        network = client.networks.get(NETWORK_NAME)
+        network.connect(container=master_name, aliases=["determined-master"])
+
+        _wait_for_master("localhost", port, cluster_name)
+
+        # Remove all cleanup methods from ExitStack.
+        exit_stack.pop_all()
+
+
+def db_up(name: str, password: str, network_name: str, cluster_name: str, volume_name: str) -> None:
+    print(f"Creating {name}...")
+    env = {"PGUSER": "postgres", "POSTGRES_DB": "determined", "POSTGRES_PASSWORD": password}
+    client = docker_client()
+    client.containers.run(
+        image="postgres:10.14",
+        environment=env,
+        mounts=[],
+        volumes=[f"{volume_name}:/var/lib/postgresql/data"],
+        name=name,
+        detach=True,
+        labels={},
+        restart_policy={"Name": "unless-stopped"},
+        device_requests=None,
+        command="--max_connections=96 --shared_buffers=512MB",
+        healthcheck={
+            "test": ["CMD-SHELL", "pg_isready", "-d", "determined"],
+            "interval": 1000000,
+        },
+    )
+    # Connect to the network separately to set alias.
+    network = client.networks.get(network_name)
+    network.connect(container=name, aliases=["determined-db"])
+
+
+def master_down(master_name: str, delete_db: bool, cluster_name: str) -> None:
+    if master_name is None:
+        master_name = f"{cluster_name}_{MASTER_NAME}"
+
+    _kill_containers(names=[master_name])
+
+    volume_name = f"{cluster_name}_{VOLUME_NAME}"
+    db_name = f"{cluster_name}_{DB_NAME}"
+
+    db_down(db_name=db_name, volume_name=volume_name, delete_volume=delete_db)
+    remove_network(NETWORK_NAME)
+
+
+def db_down(db_name: str, volume_name: str, delete_volume: bool) -> None:
+    client = docker_client()
+
+    _kill_containers([db_name])
+    if delete_volume:
+        try:
+            volume = client.volumes.get(volume_name)
+            print(f"Removing db volume {volume_name}")
+            volume.remove()
+        except docker.errors.NotFound:
+            print(f"Volume {volume_name} not found.")
+
+
+def remove_network(network_name: str) -> None:
+    client = docker_client()
+    networks = client.networks.list(names=[network_name])
+    for network in networks:
+        print(f"Removing network {network.name}")
+        network.remove()
 
 
 def cluster_up(
     num_agents: int,
     port: int,
-    master_config_path: Optional[Path],
-    storage_host_path: Path,
+    master_config_path: Optional[pathlib.Path],
+    storage_host_path: pathlib.Path,
     cluster_name: str,
-    image_repo_prefix: Optional[str],
-    version: Optional[str],
+    image_repo_prefix: str,
+    version: str,
     db_password: str,
     delete_db: bool,
     gpu: bool,
     autorestart: bool,
-    auto_work_dir: Optional[Path],
+    auto_work_dir: Optional[pathlib.Path],
 ) -> None:
     cluster_down(cluster_name, delete_db)
     master_up(
         port=port,
         master_config_path=master_config_path,
         storage_host_path=storage_host_path,
-        master_name=cluster_name,
+        master_name=f"{cluster_name}_{MASTER_NAME}",
         image_repo_prefix=image_repo_prefix,
         version=version,
         db_password=db_password,
         delete_db=delete_db,
         autorestart=autorestart,
         cluster_name=cluster_name,
         auto_work_dir=auto_work_dir,
@@ -250,26 +375,48 @@
             gpu=gpu,
             autorestart=autorestart,
             cluster_name=cluster_name,
         )
 
 
 def cluster_down(cluster_name: str, delete_db: bool) -> None:
-    master_down(master_name=cluster_name, delete_db=delete_db)
+    master_down(
+        master_name=f"{cluster_name}_{MASTER_NAME}", delete_db=delete_db, cluster_name=cluster_name
+    )
     stop_cluster_agents(cluster_name=cluster_name)
 
 
-def logs(cluster_name: str, no_follow: bool) -> None:
-    docker_compose(["logs"] if no_follow else ["logs", "-f"], cluster_name)
+def logs(cluster_name: str, follow: bool) -> None:
+    def docker_logs(container_name: str) -> None:
+        client = docker_client()
+        try:
+            container = client.containers.get(container_name)
+        except docker.errors.NotFound:
+            return
+        log_stream = container.logs(stream=follow)
+        if not follow:
+            print(log_stream.decode("utf-8"))
+            return
+        log_line = next(log_stream)
+        while log_line:
+            print(log_line.decode("utf-8").strip())
+            log_line = next(log_stream)
+
+    master_name = f"{cluster_name}_{MASTER_NAME}"
+    db_name = f"{cluster_name}_{DB_NAME}"
+    master_thread = threading.Thread(target=docker_logs, args=(master_name,))
+    db_thread = threading.Thread(target=docker_logs, args=(db_name,))
+    db_thread.start()
+    master_thread.start()
 
 
 def agent_up(
     master_host: str,
     master_port: int,
-    agent_config_path: Optional[Path],
+    agent_config_path: Optional[pathlib.Path],
     agent_name: str,
     agent_resource_pool: Optional[str],
     image_repo_prefix: Optional[str],
     version: Optional[str],
     gpu: bool,
     autorestart: bool,
     cluster_name: str,
@@ -292,18 +439,14 @@
     if master_port == MASTER_PORT_DEFAULT:
         if "master_port" in agent_conf:
             del environment["DET_MASTER_PORT"]
             master_port = agent_conf["master_port"]
 
     if agent_resource_pool is not None:
         environment["DET_RESOURCE_POOL"] = agent_resource_pool
-    if image_repo_prefix is None:
-        image_repo_prefix = "determinedai"
-    if version is None:
-        version = determined.__version__
 
     _wait_for_master(master_host, master_port, cluster_name)
 
     if master_host == "localhost":
         master_host = get_proxy_addr()
     environment["DET_MASTER_HOST"] = master_host
 
@@ -313,53 +456,52 @@
     if labels is None:
         labels = {}
     labels["ai.determined.type"] = "agent"
 
     restart_policy = {"Name": "unless-stopped"} if autorestart else None
     device_requests = [GPU_DEVICE_REQUEST] if gpu else None
 
-    docker_client = docker.from_env()
-
+    client = docker_client()
     print(f"Starting {agent_name}")
-    docker_client.containers.run(
+    client.containers.run(
         image=image,
         environment=environment,
         init=init,
         mounts=mounts,
         volumes=volumes,
         network_mode="host",
         name=agent_name,
         detach=True,
         labels=labels,
         restart_policy=restart_policy,
         device_requests=device_requests,
     )
 
 
-def _kill_containers(containers: docker.models.containers.Container) -> None:
+def _kill_containers(names: Optional[List[str]] = None, labels: Optional[List[str]] = None) -> None:
+    filters = {}
+    if names:
+        filters["name"] = names
+    if labels:
+        filters["label"] = labels
+    client = docker_client()
+    containers = client.containers.list(all=True, filters=filters)
     for container in containers:
+        # Docker will match container names containing the string instead of strictly matching.
+        if names and container.name not in names:
+            continue
         print(f"Stopping {container.name}")
         container.stop(timeout=20)
         print(f"Removing {container.name}")
         container.remove()
 
 
 def stop_all_agents() -> None:
-    docker_client = docker.from_env()
-    filters = {"label": ["ai.determined.type=agent"]}
-    to_stop = docker_client.containers.list(all=True, filters=filters)
-    _kill_containers(to_stop)
+    _kill_containers(labels=["ai.determined.type=agent"])
 
 
 def stop_cluster_agents(cluster_name: str) -> None:
-    docker_client = docker.from_env()
-    labels = [f"determined.cluster={cluster_name}"]
-    filters = {"label": labels}
-    to_stop = docker_client.containers.list(all=True, filters=filters)
-    _kill_containers(to_stop)
+    _kill_containers(labels=[f"determined.cluster={cluster_name}"])
 
 
 def stop_agent(agent_name: str) -> None:
-    docker_client = docker.from_env()
-    filters = {"name": [agent_name]}
-    to_stop = docker_client.containers.list(all=True, filters=filters)
-    _kill_containers(to_stop)
+    _kill_containers(names=[agent_name])
```

## determined/estimator/_estimator_trial.py

```diff
@@ -814,15 +814,15 @@
         return metrics
 
 
 class EstimatorTrial(det.Trial):
     """
     By default, experiments run with TensorFlow 1.x. To configure your trial to
     use TensorFlow 2.x, set a TF 2.x image in the experiment configuration
-    (e.g. ``determinedai/environments:cuda-11.3-pytorch-1.12-tf-2.8-gpu-0.21.1``).
+    (e.g. ``determinedai/environments:cuda-11.3-pytorch-1.12-tf-2.8-gpu-0.21.2``).
 
     ``EstimatorTrial`` supports TF 2.x; however it uses TensorFlow V1
     behavior. We have disabled TensorFlow V2 behavior for ``EstimatorTrial``,
     so there is no need for you to disable it.
     """
 
     trial_controller_class = EstimatorTrialController
```

## determined/keras/_tf_keras_context.py

```diff
@@ -286,15 +286,14 @@
 
         self.dataset_initialized = True
         if (
             self.distributed.size == 1
             or not isinstance(dataset, tf.data.Dataset)
             or not shard_dataset
         ):
-
             if self.distributed.size > 1 and not shard_dataset:
                 logging.info("Dataset sharding skipped.")
             return dataset
 
         hvd.require_horovod_type("tensorflow.keras", "TFKerasTrialContext.wrap_dataset was called.")
         dataset = dataset.shard(hvd.size(), hvd.rank())
         logging.debug(f"Sharded dataset to index {hvd.rank()} of {hvd.size()}.")
```

## determined/keras/_tf_keras_trial.py

```diff
@@ -22,18 +22,26 @@
 from determined.common import check
 from determined.horovod import hvd
 from determined.tensorboard.metric_writers import tensorflow
 
 # In TF 2.6, we have to import some keras internals directly from `keras`.
 if version.parse(tf.__version__) >= version.parse("2.6.0"):
     from keras.callbacks import CallbackList, make_logs, set_callback_parameters
-    from keras.saving.hdf5_format import (
-        load_optimizer_weights_from_hdf5_group,
-        save_optimizer_weights_to_hdf5_group,
-    )
+
+    # TODO MLG-444 Migrate from legacy Keras hdf5 saving methods
+    if version.parse(tf.__version__) >= version.parse("2.11.0"):
+        from keras.saving.legacy.hdf5_format import (
+            load_optimizer_weights_from_hdf5_group,
+            save_optimizer_weights_to_hdf5_group,
+        )
+    else:
+        from keras.saving.hdf5_format import (
+            load_optimizer_weights_from_hdf5_group,
+            save_optimizer_weights_to_hdf5_group,
+        )
     from keras.utils.mode_keys import ModeKeys
 else:
     from tensorflow.python.keras.callbacks import CallbackList, make_logs, set_callback_parameters
     from tensorflow.python.keras.saving.hdf5_format import (
         load_optimizer_weights_from_hdf5_group,
         save_optimizer_weights_to_hdf5_group,
     )
@@ -969,15 +977,15 @@
     :meth:`build_training_data_loader`, and :meth:`build_validation_data_loader`).
     In most cases you should provide a custom :meth:`__init__` method as well.
 
     By default, experiments use TensorFlow 2.x. To configure your trial to use
     legacy TensorFlow 1.x, specify a TensorFlow 1.x image in the
     :ref:`environment.image <exp-environment-image>` field of the experiment
     configuration (e.g.,
-    ``determinedai/environments:cuda-10.2-pytorch-1.7-tf-1.15-gpu-0.21.1``).
+    ``determinedai/environments:cuda-10.2-pytorch-1.7-tf-1.15-gpu-0.21.2``).
 
     Trials default to using eager execution with TensorFlow 2.x but not with
     TensorFlow 1.x. To override the default behavior, call the appropriate
     function at the top of your code. For example, if you want to disable
     eager execution while using TensorFlow 2.x, call
     ``tf.compat.v1.disable_eager_execution`` after your import statements.
     If you are using TensorFlow 1.x in eager mode, please add
```

## determined/launch/deepspeed.py

```diff
@@ -24,25 +24,23 @@
 from determined.common.api import certs
 
 hostfile_path = None
 deepspeed_version = version.parse(deepspeed.__version__)
 
 
 def is_using_cuda() -> bool:
-
     val = os.getenv("CUDA_VISIBLE_DEVICES")
 
     if val is None or len(val.strip()) == 0:
         return False
     else:
         return True
 
 
 def is_nccl_socket_ifname_env_var_set() -> bool:
-
     val = os.getenv("NCCL_SOCKET_IFNAME")
 
     if val is None or len(val.strip()) == 0:
         return False
     else:
         return True
```

## determined/pytorch/_data.py

```diff
@@ -51,22 +51,22 @@
         f"not, you can disable this check by calling {disable_repro_check_method} at some point "
         "in your trial's __init__() method."
     )
 
 
 class DataLoader:
     """
-    DataLoader is meant to contain a user's `Dataset`, configuration for
+    DataLoader is meant to contain a user's ``Dataset``, configuration for
     sampling data in batches, and performance configuration like
     multiprocessing.
 
     The __init__ function determines the defaults in the same way as a
-    `torch.utils.data.DataLoader` would, so the behavior should be familiar.
-    However, the `torch.utils.data.Dataloader` that is used for training and
-    validation is not created until `get_data_loader(...)` is called. This is
+    ``torch.utils.data.DataLoader`` would, so the behavior should be familiar.
+    However, the ``torch.utils.data.Dataloader`` that is used for training and
+    validation is not created until ``get_data_loader(...)`` is called. This is
     done so that Determined can ensure that sampling restarts from the right location
     and distributed sampling is handled correctly.
 
     Note that the arguments are from PyTorch.
 
     Arguments:
         dataset (Dataset): dataset from which to load the data.
@@ -95,21 +95,21 @@
         timeout (numeric, optional): if positive, the timeout value for collecting a batch
             from workers. Should always be non-negative. (default: ``0``)
         worker_init_fn (callable, optional): If not ``None``, this will be called on each
             worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as
             input, after seeding and before data loading. (default: ``None``)
         generator (torch.Generator, optional): If not ``None``, this RNG will be used
             by RandomSampler to generate random indexes and multiprocessing to generate
-            `base_seed` for workers. (default: ``None``)
+            ``base_seed`` for workers. (default: ``None``)
         prefetch_factor (int, optional, keyword-only arg): Number of samples loaded
             in advance by each worker. ``2`` means there will be a total of
             2 * num_workers samples prefetched across all workers. (default: ``2``)
         persistent_workers (bool, optional): If ``True``, the data loader will not shut down
             the worker processes after a dataset has been consumed once. This allows to
-            maintain the workers `Dataset` instances alive. (default: ``False``)
+            maintain the workers ``Dataset`` instances alive. (default: ``False``)
     """
 
     def __init__(
         self,
         dataset: Dataset,
         batch_size: Optional[int] = 1,
         shuffle: bool = False,
```

## determined/pytorch/_metric_utils.py

```diff
@@ -177,15 +177,14 @@
 def _log_tb_metrics(
     writer: Any,
     metric_type: str,
     steps_completed: int,
     metrics: Dict[str, Any],
     batch_metrics: Optional[List[Dict[str, Any]]] = None,
 ) -> None:
-
     if metric_type == "val":
         logging.debug("Write validation metrics for TensorBoard")
     elif metric_type == "train":
         logging.debug("Write training metrics for TensorBoard")
     else:
         logging.warning("Unrecognized tensorboard metric type: " + metric_type, stacklevel=2)
```

## determined/pytorch/_pytorch_context.py

```diff
@@ -298,16 +298,16 @@
         average_aggregated_gradients: Optional[bool] = None,
     ) -> torch.optim.Optimizer:
         """Returns a wrapped optimizer.
 
         The optimizer must use the models wrapped by :meth:`wrap_model`. This function
         creates a ``horovod.DistributedOptimizer`` if using parallel/distributed training.
 
-        `backward_passes_per_step` can be used to specify how many gradient aggregation
-        steps will be performed in a single `train_batch` call per optimizer step.
+        ``backward_passes_per_step`` can be used to specify how many gradient aggregation
+        steps will be performed in a single ``train_batch`` call per optimizer step.
         In most cases, this will just be the default value 1.  However, this advanced functionality
         can be used to support training loops like the one shown below:
 
         .. code-block:: python
 
             def train_batch(
                 self, batch: TorchData, epoch_idx: int, batch_idx: int
@@ -545,15 +545,15 @@
         keep_batchnorm_fp32: Optional[Union[bool, str]] = None,
         master_weights: Optional[bool] = None,
         loss_scale: Optional[Union[float, str]] = None,
         cast_model_outputs: Optional[torch.dtype] = None,
         num_losses: Optional[int] = 1,
         verbosity: Optional[int] = 1,
         min_loss_scale: Optional[float] = None,
-        max_loss_scale: Optional[float] = 2.0 ** 24,
+        max_loss_scale: Optional[float] = 2.0**24,
     ) -> Tuple:
         """
         Configure automatic mixed precision for your models and optimizers using NVIDIA's Apex
         PyTorch extension. Note that details for ``apex.amp`` are handled automatically within
         Determined after this call.
 
         This function must be called **after** you have finished constructing your models and
@@ -591,18 +591,19 @@
                 ``loss_id`` argument to ``amp.scale_loss``, enables Amp to use a different
                 loss scale per loss/backward pass, which can improve stability.
                 If ``num_losses`` is left to 1, Amp will still support multiple losses/backward
                 passes, but use a single global loss scale for all of them.
             verbosity (int, default=1):  Set to 0 to suppress Amp-related output.
             min_loss_scale (float, default=None):  Sets a floor for the loss scale values that
                 can be chosen by dynamic loss scaling.  The default value of None means that no
-                floor is imposed. If dynamic loss scaling is not used, `min_loss_scale` is ignored.
+                floor is imposed. If dynamic loss scaling is not used, ``min_loss_scale`` is
+                ignored.
             max_loss_scale (float, default=2.**24):  Sets a ceiling for the loss scale values
                 that can be chosen by dynamic loss scaling.  If dynamic loss scaling is not used,
-                `max_loss_scale` is ignored.
+                ``max_loss_scale`` is ignored.
 
         Returns:
             Model(s) and optimizer(s) modified according to the ``opt_level``.
             If  ``optimizers`` args were lists, the corresponding return value will
             also be a list.
         """
         if not enabled or not self._managed_training:
```

## determined/pytorch/_pytorch_trial.py

```diff
@@ -189,15 +189,14 @@
         searcher_metric_name: Optional[str],
         checkpoint_policy: str,
         step_zero_validation: bool,
         max_length: Optional[TrainUnit],
         det_profiler: Optional[profiler.ProfilerAgent],
         global_batch_size: Optional[int],
     ) -> None:
-
         if not isinstance(trial_inst, PyTorchTrial):
             raise TypeError("PyTorchTrialController requires a PyTorchTrial.")
         self.trial = trial_inst
         self.context = context
         self.core_context = self.context._core
         self.prof = det_profiler or profiler.DummyProfilerAgent()
         self.context._set_determined_profiler(self.prof)
@@ -1577,16 +1576,16 @@
         """
         pass
 
     def get_batch_length(self, batch: Any) -> int:
         """Count the number of records in a given batch.
 
         Override this method when you are using custom batch types, as produced
-        when iterating over the `DataLoader`.
-        For example, when using `pytorch_geometric`:
+        when iterating over the :py:class:`determined.pytorch.DataLoader`.
+        For example, when using ``pytorch_geometric``:
 
         .. code-block:: python
 
             # Extra imports:
             from determined.pytorch import DataLoader
             from torch_geometric.data.dataloader import Collater
```

## determined/pytorch/_reducer.py

```diff
@@ -153,15 +153,15 @@
         This will be called after per_slot_reduce has finished (even when there is only one worker).
 
         The per_slot_metrics will be a list containing the output of per_slot_reduce() from each
         worker.
 
         The return value should either be:
            -  A dict mapping string metric names to metric values, if the call to
-              context.wrap_reducer() omitted the `name` parameter, or
+              context.wrap_reducer() omitted the ``name`` parameter, or
            -  A non-dict metric value if the call to context.wrap_reducer() had name set to a string
               (an error will be raised if a dict-type metric is returned but name was set).
 
         This will be called after per_slot_reduce.
         """
         pass
```

## determined/searcher/_search_method.py

```diff
@@ -70,28 +70,19 @@
     USER_CANCELED = "USER_CANCELED"
     INVALID_HP = "INVALID_HP"
 
     @classmethod
     def _from_bindings(
         cls, bindings_exited_reason: bindings.v1TrialExitedEarlyExitedReason
     ) -> "ExitedReason":
-        if (
-            bindings_exited_reason
-            == bindings.v1TrialExitedEarlyExitedReason.EXITED_REASON_INVALID_HP
-        ):
+        if bindings_exited_reason == bindings.v1TrialExitedEarlyExitedReason.INVALID_HP:
             return cls.INVALID_HP
-        if (
-            bindings_exited_reason
-            == bindings.v1TrialExitedEarlyExitedReason.EXITED_REASON_USER_REQUESTED_STOP
-        ):
+        if bindings_exited_reason == bindings.v1TrialExitedEarlyExitedReason.USER_REQUESTED_STOP:
             return cls.USER_CANCELED
-        if (
-            bindings_exited_reason
-            == bindings.v1TrialExitedEarlyExitedReason.EXITED_REASON_UNSPECIFIED
-        ):
+        if bindings_exited_reason == bindings.v1TrialExitedEarlyExitedReason.UNSPECIFIED:
             return cls.ERRORED
         raise RuntimeError(f"Invalid exited reason: {bindings_exited_reason}")
 
 
 class Operation:
     """
     Abstract base class for all Operations
```

## determined/searcher/_search_runner.py

```diff
@@ -54,20 +54,20 @@
                 f" {event.trialExitedEarly.exitedReason})"
             )
             if event.trialExitedEarly.exitedReason is None:
                 raise RuntimeError("trialExitedEarly event is invalid without exitedReason")
             request_id = uuid.UUID(event.trialExitedEarly.requestId)
             if (
                 event.trialExitedEarly.exitedReason
-                == bindings.v1TrialExitedEarlyExitedReason.EXITED_REASON_INVALID_HP
+                == bindings.v1TrialExitedEarlyExitedReason.INVALID_HP
             ):
                 self.state.trial_progress.pop(request_id, None)
             elif (
                 event.trialExitedEarly.exitedReason
-                == bindings.v1TrialExitedEarlyExitedReason.EXITED_REASON_UNSPECIFIED
+                == bindings.v1TrialExitedEarlyExitedReason.UNSPECIFIED
             ):
                 self.state.failures.add(request_id)
             operations = self.search_method.on_trial_exited_early(
                 self.state,
                 request_id,
                 exited_reason=searcher.ExitedReason._from_bindings(
                     event.trialExitedEarly.exitedReason
@@ -150,21 +150,21 @@
                         if event.experimentInactive:
                             logger.info(
                                 f"experiment {self.state.experiment_id} is "
                                 f"inactive; state={event.experimentInactive.experimentState}"
                             )
                             if (
                                 event.experimentInactive.experimentState
-                                == bindings.experimentv1State.STATE_COMPLETED
+                                == bindings.experimentv1State.COMPLETED
                             ):
                                 self.state.experiment_completed = True
 
                             if (
                                 event.experimentInactive.experimentState
-                                == bindings.experimentv1State.STATE_PAUSED
+                                == bindings.experimentv1State.PAUSED
                             ):
                                 self._show_experiment_paused_msg()
                             else:
                                 experiment_is_active = False
                             break
 
                         operations = self._get_operations(event)
```

## Comparing `determined-0.21.1rc4.dist-info/METADATA` & `determined-0.21.2rc0.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: determined
-Version: 0.21.1rc4
+Version: 0.21.2rc0
 Summary: Determined Deep Learning Training Platform
 Home-page: https://determined.ai/
 Author: Determined AI
 Author-email: hello@determined.ai
 License: Apache License 2.0
 Classifier: License :: OSI Approved :: Apache Software License
 Requires-Python: >=3.6
@@ -31,14 +31,12 @@
 Requires-Dist: python-dateutil
 Requires-Dist: pytz
 Requires-Dist: tabulate (>=0.8.3)
 Requires-Dist: ruamel.yaml (>=0.15.29)
 Requires-Dist: docker[ssh] (>=3.7.3)
 Requires-Dist: google-api-python-client (>=1.12.1)
 Requires-Dist: paramiko (>=2.4.2)
-Requires-Dist: docker-compose (>=1.13.0)
 Requires-Dist: tqdm
 Requires-Dist: appdirs
-Requires-Dist: websocket-client (<1)
 Requires-Dist: analytics-python
 
 See https://docs.determined.ai/ for more information.
```

## Comparing `determined-0.21.1rc4.dist-info/RECORD` & `determined-0.21.2rc0.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 determined/__init__.py,sha256=YP_VEqU7NSA2jILM39YzVkXHiEiHElFWAvQMqF8hA0A,1131
-determined/__version__.py,sha256=8_Mdrvlc9TXJi741ntr-mT8S5Nq74-HGWInpGtjb35U,27
+determined/__version__.py,sha256=UHXTN33vnqwFsI22yOFoomuPOMawuPv1zgbFL0cRCPs,27
 determined/_env_context.py,sha256=vG1rREr-gp-rVjGUn_5mzKhRxC8qv0RwpNY4rABT_h4,1664
 determined/_execution.py,sha256=sFxJSNK3WnFxVfwCL_O8ZYOKjIvnPVi3pCQYq99Xy5w,8804
 determined/_experiment_config.py,sha256=kkHvFHQzkI3HlckUozKwjfaieWIgeEot9jyPtvQuLjs,2779
 determined/_import.py,sha256=czaz1MQU8w2tfkeD3rIAVDpRyS_7lH0K9Sn-_zm2luQ,4781
 determined/_info.py,sha256=Xm-H3VpeZwYWRPHMmg2Ig1jSeKVU5jekCnuUiqrzrXA,14901
 determined/_tf_rng.py,sha256=Z96_dEFOB2BRWyrthYN1A2FKxGe-fi0XKS_AEupLAnM,1272
 determined/_trial.py,sha256=5GAT_OKfXumm3ykuSRxY8ImTZAUTUBZ9agBVpcMjIEw,1214
@@ -14,126 +14,126 @@
 determined/gpu.py,sha256=WwjXm7jWoLYCxwhPBizcZe_lWMZ3hCFy0PoHG3y9NS0,5431
 determined/horovod.py,sha256=mCY6Ori3zjHQc5CROrkG7PwjJwtC_tAdycIoUOdd9qI,6182
 determined/ipc.py,sha256=QBYRJAW1NRs_t_5kXigcm9SaSGsnVyyW8SHTnk3X7fo,19374
 determined/load.py,sha256=4dSCrArKAatIPhyDT9ofzwBFZ2GC1_0Lvj5jYymFcPc,3065
 determined/monkey_patch.py,sha256=bw-kPbWew541z1G4W4HA7q-zQ5GJwKoysDC4bKk5q9Q,737
 determined/profiler.py,sha256=u9k319yKX2tc5NaY82z2NDZ1eKJPdEcjhn7ySWnDk_M,39670
 determined/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-determined/util.py,sha256=RHdkuXg0OzFqCtQtTqvQ-TGpK4CAbePk9KX861do5hs,15913
+determined/util.py,sha256=2bQn9GtL1L1Ydc3wfCmlvO2LNbQBoorTuFZEvRmaXEs,15909
 determined/workload.py,sha256=UG-9Dt5vGf3mfz4No77o-XjBAXGc4TmUfE1BIYJMveE,6578
 determined/cli/__init__.py,sha256=bQTdOCnCglaql6QQf6i6bUVBCujBAnfpXZItfXmj-78,476
 determined/cli/__main__.py,sha256=xd-S-mXqQ92a37G50DzPszQZ5y1FYXMcQGR52GnTQVA,75
 determined/cli/_util.py,sha256=JAGG_HGeBRRELnOkE9XmGFe0fQtK3ttoZLi0Bt7Jdyo,3423
-determined/cli/agent.py,sha256=6o-RqR3tsDBStI83Zp2g6h60k595ITHA46lB_SLu_1E,9636
-determined/cli/checkpoint.py,sha256=9mpwHxAdxSFvQvNPch1IxzibIPfNhVg--b68TpvoeyA,6425
+determined/cli/agent.py,sha256=BjdjsA5sgHtURealNwJWPn0o1FB9RKoK30jqFT6BLog,9783
+determined/cli/checkpoint.py,sha256=x6WE71x59AQoDo8zQuwC_-oAgcaKon5HDmzElbZCa3E,6409
 determined/cli/cli.py,sha256=Cld8TP59_-2u_0gt5ez39jW-hQtaadPkKxWTsSANzOI,12433
 determined/cli/command.py,sha256=8atAB9exp-6KTD4uAT7NyV6Jj2fRtNtcGdTh0R1uO1w,13446
 determined/cli/dev.py,sha256=FJaNvnJYeMa4c59ZgOHmUFBOpEdc1V_yqI-6wFLdT7U,2354
 determined/cli/errors.py,sha256=VdmkMdIuW4bmU-kFeiPTu6SSTDzBQHxDsEGoKF7ePWY,739
-determined/cli/experiment.py,sha256=p6YuLt6Dd53Nk3oWam4228bNh8btQxqhVYvb5r0MgcE,49043
-determined/cli/job.py,sha256=3uJNXfERdUpgWwEzGYm0WEw4TQR3Tebb9GaxTPulTqk,8386
+determined/cli/experiment.py,sha256=OVhhu4_ffseREetVwGQfB7T1aiA8DDhd4hsCaS5JZ74,49929
+determined/cli/job.py,sha256=sJNUS_Ox0aBMosGEAPHaHCIRzuxjaA2DAm9NwKf4FWA,8326
 determined/cli/master.py,sha256=L2woDKkfLV0GYiGyhBikAWV1p4WI9E7xkEmejzmpRwc,2110
 determined/cli/model.py,sha256=GcvittkFNWTrRt192jM8u65_YHzfqq5q2dcBlfSAHAw,8693
-determined/cli/notebook.py,sha256=RI6by3TaYqiVygevYHaXN0hrwS2e1LiNXi0iymAkyVs,5650
+determined/cli/notebook.py,sha256=Zb1MHgRLiMOaAh6RIcpxyTmfkdSpzQUZJNpDzjyIshY,5635
 determined/cli/oauth.py,sha256=dN5gSlQAO1V8WXiULqjBefA9xZXuqqibemaXXgQHTN4,1761
-determined/cli/project.py,sha256=-_cHbWyYYYGaSeChMFy_33aVeLGuuu28KM1AJ8VeBT4,12212
-determined/cli/proxy.py,sha256=sKdX4SPJ-WJLTx8S9kjKRx71stLV_Q9FsR9j7Mu8SN0,2826
+determined/cli/project.py,sha256=SJ4kQwLZldLfY8F6Xas7A-LqfmtjPUHqeNXA1N1_B9k,12180
+determined/cli/proxy.py,sha256=pWuxspqZ7syoPmmpoQuM5CY1U5Hu065U4fBXWkhv24Q,2808
 determined/cli/rbac.py,sha256=1HrQ-0AbMWVPvBl9dzY-UzE69p5Lhd-_xAHiICSGCUg,18745
-determined/cli/remote.py,sha256=I0d7cmzU4tW92i2n0Mfyi2uO7JkTHMGeJmgIJElSmlI,3527
+determined/cli/remote.py,sha256=4VYwJniNNyzmnNY-5_aVouJ_jGM5ibhdXKryOYQcejQ,3783
 determined/cli/render.py,sha256=-AbR15R8jqTcNW1xn7txCPGKDkPsx4KqSoQyNq9pQ6o,4484
 determined/cli/resources.py,sha256=dJuysbZeAVwShM5bdhIeu2PX5GqBXin8VSDL0N11l9M,2386
-determined/cli/shell.py,sha256=CLinTs6J94eILcD8XwoMDmvNhv8_qXzViz7o_mqaK_0,9719
+determined/cli/shell.py,sha256=lNVpt9smd2ZPKW46_bETXxp7jQpFyvTtn5RiJiFvWqw,9718
 determined/cli/sso.py,sha256=1F3Zr_DY49_IwXV6H4MVxuzrWgGkvgTv55jOckEKVNI,5105
-determined/cli/task.py,sha256=XdoPVcQHsK93jDJ-HYOUl9vGxHVhhUEswtUdpae3Szk,5591
+determined/cli/task.py,sha256=SSRsBy6tQf0wdvAI2NgAga9CWnYcRszLPqS0neAieag,6363
 determined/cli/template.py,sha256=hEbFk5v1YzUEKPkNpXDU2sOt2ygoXX1wYmCwuMaBQx0,2716
-determined/cli/tensorboard.py,sha256=BdmVVyNnoI9FJOk1l9XUhgtuiVN5Co6M4n9ZQ99xicI,6733
+determined/cli/tensorboard.py,sha256=EwFeSeIKV5183qbwrJFbpkX_Mk6HHkSAM_F4QPgoBkM,6718
 determined/cli/top_arg_descriptions.py,sha256=ql_tbzsAmniYKlZ3YQ4bbCNKtDDmkp9Rf8V5dnED3c8,135
-determined/cli/trial.py,sha256=3VzQG4mDfpvMglEtWbwe6VMMOohIApLT0V9P6M1iL_A,14756
+determined/cli/trial.py,sha256=oSkFzH5aXrgEslHC41tyni0ESpHbzOMZJbNLVj5Obio,15065
 determined/cli/tunnel.py,sha256=wix1PRPsjMuiH3GAvB1gP_8msYxrq0heypQ2t85msKk,8899
 determined/cli/user.py,sha256=xhGTSgd2sT1slsC0ZyfV7ENeH8z8xqwT-c97aQBpKs0,6560
 determined/cli/user_groups.py,sha256=-amrLLqra9lFYE1ae7wE4eXEJTkkgQgSxQmR7Ty_1go,9727
 determined/cli/version.py,sha256=PaKvNqv3SWlJy8jmX33pqoG5ylVINVPRHNbUgPU3mlA,2557
-determined/cli/workspace.py,sha256=Thhm1FN0SNz5Ofy4g-4N4ojZs8-EqirgBlQHVTA-wXI,15211
+determined/cli/workspace.py,sha256=H5FmczuQhU9u-mB_j7b_BnUF5vGDHLmNST5_MLYkSFU,15179
 determined/common/__init__.py,sha256=c9lF3AufKAwXW8_PMl3lbVtBN6yXmPzxI-8IsbS5RTE,352
 determined/common/_logging.py,sha256=q8wQXSRXRv0dAM5QaS8b374juPz2V3txi786PdCgSRM,498
 determined/common/check.py,sha256=7eK_uJj86VR473TCeYUgultitGJGWEGDKXC7PlU_pVM,8593
 determined/common/constants.py,sha256=nlIn6UueZ9-aIsWjDax2KbpDn7nJo6FE-WAzxcLfxvo,2255
 determined/common/context.py,sha256=aKuMa8VrkxMpdUpkEK-PUX-dFTaDjqKtl2xA5vU-zrk,8213
 determined/common/declarative_argparse.py,sha256=H440Ipryf1bBKUsdGD8QVEu1c8HF3YGJAc-MBFSpS5U,7359
 determined/common/requests.py,sha256=M6NYyQDiXxHq0lHc_OcTrnC4cGQYuXT57V8JNSIon14,1662
 determined/common/util.py,sha256=Jnf-40P5Kf8e2hixJ5Qf0kVwVck33eSlDU0pHBm-1zE,6349
-determined/common/api/__init__.py,sha256=P0VpjaVaAEM9LWGBVxyFaR79zMD_bWj_14VbXI_XfII,806
+determined/common/api/__init__.py,sha256=pfqNnfzP2FBdVafiyTQu8Yw5SVDRz00509HPKdzq4wM,799
 determined/common/api/_session.py,sha256=k6j7joLzPUt-nfz99LFswd_FwQiMrWzw_qunBDO3TTI,3383
-determined/common/api/_util.py,sha256=oYbQnQBR5EJIeQiz76fvfvY76PogGZwJzaQokNPDN5o,2855
+determined/common/api/_util.py,sha256=bWa9qzxN2hHmyNfqndVWV8kVXwEG4I5mSCeNn6IMX2I,2840
 determined/common/api/analytics.py,sha256=ijoLQ1XHYty88SBpYjTUM-4793lxFfyLM9rnEE15Ghk,1450
 determined/common/api/authentication.py,sha256=b1mOtuvlS0cp4jwTkJRVcdMUW_fhTs9l3r13Dw_Bn2Y,15676
-determined/common/api/bindings.py,sha256=GBKOTvRqAuwmdfF8dmcUwEJHHiXaRzCQoRXe5DUF6do,597398
+determined/common/api/bindings.py,sha256=fUhKiPpbX6jP0L49NnwhtjsDgXBArhS3U6jBhwa30G4,611406
 determined/common/api/certs.py,sha256=sltCyZYaGRxnS8LtBpDAMgGTBHaDF_9amKn7UID4-nw,7415
 determined/common/api/errors.py,sha256=WBftwTV5A0CW8erUGtMN1wGYIsVoQjK31FlTFxk0hto,2555
-determined/common/api/logs.py,sha256=AH8LV5A3s6wNPnJRG7fWqa37KX3yqM3I3vHojyBGMJ8,3970
+determined/common/api/logs.py,sha256=GbhR0x9RRqNd1AgeMdX8q91vwDAoEO7izFYGv1ab1XM,3438
 determined/common/api/metric.py,sha256=dR6Opp8sZjRsB3fhZkLKYxLrGkiq1se1Pxsf39udtNw,247
 determined/common/api/profiler.py,sha256=Bz8PkIAV_-3KhBZ2LF5EOqQLDVVYu51hwzJicATuCM0,2620
 determined/common/api/request.py,sha256=QUVsdp8ogN2CAbEs9lL_obtarWPKemfFm6mnv3DK4Dw,10384
 determined/common/api/checkpoint/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 determined/common/api/checkpoint/torch_load.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 determined/common/experimental/__init__.py,sha256=6WWT1MwtmYbxfotWstMocQYwbqjmLnTBTNkwra9I_z0,502
-determined/common/experimental/determined.py,sha256=Q30fh9cmfPs2cETtVkymwNrWPGotP55YUXpRkTngCi0,15863
-determined/common/experimental/experiment.py,sha256=T2Mf5wPkLNX2WA2_EFpukeR6G6NJG7DhGk4He8XeRe0,11536
-determined/common/experimental/model.py,sha256=40jcFM2M7Df_GEGFeef13R_T-Vn9k-KxXKv6XomWOtQ,12748
-determined/common/experimental/oauth2_scim_client.py,sha256=xZzYxAP1nY3dedR1GVoS0cLIIbKU-JxD4BpJAbHVggQ,307
+determined/common/experimental/determined.py,sha256=EaauuLUqNTR_yBearZfSXuXoVh8OYi235gLHiIkag-g,15864
+determined/common/experimental/experiment.py,sha256=yKB_Wkgoe_9XKWMg61qnPdCtFWKG-OEzO9WcwntSDl8,11398
+determined/common/experimental/model.py,sha256=6yxsrjQOYv_bTDvi5xw8WNfAv7l4UHgl2bVEJLpRP-k,12639
+determined/common/experimental/oauth2_scim_client.py,sha256=m1wWw4M_QRIyYXc2CC5Mi6aeyaLSIGrAr9wig4bV8qc,306
 determined/common/experimental/session.py,sha256=bcqZPDrznB_BupdqziJCpzvUuxsM4OaowXR7IIcRxs4,466
-determined/common/experimental/trial.py,sha256=X5TMJnfmLrlnfcLvPOH7wltWjojvQMeNZDx9iPQ0CXw,15977
+determined/common/experimental/trial.py,sha256=vZfKp5Sot5HNGfXsfl5ImkQinnVDgfiNdzA1VL8o_9o,15743
 determined/common/experimental/user.py,sha256=WbDJyBM87bWnE5UYUKdOZlUUUrOoQUoJaBTqREhHrO8,3503
 determined/common/experimental/checkpoint/__init__.py,sha256=aoILZ_tqIFDai-EadxakaCBVkNLEzLBTw7_7X0l1R4I,125
-determined/common/experimental/checkpoint/_checkpoint.py,sha256=kQSEaYfoP7XQFOU843NSJaAhBUF8JVLl6zzIEhkACb8,13791
+determined/common/experimental/checkpoint/_checkpoint.py,sha256=3elyfDIQ_R4DqQV6Oil1wTScXVeDxX-gr-tg26sQkzI,13755
 determined/common/storage/__init__.py,sha256=fKv8g8rTvIm40VCTMxOTYpUNeG09xfZ_oy5rT2m1Kwg,3868
 determined/common/storage/azure.py,sha256=ABXIr4sv2fQSrSfKIQ1KS8M-SRCzmOwgof-upwtB5TQ,3822
 determined/common/storage/azure_client.py,sha256=-d_-VhFNagZSzj8LKmO33wdqzxoDjlficVXjL2nUhjE,3514
 determined/common/storage/base.py,sha256=gaj3ysQH56ZYLo9EsJowkGlTN5EZkLdk5uZGmdGFL2k,7152
 determined/common/storage/boto3_credential_manager.py,sha256=pKb5OabIB53P-Q28Ba-_Vf9LzUgIcgaFhtfBTmDTyqY,4447
 determined/common/storage/cloud.py,sha256=gfv026i7syfE3rVS-1_bFXEGixagKZ7DeopG4l6DNF4,1014
 determined/common/storage/gcs.py,sha256=8-A37OF7pkDViZKWYkO55CI54_eghgEGcto-PAvHfFw,5789
 determined/common/storage/hdfs.py,sha256=QVZcRiNvLpw1vfwZ4UziaVSmy3Lwq4es1RzYcI5nX-c,2070
 determined/common/storage/s3.py,sha256=Q1BEmZJ-rKTnDG_WH8sGuPJ-JFeERH9-HOZ6Ckpc3Xs,6195
 determined/common/storage/shared.py,sha256=NjIAzl09iw4UTkZWUGnFmhBzvzLm946Cu5PqfrbfXAo,7816
 determined/core/__init__.py,sha256=DojWVPUz2esaYHa6Ew1ul4pDG5Kw5tM2jxLGb_HyW68,747
-determined/core/_checkpoint.py,sha256=ZQwaDOFr0e4iPkT2V2efA27PjtiEh__mkOBkNgqU7Rw,29477
+determined/core/_checkpoint.py,sha256=b7K-BE1_GVRuRkKYVmXSLhW5D1JSljHJIMvX2nWoPDo,29468
 determined/core/_context.py,sha256=gaMCWJNL4pTWyjaXPOn7-GWD3IyDPPOabUdtl0ZdsLY,10130
 determined/core/_distributed.py,sha256=ittb2-UpitI0vD4nMTKlYd86aB4A4eX-dhDyOSxjfv4,16194
 determined/core/_preempt.py,sha256=PRSTglEO5Rrf-QDm0HLNvuZYV2XDZGwifSGHMj6eVqQ,12738
 determined/core/_searcher.py,sha256=3xHycD5l1UPALTgukcUckjUVGtIo5me5EzY7XEPIAPk,15041
 determined/core/_tensorboard_mode.py,sha256=9A9FKYB77F26wkDTe4LNJWKBn9_4Th3cFzCMWjkl7Lc,932
 determined/core/_train.py,sha256=vfXmBAYCplIKiqRfgarfxnmQxcmseE8dlZSmoPrGYHg,10462
 determined/deploy/__init__.py,sha256=ws1TqRk5eBmOWyQ2zS5wL7-_-QU4pbC4ZJBapG7OvR4,40
 determined/deploy/cli.py,sha256=CR6S8nnR47O0p6wPfzvI0RvqApFo9g69swELLc_7bCk,1057
 determined/deploy/errors.py,sha256=6PPO8hrAczDcv3pklrGJRG-8bE1ydHP8BmGHx5T0UOg,94
 determined/deploy/healthcheck.py,sha256=8d_YE6I17pndG9PWDBM1Gs3AKxHUzSUFixPExhFPdDM,1472
 determined/deploy/aws/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-determined/deploy/aws/aws.py,sha256=9jatbwYynD0patnQADuJRw8kyK67ZdfqHyXhhXlicxM,19016
+determined/deploy/aws/aws.py,sha256=V8YNjk5h4Iv2k_Cf8oPlGxFv8jb6QOAZaua2JJAGXls,19015
 determined/deploy/aws/cli.py,sha256=QbWQYTtpLrfovyw6wPFcwaThk09aMYnXHx8sAkznaqA,24035
 determined/deploy/aws/constants.py,sha256=V5aOrV8SvW_ZrNZO4IvwGBB9L60FLELhtkSVR6io_1w,2998
-determined/deploy/aws/gen_vcpu_mapping.py,sha256=sZZKukS41pgEZC6aen2XxUkncB74GU38wqs1fBDUutk,1396
+determined/deploy/aws/gen_vcpu_mapping.py,sha256=XmbbozPADD7RxXhEc1Nkyqm7cElpCgDM_hYo4HB3ezg,1420
 determined/deploy/aws/master_config_inject.py,sha256=_ISaZQIfvDsAyOeDzA7ssE40s6RA-7kMJxqgFiu4k3w,2201
-determined/deploy/aws/preflight.py,sha256=Lki-Lpi0IKyi7L_Alm0zHhDKvZphMGAKmy4O0GhiDs0,4933
+determined/deploy/aws/preflight.py,sha256=M5T-DYr_wkn2mSjHj2irjq80Br5ODKBmQ7_kTQ9ac1w,4911
 determined/deploy/aws/vcpu_mapping.yaml,sha256=czutxPBg3wIocgCYIKTiv1u8xZtH61hFFqiVig74tPM,17324
 determined/deploy/aws/deployment_types/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 determined/deploy/aws/deployment_types/base.py,sha256=4fv1ivcYs43L5qEcG99uiJmfMig4b4gyc7-uhuN1xos,5624
 determined/deploy/aws/deployment_types/govcloud.py,sha256=BjLMhQy9QreojfjBacO2Lg-g-fIVvq4cSDQc_HsDDC4,1593
 determined/deploy/aws/deployment_types/secure.py,sha256=E-EHO1PnlZoi4dXf17PLhSE18ceRjPklTf__2PsHGgQ,3094
 determined/deploy/aws/deployment_types/simple.py,sha256=tNzqZyyWSjkHaAi8OHJJ10B6wTHN4SclOwrx3RHv4BY,1349
 determined/deploy/aws/deployment_types/vpc.py,sha256=4k4pb8ehXO_INC88KMhvwPdqzOI3U4RQtyqpR8n5gRo,1400
 determined/deploy/aws/templates/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-determined/deploy/aws/templates/efs.yaml,sha256=AFkzBpV1VMSlA8DFbm8178fCANR1p6CCoK8V6VRVDfI,29259
-determined/deploy/aws/templates/fsx.yaml,sha256=zxW3Akrcl9Serv3vihQbdOmu_NqnSpSaZoBCZHWqnK8,29969
-determined/deploy/aws/templates/govcloud.yaml,sha256=S33nGuqeDlIMugXHkFXdb7HwOCOukGT2IjWQ8SdiIKU,23518
-determined/deploy/aws/templates/secure.yaml,sha256=dxKj-WY6kykUa1dHl39nN25Shw6miYql2F0szV5nlsI,29153
-determined/deploy/aws/templates/simple.yaml,sha256=GHWGki6UnFW-6Lvsk333xMK1isCgvUttLsryZCvnzmY,24926
+determined/deploy/aws/templates/efs.yaml,sha256=bWIGQzXY3hjuTHY7LBsj_XJXm3nXT0LS6_cf2V15BLg,29259
+determined/deploy/aws/templates/fsx.yaml,sha256=81PQ-qubAYL9oun_1rDL8MhukGxu03auVRm-EkoBNU8,29969
+determined/deploy/aws/templates/govcloud.yaml,sha256=LTE0w4OOPSuFtxPEqo5tdvPskrva-vPNgn-vh0L4yWI,23518
+determined/deploy/aws/templates/secure.yaml,sha256=YRiFj-9gmDw_aALUL4gFDnNHk3Sxtnw0dziridyJvcg,29153
+determined/deploy/aws/templates/simple.yaml,sha256=OinaERy8ACzGZSOfkF9lfA6RefOMwgh_So1O3khQvt0,24926
 determined/deploy/gcp/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 determined/deploy/gcp/cli.py,sha256=Ww4a7FSRSDVFy8Qd01oqaZHG6O3BewOula9SqjcsyuU,19015
-determined/deploy/gcp/constants.py,sha256=hYJ0q860WpHOsUgx9bu8FMYqfC753-PBaU_Lmuv2L_g,773
+determined/deploy/gcp/constants.py,sha256=Kv91xkN70zY1dYDSkzEWyOwHYdyRhzsbvKfypBuIESA,772
 determined/deploy/gcp/gcp.py,sha256=f_Z5roqcXDQzYFH5OZhHFB6bzdkrAAwpZFCcSSQYiXY,12420
 determined/deploy/gcp/preflight.py,sha256=pITWn2fxVKr154OVJA4iPnvYEnNuZVd03dPyaGDX8IM,2380
 determined/deploy/gcp/terraform/main.tf,sha256=5VaCs9G0Fc4U4ypATT_6Ra9tasz398XdrkhIYTdcDNk,5376
 determined/deploy/gcp/terraform/master.yaml.tmpl,sha256=YPrqZ5sJeX_TJ-T3TUR98ov9tX_lNBXhpghSXC-3zMY,1569
 determined/deploy/gcp/terraform/outputs.tf,sha256=eO9pEL4-BpY_-5Na8bErqbLyB0Sjj-ECq667xRgpZ2U,1487
 determined/deploy/gcp/terraform/variables.tf,sha256=Xm-2lKfIrIU-inYU1T-ti7l84mIfEv6Az5B3duIXkuE,3767
 determined/deploy/gcp/terraform/modules/compute/main.tf,sha256=zaN9yYlZmGwB_KRHoFrAwvlkXXXJfzzTdvlS1PcOEMc,6105
@@ -160,22 +160,21 @@
 determined/deploy/gcp/terraform/modules/service_account/main.tf,sha256=i89pStos5n2oofzWtQ8JeQD82CvK75zpHn3VgEk_UNM,962
 determined/deploy/gcp/terraform/modules/service_account/outputs.tf,sha256=13rlFrQtnyzM971tTy_B9xMDhz0VfMeksH2lrYO1dGk,73
 determined/deploy/gcp/terraform/modules/service_account/variables.tf,sha256=MMkgflxS7aHMK0rJzPvP1nThA52n9i_wQ-xtUxRXw5g,138
 determined/deploy/gke/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 determined/deploy/gke/cli.py,sha256=iIH05GjhBY1EiFSMSb0KZpFLbar5Pl1FNf6v1iiZO2A,19568
 determined/deploy/gke/constants.py,sha256=6PLJoJcbicb35JuYfdrGfNZeCaCvlZjrGNCvHzq2_YI,408
 determined/deploy/local/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-determined/deploy/local/cli.py,sha256=jrcCvnBnYQSKoF6FtoY4N8Ve6xpPeRhvVBT_ZOmrUFc,12007
-determined/deploy/local/cluster_utils.py,sha256=lth-sVgNnYCxI11p0Tg_nL_YrJGAj5e-yCrB_CSojBo,12390
-determined/deploy/local/docker-compose.yaml,sha256=GoM3BrSOYxGy7_m_pkboHCKRVa6lPsV0sQk5d6gBfoE,870
+determined/deploy/local/cli.py,sha256=_CIqKr-OEQ6teIwQDOxNRIOQvlwjyB7yj0yZtuOBkD4,12829
+determined/deploy/local/cluster_utils.py,sha256=vcto5HLlCquvZOdzd74QjxYT8nkyuemSyg5VBCA_UY8,17140
 determined/deploy/local/preflight.py,sha256=s5Mu_H2INY5EQUv1JC1wCy-d3Pz-vMld2i27YqgMmXs,1134
 determined/estimator/__init__.py,sha256=fjPJQtS2Kj3TN3J8gkM-rt-3PUginvehCyjHmt0bybA,697
 determined/estimator/_callback.py,sha256=zXdIxEkmxg4HUnXJLD7juHzKmo2MjwzJaEYii4wSsDM,1288
 determined/estimator/_estimator_context.py,sha256=6KLc5mCtY-PxrvkdaiZjKy_V6-JcdjOCvzycpN2A6fU,7375
-determined/estimator/_estimator_trial.py,sha256=sy38EkTpr2vY3PDzhjG2bBF7GPDhYquN0ZjHWqjVNyw,40174
+determined/estimator/_estimator_trial.py,sha256=g-6xV2geUddz132EBcmAmJHyVLO9LW-vli03kzX45CM,40174
 determined/estimator/_load.py,sha256=sQy3LfLMkFFwU0YyPS9rYRU3TlMaM0Qu9jdDs3_0HnA,3086
 determined/estimator/_reducer.py,sha256=IV5ErvXlJtgylYkdRf7U-V50rWObTLEQirvow4lsWsg,13742
 determined/estimator/_util.py,sha256=wZ80Iky7WTtoDBkh2r2YVXOE5-DUvEt8kNnb0N5Gayc,9600
 determined/exec/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 determined/exec/gc_checkpoints.py,sha256=u3hVhGhhbd94HZIELtFApHWbxJ0roKIYIXeuqhpdtHQ,3836
 determined/exec/harness.py,sha256=wd_AX84iEpNbaDPljpb9htZmUGLW4U81RIu5m5akN7M,8857
 determined/exec/launch.py,sha256=Tmcl-x0iOqluYdjWlCHi629HUPiT2PFo3X9KiN41cIk,4233
@@ -187,47 +186,47 @@
 determined/experimental/_native.py,sha256=yNr0UgfrgI1pICD1nzXprr8vpLIwjV8TLc-o67_X3ls,3473
 determined/experimental/client.py,sha256=2fjLt-VLATf8j_vzx2mxiY4qjMcAOBaDkj7_Ra7zwWU,15160
 determined/keras/__init__.py,sha256=qY5vSLMwmjfT3zeBool6fUSxIsFcuj_IhpbdXS42P-0,744
 determined/keras/_data.py,sha256=whHIL5Ky1iIYAQP2LWEWIjJ2jD-yPF-xDll4Pq8V4f8,7579
 determined/keras/_enqueuer.py,sha256=Ocm7bXQPgh9pb25uYTUZbEaQv6PSqx8Ks5fBKrs1WHg,11491
 determined/keras/_load.py,sha256=pFxVc769H70CSPj1s_JjS7X5Siqta_HabpXXszIBRso,4020
 determined/keras/_tensorboard_callback.py,sha256=Tq82-HEeYFKN37A4oq5S0MopPfYqfFF08PWO5I-y1rc,486
-determined/keras/_tf_keras_context.py,sha256=5v6ZDOILs16NWgPlJCj3BaJy03ZrcT-NQ6R6JNsBC-E,19043
+determined/keras/_tf_keras_context.py,sha256=H3bvMo4hVDXK_FYY0ZxiBaIc6fnJnQdu-h_AhsCOEeU,19042
 determined/keras/_tf_keras_multi_gpu.py,sha256=mRO696HAs2ohkZ6JELUgA-TziVveLKhlGP8vWSofa1s,1204
-determined/keras/_tf_keras_trial.py,sha256=nojJAjk5Aug95kFabY_y_G-1bu_4dzASOMnjKkU4Zu0,47717
+determined/keras/_tf_keras_trial.py,sha256=JS9CQl1uSn7L6nfqrYVF2DDL3nKJ4JA1qvb13DfDUB0,48040
 determined/keras/callbacks.py,sha256=QpDwREhktvhQH79Ue7n6XdESN-BCNf3l-Pl7s1fytDs,28556
 determined/launch/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-determined/launch/deepspeed.py,sha256=XOKJiqscPoj10mByHZt5FAs2QTwXGox59GGJ8zseUdQ,13722
+determined/launch/deepspeed.py,sha256=DaE-9QbMp46e11SBJcxyVlYKLsEE7Yqhiet5vvPCO70,13720
 determined/launch/horovod.py,sha256=Grs-6CqjEj_zGJg1Sp7hN8asmlNb7520-555T9zOh6E,10813
 determined/launch/torch_distributed.py,sha256=k3ihiWJ-j5LJP60Ou7Uf3n9h8PVz-WNB6c1pKMMEx9E,4574
 determined/launch/wrap_rank.py,sha256=wuy2e9XsrXq7V7zxM8OhAyeCAEBsXOgtwnNXM5dgSbg,4502
 determined/layers/__init__.py,sha256=Uw66Sylnjbil07vwegIxDWruBXWS-Lt4l140vhwZB3U,98
 determined/layers/_workload_sequencer.py,sha256=TmEk2FR5UxjifQjz1UrD5m0aN6rpq8krWi63ChmFzcg,19401
 determined/pytorch/__init__.py,sha256=4AMCoAmNgHAFiV6f0CHfjpDViVFv41Oc1cg2HZa53Ds,1134
 determined/pytorch/_callback.py,sha256=h_gsiuGQ3BQ4wEgoBY2ICEO7mSV4x7WKsHJnbMMtjbU,5459
-determined/pytorch/_data.py,sha256=jCLMFiG2ATryaI0bnewXElcQMBzFlsiJyEvjcjHlTJw,16282
+determined/pytorch/_data.py,sha256=HBBR7xDSD8glVN6W3FufHevD_Z5c8i98J4Kr_pg9308,16294
 determined/pytorch/_experimental.py,sha256=alOckCDugmP_Q-YoKtedE9RwhZI1ud1G-63HpFEeStw,3670
 determined/pytorch/_load.py,sha256=_7H3faSeduUAgWuQe_ZHaJIPD8H-yiN8Raf1XTs2_hU,12510
 determined/pytorch/_lr_scheduler.py,sha256=5mDlNPknOaa_U-R_AiXawcuJeKh0pbBu1ONtY4IK93I,3286
-determined/pytorch/_metric_utils.py,sha256=RDGXxz2O67E3ujCCog9yhDNsNSts6jtXfgnIRFtMEHM,8276
-determined/pytorch/_pytorch_context.py,sha256=IixCUoeo1mJD5dbQE-EKkU2x-9jVEsMxuJxQZgeQ7hQ,45114
-determined/pytorch/_pytorch_trial.py,sha256=ePqubk-nijtMQf6mfAZGJtcEqi_InXCSV-v-OcJ1esE,67783
-determined/pytorch/_reducer.py,sha256=4DIbc9uCgt8B8sqpSOUGnggZMWYcu1AzZi1pG9_NlxM,21404
+determined/pytorch/_metric_utils.py,sha256=4LBvaec5GCLpPki8vmFerH8L0H6OriVUQDByiW_lPwY,8275
+determined/pytorch/_pytorch_context.py,sha256=cUTn2CkC9hVFUIiw6XYff1p9CAqeUa5Ca0GdlTuORbA,45136
+determined/pytorch/_pytorch_trial.py,sha256=QMyHTp5ZxHT58mR_IuABtvj7mCzW16eGNIDcWpr-Bmg,67813
+determined/pytorch/_reducer.py,sha256=B24ubjL1rrQE-7Lp0ObDikShaO-eN5Dp_jp8Sc_kzRI,21406
 determined/pytorch/_trainer.py,sha256=jo5tXhOTX5JVcCZOajS6jUksW0gVk4l6OrfcFHmAgSQ,13510
 determined/pytorch/samplers.py,sha256=I4fU64xjwGi2PRGyznsqHe_QUmlWBHNsTVfe_2OzwDQ,9561
 determined/pytorch/deepspeed/__init__.py,sha256=loC0PUxi7Y21cFjeP3KFxV3omSaTpw4TwOQb1sTp0q8,347
 determined/pytorch/deepspeed/_deepspeed_context.py,sha256=peq9-vLRcmbQCkUNd3AekzRH7c_oj3OIg_Idc6w0cIk,17524
 determined/pytorch/deepspeed/_deepspeed_trial.py,sha256=xeeNOM8wRT4--uPP72ZHg5u2oyi2ljXVC1cNXALNqZo,42777
 determined/pytorch/deepspeed/_mpu.py,sha256=NFyfpLc12wv5HT13WlkN7uos-YgKu08px-ksycPuABE,1987
 determined/pytorch/lightning/__init__.py,sha256=hEeHjZvrU6v8nwpuCA_z096_epOeQgY6JcGbob2kazA,67
 determined/pytorch/lightning/_adapter.py,sha256=vVZib7VrO_s1Cp_xXqoc72zkzz1HfVaIad3qMzmaF80,18157
 determined/searcher/__init__.py,sha256=-Sm5zl0JICEArfNcAdo6Zz6sodpNDC5uQY5z1DPPz-Y,343
 determined/searcher/_remote_search_runner.py,sha256=2DZ71x0dUddaEYoDsC01Xai3TkPKoA7kwe2yGTCXcIw,3563
-determined/searcher/_search_method.py,sha256=YcpKmP7B5RWEu-Xk6b3WhqerheWLe-880AP3O_P6D_o,10064
-determined/searcher/_search_runner.py,sha256=KMXguRAz0rbg9MlVS9B6PRbjBiFX2XOjrYQCjTWMHFw,14093
+determined/searcher/_search_method.py,sha256=s_UKfcpQmKYXi9l99g9uYu4CdnYmMgBMADm-KnqPhUQ,9914
+determined/searcher/_search_runner.py,sha256=2g48Ea8zSIgx4FkhcyFBm3OJjifIuxvDyK1y1qyfUOc,14053
 determined/tensorboard/__init__.py,sha256=y462s2t7U1mJ8qu_cOkL4lFO7Z6_P5665E1uoZ2_HB4,512
 determined/tensorboard/azure.py,sha256=JS7cKd0WJqtyRNC5ruHrjcK2mYrpnsh26MbJe8blOAs,1573
 determined/tensorboard/base.py,sha256=FaF4RhxQZnxWdsGAqzOmRh8xmPrFyGMna7ftT7ZYYr0,5797
 determined/tensorboard/build.py,sha256=8HSDqruyncw21cFx45JUfE1g52pTglW-ObOMvPLAD6w,4534
 determined/tensorboard/gcs.py,sha256=PlMMve-jX1xOWPImTIWdKBSJmBdl1EsF04FVop_q22Q,2305
 determined/tensorboard/hdfs.py,sha256=VUJJvANJlYAxeh8hGK2Yv5xfCinsy_BDx2-LIeAe0dY,1333
 determined/tensorboard/s3.py,sha256=kwil2SLZA2Jow1jS6BrDt8dTP_tN-J7LgG58HqfnAPA,1892
@@ -239,12 +238,12 @@
 determined/tensorboard/fetchers/gcs.py,sha256=TBCIKfdT6NsDiWRtWuo3NP-YKV2l6hf8zAVyOrFopWY,1733
 determined/tensorboard/fetchers/s3.py,sha256=8Vprb72laPQfPAwr48RMBCzei2IShcAsrQqYxxizTCA,2476
 determined/tensorboard/fetchers/shared.py,sha256=7p-l7hyCuViVaf4hiqxjBmB4Sxl3_F61ojFtaFmX3cc,1634
 determined/tensorboard/metric_writers/__init__.py,sha256=QBZlDYJslTPnllbOOwzbV_XtKlsLwf9SR8YcL3MdJtY,91
 determined/tensorboard/metric_writers/callback.py,sha256=pwNoskesXSRWaU7EimdhwEq0QP3HkkqgyCjrwd9Mc0g,2113
 determined/tensorboard/metric_writers/pytorch.py,sha256=dfEZRibe9Tkps768n_uLrYOWe8TOtrg9hfCsI2pKayc,2936
 determined/tensorboard/metric_writers/tensorflow.py,sha256=fQA1jtA6xCFEGoT5bf2YgZ4YBr9KbWRewfMOwTyCxh4,4385
-determined-0.21.1rc4.dist-info/METADATA,sha256=5HabIFAeHeZMn2cSh8bcqEsi-MRNqZ7CiOGBqN7djm4,1363
-determined-0.21.1rc4.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-determined-0.21.1rc4.dist-info/entry_points.txt,sha256=77kPxwpCZJKt-eI0btTdFhbgHTB1vYCIeaxnrbu9cPg,53
-determined-0.21.1rc4.dist-info/top_level.txt,sha256=6KMmvfzgIXKT6XhfIA5qmqlIv8-Yk90UBTekhDjhk0U,11
-determined-0.21.1rc4.dist-info/RECORD,,
+determined-0.21.2rc0.dist-info/METADATA,sha256=3-8Gzmkkagv1CJHWEiGB3GdYROh4TRmq4RgBPQ-odbU,1285
+determined-0.21.2rc0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+determined-0.21.2rc0.dist-info/entry_points.txt,sha256=77kPxwpCZJKt-eI0btTdFhbgHTB1vYCIeaxnrbu9cPg,53
+determined-0.21.2rc0.dist-info/top_level.txt,sha256=6KMmvfzgIXKT6XhfIA5qmqlIv8-Yk90UBTekhDjhk0U,11
+determined-0.21.2rc0.dist-info/RECORD,,
```

